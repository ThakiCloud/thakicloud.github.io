---
title: "OrbStackìœ¼ë¡œ ì‹œì‘í•˜ëŠ” vLLM ì¶”ë¡  ì„œë¹„ìŠ¤ ê°œë°œ - ì‹ ì… ê°œë°œìë¥¼ ìœ„í•œ ì™„ë²½ ê°€ì´ë“œ"
date: 2025-06-01
categories: 
  - dev
tags: 
  - orbstack
  - vllm
  - kubernetes
  - helm
  - llm-inference
  - macos
  - docker
  - ai-development
  - beginner-guide
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
---

macOSì—ì„œ AI ëª¨ë¸ ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•  ë•Œ Docker Desktopì€ ë¬´ê²ê³  ë¦¬ì†ŒìŠ¤ë¥¼ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤. **OrbStack**ì€ ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ëŠ” í˜ì‹ ì ì¸ ëŒ€ì•ˆì…ë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” OrbStackì„ ì‚¬ìš©í•´ ê°€ë³ê³  ë¹ ë¥¸ Kubernetes í™˜ê²½ì„ êµ¬ì¶•í•˜ê³ , Helmì„ í™œìš©í•´ vLLM ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•˜ëŠ” ì „ ê³¼ì •ì„ ì‹ ì… ê°œë°œì ê´€ì ì—ì„œ ìƒì„¸íˆ ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

## OrbStackì´ë€?

**OrbStack**ì€ macOSì™€ Linuxë¥¼ ìœ„í•œ ì°¨ì„¸ëŒ€ Docker Desktop ëŒ€ì•ˆì…ë‹ˆë‹¤. ê¸°ì¡´ Docker Desktop ëŒ€ë¹„ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤:

### OrbStack vs Docker Desktop

| íŠ¹ì§• | OrbStack | Docker Desktop |
|------|----------|----------------|
| **ì‹œì‘ ì‹œê°„** | 2-3ì´ˆ | 30-60ì´ˆ |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** | 50% ì ìŒ | ë†’ìŒ |
| **CPU ì‚¬ìš©ëŸ‰** | ìµœì í™”ë¨ | ë†’ìŒ |
| **íŒŒì¼ ì‹œìŠ¤í…œ** | ë„¤ì´í‹°ë¸Œ ì„±ëŠ¥ | ëŠë¦¼ |
| **Kubernetes** | ë‚´ì¥ ì§€ì› | ë³„ë„ ì„¤ì • í•„ìš” |

### ì£¼ìš” íŠ¹ì§•

- **ë¹ ë¥¸ ì‹œì‘**: ëª‡ ì´ˆ ë§Œì— ì»¨í…Œì´ë„ˆ ì‹¤í–‰
- **ë‚®ì€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©**: ë©”ëª¨ë¦¬ì™€ CPU íš¨ìœ¨ì„±
- **ë„¤ì´í‹°ë¸Œ ì„±ëŠ¥**: macOSì™€ ì™„ë²½ í†µí•©
- **ë‚´ì¥ Kubernetes**: ë³„ë„ ì„¤ì • ì—†ì´ ì¦‰ì‹œ ì‚¬ìš©
- **ì§ê´€ì  UI**: ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ì¸í„°í˜ì´ìŠ¤

## ê°œë°œ í™˜ê²½ ì„¤ì •

### 1. OrbStack ì„¤ì¹˜

#### ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‹¤ìš´ë¡œë“œ

```bash
# ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‹¤ìš´ë¡œë“œ
open https://orbstack.dev/download

# ë˜ëŠ” Homebrewë¡œ ì„¤ì¹˜
brew install orbstack
```

#### ì„¤ì¹˜ í›„ ì´ˆê¸° ì„¤ì •

```bash
# OrbStack ì‹¤í–‰
open -a OrbStack

# ì„¤ì¹˜ í™•ì¸
orb version

# Docker ëª…ë ¹ì–´ í™•ì¸
docker version

# Kubernetes í™•ì¸
kubectl version --client
```

### 2. í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜

#### Helm ì„¤ì¹˜

```bash
# Homebrewë¡œ Helm ì„¤ì¹˜
brew install helm

# ì„¤ì¹˜ í™•ì¸
helm version
```

#### ê¸°íƒ€ ìœ ìš©í•œ ë„êµ¬ë“¤

```bash
# k9s (Kubernetes í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§)
brew install k9s

# kubectx (ì»¨í…ìŠ¤íŠ¸ ì „í™˜)
brew install kubectx

# stern (ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°)
brew install stern

# yq (YAML ì²˜ë¦¬)
brew install yq
```

### 3. OrbStack Kubernetes í´ëŸ¬ìŠ¤í„° ì„¤ì •

#### Kubernetes í™œì„±í™”

```bash
# OrbStackì—ì„œ Kubernetes í™œì„±í™”
orb start k8s

# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
kubectl cluster-info

# ë…¸ë“œ í™•ì¸
kubectl get nodes

# ì»¨í…ìŠ¤íŠ¸ í™•ì¸
kubectl config current-context
```

#### ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±

```bash
# vLLM ì „ìš© ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace vllm-system

# ê¸°ë³¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì •
kubectl config set-context --current --namespace=vllm-system
```

## vLLM Helm Chart ê°œë°œ

### 1. Helm Chart êµ¬ì¡° ìƒì„±

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/vllm-orbstack-project
cd ~/vllm-orbstack-project

# Helm Chart ìƒì„±
helm create vllm-inference

# Chart êµ¬ì¡° í™•ì¸
brew install tree
tree vllm-inference/
```

### 2. Chart.yaml ì„¤ì •

```yaml
# vllm-inference/Chart.yaml
apiVersion: v2
name: vllm-inference
description: A Helm chart for vLLM inference service on OrbStack
type: application
version: 0.1.0
appVersion: "latest"

keywords:
  - vllm
  - llm
  - inference
  - ai

maintainers:
  - name: Your Name
    email: your.email@example.com

dependencies: []
```

### 3. values.yaml ì„¤ì •

```yaml
# vllm-inference/values.yaml
# ê¸°ë³¸ ì„¤ì •
replicaCount: 2

image:
  repository: vllm/vllm-openai
  pullPolicy: IfNotPresent
  tag: "latest"

# ì„œë¹„ìŠ¤ ì„¤ì •
service:
  type: ClusterIP
  port: 8000
  targetPort: 8000

# Ingress ì„¤ì •
ingress:
  enabled: true
  className: ""
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
  hosts:
    - host: vllm.local
      paths:
        - path: /
          pathType: Prefix
  tls: []

# vLLM ì„¤ì • (macOS CPU ì „ìš©)
vllm:
  model: "bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF"
  modelFile: "deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf"
  host: "0.0.0.0"
  port: 8000
  maxModelLen: 4096
  maxTokens: 1024
  logLevel: "INFO"
  # CPU ì „ìš© ì„¤ì •
  cpuOnly: true
  ompNumThreads: 4

# ë¦¬ì†ŒìŠ¤ ì„¤ì • (macOS CPU ìµœì í™”)
resources:
  limits:
    cpu: 4000m
    memory: 12Gi
  requests:
    cpu: 2000m
    memory: 6Gi

# ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì„¤ì •
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# í—¬ìŠ¤ì²´í¬ ì„¤ì •
healthCheck:
  enabled: true
  livenessProbe:
    httpGet:
      path: /health
      port: 8000
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 10
  readinessProbe:
    httpGet:
      path: /health
      port: 8000
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5

# ë³´ì•ˆ ì„¤ì •
securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

# ë…¸ë“œ ì„ íƒê¸°
nodeSelector: {}

# í†¨ëŸ¬ë ˆì´ì…˜
tolerations: []

# ì–´í”¼ë‹ˆí‹°
affinity: {}

# í™˜ê²½ë³€ìˆ˜
env: []

# ë³¼ë¥¨ ë§ˆìš´íŠ¸
volumeMounts: []

# ë³¼ë¥¨
volumes: []
```

### 4. Deployment í…œí”Œë¦¿ ì‘ì„±

{% raw %}
```yaml
# vllm-inference/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "vllm-inference.fullname" . }}
  labels:
    {{- include "vllm-inference.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "vllm-inference.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
      labels:
        {{- include "vllm-inference.selectorLabels" . | nindent 8 }}
    spec:
      securityContext:
        {{- toYaml .Values.securityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: {{ .Values.service.targetPort }}
          env:
            - name: MODEL_NAME
              value: {{ .Values.vllm.model | quote }}
            - name: MODEL_FILE
              value: {{ .Values.vllm.modelFile | quote }}
            - name: HOST
              value: {{ .Values.vllm.host | quote }}
            - name: PORT
              value: {{ .Values.vllm.port | quote }}
            - name: MAX_MODEL_LEN
              value: {{ .Values.vllm.maxModelLen | quote }}
            - name: MAX_TOKENS
              value: {{ .Values.vllm.maxTokens | quote }}
            - name: LOG_LEVEL
              value: {{ .Values.vllm.logLevel | quote }}
            # CPU ì „ìš© í™˜ê²½ë³€ìˆ˜
            - name: VLLM_CPU_ONLY
              value: "1"
            - name: OMP_NUM_THREADS
              value: {{ .Values.vllm.ompNumThreads | quote }}
            - name: MKL_NUM_THREADS
              value: {{ .Values.vllm.ompNumThreads | quote }}
            - name: NUMEXPR_NUM_THREADS
              value: {{ .Values.vllm.ompNumThreads | quote }}
            - name: PYTHONUNBUFFERED
              value: "1"
          {{- if .Values.healthCheck.enabled }}
          livenessProbe:
            {{- toYaml .Values.healthCheck.livenessProbe | nindent 12 }}
          readinessProbe:
            {{- toYaml .Values.healthCheck.readinessProbe | nindent 12 }}
          {{- end }}
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          {{- with .Values.volumeMounts }}
          volumeMounts:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          command:
            - python
            - -m
            - vllm.entrypoints.openai.api_server
            - --model
            - $(MODEL_NAME)
            - --revision
            - main
            - --host
            - $(HOST)
            - --port
            - $(PORT)
            - --max-model-len
            - $(MAX_MODEL_LEN)
            - --max-num-seqs
            - "8"
            - --dtype
            - auto
            - --enforce-eager
            - --disable-log-requests
            - --cpu-only
      {{- with .Values.volumes }}
      volumes:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
```
{% endraw %}

### 5. Service í…œí”Œë¦¿ ì‘ì„±

{% raw %}
```yaml
# vllm-inference/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "vllm-inference.fullname" . }}
  labels:
    {{- include "vllm-inference.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: {{ .Values.service.targetPort }}
      protocol: TCP
      name: http
  selector:
    {{- include "vllm-inference.selectorLabels" . | nindent 4 }}
```
{% endraw %}

### 6. Ingress í…œí”Œë¦¿ ì‘ì„±

{% raw %}
```yaml
# vllm-inference/templates/ingress.yaml
{{- if .Values.ingress.enabled -}}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: {{ include "vllm-inference.fullname" . }}
  labels:
    {{- include "vllm-inference.labels" . | nindent 4 }}
  {{- with .Values.ingress.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
spec:
  {{- if .Values.ingress.className }}
  ingressClassName: {{ .Values.ingress.className }}
  {{- end }}
  {{- if .Values.ingress.tls }}
  tls:
    {{- range .Values.ingress.tls }}
    - hosts:
        {{- range .hosts }}
        - {{ . | quote }}
        {{- end }}
      secretName: {{ .secretName }}
    {{- end }}
  {{- end }}
  rules:
    {{- range .Values.ingress.hosts }}
    - host: {{ .host | quote }}
      http:
        paths:
          {{- range .paths }}
          - path: {{ .path }}
            pathType: {{ .pathType }}
            backend:
              service:
                name: {{ include "vllm-inference.fullname" $ }}
                port:
                  number: {{ $.Values.service.port }}
          {{- end }}
    {{- end }}
{{- end }}
```
{% endraw %}

### 7. HPA í…œí”Œë¦¿ ì‘ì„±

{% raw %}
```yaml
# vllm-inference/templates/hpa.yaml
{{- if .Values.autoscaling.enabled }}
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{ include "vllm-inference.fullname" . }}
  labels:
    {{- include "vllm-inference.labels" . | nindent 4 }}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ include "vllm-inference.fullname" . }}
  minReplicas: {{ .Values.autoscaling.minReplicas }}
  maxReplicas: {{ .Values.autoscaling.maxReplicas }}
  metrics:
    {{- if .Values.autoscaling.targetCPUUtilizationPercentage }}
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }}
    {{- end }}
    {{- if .Values.autoscaling.targetMemoryUtilizationPercentage }}
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: {{ .Values.autoscaling.targetMemoryUtilizationPercentage }}
    {{- end }}
{{- end }}
```
{% endraw %}

### 8. ConfigMap í…œí”Œë¦¿ ì‘ì„±

{% raw %}
```yaml
# vllm-inference/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "vllm-inference.fullname" . }}-config
  labels:
    {{- include "vllm-inference.labels" . | nindent 4 }}
data:
  model: {{ .Values.vllm.model | quote }}
  model-file: {{ .Values.vllm.modelFile | quote }}
  host: {{ .Values.vllm.host | quote }}
  port: {{ .Values.vllm.port | quote }}
  max-model-len: {{ .Values.vllm.maxModelLen | quote }}
  max-tokens: {{ .Values.vllm.maxTokens | quote }}
  log-level: {{ .Values.vllm.logLevel | quote }}
  # CPU ì „ìš© ì„¤ì •
  cpu-only: "true"
  omp-num-threads: {{ .Values.vllm.ompNumThreads | quote }}
  vllm-cpu-only: "1"
```
{% endraw %}

## Helm Chart ë°°í¬ ë° í…ŒìŠ¤íŠ¸

### 1. Chart ìœ íš¨ì„± ê²€ì‚¬

```bash
# Chart ë¬¸ë²• ê²€ì‚¬
helm lint vllm-inference/

# í…œí”Œë¦¿ ë Œë”ë§ í…ŒìŠ¤íŠ¸
helm template vllm-inference vllm-inference/ --debug

# ë“œë¼ì´ëŸ° í…ŒìŠ¤íŠ¸
helm install vllm-inference vllm-inference/ --dry-run --debug
```

### 2. Chart ë°°í¬

```bash
# Helm Chart ì„¤ì¹˜
helm install vllm-inference vllm-inference/ \
  --namespace vllm-system \
  --create-namespace

# ë°°í¬ ìƒíƒœ í™•ì¸
helm status vllm-inference -n vllm-system

# ë°°í¬ëœ ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl get all -n vllm-system
```

### 3. ë°°í¬ ëª¨ë‹ˆí„°ë§

```bash
# Pod ìƒíƒœ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
kubectl get pods -n vllm-system -w

# ë¡œê·¸ í™•ì¸
kubectl logs -f deployment/vllm-inference -n vllm-system

# k9së¡œ í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§
k9s -n vllm-system
```

## ê°œë°œ ì›Œí¬í”Œë¡œìš°

### 1. ë¡œì»¬ ê°œë°œ í™˜ê²½ ì„¤ì •

```bash
# ê°œë°œìš© values íŒŒì¼ ìƒì„±
cat > values-dev.yaml << EOF
replicaCount: 1

image:
  repository: vllm/vllm-openai
  tag: latest
  pullPolicy: IfNotPresent

vllm:
  model: "bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF"
  modelFile: "deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf"
  maxModelLen: 4096
  maxTokens: 1024
  logLevel: "DEBUG"
  # CPU ì „ìš© ì„¤ì •
  cpuOnly: true
  ompNumThreads: 2

resources:
  limits:
    cpu: 2000m
    memory: 8Gi
  requests:
    cpu: 1000m
    memory: 4Gi

autoscaling:
  enabled: false

ingress:
  enabled: true
  hosts:
    - host: vllm-dev.local
      paths:
        - path: /
          pathType: Prefix
EOF
```

### 2. ê°œë°œìš© ë°°í¬

```bash
# ê°œë°œ í™˜ê²½ ë°°í¬
helm install vllm-dev vllm-inference/ \
  -f values-dev.yaml \
  --namespace vllm-dev \
  --create-namespace

# í¬íŠ¸ í¬ì›Œë”©ìœ¼ë¡œ ë¡œì»¬ í…ŒìŠ¤íŠ¸
kubectl port-forward service/vllm-dev 8000:8000 -n vllm-dev
```

### 3. API í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

```python
#!/usr/bin/env python3
"""vLLM API í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ for OrbStack"""

import requests
import json
import time
from typing import Dict, Any, List

class VLLMTester:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
        
    def test_health(self) -> bool:
        """í—¬ìŠ¤ì²´í¬ í…ŒìŠ¤íŠ¸"""
        try:
            response = self.session.get(f"{self.base_url}/health", timeout=5)
            print(f"âœ… í—¬ìŠ¤ì²´í¬: HTTP {response.status_code}")
            return response.status_code == 200
        except Exception as e:
            print(f"âŒ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return False
    
    def test_models(self) -> bool:
        """ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        try:
            response = self.session.get(f"{self.base_url}/v1/models", timeout=10)
            if response.status_code == 200:
                models = response.json()
                print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(models.get('data', []))}ê°œ")
                for model in models.get('data', []):
                    print(f"  - {model.get('id', 'Unknown')}")
                return True
            else:
                print(f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: HTTP {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return False
    
    def test_completion(self, prompt: str = "Hello, how are you?") -> bool:
        """í…ìŠ¤íŠ¸ ì™„ì„± í…ŒìŠ¤íŠ¸"""
        try:
            payload = {
                "model": "bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF",
                "prompt": prompt,
                "max_tokens": 50,
                "temperature": 0.7,
                "stream": False
            }
            
            response = self.session.post(
                f"{self.base_url}/v1/completions",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                print("âœ… í…ìŠ¤íŠ¸ ì™„ì„± ì„±ê³µ:")
                print(f"  ì…ë ¥: {payload['prompt']}")
                print(f"  ì¶œë ¥: {result['choices'][0]['text'].strip()}")
                return True
            else:
                print(f"âŒ í…ìŠ¤íŠ¸ ì™„ì„± ì‹¤íŒ¨: HTTP {response.status_code}")
                print(f"  ì‘ë‹µ: {response.text}")
                return False
        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ ì™„ì„± ì˜¤ë¥˜: {e}")
            return False
    
    def test_streaming_completion(self, prompt: str = "Tell me a story") -> bool:
        """ìŠ¤íŠ¸ë¦¬ë° ì™„ì„± í…ŒìŠ¤íŠ¸"""
        try:
            payload = {
                "model": "bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF",
                "prompt": prompt,
                "max_tokens": 100,
                "temperature": 0.7,
                "stream": True
            }
            
            response = self.session.post(
                f"{self.base_url}/v1/completions",
                json=payload,
                headers={"Content-Type": "application/json"},
                stream=True,
                timeout=30
            )
            
            if response.status_code == 200:
                print("âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ì„± ì„±ê³µ:")
                print(f"  ì…ë ¥: {prompt}")
                print("  ì¶œë ¥: ", end="")
                
                for line in response.iter_lines():
                    if line:
                        line_str = line.decode('utf-8')
                        if line_str.startswith('data: '):
                            data_str = line_str[6:]
                            if data_str.strip() == '[DONE]':
                                break
                            try:
                                data = json.loads(data_str)
                                if 'choices' in data and data['choices']:
                                    text = data['choices'][0].get('text', '')
                                    print(text, end='', flush=True)
                            except json.JSONDecodeError:
                                continue
                print()
                return True
            else:
                print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì™„ì„± ì‹¤íŒ¨: HTTP {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì™„ì„± ì˜¤ë¥˜: {e}")
            return False
    
    def benchmark_performance(self, num_requests: int = 10) -> Dict[str, float]:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        print(f"ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({num_requests}ê°œ ìš”ì²­)...")
        
        times = []
        successful_requests = 0
        
        for i in range(num_requests):
            start_time = time.time()
            
            payload = {
                "model": "bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF",
                "prompt": f"Request {i+1}: What is AI?",
                "max_tokens": 30,
                "temperature": 0.7
            }
            
            try:
                response = self.session.post(
                    f"{self.base_url}/v1/completions",
                    json=payload,
                    timeout=30
                )
                
                if response.status_code == 200:
                    successful_requests += 1
                    
                end_time = time.time()
                request_time = end_time - start_time
                times.append(request_time)
                
                print(f"  ìš”ì²­ {i+1}/{num_requests}: {request_time:.2f}s")
                
            except Exception as e:
                print(f"  ìš”ì²­ {i+1}/{num_requests}: ì‹¤íŒ¨ - {e}")
        
        if times:
            avg_time = sum(times) / len(times)
            min_time = min(times)
            max_time = max(times)
            
            print(f"\nğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
            print(f"  ì„±ê³µë¥ : {successful_requests}/{num_requests} ({successful_requests/num_requests*100:.1f}%)")
            print(f"  í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.2f}s")
            print(f"  ìµœì†Œ ì‘ë‹µ ì‹œê°„: {min_time:.2f}s")
            print(f"  ìµœëŒ€ ì‘ë‹µ ì‹œê°„: {max_time:.2f}s")
            print(f"  ì²˜ë¦¬ëŸ‰: {successful_requests/sum(times):.2f} req/s")
            
            return {
                "success_rate": successful_requests/num_requests,
                "avg_time": avg_time,
                "min_time": min_time,
                "max_time": max_time,
                "throughput": successful_requests/sum(times)
            }
        else:
            print("âŒ ëª¨ë“  ìš”ì²­ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return {}
    
    def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸ§ª vLLM API í…ŒìŠ¤íŠ¸ ì‹œì‘ (OrbStack í™˜ê²½)\n")
        
        tests = [
            ("í—¬ìŠ¤ì²´í¬", self.test_health),
            ("ëª¨ë¸ ëª©ë¡", self.test_models),
            ("í…ìŠ¤íŠ¸ ì™„ì„±", lambda: self.test_completion("ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë–¤ê°€ìš”?")),
            ("ìŠ¤íŠ¸ë¦¬ë° ì™„ì„±", lambda: self.test_streaming_completion("ê°„ë‹¨í•œ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”"))
        ]
        
        results = []
        for test_name, test_func in tests:
            print(f"ğŸ” {test_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
            result = test_func()
            results.append((test_name, result))
            print()
            time.sleep(1)
        
        # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        print("ğŸ” ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì¤‘...")
        benchmark_results = self.benchmark_performance(5)
        print()
        
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
        for test_name, result in results:
            status = "âœ… ì„±ê³µ" if result else "âŒ ì‹¤íŒ¨"
            print(f"  {test_name}: {status}")
        
        success_count = sum(1 for _, result in results if result)
        print(f"\nğŸ¯ ì´ {len(results)}ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ {success_count}ê°œ ì„±ê³µ")
        
        if benchmark_results:
            print(f"âš¡ í‰ê·  ì‘ë‹µ ì‹œê°„: {benchmark_results['avg_time']:.2f}s")
            print(f"ğŸš€ ì²˜ë¦¬ëŸ‰: {benchmark_results['throughput']:.2f} req/s")

if __name__ == "__main__":
    import sys
    
    base_url = "http://localhost:8000"
    if len(sys.argv) > 1:
        base_url = sys.argv[1]
    
    print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {base_url}")
    tester = VLLMTester(base_url)
    tester.run_all_tests()
```

### 4. í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python test_vllm_orbstack.py

# ë˜ëŠ” íŠ¹ì • URLë¡œ í…ŒìŠ¤íŠ¸
python test_vllm_orbstack.py http://vllm-dev.local
```

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 1. Prometheus ëª¨ë‹ˆí„°ë§ ì„¤ì •

```yaml
# monitoring/prometheus-values.yaml
prometheus:
  prometheusSpec:
    serviceMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelector: {}
    ruleSelector: {}
    
grafana:
  enabled: true
  adminPassword: admin123
  
alertmanager:
  enabled: true
```

```bash
# Prometheus ìŠ¤íƒ ì„¤ì¹˜
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm install monitoring prometheus-community/kube-prometheus-stack \
  -f monitoring/prometheus-values.yaml \
  --namespace monitoring \
  --create-namespace
```

### 2. vLLM ServiceMonitor ìƒì„±

{% raw %}
```yaml
# vllm-inference/templates/servicemonitor.yaml
{{- if .Values.monitoring.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: {{ include "vllm-inference.fullname" . }}
  labels:
    {{- include "vllm-inference.labels" . | nindent 4 }}
spec:
  selector:
    matchLabels:
      {{- include "vllm-inference.selectorLabels" . | nindent 6 }}
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s
{{- end }}
```
{% endraw %}

### 3. Grafana ëŒ€ì‹œë³´ë“œ

```json
# monitoring/vllm-dashboard.json
{
  "dashboard": {
    "id": null,
    "title": "vLLM Inference Metrics",
    "tags": ["vllm", "inference"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(vllm_requests_total[5m])",
            "legendFormat": "Requests/sec"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "reqps"
          }
        }
      },
      {
        "id": 2,
        "title": "Response Time",
        "type": "timeseries",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(vllm_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(vllm_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          }
        ]
      },
      {
        "id": 3,
        "title": "Memory Usage",
        "type": "timeseries",
        "targets": [
          {
            "expr": "container_memory_usage_bytes{pod=~\"vllm-inference-.*\"}",
            "legendFormat": "Memory Usage"
          }
        ]
      },
      {
        "id": 4,
        "title": "CPU Usage",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(container_cpu_usage_seconds_total{pod=~\"vllm-inference-.*\"}[5m])",
            "legendFormat": "CPU Usage"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "5s"
  }
}
```

## ì„±ëŠ¥ ìµœì í™”

### 1. OrbStack ìµœì í™” ì„¤ì • (macOS CPU ì „ìš©)

```bash
# OrbStack ë¦¬ì†ŒìŠ¤ í• ë‹¹ í™•ì¸
orb config

# CPU ë° ë©”ëª¨ë¦¬ í• ë‹¹ ì¡°ì • (CPU ì§‘ì•½ì  ì›Œí¬ë¡œë“œìš©)
orb config set cpu 8
orb config set memory 16GB

# ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (DeepSeek ëª¨ë¸ì€ í° ìš©ëŸ‰)
orb config set disk 100GB

# CPU ìµœì í™” ì„¤ì •
orb config set vm.cpuType "host"
orb config set vm.accelerated true
```

### 2. Helm Chart ì„±ëŠ¥ íŠœë‹ (macOS CPU ìµœì í™”)

```yaml
# values-production.yaml
replicaCount: 4

image:
  repository: vllm/vllm-openai
  tag: latest
  pullPolicy: IfNotPresent

vllm:
  model: "bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF"
  modelFile: "deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf"
  maxModelLen: 4096
  maxTokens: 2048
  logLevel: "INFO"
  # CPU ì „ìš© ìµœì í™”
  cpuOnly: true
  ompNumThreads: 6

resources:
  limits:
    cpu: 6000m
    memory: 16Gi
  requests:
    cpu: 3000m
    memory: 8Gi

autoscaling:
  enabled: true
  minReplicas: 4
  maxReplicas: 20
  targetCPUUtilizationPercentage: 60
  targetMemoryUtilizationPercentage: 70

# ë…¸ë“œ ì–´í”¼ë‹ˆí‹° ì„¤ì • (macOS í˜¸í™˜ì„±)
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/arch
          operator: In
          values:
          - amd64
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - vllm-inference
        topologyKey: kubernetes.io/hostname

# ë¦¬ì†ŒìŠ¤ ì¿¼í„° ì„¤ì •
resourceQuota:
  enabled: true
  hard:
    requests.cpu: "15"
    requests.memory: 50Gi
    limits.cpu: "30"
    limits.memory: 100Gi
```

### 3. ìºì‹± ì „ëµ

{% raw %}
```yaml
# vllm-inference/templates/redis.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "vllm-inference.fullname" . }}-redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        resources:
          limits:
            cpu: 500m
            memory: 1Gi
          requests:
            cpu: 100m
            memory: 256Mi
---
apiVersion: v1
kind: Service
metadata:
  name: {{ include "vllm-inference.fullname" . }}-redis
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379
```
{% endraw %}

## CI/CD íŒŒì´í”„ë¼ì¸

### 1. GitHub Actions ì›Œí¬í”Œë¡œìš°

```yaml
# .github/workflows/deploy-orbstack.yml
name: Deploy vLLM to OrbStack

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/vllm-inference

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install pytest requests
    
    - name: Run tests
      run: |
        pytest tests/
  
  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Helm Lint
      run: |
        helm lint vllm-inference/
    
    - name: Validate Kubernetes manifests
      run: |
        helm template vllm-inference vllm-inference/ | kubectl apply --dry-run=client -f -

  build:
    needs: [test, lint]
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}

  deploy-dev:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    environment: development
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Helm
      uses: azure/setup-helm@v3
      with:
        version: '3.12.0'
    
    - name: Deploy to development
      run: |
        helm upgrade --install vllm-dev vllm-inference/ \
          --namespace vllm-dev \
          --create-namespace \
          --set image.tag=${{ github.sha }} \
          -f values-dev.yaml

  deploy-prod:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Helm
      uses: azure/setup-helm@v3
      with:
        version: '3.12.0'
    
    - name: Deploy to production
      run: |
        helm upgrade --install vllm-prod vllm-inference/ \
          --namespace vllm-system \
          --create-namespace \
          --set image.tag=${{ github.sha }} \
          -f values-production.yaml
```

### 2. ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# scripts/deploy.sh

set -e

ENVIRONMENT=${1:-dev}
NAMESPACE="vllm-${ENVIRONMENT}"
VALUES_FILE="values-${ENVIRONMENT}.yaml"

echo "ğŸš€ vLLM ë°°í¬ ì‹œì‘ (í™˜ê²½: ${ENVIRONMENT})"

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸/ìƒì„±
if ! kubectl get namespace ${NAMESPACE} >/dev/null 2>&1; then
    echo "ğŸ“¦ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±: ${NAMESPACE}"
    kubectl create namespace ${NAMESPACE}
fi

# Helm Chart ìœ íš¨ì„± ê²€ì‚¬
echo "ğŸ” Helm Chart ê²€ì¦ ì¤‘..."
helm lint vllm-inference/

# ë°°í¬ ì‹¤í–‰
echo "ğŸ“¦ ë°°í¬ ì‹¤í–‰ ì¤‘..."
helm upgrade --install vllm-${ENVIRONMENT} vllm-inference/ \
    --namespace ${NAMESPACE} \
    -f ${VALUES_FILE} \
    --wait \
    --timeout 10m

# ë°°í¬ ìƒíƒœ í™•ì¸
echo "âœ… ë°°í¬ ì™„ë£Œ! ìƒíƒœ í™•ì¸ ì¤‘..."
kubectl get pods -n ${NAMESPACE}
kubectl get services -n ${NAMESPACE}

# í—¬ìŠ¤ì²´í¬
echo "ğŸ¥ í—¬ìŠ¤ì²´í¬ ì‹¤í–‰ ì¤‘..."
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=vllm-inference -n ${NAMESPACE} --timeout=300s

echo "ğŸ‰ ë°°í¬ ì„±ê³µ!"
```

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

#### 1. OrbStack ê´€ë ¨ ë¬¸ì œ (macOS CPU í™˜ê²½)

```bash
# OrbStack ìƒíƒœ í™•ì¸
orb status

# CPU ì‚¬ìš©ë¥  í™•ì¸
orb stats

# OrbStack ì¬ì‹œì‘ (CPU ì„±ëŠ¥ ìµœì í™”)
orb restart

# ë¡œê·¸ í™•ì¸
orb logs

# CPU ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
orb exec -- top -n 1
```

#### 2. Kubernetes í´ëŸ¬ìŠ¤í„° ë¬¸ì œ (CPU í™˜ê²½)

```bash
# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
kubectl cluster-info

# ë…¸ë“œ ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸ (CPU ì¤‘ì‹¬)
kubectl top nodes
kubectl describe nodes

# CPU ì‚¬ìš©ë¥ ì´ ë†’ì€ Pod í™•ì¸
kubectl top pods --sort-by=cpu -A

# ì´ë²¤íŠ¸ í™•ì¸
kubectl get events --sort-by='.lastTimestamp' -n vllm-system
```

#### 3. DeepSeek ëª¨ë¸ ë¡œë”© ë¬¸ì œ

```bash
# DeepSeek ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìƒíƒœ í™•ì¸
helm status vllm-inference -n vllm-system

# GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì§„í–‰ìƒí™©
kubectl logs -f deployment/vllm-inference -n vllm-system | grep -i "download\|loading\|model"

# ëª¨ë¸ íŒŒì¼ í¬ê¸° í™•ì¸ (DeepSeek Q4_0ì€ ì•½ 4.8GB)
kubectl exec -it deployment/vllm-inference -n vllm-system -- \
  du -h /root/.cache/huggingface/hub/ | grep deepseek

# í—ˆê¹…í˜ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
kubectl exec -it deployment/vllm-inference -n vllm-system -- \
  curl -I https://huggingface.co/bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF

# GGUF íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬
kubectl exec -it deployment/vllm-inference -n vllm-system -- \
  python -c "
from huggingface_hub import hf_hub_download
import os
file_path = hf_hub_download(
    repo_id='bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF',
    filename='deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf'
)
print(f'GGUF File size: {os.path.getsize(file_path)/1024/1024/1024:.2f} GB')
"
```

#### 4. CPU ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ë¬¸ì œ

```bash
# Helm ë¦´ë¦¬ìŠ¤ ë¦¬ì†ŒìŠ¤ ì¡°ì •
helm upgrade vllm-inference vllm-inference/ \
  --set resources.limits.cpu=6000m \
  --set resources.limits.memory=16Gi \
  --set resources.requests.cpu=3000m \
  --set resources.requests.memory=8Gi \
  -n vllm-system

# OrbStack VM ë¦¬ì†ŒìŠ¤ ì¦ê°€
orb config set cpu 12
orb config set memory 24GB
orb restart

# CPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
kubectl exec -it deployment/vllm-inference -n vllm-system -- \
  top -n 1 -b | head -20
```

#### 5. GGUF ëª¨ë¸ í˜•ì‹ ê´€ë ¨ ë¬¸ì œ

```bash
# vLLM GGUF ì§€ì› í™•ì¸
kubectl exec -it deployment/vllm-inference -n vllm-system -- \
  python -c "
import vllm
print(f'vLLM version: {vllm.__version__}')
try:
    from vllm import LLM
    print('GGUF support: Available')
except Exception as e:
    print(f'GGUF support: Error - {e}')
"

# ì§€ì›ë˜ëŠ” ëª¨ë¸ í˜•ì‹ í™•ì¸
kubectl exec -it deployment/vllm-inference -n vllm-system -- \
  python -c "
from vllm.model_executor.models import MODEL_REGISTRY
print('Supported models:')
for model in sorted(MODEL_REGISTRY.keys()):
    print(f'  - {model}')
"
```

## ê²°ë¡ 

OrbStackì„ ì‚¬ìš©í•œ CPU ê¸°ë°˜ vLLM DeepSeek ì¶”ë¡  ì„œë¹„ìŠ¤ ê°œë°œì€ ê¸°ì¡´ Docker Desktop ëŒ€ë¹„ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì„ ì œê³µí•©ë‹ˆë‹¤:

### í•µì‹¬ ì¥ì  ìš”ì•½

1. **ë¹ ë¥¸ ì‹œì‘**: 2-3ì´ˆ ë§Œì— CPU ìµœì í™” ì»¨í…Œì´ë„ˆ í™˜ê²½ ì¤€ë¹„
2. **ë‚®ì€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©**: macOSì—ì„œ ë©”ëª¨ë¦¬ì™€ CPU íš¨ìœ¨ì„± ê·¹ëŒ€í™”
3. **ë„¤ì´í‹°ë¸Œ ì„±ëŠ¥**: Apple Siliconê³¼ Intel Mac ëª¨ë‘ì—ì„œ ìµœì  ì„±ëŠ¥
4. **DeepSeek GGUF ëª¨ë¸**: ê³ ì„±ëŠ¥ ì–‘ìí™” ëª¨ë¸ë¡œ ë¹ ë¥¸ ì¶”ë¡ 
5. **Helm í™œìš©**: ì²´ê³„ì ì¸ CPU ê¸°ë°˜ Kubernetes ì• í”Œë¦¬ì¼€ì´ì…˜ ê´€ë¦¬
6. **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: CPU ì‚¬ìš©ë¥  ì¤‘ì‹¬ì˜ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### ê°œë°œ ì›Œí¬í”Œë¡œìš°

1. **ë¡œì»¬ ê°œë°œ**: OrbStack + Helmìœ¼ë¡œ CPU ìµœì í™” ê°œë°œ í™˜ê²½ êµ¬ì¶•
2. **GGUF ëª¨ë¸ í…ŒìŠ¤íŠ¸**: DeepSeek ì–‘ìí™” ëª¨ë¸ì„ í†µí•œ íš¨ìœ¨ì  ì¶”ë¡ 
3. **CPU ê¸°ë°˜ ë°°í¬**: Helm Chartë¥¼ í†µí•œ ì¼ê´€ëœ CPU ì „ìš© ë°°í¬
4. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: CPU ì‚¬ìš©ë¥ ê³¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì‹¤ì‹œê°„ ì¶”ì 
5. **ë¦¬ì†ŒìŠ¤ ìµœì í™”**: macOS í™˜ê²½ì— ë§ëŠ” CPU ë° ë©”ëª¨ë¦¬ íŠœë‹

### macOS íŠ¹í™” ì´ì 

- **ê°œë°œ í™˜ê²½ í†µí•©**: ë§¥ë¶ì—ì„œ ê°œë°œë¶€í„° ë°°í¬ê¹Œì§€ seamless ê²½í—˜
- **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±**: GPU ì—†ì´ë„ DeepSeek GGUFë¡œ ì¶©ë¶„í•œ ì¶”ë¡  ì„±ëŠ¥
- **OrbStack ìµœì í™”**: Docker Desktop ëŒ€ë¹„ 50% ì ì€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©
- **í”Œë«í¼ í˜¸í™˜ì„±**: Apple Siliconê³¼ Intel Mac ëª¨ë‘ ì§€ì›
- **ëª¨ë¸ íš¨ìœ¨ì„±**: GGUF í˜•ì‹ìœ¼ë¡œ ë¹ ë¥¸ ë¡œë”©ê³¼ ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©

### ë‹¤ìŒ ë‹¨ê³„

- **ëª¨ë¸ í™•ì¥**: ë” í° DeepSeek ëª¨ë¸ (16B, 32B) í™œìš©
- **ë©€í‹° ëª¨ë¸ ì„œë¹™**: ì—¬ëŸ¬ GGUF ëª¨ë¸ ë™ì‹œ ìš´ì˜
- **ë™ì  ìŠ¤ì¼€ì¼ë§**: CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ìë™ ìŠ¤ì¼€ì¼ë§
- **í´ë¼ìš°ë“œ í•˜ì´ë¸Œë¦¬ë“œ**: ë¡œì»¬ ê°œë°œ + í´ë¼ìš°ë“œ CPU ì¸ìŠ¤í„´ìŠ¤ ë°°í¬
- **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**: ë‹¤ì–‘í•œ GGUF ì–‘ìí™” ë ˆë²¨ ë¹„êµ

OrbStackì˜ ë›°ì–´ë‚œ ì„±ëŠ¥ê³¼ Helmì˜ ê°•ë ¥í•œ íŒ¨í‚¤ì§€ ê´€ë¦¬ ê¸°ëŠ¥, ê·¸ë¦¬ê³  DeepSeek GGUF ëª¨ë¸ì˜ íš¨ìœ¨ì„±ì„ ê²°í•©í•˜ì—¬ macOSì—ì„œ ìµœê³ ì˜ CPU ê¸°ë°˜ AI ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•í•´ë³´ì„¸ìš”!

---

*ì´ ê°€ì´ë“œëŠ” OrbStack 1.0+, Helm 3.12+, vLLM 0.4.x (CPU ì§€ì›) ë²„ì „ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.* 