---
title: "ì‹ ì… ê°œë°œìë¥¼ ìœ„í•œ macOS Kubernetes vLLM ì¶”ë¡  ì„œë¹„ìŠ¤ ê°œë°œ ì™„ë²½ ê°€ì´ë“œ"
date: 2025-06-01
categories: 
  - dev
tags: 
  - kubernetes
  - vllm
  - llm-inference
  - macos
  - docker
  - ai-deployment
  - microservices
  - beginner-guide
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
---

AI ëª¨ë¸ ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ Kubernetes í™˜ê²½ì—ì„œ ìš´ì˜í•˜ëŠ” ê²ƒì€ í˜„ëŒ€ AI ê°œë°œì˜ í•µì‹¬ ìŠ¤í‚¬ì…ë‹ˆë‹¤. íŠ¹íˆ vLLMì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ê³ ì„±ëŠ¥ ì¶”ë¡ ì„ ìœ„í•œ ìµœê³ ì˜ ë„êµ¬ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” macOS í™˜ê²½ì—ì„œ Kubernetes í´ëŸ¬ìŠ¤í„°ë¥¼ êµ¬ì¶•í•˜ê³ , vLLMì„ ì‚¬ìš©í•œ ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•˜ëŠ” ì „ ê³¼ì •ì„ ì‹ ì… ê°œë°œì ê´€ì ì—ì„œ ìƒì„¸íˆ ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

## ê°œìš” ë° ì•„í‚¤í…ì²˜

### vLLMì´ë€?

**vLLM**ì€ UC Berkeleyì—ì„œ ê°œë°œí•œ ê³ ì„±ëŠ¥ LLM ì¶”ë¡  ì—”ì§„ì…ë‹ˆë‹¤:

- **PagedAttention**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
- **ì—°ì† ë°°ì¹˜**: ë™ì  ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”
- **OpenAI í˜¸í™˜ API**: ê¸°ì¡´ OpenAI APIì™€ í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤
- **ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›**: Llama, Mistral, Qwen ë“± ì£¼ìš” ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸

### ì „ì²´ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client App    â”‚â”€â”€â”€â–¶â”‚  Load Balancer  â”‚â”€â”€â”€â–¶â”‚   vLLM Pod 1    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   (Ingress)     â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   vLLM Pod 2    â”‚
                                â–²             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                â”‚             â”‚   vLLM Pod N    â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚  Kubernetes     â”‚
                       â”‚   Cluster       â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ê°œë°œ í™˜ê²½ ì„¤ì •

### 1. í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜

#### Homebrew ì„¤ì¹˜ (ë¯¸ì„¤ì¹˜ ì‹œ)

```bash
# Homebrew ì„¤ì¹˜
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

#### Docker Desktop ì„¤ì¹˜

```bash
# Docker Desktop ì„¤ì¹˜
brew install --cask docker

# Docker Desktop ì‹¤í–‰ í›„ Kubernetes í™œì„±í™”
# Docker Desktop > Settings > Kubernetes > Enable Kubernetes
```

#### kubectl ì„¤ì¹˜

```bash
# kubectl ì„¤ì¹˜
brew install kubectl

# ì„¤ì¹˜ í™•ì¸
kubectl version --client
```

#### Helm ì„¤ì¹˜

```bash
# Helm ì„¤ì¹˜ (íŒ¨í‚¤ì§€ ê´€ë¦¬ì)
brew install helm

# ì„¤ì¹˜ í™•ì¸
helm version
```

#### ê¸°íƒ€ ìœ ìš©í•œ ë„êµ¬ë“¤

```bash
# k9s (Kubernetes í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§)
brew install k9s

# kubectx (ì»¨í…ìŠ¤íŠ¸ ì „í™˜)
brew install kubectx

# stern (ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°)
brew install stern
```

### 2. ë¡œì»¬ Kubernetes í´ëŸ¬ìŠ¤í„° ì„¤ì •

#### Docker Desktop Kubernetes í™•ì¸

```bash
# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
kubectl cluster-info

# ë…¸ë“œ í™•ì¸
kubectl get nodes

# ì»¨í…ìŠ¤íŠ¸ í™•ì¸
kubectl config current-context
```

#### ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±

```bash
# vLLM ì „ìš© ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace vllm-inference

# ê¸°ë³¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì •
kubectl config set-context --current --namespace=vllm-inference
```

## vLLM Docker ì´ë¯¸ì§€ ì¤€ë¹„ (macOS CPU ë²„ì „)

### 1. macOSìš© CPU ì „ìš© vLLM ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸

macOSì—ì„œëŠ” GPU ë²„ì „ì´ ì•„ë‹Œ CPU ì „ìš© vLLMì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤:

```bash
# CPU ì „ìš© vLLM ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° í…ŒìŠ¤íŠ¸
docker run --rm -it \
  -p 8000:8000 \
  --platform linux/amd64 \
  --memory=8g \
  --cpus=4 \
  vllm/vllm-openai:latest \
  --model bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF \
  --revision main \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype auto \
  --enforce-eager \
  --disable-log-requests
```

### 2. macOS ìµœì í™” ì»¤ìŠ¤í…€ vLLM ì´ë¯¸ì§€ ìƒì„±

í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•˜ê³  macOSì— ìµœì í™”ëœ ì»¤ìŠ¤í…€ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤:

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/vllm-k8s-project
cd ~/vllm-k8s-project
```

#### Dockerfile ì‘ì„± (macOS CPU ìµœì í™”)

```dockerfile
# Dockerfile - macOS CPU ìµœì í™” ë²„ì „
FROM --platform=linux/amd64 python:3.11-slim

# ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ë° í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    git \
    wget \
    curl \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# vLLM CPU ë²„ì „ ì„¤ì¹˜
RUN pip install --no-cache-dir \
    vllm[cpu] \
    torch \
    transformers \
    huggingface-hub \
    prometheus-client \
    structlog \
    uvicorn[standard] \
    requests

# í—¬ìŠ¤ì²´í¬ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€
COPY healthcheck.py /app/
COPY entrypoint.sh /app/

# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
RUN chmod +x /app/entrypoint.sh

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
ENV PYTHONUNBUFFERED=1
ENV VLLM_CPU_ONLY=1
ENV OMP_NUM_THREADS=4

# ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ ì„¤ì •
ENTRYPOINT ["/app/entrypoint.sh"]
```

#### í—¬ìŠ¤ì²´í¬ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

```python
# healthcheck.py
#!/usr/bin/env python3
"""vLLM ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ ìŠ¤í¬ë¦½íŠ¸ - macOS CPU ë²„ì „"""

import requests
import sys
import json
import time
from typing import Dict, Any

def check_health() -> bool:
    """vLLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    try:
        # í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ
        response = requests.get(
            "http://localhost:8000/health",
            timeout=10
        )
        
        if response.status_code == 200:
            print("âœ… vLLM CPU ì„œë¹„ìŠ¤ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.")
            return True
        else:
            print(f"âŒ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: HTTP {response.status_code}")
            return False
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ í—¬ìŠ¤ì²´í¬ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return False

def check_model_ready() -> bool:
    """ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸"""
    try:
        # ëª¨ë¸ ì •ë³´ ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ
        response = requests.get(
            "http://localhost:8000/v1/models",
            timeout=30
        )
        
        if response.status_code == 200:
            models = response.json()
            if models.get("data"):
                print(f"âœ… DeepSeek ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {len(models['data'])}ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
                for model in models.get('data', []):
                    print(f"  - {model.get('id', 'Unknown')}")
                return True
            else:
                print("âŒ ë¡œë”©ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
        else:
            print(f"âŒ ëª¨ë¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: HTTP {response.status_code}")
            return False
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ ëª¨ë¸ ìƒíƒœ í™•ì¸ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return False

def test_inference() -> bool:
    """ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    try:
        payload = {
            "model": "bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF",
            "prompt": "Hello, how are you?",
            "max_tokens": 50,
            "temperature": 0.7
        }
        
        response = requests.post(
            "http://localhost:8000/v1/completions",
            json=payload,
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            print(f"  ì‘ë‹µ: {result['choices'][0]['text'][:100]}...")
            return True
        else:
            print(f"âŒ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: HTTP {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        return False

if __name__ == "__main__":
    print("ğŸ” vLLM CPU ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì‹œì‘...")
    
    # ì„œë¹„ìŠ¤ ì‹œì‘ ëŒ€ê¸°
    print("â³ ì„œë¹„ìŠ¤ ì‹œì‘ ëŒ€ê¸° ì¤‘...")
    time.sleep(10)
    
    health_ok = check_health()
    model_ok = check_model_ready()
    inference_ok = test_inference() if health_ok and model_ok else False
    
    if health_ok and model_ok and inference_ok:
        print("ğŸ‰ ëª¨ë“  ìƒíƒœ í™•ì¸ ì™„ë£Œ! CPU ê¸°ë°˜ vLLM ì„œë¹„ìŠ¤ê°€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        sys.exit(0)
    else:
        print("ğŸ’¥ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨!")
        sys.exit(1)
```

#### ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± (macOS CPU ìµœì í™”)

```bash
# entrypoint.sh - macOS CPU ìµœì í™” ë²„ì „
#!/bin/bash
set -e

echo "ğŸš€ macOS CPU ìµœì í™” vLLM ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤..."

# í™˜ê²½ë³€ìˆ˜ ê¸°ë³¸ê°’ ì„¤ì • (DeepSeek ëª¨ë¸ ì‚¬ìš©)
MODEL_NAME=${MODEL_NAME:-"bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF"}
MODEL_FILE=${MODEL_FILE:-"deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf"}
HOST=${HOST:-"0.0.0.0"}
PORT=${PORT:-"8000"}
MAX_MODEL_LEN=${MAX_MODEL_LEN:-"4096"}
MAX_TOKENS=${MAX_TOKENS:-"1024"}

echo "ğŸ“‹ macOS CPU ìµœì í™” ì„¤ì • ì •ë³´:"
echo "  - ëª¨ë¸: $MODEL_NAME"
echo "  - ëª¨ë¸ íŒŒì¼: $MODEL_FILE"
echo "  - í˜¸ìŠ¤íŠ¸: $HOST"
echo "  - í¬íŠ¸: $PORT"
echo "  - ìµœëŒ€ ëª¨ë¸ ê¸¸ì´: $MAX_MODEL_LEN"
echo "  - CPU ì „ìš© ëª¨ë“œ: í™œì„±í™”"

# CPU ìµœì í™” í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export VLLM_CPU_ONLY=1
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4
export NUMEXPR_NUM_THREADS=4

# vLLM CPU ì„œë²„ ì‹œì‘
exec python -m vllm.entrypoints.openai.api_server \
    --model "$MODEL_NAME" \
    --revision main \
    --host "$HOST" \
    --port "$PORT" \
    --max-model-len "$MAX_MODEL_LEN" \
    --max-num-seqs 8 \
    --dtype auto \
    --enforce-eager \
    --disable-log-requests \
    --cpu-only \
    "$@"
```

#### ì´ë¯¸ì§€ ë¹Œë“œ

```bash
# macOS CPU ìµœì í™” Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build --platform linux/amd64 -t vllm-cpu-custom:latest .

# ë¹Œë“œëœ ì´ë¯¸ì§€ í™•ì¸
docker images | grep vllm-cpu-custom

# ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
docker run --rm -it \
  --platform linux/amd64 \
  -p 8000:8000 \
  --memory=8g \
  --cpus=4 \
  vllm-cpu-custom:latest
```

## Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì‘ì„±

### 1. ConfigMap ìƒì„±

```yaml
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-config
  namespace: vllm-inference
data:
  MODEL_NAME: "bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF"
  MODEL_FILE: "deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf"
  HOST: "0.0.0.0"
  PORT: "8000"
  MAX_MODEL_LEN: "4096"
  MAX_TOKENS: "1024"
  LOG_LEVEL: "INFO"
  # CPU ì „ìš© ì„¤ì •
  VLLM_CPU_ONLY: "1"
  OMP_NUM_THREADS: "4"
  MKL_NUM_THREADS: "4"
  NUMEXPR_NUM_THREADS: "4"
```

### 2. Deployment ìƒì„±

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-inference
  namespace: vllm-inference
  labels:
    app: vllm-inference
spec:
  replicas: 2
  selector:
    matchLabels:
      app: vllm-inference
  template:
    metadata:
      labels:
        app: vllm-inference
    spec:
      containers:
      - name: vllm
        image: vllm-cpu-custom:latest
        imagePullPolicy: Never  # ë¡œì»¬ ì´ë¯¸ì§€ ì‚¬ìš©
        ports:
        - containerPort: 8000
          name: http
        envFrom:
        - configMapRef:
            name: vllm-config
        env:
        - name: VLLM_CPU_ONLY
          value: "1"
        - name: OMP_NUM_THREADS
          value: "4"
        - name: PYTHONUNBUFFERED
          value: "1"
        resources:
          requests:
            memory: "6Gi"
            cpu: "2000m"
          limits:
            memory: "12Gi"
            cpu: "4000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 15
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 5
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 30"]
      terminationGracePeriodSeconds: 60
      # macOS í™˜ê²½ì—ì„œì˜ ì•ˆì •ì„±ì„ ìœ„í•œ ì„¤ì •
      nodeSelector:
        kubernetes.io/arch: amd64
```

### 3. Service ìƒì„±

```yaml
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: vllm-service
  namespace: vllm-inference
  labels:
    app: vllm-inference
spec:
  type: ClusterIP
  ports:
  - port: 8000
    targetPort: 8000
    protocol: TCP
    name: http
  selector:
    app: vllm-inference
```

### 4. Ingress ìƒì„±

```yaml
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: vllm-ingress
  namespace: vllm-inference
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
spec:
  rules:
  - host: vllm.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: vllm-service
            port:
              number: 8000
```

### 5. HorizontalPodAutoscaler ìƒì„±

```yaml
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vllm-hpa
  namespace: vllm-inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vllm-inference
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## ë°°í¬ ë° í…ŒìŠ¤íŠ¸

### 1. Kubernetes ë¦¬ì†ŒìŠ¤ ë°°í¬

```bash
# k8s ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p k8s

# ëª¨ë“  ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ì„ k8s/ ë””ë ‰í† ë¦¬ì— ì €ì¥ í›„ ë°°í¬
kubectl apply -f k8s/

# ë°°í¬ ìƒíƒœ í™•ì¸
kubectl get all -n vllm-inference
```

### 2. ë°°í¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§

```bash
# Pod ìƒíƒœ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
kubectl get pods -n vllm-inference -w

# ë¡œê·¸ í™•ì¸
kubectl logs -f deployment/vllm-inference -n vllm-inference

# ì—¬ëŸ¬ Pod ë¡œê·¸ ë™ì‹œ í™•ì¸ (stern ì‚¬ìš©)
stern vllm-inference -n vllm-inference
```

### 3. ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸

#### í¬íŠ¸ í¬ì›Œë”©ìœ¼ë¡œ ë¡œì»¬ í…ŒìŠ¤íŠ¸

```bash
# í¬íŠ¸ í¬ì›Œë”© ì„¤ì •
kubectl port-forward service/vllm-service 8000:8000 -n vllm-inference
```

#### API í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

```python
# test_vllm_api.py
#!/usr/bin/env python3
"""vLLM API í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸"""

import requests
import json
import time
from typing import Dict, Any

class VLLMTester:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        
    def test_health(self) -> bool:
        """í—¬ìŠ¤ì²´í¬ í…ŒìŠ¤íŠ¸"""
        try:
            response = requests.get(f"{self.base_url}/health")
            print(f"âœ… í—¬ìŠ¤ì²´í¬: {response.status_code}")
            return response.status_code == 200
        except Exception as e:
            print(f"âŒ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return False
    
    def test_models(self) -> bool:
        """ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                models = response.json()
                print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(models.get('data', []))}ê°œ")
                for model in models.get('data', []):
                    print(f"  - {model.get('id', 'Unknown')}")
                return True
            else:
                print(f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return False
    
    def test_completion(self) -> bool:
        """í…ìŠ¤íŠ¸ ì™„ì„± í…ŒìŠ¤íŠ¸"""
        try:
            payload = {
                "model": "bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF",
                "prompt": "Hello, how are you?",
                "max_tokens": 50,
                "temperature": 0.7
            }
            
            response = requests.post(
                f"{self.base_url}/v1/completions",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=60  # DeepSeek ëª¨ë¸ì€ ë” ê¸´ ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
            )
            
            if response.status_code == 200:
                result = response.json()
                print("âœ… í…ìŠ¤íŠ¸ ì™„ì„± ì„±ê³µ:")
                print(f"  ì…ë ¥: {payload['prompt']}")
                print(f"  ì¶œë ¥: {result['choices'][0]['text']}")
                return True
            else:
                print(f"âŒ í…ìŠ¤íŠ¸ ì™„ì„± ì‹¤íŒ¨: {response.status_code}")
                print(f"  ì‘ë‹µ: {response.text}")
                return False
        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ ì™„ì„± ì˜¤ë¥˜: {e}")
            return False
    
    def test_chat_completion(self) -> bool:
        """ì±„íŒ… ì™„ì„± í…ŒìŠ¤íŠ¸"""
        try:
            payload = {
                "model": "bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF",
                "messages": [
                    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ Python ì½”ë“œ ì˜ˆì œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”."}
                ],
                "max_tokens": 150,
                "temperature": 0.7
            }
            
            response = requests.post(
                f"{self.base_url}/v1/chat/completions",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=90  # DeepSeek ëª¨ë¸ì€ ë” ê¸´ ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
            )
            
            if response.status_code == 200:
                result = response.json()
                print("âœ… ì±„íŒ… ì™„ì„± ì„±ê³µ:")
                print(f"  ì‚¬ìš©ì: {payload['messages'][0]['content']}")
                print(f"  AI: {result['choices'][0]['message']['content']}")
                return True
            else:
                print(f"âŒ ì±„íŒ… ì™„ì„± ì‹¤íŒ¨: {response.status_code}")
                print(f"  ì‘ë‹µ: {response.text}")
                return False
        except Exception as e:
            print(f"âŒ ì±„íŒ… ì™„ì„± ì˜¤ë¥˜: {e}")
            return False
    
    def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸ§ª vLLM API í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n")
        
        tests = [
            ("í—¬ìŠ¤ì²´í¬", self.test_health),
            ("ëª¨ë¸ ëª©ë¡", self.test_models),
            ("í…ìŠ¤íŠ¸ ì™„ì„±", self.test_completion),
            ("ì±„íŒ… ì™„ì„±", self.test_chat_completion)
        ]
        
        results = []
        for test_name, test_func in tests:
            print(f"ğŸ” {test_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
            result = test_func()
            results.append((test_name, result))
            print()
            time.sleep(1)
        
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
        for test_name, result in results:
            status = "âœ… ì„±ê³µ" if result else "âŒ ì‹¤íŒ¨"
            print(f"  {test_name}: {status}")
        
        success_count = sum(1 for _, result in results if result)
        print(f"\nğŸ¯ ì´ {len(results)}ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ {success_count}ê°œ ì„±ê³µ")

if __name__ == "__main__":
    tester = VLLMTester()
    tester.run_all_tests()
```

#### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python test_vllm_api.py
```

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 1. Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```yaml
# k8s/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllm-metrics
  namespace: vllm-inference
spec:
  selector:
    matchLabels:
      app: vllm-inference
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
```

### 2. ë¡œê·¸ ìˆ˜ì§‘ ì„¤ì •

```yaml
# k8s/fluentd-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-vllm-config
  namespace: vllm-inference
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/vllm-inference-*.log
      pos_file /var/log/fluentd-vllm.log.pos
      tag kubernetes.vllm
      format json
    </source>
    
    <match kubernetes.vllm>
      @type elasticsearch
      host elasticsearch.logging.svc.cluster.local
      port 9200
      index_name vllm-logs
    </match>
```

### 3. ëŒ€ì‹œë³´ë“œ ì„¤ì •

```yaml
# k8s/grafana-dashboard.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-dashboard
  namespace: vllm-inference
data:
  dashboard.json: |
    {
      "dashboard": {
        "title": "vLLM Inference Metrics",
        "panels": [
          {
            "title": "Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(vllm_requests_total[5m])"
              }
            ]
          },
          {
            "title": "Response Time",
            "type": "graph", 
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(vllm_request_duration_seconds_bucket[5m]))"
              }
            ]
          }
        ]
      }
    }
```

## ì„±ëŠ¥ ìµœì í™”

### 1. macOS CPU ë¦¬ì†ŒìŠ¤ íŠœë‹

```yaml
# k8s/deployment-optimized.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-inference-optimized
spec:
  template:
    spec:
      containers:
      - name: vllm
        resources:
          requests:
            memory: "8Gi"
            cpu: "3000m"
          limits:
            memory: "16Gi"
            cpu: "6000m"
        env:
        - name: VLLM_CPU_ONLY
          value: "1"
        - name: OMP_NUM_THREADS
          value: "6"
        - name: MKL_NUM_THREADS
          value: "6"
        - name: NUMEXPR_NUM_THREADS
          value: "6"
        - name: VLLM_WORKER_MULTIPROC_METHOD
          value: "spawn"
        - name: VLLM_ENGINE_ITERATION_TIMEOUT_S
          value: "120"
        # macOS Docker í™˜ê²½ ìµœì í™”
        - name: MAX_MODEL_LEN
          value: "4096"
        - name: MAX_NUM_SEQS
          value: "8"
```

### 2. CPU ìµœì í™” ìºì‹± ì „ëµ

```python
# cache_manager.py
"""CPU ê¸°ë°˜ ëª¨ë¸ ìºì‹± ê´€ë¦¬ì"""

import redis
import json
import hashlib
import threading
from typing import Optional, Dict, Any
from functools import lru_cache

class CPUModelCacheManager:
    def __init__(self, redis_host: str = "redis.vllm-inference.svc.cluster.local"):
        self.redis_client = redis.Redis(host=redis_host, port=6379, db=0)
        self.cache_ttl = 7200  # 2ì‹œê°„ (CPU ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤)
        self.local_cache = {}
        self.lock = threading.RLock()
    
    def _generate_cache_key(self, prompt: str, model: str, params: Dict[str, Any]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        cache_data = {
            "prompt": prompt[:500],  # ê¸´ í”„ë¡¬í”„íŠ¸ ì œí•œ
            "model": model,
            "params": {k: v for k, v in params.items() if k in ['temperature', 'max_tokens']}
        }
        cache_string = json.dumps(cache_data, sort_keys=True)
        return f"cpu_vllm:{hashlib.md5(cache_string.encode()).hexdigest()}"
    
    @lru_cache(maxsize=100)
    def get_cached_response(self, prompt: str, model: str, params: Dict[str, Any]) -> Optional[str]:
        """ìºì‹œëœ ì‘ë‹µ ì¡°íšŒ (ë¡œì»¬ ìºì‹œ + Redis)"""
        cache_key = self._generate_cache_key(prompt, model, params)
        
        # ë¡œì»¬ ìºì‹œ ë¨¼ì € í™•ì¸
        with self.lock:
            if cache_key in self.local_cache:
                return self.local_cache[cache_key]
        
        # Redis ìºì‹œ í™•ì¸
        cached_response = self.redis_client.get(cache_key)
        if cached_response:
            response = json.loads(cached_response.decode())
            # ë¡œì»¬ ìºì‹œì—ë„ ì €ì¥
            with self.lock:
                self.local_cache[cache_key] = response
            return response
        
        return None
    
    def cache_response(self, prompt: str, model: str, params: Dict[str, Any], response: str):
        """ì‘ë‹µ ìºì‹± (ë¡œì»¬ + Redis)"""
        cache_key = self._generate_cache_key(prompt, model, params)
        
        # ë¡œì»¬ ìºì‹œì— ì €ì¥
        with self.lock:
            self.local_cache[cache_key] = response
            # ë¡œì»¬ ìºì‹œ í¬ê¸° ì œí•œ
            if len(self.local_cache) > 50:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = next(iter(self.local_cache))
                del self.local_cache[oldest_key]
        
        # Redisì— ì €ì¥
        self.redis_client.setex(
            cache_key,
            self.cache_ttl,
            json.dumps(response)
        )
```

### 3. macOS Docker í™˜ê²½ ìµœì í™”

```bash
# Docker Desktop ë¦¬ì†ŒìŠ¤ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash
# optimize_docker.sh

echo "ğŸ”§ macOS Docker í™˜ê²½ ìµœì í™” ì¤‘..."

# Docker Desktop ë©”ëª¨ë¦¬ í• ë‹¹ í™•ì¸
docker system info | grep -E "Total Memory|CPUs"

# ì»¨í…Œì´ë„ˆ ë¦¬ì†ŒìŠ¤ ì œí•œ ì„¤ì •
docker update \
  --memory=12g \
  --cpus=4 \
  --memory-swap=16g \
  $(docker ps -q --filter "ancestor=vllm-cpu-custom:latest")

# ë¶ˆí•„ìš”í•œ ì´ë¯¸ì§€ ì •ë¦¬
docker image prune -f

# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
docker system prune -f

echo "âœ… Docker í™˜ê²½ ìµœì í™” ì™„ë£Œ"
```

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

#### 1. Podê°€ ì‹œì‘ë˜ì§€ ì•ŠëŠ” ê²½ìš°

```bash
# Pod ìƒíƒœ í™•ì¸
kubectl describe pod -l app=vllm-inference -n vllm-inference

# ì´ë²¤íŠ¸ í™•ì¸
kubectl get events -n vllm-inference --sort-by='.lastTimestamp'

# ë¦¬ì†ŒìŠ¤ ë¶€ì¡± í™•ì¸
kubectl top nodes
kubectl top pods -n vllm-inference
```

#### 2. CPU ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ì˜¤ë¥˜ (macOS íŠ¹í™”)

```bash
# CPU ì‚¬ìš©ëŸ‰ í™•ì¸
kubectl top pods -n vllm-inference

# macOS Docker Desktop CPU í• ë‹¹ í™•ì¸
docker system info | grep CPUs

# ë¦¬ì†ŒìŠ¤ ì œí•œ ì¡°ì •
kubectl patch deployment vllm-inference -n vllm-inference -p '
{
  "spec": {
    "template": {
      "spec": {
        "containers": [
          {
            "name": "vllm",
            "resources": {
              "limits": {
                "cpu": "4000m",
                "memory": "10Gi"
              },
              "requests": {
                "cpu": "2000m",
                "memory": "6Gi"
              }
            }
          }
        ]
      }
    }
  }
}'
```

#### 3. DeepSeek ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨

```bash
# ìƒì„¸ ë¡œê·¸ í™•ì¸
kubectl logs -f deployment/vllm-inference -n vllm-inference

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì§„í–‰ìƒí™© í™•ì¸
kubectl exec -it deployment/vllm-inference -n vllm-inference -- \
  ls -la /root/.cache/huggingface/hub/

# ëª¨ë¸ íŒŒì¼ í¬ê¸° í™•ì¸ (DeepSeek ëª¨ë¸ì€ í¼)
kubectl exec -it deployment/vllm-inference -n vllm-inference -- \
  du -h /root/.cache/huggingface/

# ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
kubectl exec -it deployment/vllm-inference -n vllm-inference -- \
  curl -I https://huggingface.co/bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF
```

#### 4. macOS Docker í”Œë«í¼ ê´€ë ¨ ë¬¸ì œ

```bash
# í”Œë«í¼ í˜¸í™˜ì„± í™•ì¸
docker run --rm --platform linux/amd64 vllm-cpu-custom:latest --help

# Apple Siliconì—ì„œ AMD64 ì—ë®¬ë ˆì´ì…˜ ì„±ëŠ¥ í™•ì¸
docker run --rm --platform linux/amd64 ubuntu:20.04 uname -a

# Docker Desktop ì¬ì‹œì‘ (ë¬¸ì œ í•´ê²°ì˜ ê¸°ë³¸)
osascript -e 'quit app "Docker Desktop"'
sleep 5
open -a "Docker Desktop"
```

#### 5. GGUF ëª¨ë¸ í˜•ì‹ ê´€ë ¨ ë¬¸ì œ

```bash
# GGUF ëª¨ë¸ ì§€ì› í™•ì¸
kubectl exec -it deployment/vllm-inference -n vllm-inference -- \
  python -c "import vllm; print('vLLM version:', vllm.__version__)"

# ì§€ì›ë˜ëŠ” ëª¨ë¸ í˜•ì‹ í™•ì¸
kubectl exec -it deployment/vllm-inference -n vllm-inference -- \
  python -c "from vllm import LLM; print('GGUF support available')"

# ëª¨ë¸ íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬
kubectl exec -it deployment/vllm-inference -n vllm-inference -- \
  python -c "
from huggingface_hub import hf_hub_download
import os
file_path = hf_hub_download(
    repo_id='bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF',
    filename='deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf'
)
print(f'File size: {os.path.getsize(file_path)} bytes')
"
```

## í”„ë¡œë•ì…˜ ë°°í¬ ê³ ë ¤ì‚¬í•­

### 1. ë³´ì•ˆ ì„¤ì •

```yaml
# k8s/security.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vllm-service-account
  namespace: vllm-inference
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: vllm-role
  namespace: vllm-inference
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: vllm-rolebinding
  namespace: vllm-inference
subjects:
- kind: ServiceAccount
  name: vllm-service-account
  namespace: vllm-inference
roleRef:
  kind: Role
  name: vllm-role
  apiGroup: rbac.authorization.k8s.io
---
# macOS í™˜ê²½ì„ ìœ„í•œ ë„¤íŠ¸ì›Œí¬ ì •ì±…
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: vllm-network-policy
  namespace: vllm-inference
spec:
  podSelector:
    matchLabels:
      app: vllm-inference
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 8000
  egress:
  - to: []
    ports:
    - protocol: TCP
      port: 443  # HTTPS for model downloads
    - protocol: TCP
      port: 80   # HTTP for health checks
```

### 2. ë°±ì—… ë° ë³µêµ¬

```bash
# ì„¤ì • ë°±ì—…
kubectl get all -n vllm-inference -o yaml > vllm-backup.yaml

# DeepSeek ëª¨ë¸ ìºì‹œ ë°±ì—… (í° íŒŒì¼ ì£¼ì˜)
kubectl exec deployment/vllm-inference -n vllm-inference -- \
  tar -czf /tmp/deepseek-model-cache.tar.gz /root/.cache/huggingface/

# ë°±ì—… íŒŒì¼ ë‹¤ìš´ë¡œë“œ
kubectl cp vllm-inference/$(kubectl get pods -n vllm-inference -l app=vllm-inference -o jsonpath='{.items[0].metadata.name}'):/tmp/deepseek-model-cache.tar.gz ./deepseek-model-cache.tar.gz
```

### 3. CI/CD íŒŒì´í”„ë¼ì¸ (macOS ëŒ€ì‘)

```yaml
# .github/workflows/deploy-macos.yml
name: Deploy vLLM CPU to Kubernetes (macOS)

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Build macOS compatible Docker image
      run: |
        docker buildx build \
          --platform linux/amd64 \
          -t vllm-cpu-custom:${{ github.sha }} \
          --load .
        
    - name: Test DeepSeek model compatibility
      run: |
        docker run --rm --platform linux/amd64 \
          vllm-cpu-custom:${{ github.sha }} \
          --model bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF \
          --dry-run
        
    - name: Deploy to Kubernetes
      run: |
        kubectl set image deployment/vllm-inference \
          vllm=vllm-cpu-custom:${{ github.sha }} \
          -n vllm-inference
```

## ê²°ë¡ 

ì´ ê°€ì´ë“œë¥¼ í†µí•´ macOS í™˜ê²½ì—ì„œ CPU ê¸°ë°˜ Kubernetes ìœ„ì— vLLM DeepSeek ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

### í•µì‹¬ í¬ì¸íŠ¸ ìš”ì•½

1. **macOS CPU ìµœì í™”**: GPU ëŒ€ì‹  CPU ì „ìš© vLLM ì‚¬ìš©
2. **DeepSeek GGUF ëª¨ë¸**: ê³ ì„±ëŠ¥ ì–‘ìí™” ëª¨ë¸ í™œìš©
3. **Docker í”Œë«í¼ í˜¸í™˜ì„±**: linux/amd64 í”Œë«í¼ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
4. **ë¦¬ì†ŒìŠ¤ ìµœì í™”**: macOS Docker Desktop í™˜ê²½ì— ë§ëŠ” ë©”ëª¨ë¦¬/CPU ì„¤ì •
5. **ëª¨ë‹ˆí„°ë§**: CPU ì‚¬ìš©ë¥ ê³¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¤‘ì‹¬ ëª¨ë‹ˆí„°ë§

### macOS íŠ¹í™” ì¥ì 

- **ê°œë°œ í™˜ê²½ ì¼ê´€ì„±**: ë§¥ë¶ì—ì„œ ê°œë°œë¶€í„° ë°°í¬ê¹Œì§€ í†µí•© í™˜ê²½
- **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±**: GPU ì—†ì´ë„ ì¶©ë¶„í•œ ì¶”ë¡  ì„±ëŠ¥
- **Docker ìµœì í™”**: Apple Siliconê³¼ Intel Mac ëª¨ë‘ ì§€ì›
- **ëª¨ë¸ í˜¸í™˜ì„±**: GGUF í˜•ì‹ìœ¼ë¡œ ë¹ ë¥¸ ë¡œë”©ê³¼ ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©

### ë‹¤ìŒ ë‹¨ê³„

- **ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ**: ë” í° DeepSeek ëª¨ë¸ (16B, 32B) í™œìš©
- **ë©€í‹° ëª¨ë¸ ì„œë¹™**: ì—¬ëŸ¬ GGUF ëª¨ë¸ ë™ì‹œ ìš´ì˜
- **ì„±ëŠ¥ íŠœë‹**: CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ë™ì  ìŠ¤ì¼€ì¼ë§
- **í´ë¼ìš°ë“œ ë°°í¬**: AWS/GCPì˜ CPU ì¸ìŠ¤í„´ìŠ¤ë¡œ í™•ì¥

macOSì˜ ë›°ì–´ë‚œ ê°œë°œ í™˜ê²½ê³¼ CPU ê¸°ë°˜ vLLMì˜ íš¨ìœ¨ì„±ì„ ê²°í•©í•˜ì—¬ ì‹¤ìš©ì ì´ê³  ë¹„ìš© íš¨ìœ¨ì ì¸ AI ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•í•´ë³´ì„¸ìš”!

---

*ì´ ê°€ì´ë“œëŠ” vLLM 0.4.x (CPU ì§€ì›) ë° Kubernetes 1.28+ ë²„ì „, macOS 14+ í™˜ê²½ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.* 

### ë””ë²„ê¹… ë„êµ¬ í™œìš©

```bash
# k9së¡œ í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§
k9s -n vllm-inference

# ì‹¤ì‹œê°„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°
stern vllm-inference -n vllm-inference

# ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸
kubectl run debug-pod --image=nicolaka/netshoot -it --rm -n vllm-inference

# macOS íŠ¹í™” ë””ë²„ê¹…
# Docker Desktop ìƒíƒœ í™•ì¸
docker system df
docker system events

# CPU ì‚¬ìš©ë¥  ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
kubectl exec -it deployment/vllm-inference -n vllm-inference -- \
  top -n 1 -b | head -20
```

## í”„ë¡œë•ì…˜ ë°°í¬ ê³ ë ¤ì‚¬í•­

### 1. macOS í™˜ê²½ ë³´ì•ˆ ì„¤ì •

```yaml
# k8s/security.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vllm-service-account
  namespace: vllm-inference
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: vllm-role
  namespace: vllm-inference
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: vllm-rolebinding
  namespace: vllm-inference
subjects:
- kind: ServiceAccount
  name: vllm-service-account
  namespace: vllm-inference
roleRef:
  kind: Role
  name: vllm-role
  apiGroup: rbac.authorization.k8s.io
---
# macOS í™˜ê²½ì„ ìœ„í•œ ë„¤íŠ¸ì›Œí¬ ì •ì±…
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: vllm-network-policy
  namespace: vllm-inference
spec:
  podSelector:
    matchLabels:
      app: vllm-inference
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 8000
  egress:
  - to: []
    ports:
    - protocol: TCP
      port: 443  # HTTPS for model downloads
    - protocol: TCP
      port: 80   # HTTP for health checks
```

### 2. ë°±ì—… ë° ë³µêµ¬

```bash
# ì„¤ì • ë°±ì—…
kubectl get all -n vllm-inference -o yaml > vllm-backup.yaml

# DeepSeek ëª¨ë¸ ìºì‹œ ë°±ì—… (í° íŒŒì¼ ì£¼ì˜)
kubectl exec deployment/vllm-inference -n vllm-inference -- \
  tar -czf /tmp/deepseek-model-cache.tar.gz /root/.cache/huggingface/

# ë°±ì—… íŒŒì¼ ë‹¤ìš´ë¡œë“œ
kubectl cp vllm-inference/$(kubectl get pods -n vllm-inference -l app=vllm-inference -o jsonpath='{.items[0].metadata.name}'):/tmp/deepseek-model-cache.tar.gz ./deepseek-model-cache.tar.gz
```

### 3. CI/CD íŒŒì´í”„ë¼ì¸ (macOS ëŒ€ì‘)

```yaml
# .github/workflows/deploy-macos.yml
name: Deploy vLLM CPU to Kubernetes (macOS)

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Build macOS compatible Docker image
      run: |
        docker buildx build \
          --platform linux/amd64 \
          -t vllm-cpu-custom:${{ github.sha }} \
          --load .
        
    - name: Test DeepSeek model compatibility
      run: |
        docker run --rm --platform linux/amd64 \
          vllm-cpu-custom:${{ github.sha }} \
          --model bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF \
          --dry-run
        
    - name: Deploy to Kubernetes
      run: |
        kubectl set image deployment/vllm-inference \
          vllm=vllm-cpu-custom:${{ github.sha }} \
          -n vllm-inference
```

## ê²°ë¡ 

ì´ ê°€ì´ë“œë¥¼ í†µí•´ macOS í™˜ê²½ì—ì„œ CPU ê¸°ë°˜ Kubernetes ìœ„ì— vLLM DeepSeek ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

### í•µì‹¬ í¬ì¸íŠ¸ ìš”ì•½

1. **macOS CPU ìµœì í™”**: GPU ëŒ€ì‹  CPU ì „ìš© vLLM ì‚¬ìš©
2. **DeepSeek GGUF ëª¨ë¸**: ê³ ì„±ëŠ¥ ì–‘ìí™” ëª¨ë¸ í™œìš©
3. **Docker í”Œë«í¼ í˜¸í™˜ì„±**: linux/amd64 í”Œë«í¼ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
4. **ë¦¬ì†ŒìŠ¤ ìµœì í™”**: macOS Docker Desktop í™˜ê²½ì— ë§ëŠ” ë©”ëª¨ë¦¬/CPU ì„¤ì •
5. **ëª¨ë‹ˆí„°ë§**: CPU ì‚¬ìš©ë¥ ê³¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¤‘ì‹¬ ëª¨ë‹ˆí„°ë§

### macOS íŠ¹í™” ì¥ì 

- **ê°œë°œ í™˜ê²½ ì¼ê´€ì„±**: ë§¥ë¶ì—ì„œ ê°œë°œë¶€í„° ë°°í¬ê¹Œì§€ í†µí•© í™˜ê²½
- **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±**: GPU ì—†ì´ë„ ì¶©ë¶„í•œ ì¶”ë¡  ì„±ëŠ¥
- **Docker ìµœì í™”**: Apple Siliconê³¼ Intel Mac ëª¨ë‘ ì§€ì›
- **ëª¨ë¸ í˜¸í™˜ì„±**: GGUF í˜•ì‹ìœ¼ë¡œ ë¹ ë¥¸ ë¡œë”©ê³¼ ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©

### ë‹¤ìŒ ë‹¨ê³„

- **ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ**: ë” í° DeepSeek ëª¨ë¸ (16B, 32B) í™œìš©
- **ë©€í‹° ëª¨ë¸ ì„œë¹™**: ì—¬ëŸ¬ GGUF ëª¨ë¸ ë™ì‹œ ìš´ì˜
- **ì„±ëŠ¥ íŠœë‹**: CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ë™ì  ìŠ¤ì¼€ì¼ë§
- **í´ë¼ìš°ë“œ ë°°í¬**: AWS/GCPì˜ CPU ì¸ìŠ¤í„´ìŠ¤ë¡œ í™•ì¥

macOSì˜ ë›°ì–´ë‚œ ê°œë°œ í™˜ê²½ê³¼ CPU ê¸°ë°˜ vLLMì˜ íš¨ìœ¨ì„±ì„ ê²°í•©í•˜ì—¬ ì‹¤ìš©ì ì´ê³  ë¹„ìš© íš¨ìœ¨ì ì¸ AI ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•í•´ë³´ì„¸ìš”!

---

*ì´ ê°€ì´ë“œëŠ” vLLM 0.4.x (CPU ì§€ì›) ë° Kubernetes 1.28+ ë²„ì „, macOS 14+ í™˜ê²½ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.* 