---
title: "PEFT: ëŒ€í˜• ëª¨ë¸ì„ 0.2%ë§Œ í•™ìŠµí•´ë„ ì „ì²´ íŒŒì¸íŠœë‹ ì„±ëŠ¥ì„ ë‚´ëŠ” í˜ì‹  ê¸°ìˆ "
excerpt: "LoRA, AdaLoRA, IA3 ë“± ìµœì‹  PEFT ê¸°ë²•ìœ¼ë¡œ ë©”ëª¨ë¦¬ëŠ” 80% ì ˆì•½í•˜ë©´ì„œ ì„±ëŠ¥ì€ ê·¸ëŒ€ë¡œ. Llama, BERT, Stable Diffusionê¹Œì§€ ëª¨ë“  ëª¨ë¸ì— ì ìš© ê°€ëŠ¥"
date: 2025-05-30
categories:
  - llmops
tags:
  - PEFT
  - LoRA
  - AdaLoRA
  - IA3
  - PromptTuning
  - íŒŒë¼ë¯¸í„°íš¨ìœ¨í•™ìŠµ
  - HuggingFace
  - ë©”ëª¨ë¦¬ìµœì í™”
  - íŒŒì¸íŠœë‹
author_profile: true
toc: true
toc_label: "PEFT ì™„ì „ ê°€ì´ë“œ"
---

> **TL;DR** [PEFT (Parameter-Efficient Fine-Tuning)](https://github.com/huggingface/peft)ëŠ” ğŸ¤—Hugging Faceì˜ **íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  íŒŒì¸íŠœë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬**ë‹¤. LoRA, AdaLoRA, IA3 ë“±ì„ í†µí•´ **ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ 0.2%ë§Œ í•™ìŠµ**í•´ë„ ì „ì²´ íŒŒì¸íŠœë‹ê³¼ **ë™ë“±í•œ ì„±ëŠ¥**ì„ ë‹¬ì„±í•œë‹¤. **1ë§Œ 8ì²œ ê°œ ì´ìƒì˜ GitHub ìŠ¤íƒ€**ë¥¼ ë°›ìœ¼ë©° ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì—…ê³„ í‘œì¤€ ì†”ë£¨ì…˜ì´ ë˜ì—ˆë‹¤.

---

## PEFTë€?

[PEFT (Parameter-Efficient Fine-Tuning)](https://github.com/huggingface/peft)ëŠ” ëŒ€í˜• ì‚¬ì „í›ˆë ¨ ëª¨ë¸ì˜ **ê·¹ì†Œìˆ˜ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ**í•˜ì—¬ ë‹¤ì–‘í•œ í•˜ìœ„ íƒœìŠ¤í¬ì— íš¨ìœ¨ì ìœ¼ë¡œ ì ì‘ì‹œí‚¤ëŠ” í˜ì‹ ì ì¸ ê¸°ìˆ ì´ë‹¤. **1ë§Œ 8ì²œ ê°œ ì´ìƒì˜ GitHub ìŠ¤íƒ€**ë¥¼ ë°›ìœ¼ë©° í˜„ì¬ AI ì—…ê³„ì—ì„œ ê°€ì¥ ì£¼ëª©ë°›ëŠ” íŒŒì¸íŠœë‹ ë°©ë²•ë¡ ì´ë‹¤.

### í•µì‹¬ ê°œë…

- **ê·¹ì†Œìˆ˜ íŒŒë¼ë¯¸í„° í•™ìŠµ**: ì „ì²´ ëª¨ë¸ì˜ 0.1-1%ë§Œ ì—…ë°ì´íŠ¸
- **ì„±ëŠ¥ ìœ ì§€**: ì „ì²´ íŒŒì¸íŠœë‹ê³¼ ë™ë“±í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 80% ì´ìƒ ì ˆì•½
- **ì €ì¥ ê³µê°„ ì ˆì•½**: ì²´í¬í¬ì¸íŠ¸ í¬ê¸°ë¥¼ GBì—ì„œ MBë¡œ ì¶•ì†Œ
- **ë‹¤ì¤‘ íƒœìŠ¤í¬ ì§€ì›**: í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ì—¬ëŸ¬ ì–´ëŒ‘í„° ê´€ë¦¬

## ì™œ PEFTë¥¼ ì¨ì•¼ í• ê¹Œ?

### ë©”ëª¨ë¦¬ í˜ëª…

A100 80GB GPU ê¸°ì¤€ ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ:

| ëª¨ë¸ | ì „ì²´ íŒŒì¸íŠœë‹ | PEFT-LoRA | PEFT-LoRA + DeepSpeed |
|------|-------------|-----------|---------------------|
| T0-3B (3B) | 47.14GB GPU | 14.4GB GPU | 9.8GB GPU |
| MT0-XXL (12B) | **OOM** | 56GB GPU | 22GB GPU |
| BLOOMZ-7B (7B) | **OOM** | 32GB GPU | 18.1GB GPU |

### ì„±ëŠ¥ ë¹„êµ

Twitter ì»´í”Œë ˆì¸ ë¶„ë¥˜ íƒœìŠ¤í¬ ê²°ê³¼:

| ë°©ë²• | ì •í™•ë„ | ì²´í¬í¬ì¸íŠ¸ í¬ê¸° |
|------|--------|---------------|
| ì¸ê°„ ê¸°ì¤€ì„  | 0.897 | - |
| Flan-T5 (ì „ì²´) | 0.892 | 11GB |
| **LoRA T0-3B** | **0.863** | **19MB** |

### ì‹¤ì œ ë„ì… ì‚¬ë¡€

**ì£¼ìš” ê¸°ì—…ë“¤ì˜ PEFT í™œìš©:**

- **OpenAI**: GPT-4 ë§ì¶¤ í•™ìŠµì— LoRA ë³€í˜• ì‚¬ìš©
- **Google**: PaLM, Gemini ëª¨ë¸ì˜ ë„ë©”ì¸ ì ì‘
- **Meta**: Llama 2 ì‹œë¦¬ì¦ˆì˜ instruction tuning
- **Microsoft**: Azure OpenAIì˜ ì»¤ìŠ¤í…€ ëª¨ë¸ ì„œë¹„ìŠ¤
- **Stability AI**: Stable Diffusion ì»¤ìŠ¤í„°ë§ˆì´ì§•

## ì„¤ì¹˜ ë° ë¹ ë¥¸ ì‹œì‘

### ì„¤ì¹˜

```bash
pip install peft
```

### 30ì´ˆ QuickStart

```python
from transformers import AutoModelForSeq2SeqLM
from peft import get_peft_config, get_peft_model, LoraConfig, TaskType

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/mt0-large")

# LoRA ì„¤ì • (0.19%ë§Œ í•™ìŠµ!)
peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1
)

# PEFT ëª¨ë¸ ìƒì„±
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
# "trainable params: 2,359,296 || all params: 1,231,940,608 || trainable%: 0.19%"
```

### ì¶”ë¡ ìš© ëª¨ë¸ ë¡œë“œ

```python
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer

# í•™ìŠµëœ PEFT ëª¨ë¸ ë¡œë“œ
model = AutoPeftModelForCausalLM.from_pretrained("ybelkada/opt-350m-lora")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

# ì¶”ë¡  ì‹¤í–‰
inputs = tokenizer("ë ˆì‹œí”¼: ì˜¤ë¸ì„ 350ë„ë¡œ ì˜ˆì—´í•˜ê³ ", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## ì£¼ìš” PEFT ë°©ë²•ë¡  ìƒì„¸ ê°€ì´ë“œ

### LoRA (Low-Rank Adaptation)

**LoRA**ëŠ” ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” PEFT ë°©ë²•ìœ¼ë¡œ, ê¸°ì¡´ ê°€ì¤‘ì¹˜ë¥¼ ê³ ì •í•˜ê³  ì €ì°¨ì› í–‰ë ¬ì„ ì¶”ê°€í•œë‹¤.

#### ìˆ˜í•™ì  ì›ë¦¬

ê¸°ì¡´ ê°€ì¤‘ì¹˜ í–‰ë ¬ Wì— ëŒ€í•´:

```
W_new = W + BA
```

ì—¬ê¸°ì„œ:

- W: ì›ë˜ ê°€ì¤‘ì¹˜ (ê³ ì •)
- B: rÃ—d í–‰ë ¬
- A: kÃ—r í–‰ë ¬  
- r: ë­í¬ (ë³´í†µ 8-64)

#### ì‹¤ìŠµ ì˜ˆì œ

```python
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")

# LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,  # ë­í¬ ì„¤ì •
    lora_alpha=32,  # ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°
    target_modules=["c_attn", "c_proj"],  # íƒ€ê²Ÿ ëª¨ë“ˆ
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

# LoRA ì ìš©
model = get_peft_model(model, lora_config)
print(f"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {model.num_parameters(only_trainable=True):,}")
```

#### í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°€ì´ë“œ

```python
# ë³´ìˆ˜ì  ì„¤ì • (ì•ˆì •ì , ë©”ëª¨ë¦¬ ì ˆì•½)
conservative_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1
)

# ê³µê²©ì  ì„¤ì • (ë” ë†’ì€ ì„±ëŠ¥, ë” ë§ì€ íŒŒë¼ë¯¸í„°)
aggressive_config = LoraConfig(
    r=64,
    lora_alpha=128,
    lora_dropout=0.05
)
```

### AdaLoRA (Adaptive LoRA)

**AdaLoRA**ëŠ” ì¤‘ìš”ë„ì— ë”°ë¼ ë­í¬ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ê°œì„ ëœ LoRA ë°©ë²•ì´ë‹¤.

#### í•µì‹¬ ì•„ì´ë””ì–´

- **ì ì‘ì  ë­í¬**: ê° ì–´ëŒ‘í„° ëª¨ë“ˆë§ˆë‹¤ ë‹¤ë¥¸ ë­í¬ ì‚¬ìš©
- **ì¤‘ìš”ë„ ì ìˆ˜**: SVD ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš”ë„ ê³„ì‚°
- **ê°€ì§€ì¹˜ê¸°**: ëœ ì¤‘ìš”í•œ íŠ¹ì´ê°’ ì œê±°

#### ì‹¤ìŠµ ì˜ˆì œ

```python
from peft import AdaLoraConfig, get_peft_model

adalora_config = AdaLoraConfig(
    init_r=12,  # ì´ˆê¸° ë­í¬
    target_r=8,  # ëª©í‘œ ë­í¬
    beta1=0.85,  # ì§€ìˆ˜ ì´ë™ í‰ê·  ê³„ìˆ˜
    beta2=0.85,
    tinit=200,  # ì›Œë°ì—… ìŠ¤í…
    tfinal=1000,  # ê°€ì§€ì¹˜ê¸° ì™„ë£Œ ìŠ¤í…
    deltaT=10,   # ì—…ë°ì´íŠ¸ ê°„ê²©
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj"],
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, adalora_config)
```

### IA3 (Infused Adapter by Inhibiting and Amplifying Inner Activations)

**IA3**ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ì¶”ê°€í•˜ëŠ” ëŒ€ì‹  ê¸°ì¡´ í™œì„±í™”ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” ë°©ë²•ì´ë‹¤.

#### í•µì‹¬ íŠ¹ì§•

- **ê°€ì¤‘ì¹˜ ì—†ìŒ**: ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ í–‰ë ¬ ì¶”ê°€ ì—†ìŒ
- **í™œì„±í™” ìŠ¤ì¼€ì¼ë§**: í‚¤, ê°’, í”¼ë“œí¬ì›Œë“œ í™œì„±í™”ë¥¼ ìŠ¤ì¼€ì¼ë§
- **ê·¹ì†Œ íŒŒë¼ë¯¸í„°**: LoRAë³´ë‹¤ë„ ì ì€ íŒŒë¼ë¯¸í„° ì‚¬ìš©

#### ì‹¤ìŠµ ì˜ˆì œ

```python
from peft import IA3Config, get_peft_model

ia3_config = IA3Config(
    target_modules=["k_proj", "v_proj", "down_proj"],
    feedforward_modules=["down_proj"],
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, ia3_config)
print(f"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {model.num_parameters(only_trainable=True):,}")
# LoRAë³´ë‹¤ 10ë°° ì ì€ íŒŒë¼ë¯¸í„°!
```

### Prompt Tuning

**Prompt Tuning**ì€ ì…ë ¥ì— í•™ìŠµ ê°€ëŠ¥í•œ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì´ë‹¤.

#### ì‹¤ìŠµ ì˜ˆì œ

```python
from peft import PromptTuningConfig, get_peft_model

prompt_config = PromptTuningConfig(
    task_type="CAUSAL_LM",
    prompt_tuning_init="TEXT",  # í…ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
    num_virtual_tokens=20,  # í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜
    prompt_tuning_init_text="ë‹¤ìŒì€ ê³ í’ˆì§ˆ ì‘ë‹µì…ë‹ˆë‹¤:",
    tokenizer_name_or_path="gpt2"
)

model = get_peft_model(model, prompt_config)
```

### P-Tuning v2

**P-Tuning v2**ëŠ” ëª¨ë“  ë ˆì´ì–´ì— í•™ìŠµ ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì´ë‹¤.

```python
from peft import PromptEncoderConfig, get_peft_model

p_tuning_config = PromptEncoderConfig(
    task_type="CAUSAL_LM",
    num_virtual_tokens=20,
    encoder_hidden_size=128
)

model = get_peft_model(model, p_tuning_config)
```

## ì‹¤ì œ í™œìš© ì‚¬ë¡€ ì‹¬í™” ë¶„ì„

### ì‚¬ë¡€ 1: ChatGPT ìŠ¤íƒ€ì¼ ëŒ€í™” ëª¨ë¸

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Llama 2 7B ëª¨ë¸ë¡œ ChatGPT ìŠ¤íƒ€ì¼ í•™ìŠµ
model_name = "meta-llama/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,  # 4ë¹„íŠ¸ ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    device_map="auto"
)

# LoRA ì„¤ì •
lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# ëŒ€í™” ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("OpenAssistant/oasst1", split="train")

# í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./llama-chat-lora",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    warmup_ratio=0.1,
    learning_rate=2e-4,
    logging_steps=25,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500
)

# SFT íŠ¸ë ˆì´ë„ˆë¡œ í•™ìŠµ
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    args=training_args,
    max_seq_length=2048,
    dataset_text_field="text"
)

trainer.train()
```

### ì‚¬ë¡€ 2: Stable Diffusion ì»¤ìŠ¤í„°ë§ˆì´ì§•

```python
import torch
from diffusers import StableDiffusionPipeline
from peft import LoraConfig, get_peft_model

# Stable Diffusion ëª¨ë¸ ë¡œë“œ
model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id)

# UNetì— LoRA ì ìš©
unet = pipe.unet
lora_config = LoraConfig(
    r=4,  # Diffusion ëª¨ë¸ì€ ë‚®ì€ ë­í¬ ì‚¬ìš©
    lora_alpha=32,
    target_modules=["to_k", "to_q", "to_v", "to_out.0"],
    lora_dropout=0.1
)

# LoRA ì–´ëŒ‘í„° ì ìš©
unet = get_peft_model(unet, lora_config)
print(f"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {unet.num_parameters(only_trainable=True):,}")
# ì›ë³¸ UNet: 860M â†’ LoRA: 2.5M (0.3%ë§Œ í•™ìŠµ!)
```

### ì‚¬ë¡€ 3: ë‹¤êµ­ì–´ ë²ˆì—­ ëª¨ë¸

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer
from peft import LoraConfig, get_peft_model

# T5 ëª¨ë¸ë¡œ ë²ˆì—­ íƒœìŠ¤í¬
model_name = "google/flan-t5-large"
model = T5ForConditionalGeneration.from_pretrained(model_name)
tokenizer = T5Tokenizer.from_pretrained(model_name)

# LoRA ì„¤ì • (T5 íŠ¹í™”)
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q", "v", "wi_0", "wi_1", "wo"],
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_2_SEQ_LM"
)

model = get_peft_model(model, lora_config)

# ë²ˆì—­ ì˜ˆì œ
def translate(text, source_lang, target_lang):
    prompt = f"translate {source_lang} to {target_lang}: {text}"
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=512,
            num_beams=4,
            do_sample=False
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ì‚¬ìš© ì˜ˆì‹œ
result = translate("ì•ˆë…•í•˜ì„¸ìš”", "Korean", "English")
print(result)  # "Hello"
```

## PEFTì™€ ë‹¤ë¥¸ ê¸°ìˆ ì˜ í†µí•©

### TRLê³¼ì˜ í†µí•©

```python
from trl import DPOTrainer, DPOConfig
from peft import LoraConfig, get_peft_model

# DPO (Direct Preference Optimization)ì— PEFT ì ìš©
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

# LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# DPO í•™ìŠµ
dpo_config = DPOConfig(
    output_dir="./llama-dpo-lora",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    learning_rate=1e-6,
    beta=0.1
)

dpo_trainer = DPOTrainer(
    model=model,
    args=dpo_config,
    train_dataset=preference_dataset,
    tokenizer=tokenizer
)

dpo_trainer.train()
```

### Quantizationê³¼ì˜ ê²°í•©

```python
from transformers import BitsAndBytesConfig
import torch

# 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/DialoGPT-large",
    quantization_config=bnb_config,
    device_map="auto"
)

# QLoRA ì ìš©
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["c_attn"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {model.get_memory_footprint() / 1e9:.2f} GB")
```

### Accelerateì™€ì˜ í†µí•©

```python
from accelerate import Accelerator
from peft import LoraConfig, get_peft_model

# ë¶„ì‚° í•™ìŠµ ì„¤ì •
accelerator = Accelerator()

# ëª¨ë¸ê³¼ LoRA ì„¤ì •
model = AutoModelForCausalLM.from_pretrained("gpt2-large")
lora_config = LoraConfig(r=16, target_modules=["c_attn"], task_type="CAUSAL_LM")
model = get_peft_model(model, lora_config)

# Acceleratorë¡œ ì¤€ë¹„
model, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)

# ë¶„ì‚° í•™ìŠµ ë£¨í”„
for batch in dataloader:
    outputs = model(**batch)
    loss = outputs.loss
    accelerator.backward(loss)
    optimizer.step()
    optimizer.zero_grad()
```

## ê³ ê¸‰ í™œìš© íŒ

### ë‹¤ì¤‘ ì–´ëŒ‘í„° ê´€ë¦¬

```python
from peft import PeftModel

# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained("gpt2")

# ì—¬ëŸ¬ íƒœìŠ¤í¬ìš© ì–´ëŒ‘í„° ë¡œë“œ
model = PeftModel.from_pretrained(base_model, "path/to/sentiment-adapter")
model.load_adapter("path/to/translation-adapter", adapter_name="translation")
model.load_adapter("path/to/summarization-adapter", adapter_name="summarization")

# ì–´ëŒ‘í„° ì „í™˜
model.set_adapter("translation")  # ë²ˆì—­ ëª¨ë“œ
outputs = model.generate(translation_input)

model.set_adapter("summarization")  # ìš”ì•½ ëª¨ë“œ  
outputs = model.generate(summarization_input)

# ì–´ëŒ‘í„° ê°€ì¤‘ ì¡°í•©
model.add_weighted_adapter(
    adapters=["sentiment", "translation"],
    weights=[0.7, 0.3],
    adapter_name="mixed"
)
```

### ì–´ëŒ‘í„° ìœµí•©

```python
# í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ë¥¼ ì›ë³¸ ëª¨ë¸ì— ë³‘í•©
model = PeftModel.from_pretrained(base_model, "path/to/lora-adapter")
merged_model = model.merge_and_unload()

# ë³‘í•©ëœ ëª¨ë¸ ì €ì¥
merged_model.save_pretrained("./merged-model")
```

### ì ì§„ì  ë­í¬ ì¦ê°€

```python
# ì‘ì€ ë­í¬ë¡œ ì‹œì‘
initial_config = LoraConfig(r=8, lora_alpha=16, target_modules=["c_attn"])
model = get_peft_model(base_model, initial_config)

# ì²« ë²ˆì§¸ ë‹¨ê³„ í•™ìŠµ
trainer.train()

# ë­í¬ ì¦ê°€
expanded_config = LoraConfig(r=16, lora_alpha=32, target_modules=["c_attn"])
model = expand_lora_rank(model, expanded_config)  # ì»¤ìŠ¤í…€ í•¨ìˆ˜

# ë‘ ë²ˆì§¸ ë‹¨ê³„ í•™ìŠµ
trainer.train()
```

## ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

### ë©”ëª¨ë¦¬ ìµœì í™”

```python
# ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ê³¼ í•¨ê»˜ ì‚¬ìš©
from transformers import TrainingArguments

training_args = TrainingArguments(
    gradient_checkpointing=True,  # ë©”ëª¨ë¦¬ ì ˆì•½
    dataloader_pin_memory=False,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    warmup_ratio=0.03,
    learning_rate=2e-4,
    bf16=True,  # BF16 ì‚¬ìš©
    logging_steps=10,
    optim="paged_adamw_32bit"  # í˜ì´ì§€ë“œ ì˜µí‹°ë§ˆì´ì €
)
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

```python
# íƒœìŠ¤í¬ë³„ ìµœì  ì„¤ì •
TASK_CONFIGS = {
    "text_classification": {
        "r": 8, "lora_alpha": 16, "lora_dropout": 0.1
    },
    "text_generation": {
        "r": 16, "lora_alpha": 32, "lora_dropout": 0.05
    },
    "translation": {
        "r": 32, "lora_alpha": 64, "lora_dropout": 0.1
    },
    "summarization": {
        "r": 16, "lora_alpha": 32, "lora_dropout": 0.1
    }
}

def get_optimal_config(task_type, model_size):
    base_config = TASK_CONFIGS[task_type]
    
    # ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ì¡°ì •
    if model_size > 10e9:  # 10B+ ëª¨ë¸
        base_config["r"] *= 2
        base_config["lora_alpha"] *= 2
    
    return LoraConfig(**base_config, task_type="CAUSAL_LM")
```

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. ìˆ˜ë ´í•˜ì§€ ì•ŠëŠ” ë¬¸ì œ

```python
# í•´ê²°ì±…: ë” ë†’ì€ ë­í¬ì™€ í•™ìŠµë¥  ì‚¬ìš©
problematic_config = LoraConfig(r=4, lora_alpha=8, learning_rate=1e-5)

# ê°œì„ ëœ ì„¤ì •
improved_config = LoraConfig(
    r=16,  # ë­í¬ ì¦ê°€
    lora_alpha=32,  # ì•ŒíŒŒ ì¦ê°€
    lora_dropout=0.05,  # ë“œë¡­ì•„ì›ƒ ê°ì†Œ
    learning_rate=2e-4  # í•™ìŠµë¥  ì¦ê°€
)
```

#### 2. ê³¼ì í•© ë¬¸ì œ

```python
# í•´ê²°ì±…: ì •ê·œí™” ê°•í™”
anti_overfitting_config = LoraConfig(
    r=8,  # ë­í¬ ê°ì†Œ
    lora_alpha=16,
    lora_dropout=0.2,  # ë“œë¡­ì•„ì›ƒ ì¦ê°€
    weight_decay=0.01  # ê°€ì¤‘ì¹˜ ê°ì‡  ì¶”ê°€
)
```

#### 3. ë©”ëª¨ë¦¬ ë¶€ì¡±

```python
# í•´ê²°ì±…: ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°ì™€ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
memory_efficient_args = TrainingArguments(
    per_device_train_batch_size=1,  # ìµœì†Œ ë°°ì¹˜
    gradient_accumulation_steps=32,  # ë†’ì€ ëˆ„ì 
    gradient_checkpointing=True,
    dataloader_pin_memory=False,
    optim="adafactor"  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì˜µí‹°ë§ˆì´ì €
)
```

## ìµœì‹  ë™í–¥ ë° ë¡œë“œë§µ

### 2025ë…„ ì£¼ìš” ì—…ë°ì´íŠ¸

- **QA-LoRA**: ì–‘ìí™” ì¸ì‹ LoRA í•™ìŠµ
- **MoRA**: Mixture-of-Rank Adaptation
- **DoRA**: Weight-Decomposed Low-Rank Adaptation
- **Delta-LoRA**: ì°¨ë¶„ ê¸°ë°˜ ë­í¬ ì ì‘
- **Vera**: Vector-based Random Matrix Adaptation

### ì°¨ì„¸ëŒ€ PEFT ê¸°ìˆ 

```python
# DoRA (Weight-Decomposed LoRA) ì˜ˆì‹œ
from peft import DoRAConfig

dora_config = DoRAConfig(
    r=8,
    lora_alpha=16,
    use_dora=True,  # DoRA í™œì„±í™”
    target_modules=["q_proj", "v_proj"]
)
```

## ì‹¤ì „ í”„ë¡œì íŠ¸ ì˜ˆì œ

### í”„ë¡œì íŠ¸ 1: ì½”ë“œ ìƒì„± ëª¨ë¸

```python
# CodeLlama ëª¨ë¸ë¡œ Python ì½”ë“œ ìƒì„±ê¸° êµ¬ì¶•
from transformers import CodeLlamaTokenizer, LlamaForCausalLM
from peft import LoraConfig, get_peft_model

model_name = "codellama/CodeLlama-7b-Python-hf"
model = LlamaForCausalLM.from_pretrained(model_name, load_in_4bit=True)
tokenizer = CodeLlamaTokenizer.from_pretrained(model_name)

# ì½”ë“œ ìƒì„± íŠ¹í™” LoRA ì„¤ì •
code_config = LoraConfig(
    r=32,  # ì½”ë“œëŠ” ë†’ì€ ë­í¬ í•„ìš”
    lora_alpha=64,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, code_config)

# ì½”ë“œ ìƒì„± í•¨ìˆ˜
def generate_code(prompt, max_length=512):
    inputs = tokenizer(f"# {prompt}\n", return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=0.2,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ì‚¬ìš© ì˜ˆì‹œ
code = generate_code("íŒŒì´ì¬ì—ì„œ ì´ì§„ ê²€ìƒ‰ êµ¬í˜„")
print(code)
```

### í”„ë¡œì íŠ¸ 2: ë©€í‹°ëª¨ë‹¬ ì´ë¯¸ì§€ ìº¡ì…”ë‹

```python
from transformers import BlipProcessor, BlipForConditionalGeneration
from peft import LoraConfig, get_peft_model

# BLIP ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ìº¡ì…”ë‹
model_name = "Salesforce/blip-image-captioning-large"
processor = BlipProcessor.from_pretrained(model_name)
model = BlipForConditionalGeneration.from_pretrained(model_name)

# ì´ë¯¸ì§€ ì¸ì½”ë”ëŠ” ê³ ì •, í…ìŠ¤íŠ¸ ë””ì½”ë”ë§Œ LoRA ì ìš©
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # í…ìŠ¤íŠ¸ ë””ì½”ë”ë§Œ
    modules_to_save=["classifier"],  # ë¶„ë¥˜ê¸° í—¤ë“œ ì €ì¥
    lora_dropout=0.1,
    bias="none",
    task_type="VISION_2_SEQ"
)

model = get_peft_model(model, lora_config)

# ì»¤ìŠ¤í…€ ìº¡ì…”ë‹ í•¨ìˆ˜
def generate_caption(image, style="detailed"):
    if style == "detailed":
        prompt = "a detailed description of"
    elif style == "artistic":
        prompt = "an artistic interpretation of"
    else:
        prompt = ""
    
    inputs = processor(image, prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=50,
            num_beams=5,
            temperature=0.7
        )
    
    return processor.decode(outputs[0], skip_special_tokens=True)
```

## ì»¤ë®¤ë‹ˆí‹°ì™€ ìë£Œ

### ê³µì‹ ìë£Œ

- **GitHub**: [https://github.com/huggingface/peft](https://github.com/huggingface/peft)
- **ë¬¸ì„œ**: [https://huggingface.co/docs/peft](https://huggingface.co/docs/peft)
- **PEFT Organization**: [https://huggingface.co/peft](https://huggingface.co/peft)

### ì‹¤ìŠµ ìë£Œ

- **ê³µì‹ ì˜ˆì œ**: [PEFT Examples](https://github.com/huggingface/peft/tree/main/examples)
- **ë…¸íŠ¸ë¶ ì»¬ë ‰ì…˜**: ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë³„ ì‹¤ìŠµ ë…¸íŠ¸ë¶
- **ëª¨ë¸ ì»¬ë ‰ì…˜**: ì‚¬ì „ í•™ìŠµëœ PEFT ì–´ëŒ‘í„°ë“¤

### ì—°êµ¬ ë…¼ë¬¸

- **LoRA**: "LoRA: Low-Rank Adaptation of Large Language Models"
- **AdaLoRA**: "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
- **IA3**: "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"
- **Prompt Tuning**: "The Power of Scale for Parameter-Efficient Prompt Tuning"

## ë§ˆë¬´ë¦¬

PEFTëŠ” í˜„ì¬ AI ì—…ê³„ì—ì„œ **ê°€ì¥ í˜ì‹ ì ì¸ íŒŒì¸íŠœë‹ ê¸°ìˆ **ì´ë‹¤. **ê·¹ì†Œìˆ˜ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ**í•´ë„ ì „ì²´ íŒŒì¸íŠœë‹ê³¼ ë™ë“±í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©´ì„œ, **ë©”ëª¨ë¦¬ì™€ ì €ì¥ ê³µê°„ì„ íšê¸°ì ìœ¼ë¡œ ì ˆì•½**í•œë‹¤.

íŠ¹íˆ **GPU ìì›ì´ ì œí•œëœ í™˜ê²½**ì—ì„œë„ ìµœì‹  ëŒ€í˜• ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ì–´, **AI ë¯¼ì£¼í™”**ì— í¬ê²Œ ê¸°ì—¬í•˜ê³  ìˆë‹¤. LoRA, AdaLoRA, IA3 ë“± ë‹¤ì–‘í•œ ë°©ë²•ë¡ ì„ ì œê³µí•˜ì—¬ ê° ìƒí™©ì— ìµœì í™”ëœ ì†”ë£¨ì…˜ì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤.

OpenAI, Google, Meta ë“± ì£¼ìš” ê¸°ì—…ë“¤ì´ ì´ë¯¸ PEFTë¥¼ ì ê·¹ ë„ì…í•˜ê³  ìˆìœ¼ë‹ˆ, LLM ê°œë°œì´ë‚˜ ì—°êµ¬ì— ê´€ì‹¬ì´ ìˆë‹¤ë©´ **ë°˜ë“œì‹œ ë§ˆìŠ¤í„°í•´ì•¼ í•  í•µì‹¬ ê¸°ìˆ **ì´ë‹¤.

---

*ì´ ê¸€ì´ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´, PEFT GitHubì— â­ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!*
