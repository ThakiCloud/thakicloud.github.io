---
title: "LM-Evaluation-Harness ì™„ì „ ê°€ì´ë“œ: ì–¸ì–´ ëª¨ë¸ í‰ê°€ì˜ í‘œì¤€ í”„ë ˆì„ì›Œí¬"
excerpt: "EleutherAIì˜ LM-Evaluation-Harnessë¡œ GPT, Claude, Llama ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì•Œì•„ë´…ë‹ˆë‹¤."
date: 2025-06-14
categories: 
  - llmops
  - dev
tags: 
  - lm-evaluation-harness
  - language-model-evaluation
  - eleutherai
  - benchmarking
  - few-shot-learning
  - model-evaluation
  - mlops
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
---

## ì†Œê°œ

[LM-Evaluation-Harness](https://github.com/EleutherAI/lm-evaluation-harness)ëŠ” EleutherAIì—ì„œ ê°œë°œí•œ ì–¸ì–´ ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ í‘œì¤€ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. 9.3k+ ìŠ¤íƒ€ë¥¼ ë°›ì€ ì´ ë„êµ¬ëŠ” GPT, Claude, Llama ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¼ê´€ë˜ê³  ì¬í˜„ ê°€ëŠ¥í•œ ë°©ì‹ìœ¼ë¡œ ì¸¡ì •í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œëŠ” ì„¤ì¹˜ë¶€í„° ê³ ê¸‰ í™œìš©ë²•ê¹Œì§€, LM-Evaluation-Harnessë¥¼ ì™„ì „íˆ ë§ˆìŠ¤í„°í•˜ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

## ì£¼ìš” íŠ¹ì§•

### í•µì‹¬ ê¸°ëŠ¥
- **Few-shot í‰ê°€**: ì–¸ì–´ ëª¨ë¸ì˜ few-shot learning ëŠ¥ë ¥ ì¸¡ì •
- **ë‹¤ì–‘í•œ ë°±ì—”ë“œ ì§€ì›**: HuggingFace, vLLM, OpenAI API, Anthropic API ë“±
- **í‘œì¤€í™”ëœ ë²¤ì¹˜ë§ˆí¬**: MMLU, HellaSwag, ARC, GSM8K ë“± ì£¼ìš” ë²¤ì¹˜ë§ˆí¬ ë‚´ì¥
- **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: ìƒˆë¡œìš´ íƒœìŠ¤í¬ì™€ ë©”íŠ¸ë¦­ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥
- **ì‹œê°í™” ë„êµ¬**: Weights & Biases, Zeno ì—°ë™ ì§€ì›

### ì§€ì›í•˜ëŠ” ëª¨ë¸ ë°±ì—”ë“œ

| ë°±ì—”ë“œ | ì„¤ëª… | ì‚¬ìš© ì˜ˆì‹œ |
|:-------|:-----|:----------|
| `hf` | HuggingFace Transformers | ë¡œì»¬ ëª¨ë¸ í‰ê°€ |
| `vllm` | vLLM ì¶”ë¡  ì—”ì§„ | ê³ ì„±ëŠ¥ GPU ì¶”ë¡  |
| `openai` | OpenAI API | GPT-4, GPT-3.5 ë“± |
| `anthropic` | Anthropic API | Claude ì‹œë¦¬ì¦ˆ |
| `local-chat-completions` | ë¡œì»¬ OpenAI í˜¸í™˜ ì„œë²„ | ìì²´ ë°°í¬ ëª¨ë¸ |

## ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### ê¸°ë³¸ ì„¤ì¹˜

```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install lm-eval

# ë˜ëŠ” ê°œë°œ ë²„ì „ ì„¤ì¹˜
git clone https://github.com/EleutherAI/lm-evaluation-harness
cd lm-evaluation-harness
pip install -e .
```

### ì„ íƒì  ì˜ì¡´ì„± ì„¤ì¹˜

LM-Evaluation-HarnessëŠ” ë‹¤ì–‘í•œ extrasë¥¼ ì œê³µí•©ë‹ˆë‹¤:

```bash
# API ëª¨ë¸ ì‚¬ìš© (OpenAI, Anthropic)
pip install lm-eval[api]

# vLLM ë°±ì—”ë“œ ì‚¬ìš©
pip install lm-eval[vllm]

# Weights & Biases ì—°ë™
pip install lm-eval[wandb]

# ì‹œê°í™” ë„êµ¬ Zeno ì‚¬ìš©
pip install lm-eval[zeno]

# ìˆ˜í•™ ë¬¸ì œ í‰ê°€
pip install lm-eval[math]

# ëª¨ë“  extras ì„¤ì¹˜ (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
pip install lm-eval[all]
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

API ê¸°ë°˜ ëª¨ë¸ ì‚¬ìš© ì‹œ í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜:

```bash
# OpenAI API
export OPENAI_API_KEY="your-openai-api-key"

# Anthropic API
export ANTHROPIC_API_KEY="your-anthropic-api-key"

# HuggingFace Hub (private ëª¨ë¸ìš©)
export HF_TOKEN="your-huggingface-token"

# Weights & Biases
export WANDB_API_KEY="your-wandb-api-key"

# Zeno ì‹œê°í™”
export ZENO_API_KEY="your-zeno-api-key"
```

## ê¸°ë³¸ ì‚¬ìš©ë²•

### ê°„ë‹¨í•œ í‰ê°€ ì‹¤í–‰

```bash
# HuggingFace ëª¨ë¸ë¡œ HellaSwag í‰ê°€
lm_eval --model hf \
        --model_args pretrained=microsoft/DialoGPT-medium \
        --tasks hellaswag \
        --device cuda:0 \
        --batch_size 8
```

### ì£¼ìš” CLI ì˜µì…˜

| ì˜µì…˜ | ì„¤ëª… | ì˜ˆì‹œ |
|:-----|:-----|:-----|
| `--model` | ì‚¬ìš©í•  ëª¨ë¸ ë°±ì—”ë“œ | `hf`, `openai`, `vllm` |
| `--model_args` | ëª¨ë¸ë³„ ì„¤ì • | `pretrained=gpt2,device=cuda` |
| `--tasks` | ì‹¤í–‰í•  íƒœìŠ¤í¬ ëª©ë¡ | `hellaswag,arc_easy,mmlu` |
| `--num_fewshot` | Few-shot ì˜ˆì‹œ ê°œìˆ˜ | `0`, `5`, `10` |
| `--batch_size` | ë°°ì¹˜ í¬ê¸° | `1`, `8`, `16` |
| `--output_path` | ê²°ê³¼ ì €ì¥ ê²½ë¡œ | `results/gpt2_results.json` |
| `--limit` | í‰ê°€í•  ìƒ˜í”Œ ìˆ˜ ì œí•œ | `100`, `1000` |
| `--log_samples` | ê°œë³„ ìƒ˜í”Œ ë¡œê·¸ ì €ì¥ | í”Œë˜ê·¸ë§Œ ì‚¬ìš© |

## ë‹¤ì–‘í•œ ëª¨ë¸ ë°±ì—”ë“œ í™œìš©

### HuggingFace ëª¨ë¸ í‰ê°€

```bash
# ê¸°ë³¸ HuggingFace ëª¨ë¸
lm_eval --model hf \
        --model_args pretrained=EleutherAI/gpt-j-6B \
        --tasks hellaswag,arc_easy \
        --device cuda:0 \
        --batch_size 8

# ì–‘ìí™”ëœ ëª¨ë¸
lm_eval --model hf \
        --model_args pretrained=microsoft/DialoGPT-medium,load_in_8bit=True \
        --tasks mmlu \
        --device cuda:0
```

### vLLM ë°±ì—”ë“œ ì‚¬ìš©

```bash
# vLLMìœ¼ë¡œ ê³ ì„±ëŠ¥ ì¶”ë¡ 
lm_eval --model vllm \
        --model_args pretrained=meta-llama/Llama-2-7b-hf,tensor_parallel_size=2 \
        --tasks gsm8k,math \
        --batch_size 16
```

### OpenAI API ëª¨ë¸ í‰ê°€

```bash
# GPT-4 í‰ê°€
lm_eval --model openai \
        --model_args model=gpt-4-turbo \
        --tasks mmlu,hellaswag \
        --batch_size 5

# GPT-3.5 with custom settings
lm_eval --model openai \
        --model_args model=gpt-3.5-turbo,max_tokens=512 \
        --tasks arc_challenge \
        --num_fewshot 5
```

### Anthropic Claude í‰ê°€

```bash
# Claude-3 í‰ê°€
lm_eval --model anthropic \
        --model_args model=claude-3-opus-20240229 \
        --tasks mmlu,arc_challenge \
        --batch_size 3
```



## ì‚¬ë‚´ vLLM ì„œë²„ì™€ LM Studio í‰ê°€ ì™„ë²½ ê°€ì´ë“œ

ì‚¬ë‚´ vLLM ì„œë²„ì™€ LM Studioì—ì„œ ëª¨ë¸ì„ í˜¸ì¶œâ€§í‰ê°€í•  ë•Œ ê¼­ ì•Œì•„ì•¼ í•  í•µì‹¬ ì ˆì°¨ë¥¼ ë‹¨ê³„ë³„ë¡œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. ë‘ í™˜ê²½ ëª¨ë‘ **OpenAI API í˜¸í™˜ ëª¨ë“œ**ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, LM-Evaluation-Harnessë‚˜ OpenAI í´ë¼ì´ì–¸íŠ¸ ì½”ë“œë¥¼ ê±°ì˜ ê·¸ëŒ€ë¡œ ì¬í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### í•µì‹¬ ìš”ì•½

- vLLMì€ `python -m vllm.entrypoints.openai.api_server` ëª…ë ¹ìœ¼ë¡œ **OpenAI Compatible Server**ë¥¼ ë„ìš¸ ìˆ˜ ìˆìœ¼ë©°, ê¸°ë³¸ì ìœ¼ë¡œ `http://<host>:8000/v1` ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- LM-Evaluation-HarnessëŠ” `--model openai-completions`(ë˜ëŠ” `openai-chat-completions`) ë“œë¼ì´ë²„ë¥¼ í†µí•´ **base_url**Â·**api_key**ë¥¼ ì™¸ë¶€ ì„œë²„ë¡œ ì§€ì •í•´ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- LM StudioëŠ” UIì˜ **Developer â–¸ Start Server** ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ë¡œì»¬ì— `http://localhost:1234/v1` ì„œë²„ê°€ ì—´ë¦¬ê³ , `/v1/chat/completions` ë“± ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì œê³µí•©ë‹ˆë‹¤.
- API í‚¤ ì¸ì¦ì´ ì—†ëŠ” í™˜ê²½ì´ë¼ë©´ ì„ì˜ì˜ "dummy" ë¬¸ìì—´ì„ `api_key`ì— ë„£ì–´ì£¼ë©´ ë©ë‹ˆë‹¤. í‰ê°€ ì½”ë“œ ë‚´ë¶€ì—ì„œ í‚¤ ìœ ë¬´ë§Œ í™•ì¸í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

### ì‚¬ë‚´ vLLM ì„œë²„ í‰ê°€ ì ˆì°¨

#### 1. ì„œë²„ ì‹¤í–‰

```bash
python -m vllm.entrypoints.openai.api_server \
       --model <ëª¨ë¸ì´ë¦„> \
       --host 0.0.0.0 \
       --port 8000
```

ìœ„ ëª…ë ¹ì€ GPUê°€ ì—°ê²°ëœ ë¨¸ì‹ ì—ì„œ OpenAI APIì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” HTTP ì„œë²„ë¥¼ ë„ì›ë‹ˆë‹¤.

#### 2. LM-Evaluation-Harness ì„¤ì¹˜

```bash
pip install lm-eval
```

âš ï¸ ê¶Œì¥ ë²„ì „ì€ Harness `v0.4.0` ì´ìƒì…ë‹ˆë‹¤. ìµœì‹  ë¦´ë¦¬ìŠ¤ì—ì„œ ë‹¤ìˆ˜ì˜ íƒœìŠ¤í¬ê°€ ì¶”ê°€Â·ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.

#### 3. í‰ê°€ ì‹¤í–‰ ì˜ˆì‹œ

```bash
lm_eval --model openai-chat-completions \
        --model_args model=<ëª¨ë¸ì´ë¦„>,base_url=http://<ì„œë²„IP>:8000/v1,api_key=dummy \
        --tasks hellaswag,arc_easy,arc_challenge \
        --batch_size 4 \
        --output_path ./results/


```

**ì£¼ìš” ì˜µì…˜ ì„¤ëª…:**
- `openai-completions` â†’ `text-completion` ë°©ì‹, `openai-chat-completions` â†’ ì±— ëª¨ë¸ìš© ë“œë¼ì´ë²„
- `base_url`ì€ **ê¼­** `/v1`ê¹Œì§€ í¬í•¨í•´ì•¼ ì—”ë“œí¬ì¸íŠ¸ í•´ì„ì´ ì •í™•í•©ë‹ˆë‹¤
- ë„¤íŠ¸ì›Œí¬ ì§€ì—°Â·GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ `--batch_size`ë¥¼ ì¡°ì •í•˜ì„¸ìš”. ë„ˆë¬´ í¬ê²Œ ì¡ìœ¼ë©´ `context length` ì´ˆê³¼ ì˜¤ë¥˜ê°€ ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤

#### 4. ê²°ê³¼ ì½ê¸°

`--output_path`ë¥¼ ì£¼ë©´ JSON ë¦¬í¬íŠ¸ê°€ ì €ì¥ë©ë‹ˆë‹¤. íƒœìŠ¤í¬ë³„ ì •í™•ë„Â·F1Â·í˜ë„í‹° ë“± í•µì‹¬ ì§€í‘œë¥¼ í™•ì¸í•˜ê³  ì‚¬ë‚´ MLOps ëŒ€ì‹œë³´ë“œë¡œ ë„˜ê²¨ì„œ íŠ¸ë Œë“œë¥¼ ì¶”ì í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.

### LM Studio ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì ˆì°¨

#### 1. ë¡œì»¬ ì„œë²„ ì¼œê¸°

1. LM Studio ì‹¤í–‰ â†’ **Developer** íƒ­ â†’ **Start Server** í´ë¦­
2. ê¸°ë³¸ í¬íŠ¸ëŠ” `1234`, ë² ì´ìŠ¤ URLì€ `http://localhost:1234/v1`ì…ë‹ˆë‹¤

#### 2. ë¹ ë¥¸ ë™ì‘ í™•ì¸

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
     -H "Content-Type: application/json" \
     -d '{
           "model":"<ë¡œë”©í•œ-ëª¨ë¸ëª…>",
           "messages":[{"role":"user","content":"Hello"}]
         }'
```

ì •ìƒ ì‘ë‹µì´ ì˜¤ë©´ ì„œë²„-ëª¨ë¸ ì—°ê²°ì´ ì„±ê³µí•œ ê²ƒì…ë‹ˆë‹¤.

#### 3. Python / OpenAI í´ë¼ì´ì–¸íŠ¸ ì˜ˆì‹œ

```python
from openai import OpenAI

client = OpenAI(
    api_key="lm-studio",               # ì„ì˜ ë¬¸ìì—´
    base_url="http://localhost:1234/v1"
)

resp = client.chat.completions.create(
    model="<ëª¨ë¸ëª…>",
    messages=[{"role":"user","content":"Introduce yourself"}]
)

print(resp.choices[0].message.content)
```

OpenAI ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì“°ë˜ `base_url`ë§Œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤.

#### 4. LM-Evaluation-Harnessë¡œ ë²¤ì¹˜ë§ˆí‚¹

```bash
lm_eval --model openai-chat-completions \
        --model_args model=<ëª¨ë¸ëª…>,base_url=http://localhost:1234/v1,api_key=dummy \
        --tasks kobest_hellaswag,kobest_copa \
        --batch_size 2

lm_eval --model openai-chat-completions \
        --model_args model=deepseek-ai/deepseek-ai_deepseek-r1-0528-qwen3-8b,base_url=http://localhost:1234/v1,api_key=dummy \
        --tasks kobest_hellaswag \
        --batch_size 1
```

LM Studioë„ OpenAI ê·œê²©ì„ ë”°ë¥´ë¯€ë¡œ Harness ì˜µì…˜ì´ vLLMê³¼ ë™ì¼í•©ë‹ˆë‹¤. ë‹¨, **context window**(ex. 8KÂ·32KÂ·128K) í•œë„ë¥¼ ëª¨ë¸/ë³€í™˜ê¸°ì— ë§ì¶°ì•¼ ì˜¤ë¥˜ê°€ ì•ˆ ë‚©ë‹ˆë‹¤.

#### âš ï¸ LM Studioì˜ ì¤‘ìš”í•œ ì œí•œì‚¬í•­

í˜„ì¬ LM Studioì˜ OpenAI í˜¸í™˜ APIë¥¼ í†µí•´ multiple-choice ìœ í˜•ì˜ íƒœìŠ¤í¬(ì˜ˆ: hellaswag)ë¥¼ í‰ê°€í•˜ë ¤ê³  í•  ë•Œ, **loglikelihood ê³„ì‚°ì„ ì§€ì›í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—** ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ì´ëŠ” LM-Evaluation-Harnessì—ì„œ multiple-choice íƒœìŠ¤í¬ë¥¼ í‰ê°€í•  ë•Œ loglikelihood ê³„ì‚°ì´ í•„ìˆ˜ì ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

**âŒ LM Studioì˜ ì œí•œ ì‚¬í•­:**

- **Chat Completions APIì˜ logprobs ë¯¸ì§€ì›**: í˜„ì¬ LM Studioì˜ Chat Completions APIëŠ” logprobsë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, loglikelihood ê³„ì‚°ì´ í•„ìš”í•œ multiple-choice íƒœìŠ¤í¬ë¥¼ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
- **Completions APIì˜ ì œí•œ**: LM StudioëŠ” OpenAIì˜ Completions APIë¥¼ ì§€ì›í•˜ì§€ë§Œ, ì´ ë˜í•œ logprobsë¥¼ ì§€ì›í•˜ì§€ ì•Šê±°ë‚˜ ì œí•œì ìœ¼ë¡œ ì§€ì›í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, loglikelihood ê³„ì‚°ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì–»ê¸° ì–´ë µìŠµë‹ˆë‹¤.

**âœ… ê¶Œì¥ ëŒ€ì•ˆ:**

1. **vLLM ì„œë²„ ì‚¬ìš©**: vLLMì€ OpenAI í˜¸í™˜ APIë¥¼ ì œê³µí•˜ë©°, logprobsë¥¼ ì§€ì›í•˜ë¯€ë¡œ multiple-choice íƒœìŠ¤í¬ í‰ê°€ì— ì í•©í•©ë‹ˆë‹¤.

   ```bash
   lm_eval --model openai-completions \
           --model_args model=your-model-name,base_url=http://your-vllm-server:8000/v1,api_key=dummy \
           --tasks hellaswag \
           --batch_size 1
   ```

2. **ì§€ì›ë˜ëŠ” íƒœìŠ¤í¬ë¡œ ë³€ê²½**: loglikelihood ê³„ì‚°ì´ í•„ìš”í•˜ì§€ ì•Šì€ íƒœìŠ¤í¬ë¥¼ ì„ íƒí•˜ì—¬ í‰ê°€ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

3. **LM Studioì˜ REST API í™œìš©**: LM Studioì˜ REST APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ logprobsë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì»¤ìŠ¤í…€ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì´ ë°©ë²•ì€ ì¶”ê°€ì ì¸ ê°œë°œ ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.

**ğŸ” ê²°ë¡ :**

í˜„ì¬ LM Studioì˜ OpenAI í˜¸í™˜ APIëŠ” multiple-choice íƒœìŠ¤í¬ í‰ê°€ì— í•„ìš”í•œ loglikelihood ê³„ì‚°ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, í•´ë‹¹ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ë ¤ë©´ vLLMê³¼ ê°™ì€ ëŒ€ì•ˆì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤. ë˜ëŠ” loglikelihood ê³„ì‚°ì´ í•„ìš”í•˜ì§€ ì•Šì€ íƒœìŠ¤í¬ë¥¼ ì„ íƒí•˜ì—¬ í‰ê°€ë¥¼ ì§„í–‰í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

#### 5. ì£¼ì˜ì‚¬í•­

- LM Studio íŒŒì¼ ìºì‹œì— ëª¨ë¸ì´ ì™„ì „íˆ ë¡œë“œë˜ê¸° ì „ì—ëŠ” ì²« ìš”ì²­ì´ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì¼ë¶€ êµ¬í˜• GGML í¬ë§· ëª¨ë¸ì€ Chat APIë¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì„¤ëª…ì˜ "chat compatible" í‘œì‹œë¥¼ í™•ì¸í•˜ì„¸ìš”
- LM Studio UIì—ì„œ **Enable network access** ì˜µì…˜ì„ ì¼œë©´ ê°™ì€ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­ì˜ ë‹¤ë¥¸ PCì—ì„œë„ `http://<PC-IP>:1234/v1`ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

### ê³µí†µ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

| í•­ëª© | íŒ |
|:-----|:---|
| **API í‚¤ ê´€ë¦¬** | ì¸ì¦ì´ í•„ìš” ì—†ëŠ” ë‚´ë¶€ í™˜ê²½ì´ë¼ë„ `api_key=dummy` íŒ¨í„´ì„ í†µì¼í•´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¬ì‚¬ìš©í•˜ì„¸ìš” |
| **í™˜ê²½ ë³€ìˆ˜** | `OPENAI_API_BASE`, `OPENAI_API_KEY` í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¸íŒ…í•˜ë©´ ì—¬ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤ |
| **ëª¨ë¸ ëª…ì¹­** | HarnessëŠ” `engine=` ë˜ëŠ” `model=` íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ì´ë¦„ì„ ë°›ìœ¼ë‹ˆ, vLLMÂ·LM Studio ëª¨ë‘ ì‹¤ì œ ë¡œë“œí•œ ì´ë¦„ê³¼ ì¼ì¹˜ì‹œí‚¤ì„¸ìš” |
| **ë°°ì¹˜ ê´€ë¦¬** | `--batch_size`ëŠ” API í˜¸ì¶œ ë°°ì¹˜ ìˆ˜ì´ì§€ GPU ë§ˆì´í¬ë¡œë°°ì¹˜ì™€ ë‹¤ë¦…ë‹ˆë‹¤. ì ì • ê°’(1~8)ë¶€í„° ì ì§„ì ìœ¼ë¡œ ëŠ˜ë¦¬ì„¸ìš” |
| **ìŠ¤íŠ¸ë¦¬ë°** | Harness ìì²´ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì´ ì—†ì§€ë§Œ, OpenAI í´ë¼ì´ì–¸íŠ¸ë¡œ ì§ì ‘ í…ŒìŠ¤íŠ¸í•  ë•ŒëŠ” `stream=True`ë¡œ ì‹¤ì‹œê°„ í† í°ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ |
| **íƒœìŠ¤í¬ ì„ íƒ** | í•œêµ­ì–´ ëª¨ë¸ì´ë©´ `kobest_*`, ë‹¤êµ­ì–´ ëª¨ë¸ì´ë©´ `mmlu_zh`, `tydiqa` ë“± ì§€ì› íƒœìŠ¤í¬ë¥¼ í™•ì¸í•˜ì„¸ìš”. Harness v0.4.0ë¶€í„° ë¦¬ë”ë³´ë“œ íƒœìŠ¤í¬ê°€ ëŒ€í­ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤ |

### ì‹¤ë¬´ í™œìš© íŒ

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼ **vLLM**ê³¼ **LM Studio** ëª¨ë‘ì—ì„œ ì†ì‰½ê²Œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ëŒë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‚´ë¶€ ì‹¤í—˜ â†’ ê²°ê³¼ JSON ì €ì¥ â†’ ì‚¬ë‚´ ëŒ€ì‹œë³´ë“œ ì‹œê°í™”ê¹Œì§€ ì¼ê´€ëœ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•´ë‘ë©´, ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œë‚˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ íš¨ê³¼ë¥¼ ë¹ ë¥´ê²Œ íŒŒì•…í•  ìˆ˜ ìˆìœ¼ë‹ˆ ì ê·¹ í™œìš©í•´ë³´ì„¸ìš”!

## ì£¼ìš” ë²¤ì¹˜ë§ˆí¬ íƒœìŠ¤í¬

### ì¼ë°˜ ì§€ëŠ¥ í‰ê°€

```bash
# MMLU (Massive Multitask Language Understanding)
lm_eval --model hf \
        --model_args pretrained=microsoft/DialoGPT-medium \
        --tasks mmlu \
        --num_fewshot 5

# HellaSwag (ìƒì‹ ì¶”ë¡ )
lm_eval --model hf \
        --model_args pretrained=microsoft/DialoGPT-medium \
        --tasks hellaswag \
        --num_fewshot 10
```

### ìˆ˜í•™ ë° ë…¼ë¦¬ ì¶”ë¡ 

```bash
# GSM8K (ì´ˆë“± ìˆ˜í•™ ë¬¸ì œ)
lm_eval --model hf \
        --model_args pretrained=microsoft/DialoGPT-medium \
        --tasks gsm8k \
        --num_fewshot 5

# ARC (AI2 Reasoning Challenge)
lm_eval --model hf \
        --model_args pretrained=microsoft/DialoGPT-medium \
        --tasks arc_easy,arc_challenge \
        --num_fewshot 25
```

### ì½”ë”© ëŠ¥ë ¥ í‰ê°€

```bash
# HumanEval (ì½”ë“œ ìƒì„±)
lm_eval --model hf \
        --model_args pretrained=microsoft/DialoGPT-medium \
        --tasks humaneval \
        --batch_size 4

# MBPP (Python í”„ë¡œê·¸ë˜ë°)
lm_eval --model hf \
        --model_args pretrained=microsoft/DialoGPT-medium \
        --tasks mbpp \
        --batch_size 4
```

## ê³ ê¸‰ í™œìš©ë²•

### ë°°ì¹˜ ì²˜ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”

```bash
# ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬
lm_eval --model vllm \
        --model_args pretrained=meta-llama/Llama-2-13b-hf,tensor_parallel_size=4 \
        --tasks mmlu,hellaswag,arc_easy,arc_challenge \
        --batch_size 32 \
        --output_path results/llama2_13b_comprehensive.json
```

### ìƒ˜í”Œë§ ë° ì œí•œ ì„¤ì •

```bash
# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìƒ˜í”Œ ì œí•œ
lm_eval --model hf \
        --model_args pretrained=gpt2 \
        --tasks mmlu \
        --limit 100 \
        --output_path results/gpt2_quick_test.json

# íŠ¹ì • ì„œë¸ŒíƒœìŠ¤í¬ë§Œ ì‹¤í–‰
lm_eval --model hf \
        --model_args pretrained=microsoft/DialoGPT-medium \
        --tasks mmlu_anatomy,mmlu_astronomy \
        --num_fewshot 5
```

### ê²°ê³¼ ë¡œê¹… ë° ë¶„ì„

```bash
# ìƒì„¸ ë¡œê·¸ì™€ í•¨ê»˜ í‰ê°€
lm_eval --model hf \
        --model_args pretrained=microsoft/DialoGPT-medium \
        --tasks hellaswag \
        --log_samples \
        --output_path results/detailed_results \
        --batch_size 8
```

## ì‹œê°í™” ë° ë¶„ì„

### Weights & Biases ì—°ë™

```bash
# W&B ë¡œê¹… í™œì„±í™”
lm_eval --model hf \
        --model_args pretrained=microsoft/DialoGPT-medium \
        --tasks mmlu,hellaswag \
        --wandb_args project=llm-evaluation,name=dialoGPT-experiment \
        --log_samples \
        --output_path results/wandb_experiment
```

### Zeno ì‹œê°í™”

```bash
# Zenoìš© ê²°ê³¼ ìƒì„±
lm_eval --model hf \
        --model_args pretrained=EleutherAI/gpt-j-6B \
        --tasks hellaswag \
        --log_samples \
        --output_path output/gpt-j-6B

# Zeno ì—…ë¡œë“œ
python scripts/zeno_visualize.py \
        --data_path output \
        --project_name "GPT-J Evaluation"
```

## ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ êµ¬í˜„

### ìƒˆë¡œìš´ íƒœìŠ¤í¬ ì¶”ê°€

```python
# custom_task.py
from lm_eval.api.task import Task
from lm_eval.api.metrics import mean

class CustomTask(Task):
    VERSION = 0
    DATASET_PATH = "path/to/your/dataset"
    DATASET_NAME = "custom_dataset"
    
    def has_training_docs(self):
        return True
    
    def has_validation_docs(self):
        return True
    
    def training_docs(self):
        return self.dataset["train"]
    
    def validation_docs(self):
        return self.dataset["validation"]
    
    def doc_to_text(self, doc):
        return f"Question: {doc['question']}\nAnswer:"
    
    def doc_to_target(self, doc):
        return doc['answer']
    
    def construct_requests(self, doc, ctx):
        return rf.greedy_until(ctx, ["\n"])
    
    def process_results(self, doc, results):
        return {"acc": results[0] == doc['answer']}
    
    def aggregation(self):
        return {"acc": mean}
```

### YAML ê¸°ë°˜ íƒœìŠ¤í¬ ì •ì˜

```yaml
# custom_task.yaml
task: custom_math_task
dataset_path: math_problems.json
training_split: train
validation_split: test
output_type: generate_until
generation_kwargs:
  until: ["\n"]
  max_gen_toks: 256
metric_list:
  - metric: exact_match
    aggregation: mean
    higher_is_better: true
metadata:
  version: 1.0
  description: "Custom math problem solving task"
```

## ì„±ëŠ¥ ìµœì í™” íŒ

### ë©”ëª¨ë¦¬ ìµœì í™”

```bash
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„¤ì •
lm_eval --model hf \
        --model_args pretrained=microsoft/DialoGPT-medium,load_in_8bit=True,device_map=auto \
        --tasks mmlu \
        --batch_size 4 \
        --max_batch_size 8
```

### ë³‘ë ¬ ì²˜ë¦¬

```bash
# ë©€í‹° GPU í™œìš©
lm_eval --model vllm \
        --model_args pretrained=meta-llama/Llama-2-70b-hf,tensor_parallel_size=8 \
        --tasks mmlu,hellaswag,arc_easy,arc_challenge \
        --batch_size 64
```

### ìºì‹± í™œìš©

```bash
# ê²°ê³¼ ìºì‹±ìœ¼ë¡œ ì¬ì‹¤í–‰ ì‹œê°„ ë‹¨ì¶•
lm_eval --model hf \
        --model_args pretrained=microsoft/DialoGPT-medium \
        --tasks mmlu \
        --cache_requests \
        --output_path results/cached_results.json
```

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜ì™€ í•´ê²°ì±…

| ì˜¤ë¥˜ | ì›ì¸ | í•´ê²°ì±… |
|:-----|:-----|:-------|
| `CUDA out of memory` | GPU ë©”ëª¨ë¦¬ ë¶€ì¡± | `--batch_size` ê°ì†Œ, `load_in_8bit=True` ì‚¬ìš© |
| `Task not found` | íƒœìŠ¤í¬ëª… ì˜¤íƒ€ | `lm_eval --tasks list` ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ íƒœìŠ¤í¬ í™•ì¸ |
| `API rate limit` | API í˜¸ì¶œ í•œë„ ì´ˆê³¼ | `--batch_size` ê°ì†Œ, ìš”ì²­ ê°„ê²© ì¡°ì • |
| `Model loading failed` | ëª¨ë¸ ê²½ë¡œ/ê¶Œí•œ ì˜¤ë¥˜ | HF_TOKEN ì„¤ì •, ëª¨ë¸ëª… í™•ì¸ |

### ë””ë²„ê¹… íŒ

```bash
# ìƒì„¸ ë¡œê·¸ ì¶œë ¥
lm_eval --model hf \
        --model_args pretrained=gpt2 \
        --tasks hellaswag \
        --limit 10 \
        --log_samples \
        --verbosity DEBUG

# ë‹¨ì¼ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
lm_eval --model hf \
        --model_args pretrained=gpt2 \
        --tasks hellaswag \
        --limit 1 \
        --log_samples
```

## ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€

### ëª¨ë¸ ë¹„êµ í‰ê°€

```bash
#!/bin/bash
# ì—¬ëŸ¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸

models=(
    "gpt2"
    "microsoft/DialoGPT-medium"
    "EleutherAI/gpt-neo-1.3B"
)

tasks="mmlu,hellaswag,arc_easy"

for model in "${models[@]}"; do
    echo "Evaluating $model..."
    lm_eval --model hf \
            --model_args pretrained="$model" \
            --tasks "$tasks" \
            --batch_size 8 \
            --output_path "results/$(echo $model | tr '/' '_').json" \
            --log_samples
done
```

### CI/CD íŒŒì´í”„ë¼ì¸ í†µí•©

```yaml
# .github/workflows/model-evaluation.yml
name: Model Evaluation

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        model: ["gpt2", "microsoft/DialoGPT-small"]
        task: ["hellaswag", "arc_easy"]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install lm-eval[api]
    
    - name: Run evaluation
      run: |
        lm_eval --model hf \
                --model_args pretrained=${{ matrix.model }} \
                --tasks ${{ matrix.task }} \
                --limit 100 \
                --output_path results/${{ matrix.model }}_${{ matrix.task }}.json
    
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: evaluation-results
        path: results/
```

## ê²°ê³¼ ë¶„ì„ ë° í•´ì„

### ê²°ê³¼ íŒŒì¼ êµ¬ì¡°

```json
{
  "results": {
    "hellaswag": {
      "acc": 0.5234,
      "acc_stderr": 0.0123,
      "acc_norm": 0.5456,
      "acc_norm_stderr": 0.0134
    }
  },
  "configs": {
    "model": "hf",
    "model_args": "pretrained=gpt2",
    "batch_size": 8,
    "num_fewshot": 10
  },
  "versions": {
    "hellaswag": 1
  }
}
```

### Pythonìœ¼ë¡œ ê²°ê³¼ ë¶„ì„

```python
import json
import pandas as pd
import matplotlib.pyplot as plt

def analyze_results(result_files):
    """ì—¬ëŸ¬ ëª¨ë¸ ê²°ê³¼ ë¹„êµ ë¶„ì„"""
    results = {}
    
    for file_path in result_files:
        with open(file_path, 'r') as f:
            data = json.load(f)
            model_name = data['configs']['model_args'].split('=')[1]
            results[model_name] = data['results']
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(results).T
    
    # ì‹œê°í™”
    df.plot(kind='bar', figsize=(12, 6))
    plt.title('Model Performance Comparison')
    plt.ylabel('Accuracy')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    
    return df

# ì‚¬ìš© ì˜ˆì‹œ
result_files = [
    'results/gpt2.json',
    'results/microsoft_DialoGPT-medium.json',
    'results/EleutherAI_gpt-neo-1.3B.json'
]

comparison_df = analyze_results(result_files)
print(comparison_df)
```

## ëª¨ë²” ì‚¬ë¡€

### í‰ê°€ ì„¤ê³„ ì›ì¹™

1. **ì¼ê´€ì„±**: ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ ëª¨ë“  ëª¨ë¸ í‰ê°€
2. **ì¬í˜„ì„±**: ì‹œë“œ ê³ ì • ë° í™˜ê²½ ë¬¸ì„œí™”
3. **íˆ¬ëª…ì„±**: í‰ê°€ ê³¼ì •ê³¼ ê²°ê³¼ ê³µê°œ
4. **ë‹¤ì–‘ì„±**: ì—¬ëŸ¬ íƒœìŠ¤í¬ë¡œ ì¢…í•©ì  í‰ê°€

### ê¶Œì¥ ì„¤ì •

```bash
# í‘œì¤€ í‰ê°€ ì„¤ì •
lm_eval --model hf \
        --model_args pretrained=your-model,trust_remote_code=True \
        --tasks mmlu,hellaswag,arc_easy,arc_challenge,gsm8k \
        --num_fewshot 5 \
        --batch_size 8 \
        --output_path results/comprehensive_eval.json \
        --log_samples \
        --wandb_args project=model-evaluation
```

## ê²°ë¡ 

LM-Evaluation-HarnessëŠ” ì–¸ì–´ ëª¨ë¸ í‰ê°€ì˜ í‘œì¤€ ë„êµ¬ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤. ì´ ê°€ì´ë“œë¥¼ í†µí•´ ë‹¤ìŒì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### í•µì‹¬ ì„±ê³¼
- **í‘œì¤€í™”ëœ í‰ê°€**: ì¼ê´€ë˜ê³  ì¬í˜„ ê°€ëŠ¥í•œ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
- **ë‹¤ì–‘í•œ ë°±ì—”ë“œ ì§€ì›**: HuggingFaceë¶€í„° API ëª¨ë¸ê¹Œì§€ í†µí•© í‰ê°€
- **í™•ì¥ì„±**: ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ì™€ ë©”íŠ¸ë¦­ ì‰½ê²Œ ì¶”ê°€
- **ì‹œê°í™”**: W&B, Zenoë¥¼ í†µí•œ ì§ê´€ì ì¸ ê²°ê³¼ ë¶„ì„

### í™œìš© ë¶„ì•¼
- **ëª¨ë¸ ê°œë°œ**: ìƒˆë¡œìš´ ëª¨ë¸ì˜ ì„±ëŠ¥ ê²€ì¦
- **ëª¨ë¸ ì„ íƒ**: ìš©ë„ì— ë§ëŠ” ìµœì  ëª¨ë¸ ì„ íƒ
- **ì—°êµ¬**: í•™ìˆ  ì—°êµ¬ì˜ ì‹¤í—˜ ì¬í˜„ì„± í™•ë³´
- **í”„ë¡œë•ì…˜**: ë°°í¬ ì „ ëª¨ë¸ í’ˆì§ˆ ë³´ì¦

LM-Evaluation-Harnessë¥¼ MLOps íŒŒì´í”„ë¼ì¸ì— í†µí•©í•˜ë©´ ì²´ê³„ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì–¸ì–´ ëª¨ë¸ í‰ê°€ ì²´ê³„ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ íƒœìŠ¤í¬ë¥¼ ì§€ì›í•˜ëŠ” í™•ì¥ì„± ë•ë¶„ì— ë¹ ë¥´ê²Œ ë³€í™”í•˜ëŠ” LLM ìƒíƒœê³„ì—ì„œ ê²½ìŸë ¥ì„ ìœ ì§€í•˜ëŠ” ë° í•„ìˆ˜ì ì¸ ë„êµ¬ì…ë‹ˆë‹¤.

---

**ì°¸ê³  ìë£Œ:**
- [LM-Evaluation-Harness GitHub](https://github.com/EleutherAI/lm-evaluation-harness)
- [EleutherAI ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://www.eleuther.ai)
- [Weights & Biases ë¬¸ì„œ](https://docs.wandb.ai/)
- [Zeno ì‹œê°í™” í”Œë«í¼](https://hub.zenoml.com/)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers/) 