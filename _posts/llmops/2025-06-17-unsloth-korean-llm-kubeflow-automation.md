---
title: "Unsloth í•œêµ­ì–´ LLM í•™ìŠµ ìë™í™” - 3í¸: Kubeflow ë° MLOps í”„ë ˆì„ì›Œí¬ í™œìš©"
excerpt: "Kubeflow, MLflow, DVCë¥¼ í™œìš©í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ í•œêµ­ì–´ LLM í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"
date: 2025-06-17
categories:
  - tutorials
tags:
  - Kubeflow
  - MLflow
  - DVC
  - MLOps
  - Korean LLM
  - Unsloth
  - Pipeline Automation
  - Model Registry
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
---

## ê°œìš”

ë³¸ ê°€ì´ë“œëŠ” [Unsloth í•œêµ­ì–´ LLM í•™ìŠµ ì‹œë¦¬ì¦ˆ]({% post_url 2025-06-17-unsloth-korean-llm-training-guide %})ì˜ 3í¸ìœ¼ë¡œ, Kubeflow, MLflow, DVC ë“± ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ MLOps í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ í•œêµ­ì–´ LLM í•™ìŠµì„ ì™„ì „ ìë™í™”í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

**í•™ìŠµ ëª©í‘œ**:
- ğŸš€ **Kubeflow Pipelines**: ì‹œê°ì  ML ì›Œí¬í”Œë¡œìš° êµ¬ì¶•
- ğŸ“Š **MLflow**: ì‹¤í—˜ ì¶”ì  ë° ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê´€ë¦¬
- ğŸ”„ **DVC**: ë°ì´í„° ë²„ì „ ê´€ë¦¬ ë° ì¬í˜„ ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸
- âš¡ **í†µí•© MLOps**: ì—”ë“œíˆ¬ì—”ë“œ ìë™í™” ì‹œìŠ¤í…œ

---

## 1. Kubeflow ì„¤ì¹˜ ë° ì„¤ì •

### 1.1 Kubeflow ì„¤ì¹˜

```bash
# Kubeflow 1.8 ì„¤ì¹˜
export PIPELINE_VERSION=1.8.5
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref=$PIPELINE_VERSION"

# Kubeflow Dashboard ì ‘ê·¼
kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80
```

### 1.2 Kubeflow Notebooks ì„¤ì •

```yaml
# kubeflow-notebook.yaml
apiVersion: kubeflow.org/v1
kind: Notebook
metadata:
  name: korean-llm-notebook
  namespace: kubeflow-user-example-com
spec:
  template:
    spec:
      containers:
      - name: notebook
        image: jupyter/tensorflow-notebook:latest
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: 16Gi
            cpu: 4
          limits:
            nvidia.com/gpu: 1
            memory: 16Gi
            cpu: 4
        volumeMounts:
        - name: workspace-pvc
          mountPath: /home/jovyan/work
      volumes:
      - name: workspace-pvc
        persistentVolumeClaim:
          claimName: workspace-pvc
```

---

## 2. MLflow í†µí•©

### 2.1 MLflow ì„œë²„ ì„¤ì •

```yaml
# mlflow-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlflow-server
  namespace: mlops
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mlflow-server
  template:
    metadata:
      labels:
        app: mlflow-server
    spec:
      containers:
      - name: mlflow
        image: python:3.9-slim
        command:
        - /bin/bash
        - -c
        - |
          pip install mlflow==2.8.1 psycopg2-binary boto3
          mlflow server \
            --backend-store-uri postgresql://mlflow:password@postgres:5432/mlflow \
            --default-artifact-root s3://mlflow-artifacts/ \
            --host 0.0.0.0 \
            --port 5000
        ports:
        - containerPort: 5000
        env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: s3-credentials
              key: access-key
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: s3-credentials
              key: secret-key
        - name: MLFLOW_S3_ENDPOINT_URL
          value: "https://s3.amazonaws.com"
```

### 2.2 ì‹¤í—˜ ì¶”ì  ì½”ë“œ

```python
# mlflow_tracker.py
import mlflow
import mlflow.pytorch
from mlflow.tracking import MlflowClient
import torch
from datetime import datetime

class KoreanLLMTracker:
    def __init__(self, experiment_name="korean-llm-training"):
        self.client = MlflowClient()
        mlflow.set_tracking_uri("http://mlflow-server:5000")
        
        # ì‹¤í—˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        try:
            experiment = self.client.get_experiment_by_name(experiment_name)
            self.experiment_id = experiment.experiment_id
        except:
            self.experiment_id = self.client.create_experiment(experiment_name)
    
    def start_run(self, run_name, stage="CPT"):
        """ì‹¤í—˜ ì‹¤í–‰ ì‹œì‘"""
        self.run = mlflow.start_run(
            experiment_id=self.experiment_id,
            run_name=f"{run_name}-{stage}-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        )
        return self.run
    
    def log_parameters(self, params):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…"""
        mlflow.log_params(params)
    
    def log_metrics(self, metrics, step=None):
        """ë©”íŠ¸ë¦­ ë¡œê¹…"""
        mlflow.log_metrics(metrics, step=step)
    
    def log_model(self, model, model_name, signature=None):
        """ëª¨ë¸ ë¡œê¹…"""
        mlflow.pytorch.log_model(
            pytorch_model=model,
            artifact_path=model_name,
            signature=signature
        )
    
    def register_model(self, model_name, stage="Staging"):
        """ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡"""
        model_uri = f"runs:/{self.run.info.run_id}/{model_name}"
        mlflow.register_model(model_uri, model_name)
        
        # ëª¨ë¸ ìŠ¤í…Œì´ì§€ ì„¤ì •
        client = MlflowClient()
        client.transition_model_version_stage(
            name=model_name,
            version=1,
            stage=stage
        )
    
    def end_run(self):
        """ì‹¤í—˜ ì¢…ë£Œ"""
        mlflow.end_run()
```

---

## 3. DVC ë°ì´í„° ë²„ì „ ê´€ë¦¬

### 3.1 DVC ì„¤ì •

```bash
# DVC ì´ˆê¸°í™”
dvc init --no-scm
dvc remote add -d myremote s3://korean-llm-data

# ë°ì´í„° ì¶”ì 
dvc add data/korean_corpus.jsonl
dvc add data/instruction_dataset.jsonl

# íŒŒì´í”„ë¼ì¸ ì •ì˜
dvc stage add -n preprocess \
  -d data/raw_korean_text.txt \
  -o data/korean_corpus.jsonl \
  python scripts/preprocess.py

dvc stage add -n cpt_training \
  -d data/korean_corpus.jsonl \
  -d scripts/cpt_train.py \
  -o models/cpt_model \
  -M metrics/cpt_metrics.json \
  python scripts/cpt_train.py

dvc stage add -n sft_training \
  -d models/cpt_model \
  -d data/instruction_dataset.jsonl \
  -o models/sft_model \
  -M metrics/sft_metrics.json \
  python scripts/sft_train.py
```

### 3.2 DVC íŒŒì´í”„ë¼ì¸ ì •ì˜

```yaml
# dvc.yaml
stages:
  preprocess:
    cmd: python scripts/preprocess.py
    deps:
    - data/raw_korean_text.txt
    - scripts/preprocess.py
    outs:
    - data/korean_corpus.jsonl
    
  cpt_training:
    cmd: python scripts/cpt_train.py
    deps:
    - data/korean_corpus.jsonl
    - scripts/cpt_train.py
    outs:
    - models/cpt_model
    metrics:
    - metrics/cpt_metrics.json
    
  sft_training:
    cmd: python scripts/sft_train.py
    deps:
    - models/cpt_model
    - data/instruction_dataset.jsonl
    - scripts/sft_train.py
    outs:
    - models/sft_model
    metrics:
    - metrics/sft_metrics.json
    
  evaluation:
    cmd: python scripts/evaluate.py
    deps:
    - models/sft_model
    - scripts/evaluate.py
    metrics:
    - metrics/evaluation_metrics.json
```

---

## 4. Kubeflow Pipelines êµ¬ì„±

### 4.1 íŒŒì´í”„ë¼ì¸ ì»´í¬ë„ŒíŠ¸ ì •ì˜

```python
# kubeflow_components.py
from kfp import dsl, components
from kfp.components import InputPath, OutputPath
import kfp

def preprocess_data_op(
    raw_data_path: InputPath(str),
    processed_data_path: OutputPath(str)
):
    """ë°ì´í„° ì „ì²˜ë¦¬ ì»´í¬ë„ŒíŠ¸"""
    import json
    import re
    from pathlib import Path
    
    def clean_korean_text(text):
        # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ê·œí™”
        text = re.sub(r'\s+', ' ', text)
        text = text.replace('ã†', 'Â·')
        return text.strip()
    
    # ì›ë³¸ ë°ì´í„° ë¡œë“œ
    with open(raw_data_path, 'r', encoding='utf-8') as f:
        raw_text = f.read()
    
    # ì „ì²˜ë¦¬
    processed_text = clean_korean_text(raw_text)
    
    # ê²°ê³¼ ì €ì¥
    processed_data = {
        "text": processed_text,
        "length": len(processed_text),
        "language": "korean"
    }
    
    with open(processed_data_path, 'w', encoding='utf-8') as f:
        json.dump(processed_data, f, ensure_ascii=False, indent=2)

def cpt_training_op(
    data_path: InputPath(str),
    model_output_path: OutputPath(str),
    metrics_path: OutputPath(str),
    model_size: str = "7B",
    learning_rate: float = 1e-5,
    num_epochs: int = 2
):
    """CPT í•™ìŠµ ì»´í¬ë„ŒíŠ¸"""
    import torch
    from unsloth import FastLanguageModel
    from datasets import load_dataset
    from transformers import TrainingArguments
    from trl import SFTTrainer
    import json
    import mlflow
    
    # MLflow ì„¤ì •
    mlflow.set_tracking_uri("http://mlflow-server:5000")
    
    with mlflow.start_run():
        # íŒŒë¼ë¯¸í„° ë¡œê¹…
        mlflow.log_params({
            "model_size": model_size,
            "learning_rate": learning_rate,
            "num_epochs": num_epochs,
            "stage": "CPT"
        })
        
        # ëª¨ë¸ ë¡œë”©
        model_name = f"unsloth/Qwen2.5-{model_size}-bnb-4bit"
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_name,
            max_seq_length=4096,
            dtype=None,
            load_in_4bit=True,
        )
        
        # LoRA ì„¤ì •
        model = FastLanguageModel.get_peft_model(
            model,
            r=64,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            lora_alpha=16,
            lora_dropout=0.1,
            bias="none",
            use_gradient_checkpointing="unsloth",
        )
        
        # ë°ì´í„°ì…‹ ë¡œë”©
        dataset = load_dataset("json", data_files=data_path, split="train")
        
        # í•™ìŠµ ì„¤ì •
        training_args = TrainingArguments(
            output_dir=model_output_path,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=2,
            learning_rate=learning_rate,
            logging_steps=100,
            save_steps=1000,
            fp16=True,
        )
        
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ë° í•™ìŠµ
        trainer = SFTTrainer(
            model=model,
            tokenizer=tokenizer,
            args=training_args,
            train_dataset=dataset,
            dataset_text_field="text",
        )
        
        # í•™ìŠµ ì‹¤í–‰
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        trainer.save_model(model_output_path)
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        metrics = {
            "final_loss": trainer.state.log_history[-1]["train_loss"],
            "total_steps": trainer.state.global_step,
            "model_size": model_size
        }
        
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        # MLflowì— ë©”íŠ¸ë¦­ ë¡œê¹…
        mlflow.log_metrics(metrics)
        mlflow.pytorch.log_model(model, "cpt_model")

def sft_training_op(
    base_model_path: InputPath(str),
    instruction_data_path: InputPath(str),
    model_output_path: OutputPath(str),
    metrics_path: OutputPath(str),
    learning_rate: float = 2e-5,
    num_epochs: int = 3
):
    """SFT í•™ìŠµ ì»´í¬ë„ŒíŠ¸"""
    import torch
    from unsloth import FastLanguageModel
    from datasets import load_dataset
    from transformers import TrainingArguments
    from trl import SFTTrainer
    import json
    import mlflow
    
    mlflow.set_tracking_uri("http://mlflow-server:5000")
    
    with mlflow.start_run():
        # íŒŒë¼ë¯¸í„° ë¡œê¹…
        mlflow.log_params({
            "learning_rate": learning_rate,
            "num_epochs": num_epochs,
            "stage": "SFT"
        })
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=base_model_path,
            max_seq_length=2048,
            dtype=None,
            load_in_4bit=True,
        )
        
        # Instruction ë°ì´í„°ì…‹ ë¡œë”©
        dataset = load_dataset("json", data_files=instruction_data_path, split="train")
        
        # í•™ìŠµ ì„¤ì •
        training_args = TrainingArguments(
            output_dir=model_output_path,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=4,
            learning_rate=learning_rate,
            logging_steps=50,
            save_steps=500,
            fp16=True,
        )
        
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ë° í•™ìŠµ
        trainer = SFTTrainer(
            model=model,
            tokenizer=tokenizer,
            args=training_args,
            train_dataset=dataset,
            dataset_text_field="text",
        )
        
        trainer.train()
        trainer.save_model(model_output_path)
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        metrics = {
            "final_loss": trainer.state.log_history[-1]["train_loss"],
            "total_steps": trainer.state.global_step,
            "stage": "SFT"
        }
        
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        mlflow.log_metrics(metrics)
        mlflow.pytorch.log_model(model, "sft_model")

def evaluation_op(
    model_path: InputPath(str),
    evaluation_results_path: OutputPath(str),
    test_data_path: str = "/data/test_dataset.jsonl"
):
    """ëª¨ë¸ í‰ê°€ ì»´í¬ë„ŒíŠ¸"""
    import torch
    from unsloth import FastLanguageModel
    from datasets import load_dataset
    import json
    import mlflow
    
    mlflow.set_tracking_uri("http://mlflow-server:5000")
    
    with mlflow.start_run():
        # ëª¨ë¸ ë¡œë”©
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_path,
            max_seq_length=2048,
            dtype=None,
            load_in_4bit=True,
        )
        
        # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •
        FastLanguageModel.for_inference(model)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©
        test_dataset = load_dataset("json", data_files=test_data_path, split="train")
        
        # í‰ê°€ ì‹¤í–‰
        total_samples = len(test_dataset)
        correct_predictions = 0
        
        for sample in test_dataset[:100]:  # ìƒ˜í”Œ 100ê°œë¡œ ì œí•œ
            prompt = sample["prompt"]
            expected = sample["expected"]
            
            inputs = tokenizer(prompt, return_tensors="pt")
            outputs = model.generate(**inputs, max_new_tokens=256)
            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ê°„ë‹¨í•œ ì •í™•ë„ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë©”íŠ¸ë¦­ í•„ìš”)
            if expected.lower() in prediction.lower():
                correct_predictions += 1
        
        accuracy = correct_predictions / min(100, total_samples)
        
        # ê²°ê³¼ ì €ì¥
        results = {
            "accuracy": accuracy,
            "total_samples": min(100, total_samples),
            "correct_predictions": correct_predictions
        }
        
        with open(evaluation_results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        mlflow.log_metrics(results)

# ì»´í¬ë„ŒíŠ¸ ìƒì„±
preprocess_data_component = components.create_component_from_func(
    preprocess_data_op,
    base_image="python:3.9-slim",
    packages_to_install=["datasets", "transformers"]
)

cpt_training_component = components.create_component_from_func(
    cpt_training_op,
    base_image="nvidia/cuda:11.8-devel-ubuntu20.04",
    packages_to_install=[
        "torch", "transformers", "datasets", 
        "unsloth @ git+https://github.com/unslothai/unsloth.git",
        "mlflow"
    ]
)

sft_training_component = components.create_component_from_func(
    sft_training_op,
    base_image="nvidia/cuda:11.8-devel-ubuntu20.04",
    packages_to_install=[
        "torch", "transformers", "datasets",
        "unsloth @ git+https://github.com/unslothai/unsloth.git",
        "mlflow"
    ]
)

evaluation_component = components.create_component_from_func(
    evaluation_op,
    base_image="nvidia/cuda:11.8-devel-ubuntu20.04",
    packages_to_install=[
        "torch", "transformers", "datasets",
        "unsloth @ git+https://github.com/unslothai/unsloth.git",
        "mlflow"
    ]
)
```

### 4.2 íŒŒì´í”„ë¼ì¸ ì •ì˜

```python
# korean_llm_pipeline.py
from kfp import dsl
from kubeflow_components import *

@dsl.pipeline(
    name="Korean LLM Training Pipeline",
    description="Complete pipeline for training Korean-specialized LLM using Unsloth"
)
def korean_llm_training_pipeline(
    raw_data_path: str = "/data/raw_korean_text.txt",
    instruction_data_path: str = "/data/instruction_dataset.jsonl",
    model_size: str = "7B",
    cpt_learning_rate: float = 1e-5,
    sft_learning_rate: float = 2e-5,
    cpt_epochs: int = 2,
    sft_epochs: int = 3
):
    """í•œêµ­ì–´ LLM í•™ìŠµ íŒŒì´í”„ë¼ì¸"""
    
    # 1. ë°ì´í„° ì „ì²˜ë¦¬
    preprocess_task = preprocess_data_component(
        raw_data_path=raw_data_path
    )
    
    # 2. CPT í•™ìŠµ
    cpt_task = cpt_training_component(
        data_path=preprocess_task.outputs["processed_data_path"],
        model_size=model_size,
        learning_rate=cpt_learning_rate,
        num_epochs=cpt_epochs
    )
    
    # GPU ë¦¬ì†ŒìŠ¤ ìš”ì²­
    cpt_task.set_gpu_limit(4)
    cpt_task.set_memory_limit("200Gi")
    cpt_task.set_cpu_limit("32")
    
    # 3. SFT í•™ìŠµ
    sft_task = sft_training_component(
        base_model_path=cpt_task.outputs["model_output_path"],
        instruction_data_path=instruction_data_path,
        learning_rate=sft_learning_rate,
        num_epochs=sft_epochs
    )
    
    # GPU ë¦¬ì†ŒìŠ¤ ìš”ì²­
    sft_task.set_gpu_limit(2)
    sft_task.set_memory_limit("100Gi")
    sft_task.set_cpu_limit("16")
    
    # 4. ëª¨ë¸ í‰ê°€
    evaluation_task = evaluation_component(
        model_path=sft_task.outputs["model_output_path"]
    )
    
    evaluation_task.set_gpu_limit(1)
    evaluation_task.set_memory_limit("50Gi")
    evaluation_task.set_cpu_limit("8")
    
    # íŒŒì´í”„ë¼ì¸ ì„¤ì •
    preprocess_task.set_retry(3)
    cpt_task.set_retry(2)
    sft_task.set_retry(2)
    evaluation_task.set_retry(3)

# íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼
if __name__ == "__main__":
    from kfp.compiler import Compiler
    
    Compiler().compile(
        pipeline_func=korean_llm_training_pipeline,
        package_path="korean_llm_pipeline.yaml"
    )
```

---

## 5. í†µí•© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

### 5.1 Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì •

```json
{
  "dashboard": {
    "title": "Korean LLM Training - MLOps Dashboard",
    "panels": [
      {
        "title": "Pipeline Execution Status",
        "type": "stat",
        "targets": [
          {
            "expr": "kubeflow_pipeline_runs_total",
            "legendFormat": "Total Runs"
          }
        ]
      },
      {
        "title": "Training Loss Over Time",
        "type": "graph",
        "targets": [
          {
            "expr": "mlflow_metric_train_loss",
            "legendFormat": "{{stage}} Loss"
          }
        ]
      },
      {
        "title": "GPU Utilization",
        "type": "graph",
        "targets": [
          {
            "expr": "nvidia_gpu_utilization",
            "legendFormat": "GPU {{gpu}}"
          }
        ]
      },
      {
        "title": "Model Performance Metrics",
        "type": "table",
        "targets": [
          {
            "expr": "mlflow_metric_accuracy",
            "legendFormat": "Accuracy"
          }
        ]
      }
    ]
  }
}
```

### 5.2 ì•Œë¦¼ ì„¤ì •

```yaml
# alerting-rules.yaml
groups:
- name: korean-llm-training
  rules:
  - alert: TrainingJobFailed
    expr: kubeflow_pipeline_run_status{status="Failed"} > 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Korean LLM training job failed"
      description: "Pipeline {{ $labels.pipeline_name }} failed"
  
  - alert: LowGPUUtilization
    expr: nvidia_gpu_utilization < 50
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Low GPU utilization detected"
      description: "GPU utilization is below 50% for 10 minutes"
  
  - alert: HighTrainingLoss
    expr: increase(mlflow_metric_train_loss[1h]) > 0.1
    for: 30m
    labels:
      severity: warning
    annotations:
      summary: "Training loss is increasing"
      description: "Training loss increased by more than 0.1 in the last hour"
```

---

## 6. ìë™í™”ëœ ëª¨ë¸ ë°°í¬

### 6.1 ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—°ë™

```python
# model_deployment.py
from mlflow.tracking import MlflowClient
import kubernetes
from kubernetes import client, config
import yaml

class ModelDeploymentManager:
    def __init__(self):
        self.mlflow_client = MlflowClient("http://mlflow-server:5000")
        config.load_incluster_config()
        self.k8s_apps_v1 = client.AppsV1Api()
        self.k8s_core_v1 = client.CoreV1Api()
    
    def get_latest_model(self, model_name, stage="Production"):
        """ìµœì‹  í”„ë¡œë•ì…˜ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        latest_version = self.mlflow_client.get_latest_versions(
            model_name, 
            stages=[stage]
        )[0]
        return latest_version
    
    def create_inference_deployment(self, model_version, namespace="inference"):
        """ì¶”ë¡  ì„œë¹„ìŠ¤ ë°°í¬"""
        deployment_yaml = f"""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: korean-llm-inference
  namespace: {namespace}
spec:
  replicas: 2
  selector:
    matchLabels:
      app: korean-llm-inference
  template:
    metadata:
      labels:
        app: korean-llm-inference
    spec:
      containers:
      - name: inference-server
        image: your-registry/korean-llm-inference:latest
        env:
        - name: MODEL_URI
          value: "{model_version.source}"
        - name: MODEL_VERSION
          value: "{model_version.version}"
        ports:
        - containerPort: 8000
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: 16Gi
            cpu: 4
          limits:
            nvidia.com/gpu: 1
            memory: 16Gi
            cpu: 4
---
apiVersion: v1
kind: Service
metadata:
  name: korean-llm-service
  namespace: {namespace}
spec:
  selector:
    app: korean-llm-inference
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
"""
        
        # YAML íŒŒì‹± ë° ë°°í¬
        deployment_dict = yaml.safe_load(deployment_yaml)
        
        # Deployment ìƒì„±
        self.k8s_apps_v1.create_namespaced_deployment(
            namespace=namespace,
            body=deployment_dict
        )
    
    def auto_deploy_on_model_update(self, model_name):
        """ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ ìë™ ë°°í¬"""
        latest_model = self.get_latest_model(model_name)
        
        # í˜„ì¬ ë°°í¬ëœ ëª¨ë¸ ë²„ì „ í™•ì¸
        current_deployment = self.k8s_apps_v1.read_namespaced_deployment(
            name="korean-llm-inference",
            namespace="inference"
        )
        
        current_version = None
        for env in current_deployment.spec.template.spec.containers[0].env:
            if env.name == "MODEL_VERSION":
                current_version = env.value
                break
        
        # ìƒˆ ë²„ì „ì´ ìˆìœ¼ë©´ ë°°í¬ ì—…ë°ì´íŠ¸
        if current_version != latest_model.version:
            print(f"Updating deployment from version {current_version} to {latest_model.version}")
            self.update_deployment(latest_model)
    
    def update_deployment(self, model_version):
        """ë°°í¬ ì—…ë°ì´íŠ¸"""
        # ë°°í¬ ì—…ë°ì´íŠ¸ ë¡œì§
        patch = {
            "spec": {
                "template": {
                    "spec": {
                        "containers": [{
                            "name": "inference-server",
                            "env": [
                                {"name": "MODEL_URI", "value": model_version.source},
                                {"name": "MODEL_VERSION", "value": model_version.version}
                            ]
                        }]
                    }
                }
            }
        }
        
        self.k8s_apps_v1.patch_namespaced_deployment(
            name="korean-llm-inference",
            namespace="inference",
            body=patch
        )
```

### 6.2 A/B í…ŒìŠ¤íŠ¸ ì„¤ì •

```yaml
# ab-testing-setup.yaml
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: korean-llm-rollout
  namespace: inference
spec:
  replicas: 4
  strategy:
    canary:
      steps:
      - setWeight: 25
      - pause: {duration: 10m}
      - setWeight: 50
      - pause: {duration: 10m}
      - setWeight: 75
      - pause: {duration: 10m}
      analysis:
        templates:
        - templateName: success-rate
        args:
        - name: service-name
          value: korean-llm-service
  selector:
    matchLabels:
      app: korean-llm-inference
  template:
    metadata:
      labels:
        app: korean-llm-inference
    spec:
      containers:
      - name: inference-server
        image: your-registry/korean-llm-inference:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: 16Gi
---
apiVersion: argoproj.io/v1alpha1
kind: AnalysisTemplate
metadata:
  name: success-rate
spec:
  args:
  - name: service-name
  metrics:
  - name: success-rate
    interval: 2m
    count: 5
    successCondition: result[0] >= 0.95
    provider:
      prometheus:
        address: http://prometheus:9090
        query: |
          sum(rate(http_requests_total{service="{{args.service-name}}",code=~"2.."}[2m])) /
          sum(rate(http_requests_total{service="{{args.service-name}}"}[2m]))
```

---

## 7. ì‹¤í–‰ ë° ê´€ë¦¬

### 7.1 íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

```python
# run_pipeline.py
import kfp
from kfp import dsl

# Kubeflow Pipelines í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = kfp.Client(host="http://kubeflow-pipelines:8080")

# íŒŒì´í”„ë¼ì¸ ì—…ë¡œë“œ
pipeline_id = client.upload_pipeline(
    pipeline_package_path="korean_llm_pipeline.yaml",
    pipeline_name="Korean LLM Training Pipeline v1.0"
)

# ì‹¤í—˜ ìƒì„±
experiment = client.create_experiment(
    name="Korean LLM Training Experiment",
    description="Automated training of Korean-specialized LLM"
)

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
run = client.run_pipeline(
    experiment_id=experiment.id,
    job_name="korean-llm-training-run-1",
    pipeline_id=pipeline_id,
    params={
        "model_size": "7B",
        "cpt_learning_rate": 1e-5,
        "sft_learning_rate": 2e-5,
        "cpt_epochs": 2,
        "sft_epochs": 3
    }
)

print(f"Pipeline run started: {run.id}")
```

### 7.2 ìë™í™” ìŠ¤ì¼€ì¤„ë§

```python
# scheduler.py
from kfp import dsl
import schedule
import time

class PipelineScheduler:
    def __init__(self, client):
        self.client = client
        self.pipeline_id = None
        self.experiment_id = None
    
    def setup_pipeline(self):
        """íŒŒì´í”„ë¼ì¸ ë° ì‹¤í—˜ ì„¤ì •"""
        # íŒŒì´í”„ë¼ì¸ ì—…ë¡œë“œ
        self.pipeline_id = self.client.upload_pipeline(
            pipeline_package_path="korean_llm_pipeline.yaml",
            pipeline_name="Korean LLM Scheduled Training"
        )
        
        # ì‹¤í—˜ ìƒì„±
        experiment = self.client.create_experiment(
            name="Scheduled Korean LLM Training"
        )
        self.experiment_id = experiment.id
    
    def run_training_pipeline(self):
        """í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        run = self.client.run_pipeline(
            experiment_id=self.experiment_id,
            job_name=f"scheduled-run-{int(time.time())}",
            pipeline_id=self.pipeline_id,
            params={
                "model_size": "7B",
                "cpt_learning_rate": 1e-5,
                "sft_learning_rate": 2e-5
            }
        )
        print(f"Scheduled pipeline run started: {run.id}")
    
    def start_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        # ë§¤ì£¼ ì¼ìš”ì¼ ì˜¤ì „ 2ì‹œì— ì‹¤í–‰
        schedule.every().sunday.at("02:00").do(self.run_training_pipeline)
        
        # ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì¶”ê°€ë  ë•Œë§ˆë‹¤ ì‹¤í–‰ (ì˜ˆ: ë§¤ì¼ ì²´í¬)
        schedule.every().day.at("06:00").do(self.check_and_run_if_new_data)
        
        while True:
            schedule.run_pending()
            time.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤ ì²´í¬
    
    def check_and_run_if_new_data(self):
        """ìƒˆë¡œìš´ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ì‹¤í–‰"""
        # DVCë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë³€ê²½ í™•ì¸
        import subprocess
        result = subprocess.run(["dvc", "status"], capture_output=True, text=True)
        
        if "changed" in result.stdout:
            print("New data detected, starting training pipeline...")
            self.run_training_pipeline()
        else:
            print("No new data, skipping training...")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    client = kfp.Client(host="http://kubeflow-pipelines:8080")
    scheduler = PipelineScheduler(client)
    scheduler.setup_pipeline()
    scheduler.start_scheduler()
```

---

## ê²°ë¡ 

ë³¸ ê°€ì´ë“œë¥¼ í†µí•´ Kubeflow, MLflow, DVCë¥¼ í™œìš©í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ í•œêµ­ì–´ LLM í•™ìŠµ ìë™í™” ì‹œìŠ¤í…œì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

**ì£¼ìš” ì„±ê³¼**:
- ğŸš€ **ì™„ì „ ìë™í™”**: ë°ì´í„° â†’ í•™ìŠµ â†’ í‰ê°€ â†’ ë°°í¬ ì „ ê³¼ì •
- ğŸ“Š **ì‹¤í—˜ ê´€ë¦¬**: MLflowë¥¼ í†µí•œ ì²´ê³„ì  ì‹¤í—˜ ì¶”ì 
- ğŸ”„ **ë°ì´í„° ë²„ì „ ê´€ë¦¬**: DVCë¡œ ì¬í˜„ ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸
- âš¡ **í™•ì¥ì„±**: Kubeflowì˜ ë¶„ì‚° ì²˜ë¦¬ ë° ìŠ¤ì¼€ì¼ë§

**ì‹¤ë¬´ì  ê°€ì¹˜**:
- **ìš´ì˜ íš¨ìœ¨ì„±**: GUI ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬
- **í’ˆì§ˆ ë³´ì¦**: ìë™í™”ëœ í‰ê°€ ë° ë°°í¬ ê²€ì¦
- **ë¹„ìš© ìµœì í™”**: ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ìµœì í™”
- **ê±°ë²„ë„ŒìŠ¤**: ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¥¼ í†µí•œ ëª¨ë¸ ìƒëª…ì£¼ê¸° ê´€ë¦¬

ì´ ì‹œë¦¬ì¦ˆì˜ ë‹¤ë¥¸ ê¸€ ë³´ê¸°:
- [1í¸: Unslothë¥¼ í™œìš©í•œ í•œêµ­ì–´ íŠ¹í™” LLM í•™ìŠµ ì™„ì „ ê°€ì´ë“œ]({% post_url 2025-06-17-unsloth-korean-llm-training-guide %})
- [2í¸: ì¿ ë²„ë„¤í‹°ìŠ¤ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•]({% post_url 2025-06-17-unsloth-korean-llm-kubernetes-automation %})
- 3í¸: Kubeflow ë° MLOps í”„ë ˆì„ì›Œí¬ í™œìš© (í˜„ì¬ ê¸€)

ì´ëŸ¬í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ MLOps ì‹œìŠ¤í…œì„ í†µí•´ í•œêµ­ì–´ íŠ¹í™” LLM ê°œë°œì˜ ìƒì‚°ì„±ê³¼ í’ˆì§ˆì„ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 