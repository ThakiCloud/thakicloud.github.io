---
title: "Hugging Face Kernel Hub ì™„ë²½ ê°€ì´ë“œ: ëª¨ë¸ ì„±ëŠ¥ì„ 5ë¶„ ë§Œì— í–¥ìƒì‹œí‚¤ê¸°"
excerpt: "Kernel Hubë¥¼ í™œìš©í•´ FlashAttentionÂ·GELUÂ·RMSNorm ë“± ê³ ì„±ëŠ¥ ì»¤ë„ì„ ì¦‰ì‹œ ì ìš©í•˜ì—¬ LLM ì¶”ë¡ Â·í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì†Œê°œí•©ë‹ˆë‹¤."
date: 2025-06-16
categories:
  - llmops
  - tutorials
tags:
  - huggingface
  - kernel-hub
  - kernels
  - flashattention
  - llmops
author_profile: true
toc: true
toc_label: HF Kernel Hub Guide
---

## ì„œë¡ 

ëª¨ë¸ì˜ ì¶”ë¡ Â·í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ë ¤ë©´ ë³´í†µ **CUDA ì»¤ë„**ì„ ì§ì ‘ ë¹Œë“œí•˜ê±°ë‚˜ ë³µì¡í•œ íŠ¸ë¦¬í†¤(Triton) ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. **Hugging Face Kernel Hub**ëŠ” ì´ëŸ¬í•œ ë²ˆê±°ë¡œì›€ì„ ì—†ì• ê³ , `kernels` íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ìµœì í™”ëœ ì»¤ë„ì„ *ì›í´ë¦­*ìœ¼ë¡œ ë‚´ë ¤ë°›ì•„ ì‚¬ìš©í•˜ëŠ” ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤. ë³¸ ê¸€ì€ [ê³µì‹ ì†Œê°œ ê¸€](https://huggingface.co/blog/hello-hf-kernels) \[HF Blog\]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, Kernel Hubì˜ êµ¬ì¡°, í•µì‹¬ ê°œë…, ì‹¤ì „ ì½”ë“œ ì˜ˆì œ, ë²¤ì¹˜ë§ˆí‚¹ ë°©ë²•, ê·¸ë¦¬ê³  ìš´ì˜ í™˜ê²½ì—ì„œì˜ ì ìš© ì „ëµê¹Œì§€ **LLMOps** ê´€ì ì—ì„œ ì¢…í•©ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤.

> ë³¸ë¬¸ ì˜ˆì œ ì½”ë“œëŠ” PyTorch â‰¥ 2.2, CUDA â‰¥ 12.1 í™˜ê²½ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

## 1. Kernel Hubë€ ë¬´ì—‡ì¸ê°€?

| íŠ¹ì§• | ì„¤ëª… |
| --- | --- |
| **ì €ì¥ì†Œ êµ¬ì¡°** | ëª¨ë¸ í—ˆë¸Œì™€ ìœ ì‚¬í•˜ê²Œ *ì»¤ë„ íŒ¨í‚¤ì§€*ë¥¼ Hugging Face Hubì— ì—…ë¡œë“œ ë° ë²„ì „ ê´€ë¦¬ |
| **`kernels` ë¼ì´ë¸ŒëŸ¬ë¦¬** | PythonÂ·CUDAÂ·PyTorch ë²„ì „ì„ ìë™ ê°ì§€í•´ í˜¸í™˜ë˜ëŠ” ë°”ì´ë„ˆë¦¬(.so)ë§Œ ë‹¤ìš´ë¡œë“œ |
| **ì§€ì› í•˜ë“œì›¨ì–´** | NVIDIAÂ·AMD GPU(ì¶”ê°€ ì§€ì› ì˜ˆì •), CPU(ì¼ë¶€) |
| **ì£¼ìš” ì»¤ë„** | FlashAttention, RMSNorm, GELU Fast, INT4/8 ì–‘ìí™” ë“± |

ì»¤ë„ í—ˆë¸Œë¥¼ ì‚¬ìš©í•˜ë©´ FlashAttentionì²˜ëŸ¼ ìˆ˜ ì‹œê°„ ë¹Œë“œê°€ ê±¸ë¦¬ëŠ” ê¸°ëŠ¥ë„ **í•œ ì¤„**ë¡œ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from kernels import get_kernel
flash_attn = get_kernel("kernels-community/flash-attn")
```

ì´ë¥¼ í†µí•´ **ë¹Œë“œ ì‹œê°„ ì ˆê°**, **ë³µì¡ë„ ê°ì†Œ**, **ì¬í˜„ì„± í–¥ìƒ**ì´ë¼ëŠ” ì„¸ ê°€ì§€ ì´ì ì„ ì¦‰ì‹œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \[HF Blog\]

## 2. ì„¤ì¹˜ ë° ì²« ì‹¤í–‰

### 2.1 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
pip install kernels torch --upgrade
```

`kernels`ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ PyTorchÂ·CUDA ë²„ì „ì„ í™•ì¸í•œ ë’¤ ê°€ì¥ ì í•©í•œ ë°”ì´ë„ˆë¦¬ë¥¼ ë‚´ë ¤ë°›ìŠµë‹ˆë‹¤.

### 2.2 ê¸°ë³¸ ì˜ˆì œ: GELU Fast ì»¤ë„

```python
import torch
from kernels import get_kernel

DEVICE = "cuda"

a = torch.randn(8, 8, dtype=torch.float16, device=DEVICE)
activation = get_kernel("kernels-community/activation")

out = torch.empty_like(a)
activation.gelu_fast(out, a)
print(out)
```

ìœ„ ì½”ë“œë§Œìœ¼ë¡œ íŠ¸ë¦¬í†¤ ê¸°ë°˜ ë¹ ë¥¸ GELUê°€ ì ìš©ë©ë‹ˆë‹¤.

## 3. ëª¨ë¸ í†µí•©: FlashAttentionë¥¼ í†µí•œ Llama-7B ê°€ì†

### 3.1 ì „ì œ ì¡°ê±´

- `transformers` â‰¥ 4.43
- GPU ë©”ëª¨ë¦¬ â‰¥ 24 GB (Ampere A10G ë“±)

```python
from kernels import get_kernel
from transformers import AutoModelForCausalLM, AutoTokenizer

flash_attn = get_kernel("kernels-community/flash-attn")  # ì»¤ë„ ë¡œë“œ

tok = AutoTokenizer.from_pretrained("NousResearch/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("NousResearch/Llama-2-7b-chat-hf", torch_dtype=torch.float16, device_map="auto")

prompt = "Explain Retrieval-Augmented Generation in one sentence."
inputs = tok(prompt, return_tensors="pt").to("cuda")

with torch.inference_mode():
    output = model.generate(**inputs, max_new_tokens=64)
print(tok.decode(output[0], skip_special_tokens=True))
```

`kernels-community/flash-attn`ì—ëŠ” **`mha_fwd`**/`mha_bwd` í•¨ìˆ˜ê°€ í¬í•¨ë˜ì–´ `scaled_dot_product_attention` ë‚´ë¶€ë¥¼ ìë™ í›„í‚¹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ê°€ ì œê³µë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì½”ë“œ ë³€ê²½ ì—†ì´ë„ ëª¨ë¸ ì „ì—­ì— FlashAttentionì´ ì ìš©ë©ë‹ˆë‹¤. \[HF Blog\]

## 4. ë²¤ì¹˜ë§ˆí‚¹: RMSNorm ì»¤ë„ ì„±ëŠ¥ ì¸¡ì •

ì•„ë˜ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë°°ì¹˜ ì‚¬ì´ì¦ˆë³„ **baseline LayerNorm** vs **Kernel RMSNorm** ì²˜ë¦¬ ì†ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.

```python
import torch, time
from kernels import get_kernel

DEVICE = "cuda"
DTYPE = torch.float16

# ì»¤ë„ ë¡œë“œ
rms = get_kernel("kernels-community/rmsnorm")

hidden = 4096
batch_sizes = [256, 512, 1024, 2048, 4096]

base_layer = torch.nn.LayerNorm(hidden, eps=1e-5).to(DEVICE, DTYPE)
kernel_layer = rms.RMSNorm(hidden, eps=1e-5).to(DEVICE, DTYPE)

for b in batch_sizes:
    x = torch.randn(b, hidden, device=DEVICE, dtype=DTYPE)
    torch.cuda.synchronize()

    # baseline
    t0 = time.perf_counter(); _ = base_layer(x); torch.cuda.synchronize(); t1 = time.perf_counter()

    # kernel
    t2 = time.perf_counter(); _ = kernel_layer(x); torch.cuda.synchronize(); t3 = time.perf_counter()

    print(f"{b:>6} | baseline {1000*(t1-t0):.3f} ms | kernel {1000*(t3-t2):.3f} ms | speedup {(t1-t0)/(t3-t2):.2f}x")
```

ìƒ˜í”Œ ê²°ê³¼(A100 80GB, CUDA 12.1):

| Batch | Baseline(ms) | Kernel(ms) | Speedup |
|-----:|------------:|-----------:|--------:|
|  256 | 0.21 | 0.14 | 1.50x |
| 4096 | 4.43 | 2.25 | 1.97x |

GPUÂ·ì…ë ¥ í˜•ìƒÂ·dtypeì— ë”°ë¼ í¸ì°¨ê°€ ìˆìœ¼ë¯€ë¡œ ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ìˆ˜í–‰í•œ ë’¤ ì ìš©í•˜ì„¸ìš”.

## 5. ìš´ì˜ í™˜ê²½ ì ìš© ì „ëµ

### 5.1 Docker ì´ë¯¸ì§€ì— í¬í•¨í•˜ê¸°

1. **ìºì‹±**: ìµœì´ˆ `get_kernel` í˜¸ì¶œ ì‹œ `~/.cache/hf/kernels/`ì— ë°”ì´ë„ˆë¦¬ê°€ ì €ì¥ë©ë‹ˆë‹¤. Dockerfileì—ì„œ `--mount=type=cache`ë¥¼ í™œìš©í•´ ë¹Œë“œ ì‹œì ì— ì»¤ë„ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ë©´ ëŸ°íƒ€ì„ ì§€ì—°ì„ ì—†ì•¨ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **ë²„ì „ ê³ ì •**: `get_kernel("user/kernel@v1.0.0")` í˜•íƒœë¡œ ë²„ì „ì„ ëª…ì‹œí•´ ì¬í˜„ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.
3. **ì—”í„°í”„ë¼ì´ì¦ˆ í”„ë¡ì‹œ**: ì‚¬ë‚´ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì‚¬ìš© ì‹œ `HUGGINGFACE_HUB_CACHE` ë° `HF_ENDPOINT` í™˜ê²½ë³€ìˆ˜ë¥¼ ì´ìš©í•´ ë¯¸ëŸ¬ ì €ì¥ì†Œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.

### 5.2 CanaryÂ·Rollback ì „ëµ

- **A/B í…ŒìŠ¤íŠ¸**: `torch.nn.functional.scaled_dot_product_attention`ë¥¼ ëŸ°íƒ€ì„ì— íŒ¨ì¹˜í•˜ëŠ” ë°©ì‹ì€ ìœ„í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `env var` ìŠ¤ìœ„ì¹˜ë¥¼ ë‘ê³  Canary íŠ¸ë˜í”½ì—ë§Œ ì»¤ë„ì„ í™œì„±í™”í•˜ëŠ” ë°©ì‹ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
- **Fallback**: `get_kernel` ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ìºì¹˜í•´ PyTorch ê¸°ë³¸ ì—°ì‚°ìœ¼ë¡œ ë˜ëŒì•„ê°€ëŠ” wrapperë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

## 6. ì§ì ‘ ì»¤ë„ ë§Œë“¤ê¸° & ê³µìœ í•˜ê¸°

1. íŠ¸ë¦¬í†¤(Triton)Â·CUDAë¡œ `.cu` ë˜ëŠ” `.py`(Triton) ì»¤ë„ ì‘ì„±
2. `setup.py` ë˜ëŠ” `pyproject.toml`ì— ë¹Œë“œ ì„¤ì • ì¶”ê°€
3. `kernels` CLI(ì˜ˆ: `kernels build && kernels upload`)ë¡œ Hubì— ì—…ë¡œë“œ
4. READMEì— **ì…ë ¥Â·ì¶œë ¥ ì‹œê·¸ë‹ˆì²˜**, **ì§€ì› ë²„ì „**, **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**ë¥¼ ëª…ì‹œí•´ ì‚¬ìš©ì í˜¼ë€ì„ ì¤„ì…ë‹ˆë‹¤.

ì»¤ë„ í—ˆë¸ŒëŠ” ì˜¤í”ˆì†ŒìŠ¤ í˜‘ì—… ëª¨ë¸ì´ë¯€ë¡œ, ìì‹ ì´ ê°œë°œí•œ ìµœì í™” ê¸°ë²•ì„ ì»¤ë®¤ë‹ˆí‹°ì— ê³µìœ í•˜ì—¬ ìƒíƒœê³„ ë°œì „ì— ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 7. í•œê³„ì™€ ì£¼ì˜ì‚¬í•­

- **GPU ì˜ì¡´ì„±**: ëŒ€ë¶€ë¶„ì˜ ì»¤ë„ì€ NVIDIA GPU ì „ìš©ì…ë‹ˆë‹¤. AMDÂ·CPU ëŒ€ì‘ ì—¬ë¶€ë¥¼ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”.
- **ì •í™•ë„ ê²€ì¦**: ì»¤ë„ ìµœì í™”ë¡œ ì¸í•´ ë¯¸ì„¸í•œ ìˆ˜ì¹˜ ì°¨ì´ê°€ ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `torch.testing.assert_close`ë¡œ í—ˆìš© ì˜¤ì°¨ë¥¼ ê²€ì¦í•˜ì„¸ìš”.
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: FlashAttentionì²˜ëŸ¼ ë©”ëª¨ë¦¬ë¥¼ ì ˆê°í•˜ê¸´ í•˜ì§€ë§Œ, ë°°ì¹˜Â·ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¼ peak ë©”ëª¨ë¦¬ê°€ ì ê¹ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ê²°ë¡ 

Hugging Face Kernel HubëŠ” **ì»´íŒŒì¼ ë²ˆê±°ë¡œì›€ ì—†ì´** ìµœì í™”ëœ CUDAÂ·Triton ì»¤ë„ì„ ì†ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ” í˜ì‹ ì ì¸ í”Œë«í¼ì…ë‹ˆë‹¤. LLM ì¶”ë¡  Latencyë¥¼ ë‚®ì¶”ê³ , í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ë©°, DevOps ë³µì¡ë„ë¥¼ ëŒ€í­ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§€ê¸ˆ ë°”ë¡œ `pip install kernels` í›„ FlashAttentionÂ·RMSNorm ì»¤ë„ì„ ì ìš©í•´ ë³´ì„¸ìš”!

> ë” ìì„¸í•œ ë‚´ìš©ì€ Hugging Face ê³µì‹ ë¸”ë¡œê·¸ ê¸€ì„ ì°¸ê³ í•˜ì„¸ìš”: [ğŸï¸ Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub](https://huggingface.co/blog/hello-hf-kernels) \[HF Blog\] 