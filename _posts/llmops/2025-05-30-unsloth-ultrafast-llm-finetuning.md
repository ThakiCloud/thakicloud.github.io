---
title: "Unsloth: LLM νμΈνλ‹μ„ 2λ°° λΉ λ¥΄κ², λ©”λ¨λ¦¬λ” 80% μ μ•½ν•λ” νμ‹ μ  ν”„λ μ„μ›ν¬"
excerpt: "Qwen3, Llama 4, Gemma 3λ¥Ό 2λ°° λΉ λ¥΄κ² νμΈνλ‹ν•λ©΄μ„ VRAMμ„ 80%κΉμ§€ μ μ•½. OpenAI Triton κΈ°λ°μ μ •ν™•λ„ μ†μ‹¤ μ—†λ” μµμ ν™” μ—”μ§„"
date: 2025-05-30
categories:
  - llmops
tags:
  - Unsloth
  - LLM
  - νμΈνλ‹
  - LoRA
  - QLoRA
  - λ©”λ¨λ¦¬μµμ ν™”
  - Triton
  - μ„±λ¥μµμ ν™”
author_profile: true
toc: true
toc_label: "Unsloth μ™„μ „ κ°€μ΄λ“"
---

> **TL;DR** Unslothλ” Qwen3, Llama 4, Gemma 3, Phi-4 λ“± μµμ‹  LLMμ„ **2λ°° λΉ λ¥΄κ²** νμΈνλ‹ν•λ©΄μ„ **VRAMμ„ 80%κΉμ§€ μ μ•½**ν•λ” μ¤ν”μ†μ¤ λΌμ΄λΈλ¬λ¦¬λ‹¤. OpenAI TritonμΌλ΅ μ‘μ„±λ μ»¤λ„κ³Ό μλ™ μ—­μ „ν μ—”μ§„μΌλ΅ **μ •ν™•λ„ μ†μ‹¤ μ—†μ΄** λ©”λ¨λ¦¬ ν¨μ¨μ„±μ„ κ·Ήλ€ν™”ν•λ‹¤. μ΄λ³΄μλ¶€ν„° μ „λ¬Έκ°€κΉμ§€ λ„κµ¬λ‚ μ‚¬μ©ν•  μ μλ” λ¬΄λ£ λ…ΈνΈλ¶κ³Ό μƒμ„Έν• λ¬Έμ„λ¥Ό μ κ³µν•λ‹¤.

---

## Unslothλ€?

[Unsloth](https://github.com/unslothai/unsloth)λ” λ€ν• μ–Έμ–΄ λ¨λΈ(LLM) νμΈνλ‹μ„ μ„ν• νμ‹ μ μΈ μµμ ν™” λΌμ΄λΈλ¬λ¦¬λ‹¤. 2023λ…„λ¶€ν„° κ°λ°λμ–΄ ν„μ¬ **3λ§ 9μ² κ° μ΄μƒμ GitHub μ¤νƒ€**λ¥Ό λ°›μΌλ©° LLM μ»¤λ®¤λ‹ν‹°μ—μ„ κ°€μ¥ μ£Όλ©λ°›λ” ν”„λ μ„μ›ν¬ μ¤‘ ν•λ‚κ°€ λμ—λ‹¤.

### ν•µμ‹¬ νΉμ§•

- **2λ°° λΉ λ¥Έ μ†λ„**: κΈ°μ΅΄ Hugging Face + Flash Attention 2 λ€λΉ„ 2λ°° μ΄μƒ λΉ λ¥Έ ν•™μµ
- **80% λ©”λ¨λ¦¬ μ μ•½**: VRAM μ‚¬μ©λ‰μ„ μµλ€ 80%κΉμ§€ κ°μ†
- **μ •ν™•λ„ μ†μ‹¤ μ—†μ**: κ·Όμ‚¬ν™” λ°©λ²• μ—†μ΄ μ •ν™•ν• κ³„μ‚° μ μ§€
- **κ΄‘λ²”μ„ν• λ¨λΈ μ§€μ›**: Qwen3, Llama 4, Gemma 3, Phi-4, Mistral λ“±
- **μ™„μ „ν• νΈν™μ„±**: 2018λ…„ μ΄ν›„ NVIDIA GPU μ§€μ› (CUDA Capability 7.0+)

## μ™ Unslothλ¥Ό μ¨μ•Ό ν• κΉ?

### μ„±λ¥ νμ‹ 

κΈ°μ΅΄ νμΈνλ‹ λ°©μ‹μ ν•κ³„λ¥Ό λ›°μ–΄λ„λ” μ„±λ¥μ„ μ κ³µν•λ‹¤:

| λ¨λΈ | VRAM | π¦¥ Unsloth μ†λ„ | π¦¥ VRAM μ μ•½ | π¦¥ κΈ΄ μ»¨ν…μ¤νΈ | π Hugging Face + FA2 |
|------|------|----------------|-------------|--------------|-------------------|
| Llama 3.3 (70B) | 80GB | 2λ°° | >75% | 13λ°° | 1λ°° |
| Llama 3.1 (8B) | 80GB | 2λ°° | >70% | 12λ°° | 1λ°° |

### μ»¨ν…μ¤νΈ κΈΈμ΄ νμ‹ 

Llama 3.1 (8B) κΈ°μ¤€μΌλ΅ λ‹¤μ–‘ν• VRAM ν™κ²½μ—μ„μ μµλ€ μ»¨ν…μ¤νΈ κΈΈμ΄:

| GPU VRAM | π¦¥ Unsloth | Hugging Face + FA2 |
|----------|------------|-------------------|
| 8GB | 2,972 | OOM |
| 16GB | 40,724 | 2,551 |
| 24GB | 78,475 | 5,789 |
| 80GB | 342,733 | 28,454 |

## μ§€μ›ν•λ” λ¨λΈλ“¤

Unslothλ” μµμ‹  LLMλ“¤μ„ κ΄‘λ²”μ„ν•κ² μ§€μ›ν•λ‹¤:

### μµμ‹  λ¨λΈ μ§€μ›

- **Qwen3** (4B, 14B): GRPO ν¬ν•¨ λ¨λ“  κΈ°λ¥ μ§€μ›
- **Llama 4**: Metaμ Scout & Maverick ν¬ν•¨
- **Gemma 3** (4B): Googleμ μµμ‹  λ¨λΈ
- **Phi-4** (14B): Microsoftμ μ°¨μ„Έλ€ λ¨λΈ
- **DeepSeek-R1**: μ¶”λ΅  νΉν™” λ¨λΈ
- **Mistral v0.3** (7B): ν–¥μƒλ μ„±λ¥

### λ©€ν‹°λ¨λ‹¬ μ§€μ›

- **Llama 3.2 Vision** (11B): μ΄λ―Έμ§€-ν…μ¤νΈ μ²λ¦¬
- **TTS/STT λ¨λΈ**: `sesame/csm-1b`, `openai/whisper-large-v3`

## μ„¤μΉ λ°©λ²•

### κΈ°λ³Έ μ„¤μΉ (κ¶μ¥)

```bash
pip install unsloth
```

### Windows μ„¤μΉ

Windows μ‚¬μ©μλ” λ‹¤μ μ‚¬μ „ μ”κµ¬μ‚¬ν•­μ΄ ν•„μ”ν•λ‹¤:

1. **NVIDIA λ“λΌμ΄λ²„**: [μµμ‹  λ²„μ „ λ‹¤μ΄λ΅λ“](https://www.nvidia.com/Download/index.aspx)
2. **Visual Studio C++**: C++ μµμ…κ³Ό Windows SDK ν¬ν•¨
3. **CUDA Toolkit**: [κ³µμ‹ κ°€μ΄λ“](https://developer.nvidia.com/cuda-toolkit)
4. **PyTorch**: [νΈν™ λ²„μ „ μ„¤μΉ](https://pytorch.org/)

```bash
pip install unsloth
```

### κ³ κΈ‰ μ„¤μΉ

νΉμ • CUDA/PyTorch λ²„μ „μ— λ§μ¶ μ„¤μΉ:

```bash
# CUDA 12.1 + PyTorch 2.4
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"

# CUDA 12.4 + PyTorch 2.5
pip install "unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git"

# Ampere GPU (A100, H100, RTX 30/40 μ‹λ¦¬μ¦)
pip install "unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
```

## λΉ λ¥Έ μ‹μ‘ κ°€μ΄λ“

### κΈ°λ³Έ νμΈνλ‹ μμ 

```python
from unsloth import FastLanguageModel, FastModel
import torch
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset

max_seq_length = 2048  # RoPE Scaling λ‚΄λ¶€ μ§€μ›
dataset = load_dataset("json", data_files={"train": "your_dataset.jsonl"}, split="train")

# 4λΉ„νΈ μ‚¬μ „ μ–‘μν™” λ¨λΈ λ΅λ“
model, tokenizer = FastModel.from_pretrained(
    model_name="unsloth/gemma-3-4B-it",
    max_seq_length=2048,
    load_in_4bit=True,
    load_in_8bit=False,
    full_finetuning=False,
)

# LoRA μ–΄λ‘ν„° μ¶”κ°€
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",  # 30% μ¶”κ°€ VRAM μ μ•½
    random_state=3407,
    max_seq_length=max_seq_length,
)

# νΈλ μ΄λ„ μ„¤μ • λ° ν•™μµ
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    tokenizer=tokenizer,
    args=SFTConfig(
        max_seq_length=max_seq_length,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        max_steps=60,
        logging_steps=1,
        output_dir="outputs",
        optim="adamw_8bit",
        seed=3407,
    ),
)
trainer.train()
```

### κ°•ν™”ν•™μµ (DPO) μμ 

```python
from trl import DPOTrainer, DPOConfig

# λ¨λΈ λ΅λ“ (μ„μ™€ λ™μΌ)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/zephyr-sft-bnb-4bit",
    max_seq_length=2048,
    load_in_4bit=True,
)

# DPO νΈλ μ΄λ„ μ„¤μ •
dpo_trainer = DPOTrainer(
    model=model,
    ref_model=None,
    train_dataset=YOUR_DATASET_HERE,
    tokenizer=tokenizer,
    args=DPOConfig(
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,
        warmup_ratio=0.1,
        num_train_epochs=3,
        logging_steps=1,
        optim="adamw_8bit",
        seed=42,
        output_dir="outputs",
        max_length=1024,
        max_prompt_length=512,
        beta=0.1,
    ),
)
dpo_trainer.train()
```

## κ°•ν™”ν•™μµ μ§€μ›

Unslothλ” λ‹¤μ–‘ν• κ°•ν™”ν•™μµ μ•κ³ λ¦¬μ¦μ„ μ§€μ›ν•λ©°, π¤—Hugging Face κ³µμ‹ λ¬Έμ„μ—λ„ ν¬ν•¨λμ–΄ μλ‹¤:

### μ§€μ› μ•κ³ λ¦¬μ¦

- **DPO** (Direct Preference Optimization)
- **GRPO** (Group Relative Policy Optimization)
- **PPO** (Proximal Policy Optimization)
- **ORPO** (Odds Ratio Preference Optimization)
- **KTO** (Kahneman-Tversky Optimization)
- **SimPO** (Simple Preference Optimization)
- **Reward Modelling**
- **Online DPO**

### GRPO λ…ΈνΈλ¶

νΉν **GRPO**λ” κΈ΄ μ»¨ν…μ¤νΈ μ¶”λ΅ μ„ ν•™μµν•λ” νμ‹ μ μΈ λ°©λ²•μΌλ΅, Unslothλ¥Ό ν†µν•΄ **5GB VRAM**λ§μΌλ΅λ„ μ¶”λ΅  λ¨λΈμ„ ν›λ ¨ν•  μ μλ‹¤.

## κΈ°μ μ  νμ‹ 

### OpenAI Triton κΈ°λ° μ»¤λ„

λ¨λ“  μ»¤λ„μ΄ **OpenAI Triton** μ–Έμ–΄λ΅ μ‘μ„±λμ–΄ μµμ ν™”λμ—λ‹¤:

- **μλ™ μ—­μ „ν μ—”μ§„**: μλ™ λ―Έλ¶„ λ€μ‹  μ •ν™•ν• μ—­μ „ν κµ¬ν„
- **λ©”λ¨λ¦¬ μµμ ν™”**: λ¶ν•„μ”ν• μ¤‘κ°„ ν…μ„ μ κ±°
- **μ»¤λ„ μµν•©**: μ—¬λ¬ μ—°μ‚°μ„ λ‹¨μΌ μ»¤λ„λ΅ κ²°ν•©

### μ§€μ› ν•λ“μ›¨μ–΄

- **NVIDIA GPU**: 2018λ…„ μ΄ν›„ λ¨λ“  GPU (CUDA Capability 7.0+)
- **μ΄μμ²΄μ **: Linux, Windows
- **νΉλ³„ μ§€μ›**: V100, T4, RTX 20/30/40 μ‹λ¦¬μ¦, A100, H100, L40

### μ „μ²΄ νμΈνλ‹ μ§€μ›

LoRAλΏλ§ μ•„λ‹λΌ **μ „μ²΄ νμΈνλ‹**λ„ μ§€μ›ν•λ‹¤:

```python
model = FastModel.from_pretrained(
    model_name="model_name",
    full_finetuning=True,  # μ „μ²΄ νμΈνλ‹ ν™μ„±ν™”
    load_in_8bit=True,     # 8λΉ„νΈ μ–‘μν™”
)
```

## λ¬΄λ£ λ…ΈνΈλ¶κ³Ό μλ£

Unslothλ” μ΄λ³΄μλ¥Ό μ„ν• ν’λ¶€ν• ν•™μµ μλ£λ¥Ό μ κ³µν•λ‹¤:

### μ§€μ› ν”λ«νΌλ³„ λ…ΈνΈλ¶

| λ¨λΈ | λ¬΄λ£ λ…ΈνΈλ¶ | μ„±λ¥ ν–¥μƒ | λ©”λ¨λ¦¬ μ μ•½ |
|------|------------|---------|----------|
| Qwen3 (14B) | β–¶οΈ λ°”λ΅ μ‹μ‘ | 2λ°° | 70% |
| Qwen3 (4B): GRPO | β–¶οΈ λ°”λ΅ μ‹μ‘ | 2λ°° | 80% |
| Gemma 3 (4B) | β–¶οΈ λ°”λ΅ μ‹μ‘ | 1.6λ°° | 60% |
| Llama 3.2 (3B) | β–¶οΈ λ°”λ΅ μ‹μ‘ | 2λ°° | 70% |
| Phi-4 (14B) | β–¶οΈ λ°”λ΅ μ‹μ‘ | 2λ°° | 70% |

### νΉμ μ©λ„ λ…ΈνΈλ¶

- **TTS & Vision**: ν…μ¤νΈ-μμ„± λ³€ν™ λ° λ©€ν‹°λ¨λ‹¬
- **Kaggle**: κ²½μ§„λ€ν νΉν™” λ…ΈνΈλ¶
- **Synthetic Dataset**: Metaμ™€ ν‘λ ¥ν• ν•©μ„± λ°μ΄ν„°μ…‹

## κ³ κΈ‰ κΈ°λ¥

### λ¨λΈ λ‚΄λ³΄λ‚΄κΈ°

ν•™μµλ λ¨λΈμ„ λ‹¤μ–‘ν• ν•νƒλ΅ λ‚΄λ³΄λ‚Ό μ μλ‹¤:

- **GGUF**: llama.cpp νΈν™ ν•μ‹
- **Ollama**: λ΅μ»¬ λ°°ν¬μ©
- **vLLM**: κ³ μ„±λ¥ μ¶”λ΅  μ„λ²„
- **Hugging Face**: λ¨λΈ ν—λΈ μ—…λ΅λ“

### μ²΄ν¬ν¬μΈνΈ κ΄€λ¦¬

```python
# LoRA μ–΄λ‘ν„° μ €μ¥
model.save_pretrained("lora_model")

# κ³„μ† ν•™μµ
model = FastLanguageModel.from_pretrained(
    "lora_model",  # μ €μ¥λ μ–΄λ‘ν„° λ΅λ“
    load_in_4bit=True,
)
```

### μ‚¬μ©μ μ •μ μ±„ν… ν…ν”λ¦Ώ

```python
# μ»¤μ¤ν…€ μ±„ν… ν…ν”λ¦Ώ μ„¤μ •
tokenizer.chat_template = "your_custom_template"
```

## μ»¤λ®¤λ‹ν‹°μ™€ μ§€μ›

### κ³µμ‹ μλ£

- **GitHub**: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)
- **κ³µμ‹ μ›Ήμ‚¬μ΄νΈ**: [unsloth.ai](https://unsloth.ai)
- **μƒμ„Έ λ¬Έμ„**: [Unsloth Wiki](https://github.com/unslothai/unsloth/wiki)

### μ„±λ¥ λ²¤μΉλ§ν¬

μμ„Έν• λ²¤μΉλ§ν¬ κ²°κ³Όλ” λ‹¤μμ—μ„ ν™•μΈν•  μ μλ‹¤:
- **Llama 3.3 λΈ”λ΅κ·Έ**: κ³µμ‹ μ„±λ¥ λ¶„μ„
- **π¤—Hugging Face λ²¤μΉλ§ν¬**: λ…λ¦½μ μΈ μ„±λ¥ κ²€μ¦

### λΌμ΄μ„ μ¤

- **ν”„λ μ„μ›ν¬**: Apache-2.0 λΌμ΄μ„ μ¤
- **λ¨λΈ**: κ° λ¨λΈμ κ³ μ  λΌμ΄μ„ μ¤ μ¤€μ ν•„μ”

## μ‹¤μ „ ν™μ© ν

### λ©”λ¨λ¦¬ μµμ ν™”

```python
# μµλ€ λ©”λ¨λ¦¬ μ μ•½μ„ μ„ν• μ„¤μ •
model = FastLanguageModel.get_peft_model(
    model,
    use_gradient_checkpointing="unsloth",  # 30% μ¶”κ°€ μ μ•½
    lora_dropout=0,  # μµμ ν™”λ¨
    bias="none",     # μµμ ν™”λ¨
)
```

### κΈ΄ μ»¨ν…μ¤νΈ μ²λ¦¬

```python
# RoPE Scaling ν™μ©
max_seq_length = 32768  # κΈ΄ μ»¨ν…μ¤νΈ μ„¤μ •
model, tokenizer = FastModel.from_pretrained(
    model_name="model_name",
    max_seq_length=max_seq_length,  # μλ™ RoPE μ¤μΌ€μΌλ§
)
```

### Windows νΉλ³„ μ„¤μ •

```python
# Windowsμ—μ„ μ•μ •μ„±μ„ μ„ν• μ„¤μ •
trainer = SFTTrainer(
    dataset_num_proc=1,  # Windows μ¶©λ λ°©μ§€
    # ... κΈ°νƒ€ μ„¤μ •
)
```

## λ§λ¬΄λ¦¬

Unslothλ” LLM νμΈνλ‹μ ν¨λ¬λ‹¤μ„μ„ λ°”κΎΈλ” νμ‹ μ μΈ λ„κµ¬λ‹¤. **μ •ν™•λ„ μ†μ‹¤ μ—†μ΄** 2λ°° λΉ λ¥Έ μ†λ„μ™€ 80% λ©”λ¨λ¦¬ μ μ•½μ„ λ‹¬μ„±ν•λ©°, μ΄λ³΄μλ¶€ν„° μ „λ¬Έκ°€κΉμ§€ λ¨λ“  μμ¤€μ μ‚¬μ©μλ¥Ό μ§€μ›ν•λ‹¤.

νΉν **μ ν•λ GPU μμ›**μΌλ΅λ„ μµμ‹  λ€ν• λ¨λΈμ„ ν¨κ³Όμ μΌλ΅ νμΈνλ‹ν•  μ μκ² ν•΄μ£Όμ–΄, AI μ—°κµ¬μ™€ κ°λ°μ μ§„μ… μ¥λ²½μ„ ν¬κ² λ‚®μ¶λ‹¤λ” μ μ—μ„ μλ―Έκ°€ ν¬λ‹¤.

λ¬΄λ£ λ…ΈνΈλ¶κ³Ό μƒμ„Έν• λ¬Έμ„λ¥Ό ν†µν•΄ λ„κµ¬λ‚ μ‰½κ² μ‹μ‘ν•  μ μμΌλ‹, LLM νμΈνλ‹μ— κ΄€μ‹¬μ΄ μλ‹¤λ©΄ κΌ­ ν•λ² μ‹λ„ν•΄λ³Ό λ§ν•λ‹¤.

---

*μ΄ κΈ€μ΄ λ„μ›€μ΄ λμ—λ‹¤λ©΄, Unsloth GitHubμ— β­λ¥Ό λλ¬μ£Όμ„Έμ”!* 