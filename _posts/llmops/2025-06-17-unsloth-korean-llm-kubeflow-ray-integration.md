---
title: "Unsloth+TRL í•œêµ­ì–´ LLM í•™ìŠµ ìë™í™” - 5í¸: Kubeflow + Ray í†µí•© ì—”í„°í”„ë¼ì´ì¦ˆ MLOps ì‹œìŠ¤í…œ"
excerpt: "Kubeflow Pipelinesë¥¼ ì›Œí¬í”Œë¡œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¡œ, Rayë¥¼ ë¶„ì‚° ì‹¤í–‰ ì—”ì§„ìœ¼ë¡œ í™œìš©í•œ ì™„ì „í•œ MLOps ì‹œìŠ¤í…œ êµ¬ì¶•"
date: 2025-06-17
categories:
  - llmops
tags:
  - Kubeflow
  - Ray
  - RayJob
  - RayService
  - MLflow
  - DVC
  - Korean LLM
  - MLOps
  - Argo Rollout
  - Unsloth
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
---

## 1. ì•„í‚¤í…ì²˜ ì„¤ê³„ ë° ì„¤ì¹˜

### 1.1 í†µí•© ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    subgraph "Kubeflow Pipelines"
        KFP[Pipeline Controller]
        KFP --> PP[Preprocess Step]
        KFP --> CPT[CPT Step]
        KFP --> SFT[SFT Step]
        KFP --> RLHF[RLHF Step]
        KFP --> EVAL[Evaluate Step]
    end
    
    subgraph "Ray Cluster"
        RC[Ray Head]
        RW1[Ray Worker 1]
        RW2[Ray Worker 2]
        RWN[Ray Worker N]
    end
    
    subgraph "MLOps Infrastructure"
        MLF[MLflow Server]
        DVC[DVC Remote]
        S3[S3 Storage]
        REG[Model Registry]
    end
    
    subgraph "Serving Layer"
        RS[RayService]
        AR[Argo Rollout]
        LB[LoadBalancer]
    end
    
    PP --> RC
    CPT --> RC
    SFT --> RC
    RLHF --> RC
    EVAL --> RC
    
    RC --> MLF
    RC --> S3
    MLF --> REG
    
    REG --> RS
    RS --> AR
    AR --> LB
```

### 1.2 í•„ìˆ˜ êµ¬ì„± ìš”ì†Œ ì„¤ì¹˜

```bash
# 1. Kubeflow Pipelines ì„¤ì¹˜
export PIPELINE_VERSION=2.0.3
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref=$PIPELINE_VERSION"

# 2. KubeRay Operator ì„¤ì¹˜
helm repo add kuberay https://ray-project.github.io/kuberay-helm/
helm install kuberay-operator kuberay/kuberay-operator --version 1.0.0

# 3. Argo Rollouts ì„¤ì¹˜
kubectl create namespace argo-rollouts
kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml
```

### 1.3 Ray Cluster ì„¤ì •

```yaml
# ray-cluster-for-kubeflow.yaml
apiVersion: ray.io/v1alpha1
kind: RayCluster
metadata:
  name: kubeflow-ray-cluster
  namespace: kubeflow
spec:
  rayVersion: '2.8.0'
  headGroupSpec:
    rayStartParams:
      dashboard-host: '0.0.0.0'
      num-cpus: '0'
    template:
      spec:
        containers:
        - name: ray-head
          image: rayproject/ray:2.8.0-py310-gpu
          ports:
          - containerPort: 6379
            name: gcs-server
          - containerPort: 8265
            name: dashboard
          - containerPort: 10001
            name: client
          resources:
            limits:
              cpu: "8"
              memory: "32Gi"
            requests:
              cpu: "4"
              memory: "16Gi"
          env:
          - name: RAY_CLUSTER_NAME
            value: "kubeflow-ray-cluster"
          volumeMounts:
          - name: shared-storage
            mountPath: /shared
        volumes:
        - name: shared-storage
          persistentVolumeClaim:
            claimName: kubeflow-shared-pvc
  
  workerGroupSpecs:
  - replicas: 4
    minReplicas: 2
    maxReplicas: 20
    groupName: gpu-workers
    rayStartParams:
      num-cpus: '16'
      num-gpus: '4'
    template:
      spec:
        containers:
        - name: ray-worker
          image: rayproject/ray:2.8.0-py310-gpu
          resources:
            limits:
              cpu: "32"
              memory: "128Gi"
              nvidia.com/gpu: "4"
            requests:
              cpu: "16"
              memory: "64Gi"
              nvidia.com/gpu: "4"
          volumeMounts:
          - name: shared-storage
            mountPath: /shared
        volumes:
        - name: shared-storage
          persistentVolumeClaim:
            claimName: kubeflow-shared-pvc
```

## 2. RayJob ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ì»´í¬ë„ŒíŠ¸

### 2.1 ì „ì²˜ë¦¬ ì»´í¬ë„ŒíŠ¸ (Ray Data)

```python
# components/preprocess_component.py
from kfp import dsl
from kfp.kubernetes import use_k8s_secret
import kfp

@dsl.component(
    base_image="rayproject/ray:2.8.0-py310-gpu",
    packages_to_install=["ray[data]", "datasets", "transformers", "mlflow", "dvc"]
)
def preprocess_with_ray(
    input_data_path: str,
    output_data_path: str,
    ray_cluster_address: str = "ray://kubeflow-ray-cluster-head:10001",
    mlflow_tracking_uri: str = "http://mlflow-server:5000"
) -> str:
    """Ray Dataë¥¼ í™œìš©í•œ ëŒ€ê·œëª¨ ë°ì´í„° ì „ì²˜ë¦¬"""
    import ray
    import mlflow
    from ray import data
    import json
    from datetime import datetime
    
    # Ray í´ëŸ¬ìŠ¤í„° ì—°ê²°
    ray.init(address=ray_cluster_address)
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    
    with mlflow.start_run(run_name="preprocess-ray-data"):
        # Ray Dataë¡œ ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬
        ds = data.read_json(input_data_path)
        
        def clean_korean_text(batch):
            """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ì œ"""
            import re
            
            cleaned_texts = []
            for text in batch["text"]:
                # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ê·œí™”
                text = re.sub(r'\s+', ' ', text)
                text = re.sub(r'[^\w\sê°€-í£]', '', text)
                text = text.strip()
                
                if len(text) > 50:  # ìµœì†Œ ê¸¸ì´ í•„í„°ë§
                    cleaned_texts.append(text)
            
            return {"text": cleaned_texts}
        
        # ë¶„ì‚° ì²˜ë¦¬ë¡œ ë°ì´í„° ì •ì œ
        processed_ds = ds.map_batches(
            clean_korean_text,
            batch_size=1000,
            num_cpus=2
        )
        
        # ê²°ê³¼ ì €ì¥
        processed_ds.write_json(output_data_path)
        
        # ë©”íŠ¸ë¦­ ë¡œê¹…
        total_records = processed_ds.count()
        mlflow.log_metrics({
            "total_records": total_records,
            "processing_time": 0  # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹œê°„ ì¸¡ì •
        })
        
        ray.shutdown()
        return output_data_path
```

### 2.2 CPT í•™ìŠµ ì»´í¬ë„ŒíŠ¸ (Ray Train)

```python
# components/cpt_component.py
from kfp import dsl
import kfp

@dsl.component(
    base_image="rayproject/ray:2.8.0-py310-gpu",
    packages_to_install=[
        "ray[train]", "torch", "transformers", "datasets", 
        "unsloth @ git+https://github.com/unslothai/unsloth.git",
        "mlflow", "dvc"
    ]
)
def cpt_training_with_ray(
    input_data_path: str,
    model_output_path: str,
    model_name: str = "unsloth/Qwen2.5-7B-bnb-4bit",
    learning_rate: float = 1e-5,
    num_epochs: int = 2,
    ray_cluster_address: str = "ray://kubeflow-ray-cluster-head:10001",
    mlflow_tracking_uri: str = "http://mlflow-server:5000"
) -> str:
    """Ray Trainì„ í™œìš©í•œ CPT ë¶„ì‚° í•™ìŠµ"""
    import ray
    from ray import train
    from ray.train.torch import TorchTrainer
    from ray.train import ScalingConfig, RunConfig
    import mlflow
    import torch
    from unsloth import FastLanguageModel
    from datasets import load_dataset
    from transformers import TrainingArguments
    from trl import SFTTrainer
    
    # Ray í´ëŸ¬ìŠ¤í„° ì—°ê²°
    ray.init(address=ray_cluster_address)
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    
    def train_func(config):
        """ë¶„ì‚° í•™ìŠµ í•¨ìˆ˜"""
        import torch.distributed as dist
        from torch.nn.parallel import DistributedDataParallel as DDP
        
        # ë¶„ì‚° í™˜ê²½ ì„¤ì •
        rank = train.get_context().get_world_rank()
        local_rank = train.get_context().get_local_rank()
        world_size = train.get_context().get_world_size()
        
        device = torch.device(f"cuda:{local_rank}")
        torch.cuda.set_device(device)
        
        # MLflow ë¡œê¹… (rank 0ì—ì„œë§Œ)
        if rank == 0:
            mlflow.start_run(run_name="cpt-distributed-training")
            mlflow.log_params(config)
        
        # ëª¨ë¸ ë¡œë”©
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=config["model_name"],
            max_seq_length=4096,
            dtype=None,
            load_in_4bit=True,
        )
        
        # LoRA ì„¤ì •
        model = FastLanguageModel.get_peft_model(
            model,
            r=64,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            lora_alpha=16,
            lora_dropout=0.1,
            bias="none",
            use_gradient_checkpointing="unsloth",
        )
        
        model = model.to(device)
        
        # DDP ë˜í•‘
        if world_size > 1:
            model = DDP(model, device_ids=[local_rank])
        
        # ë°ì´í„°ì…‹ ë¡œë”©
        dataset = load_dataset("json", data_files=config["input_data_path"], split="train")
        
        # í•™ìŠµ ì„¤ì •
        training_args = TrainingArguments(
            output_dir="/tmp/cpt_output",
            num_train_epochs=config["num_epochs"],
            per_device_train_batch_size=4,
            learning_rate=config["learning_rate"],
            logging_steps=100,
            save_steps=1000,
            fp16=True,
            dataloader_num_workers=4,
        )
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        trainer = SFTTrainer(
            model=model,
            tokenizer=tokenizer,
            args=training_args,
            train_dataset=dataset,
            dataset_text_field="text",
        )
        
        # í•™ìŠµ ì‹¤í–‰
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥ (rank 0ì—ì„œë§Œ)
        if rank == 0:
            trainer.save_model(config["model_output_path"])
            
            # MLflowì— ëª¨ë¸ ë¡œê¹…
            final_loss = trainer.state.log_history[-1]["train_loss"]
            mlflow.log_metrics({
                "final_loss": final_loss,
                "total_steps": trainer.state.global_step
            })
            mlflow.pytorch.log_model(model, "cpt_model")
            mlflow.end_run()
        
        # Ray Train ë©”íŠ¸ë¦­ ë¦¬í¬íŒ…
        train.report({"loss": trainer.state.log_history[-1]["train_loss"]})
    
    # í•™ìŠµ ì„¤ì •
    config = {
        "model_name": model_name,
        "input_data_path": input_data_path,
        "model_output_path": model_output_path,
        "learning_rate": learning_rate,
        "num_epochs": num_epochs
    }
    
    # TorchTrainer ì„¤ì • ë° ì‹¤í–‰
    trainer = TorchTrainer(
        train_loop_per_worker=train_func,
        train_loop_config=config,
        scaling_config=ScalingConfig(
            num_workers=4,
            use_gpu=True,
            resources_per_worker={"CPU": 4, "GPU": 1}
        ),
        run_config=RunConfig(
            name="cpt-training",
            storage_path="/shared/ray_results"
        )
    )
    
    result = trainer.fit()
    ray.shutdown()
    
    return model_output_path
```

### 2.3 RLHF ì»´í¬ë„ŒíŠ¸ (Ray Train + Tune)

```python
# components/rlhf_component.py
from kfp import dsl
import kfp

@dsl.component(
    base_image="rayproject/ray:2.8.0-py310-gpu",
    packages_to_install=[
        "ray[train,tune]", "torch", "transformers", "datasets", 
        "unsloth @ git+https://github.com/unslothai/unsloth.git",
        "trl", "mlflow", "dvc"
    ]
)
def rlhf_training_with_ray(
    base_model_path: str,
    preference_data_path: str,
    model_output_path: str,
    ray_cluster_address: str = "ray://kubeflow-ray-cluster-head:10001",
    mlflow_tracking_uri: str = "http://mlflow-server:5000"
) -> str:
    """Ray Train + Tuneì„ í™œìš©í•œ RLHF í•™ìŠµ"""
    import ray
    from ray import train, tune
    from ray.train.torch import TorchTrainer
    from ray.tune.schedulers import ASHAScheduler
    import mlflow
    import torch
    from unsloth import FastLanguageModel
    from datasets import load_dataset
    from trl import PPOTrainer, PPOConfig
    
    # Ray í´ëŸ¬ìŠ¤í„° ì—°ê²°
    ray.init(address=ray_cluster_address)
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    
    def rlhf_objective(config):
        """RLHF íŠœë‹ ëª©ì  í•¨ìˆ˜"""
        with mlflow.start_run(nested=True):
            mlflow.log_params(config)
            
            # ëª¨ë¸ ë¡œë”©
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=base_model_path,
                max_seq_length=2048,
                dtype=None,
                load_in_4bit=True,
            )
            
            # PPO ì„¤ì •
            ppo_config = PPOConfig(
                model_name=base_model_path,
                learning_rate=config["learning_rate"],
                batch_size=config["batch_size"],
                mini_batch_size=config["mini_batch_size"],
                ppo_epochs=config["ppo_epochs"],
                gamma=config["gamma"],
                lam=config["lam"],
                cliprange=config["cliprange"],
            )
            
            # ì„ í˜¸ë„ ë°ì´í„°ì…‹ ë¡œë”©
            dataset = load_dataset("json", data_files=preference_data_path, split="train")
            
            # PPO íŠ¸ë ˆì´ë„ˆ ì„¤ì •
            ppo_trainer = PPOTrainer(
                config=ppo_config,
                model=model,
                tokenizer=tokenizer,
                dataset=dataset,
            )
            
            # RLHF í•™ìŠµ ì‹¤í–‰
            for epoch in range(3):  # ê°„ì†Œí™”ëœ í•™ìŠµ
                for batch in ppo_trainer.dataloader:
                    # PPO ìŠ¤í… ì‹¤í–‰
                    stats = ppo_trainer.step(
                        queries=batch["query"],
                        responses=batch["response"],
                        scores=batch["score"]
                    )
                    
                    # ë©”íŠ¸ë¦­ ë¡œê¹…
                    if stats:
                        mlflow.log_metrics(stats, step=epoch)
            
            # ëª¨ë¸ ì €ì¥
            model.save_pretrained(model_output_path)
            
            # ìµœì¢… ì„±ëŠ¥ í‰ê°€
            reward_score = 0.8  # ì‹¤ì œë¡œëŠ” ë³´ìƒ ëª¨ë¸ë¡œ í‰ê°€
            tune.report(reward=reward_score)
            mlflow.log_metric("final_reward", reward_score)
            
            return reward_score
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰ ê³µê°„
    search_space = {
        "learning_rate": tune.loguniform(1e-6, 1e-3),
        "batch_size": tune.choice([4, 8, 16]),
        "mini_batch_size": tune.choice([2, 4]),
        "ppo_epochs": tune.choice([2, 4, 6]),
        "gamma": tune.uniform(0.9, 0.99),
        "lam": tune.uniform(0.9, 0.99),
        "cliprange": tune.uniform(0.1, 0.3),
    }
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    scheduler = ASHAScheduler(
        time_attr="training_iteration",
        metric="reward",
        mode="max",
        max_t=10,
        grace_period=2
    )
    
    # íŠœë‹ ì‹¤í–‰
    tuner = tune.Tuner(
        rlhf_objective,
        param_space=search_space,
        tune_config=tune.TuneConfig(
            scheduler=scheduler,
            num_samples=5,
            max_concurrent_trials=2
        )
    )
    
    results = tuner.fit()
    best_result = results.get_best_result("reward", "max")
    
    # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ
    final_model = rlhf_objective(best_result.config)
    
    ray.shutdown()
    return model_output_path
```

---

## 3. Kubeflow Pipeline ì •ì˜ ë° í†µí•©

### 3.1 í†µí•© íŒŒì´í”„ë¼ì¸ ì •ì˜

```python
# korean_llm_pipeline.py
from kfp import dsl, compiler
from kfp.kubernetes import use_k8s_secret
from components.preprocess_component import preprocess_with_ray
from components.cpt_component import cpt_training_with_ray
from components.rlhf_component import rlhf_training_with_ray
import kfp

@dsl.pipeline(
    name="Korean LLM Training with Ray Integration",
    description="Complete MLOps pipeline using Kubeflow + Ray for Korean LLM training"
)
def korean_llm_ray_pipeline(
    raw_data_path: str = "s3://korean-llm-data/raw_corpus.jsonl",
    instruction_data_path: str = "s3://korean-llm-data/instruction_dataset.jsonl",
    preference_data_path: str = "s3://korean-llm-data/preference_dataset.jsonl",
    model_name: str = "unsloth/Qwen2.5-7B-bnb-4bit",
    cpt_learning_rate: float = 1e-5,
    sft_learning_rate: float = 2e-5,
    ray_cluster_address: str = "ray://kubeflow-ray-cluster-head:10001",
    mlflow_tracking_uri: str = "http://mlflow-server:5000"
):
    """í•œêµ­ì–´ LLM í•™ìŠµì„ ìœ„í•œ Kubeflow + Ray í†µí•© íŒŒì´í”„ë¼ì¸"""
    
    # 1ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ (Ray Data)
    preprocess_task = preprocess_with_ray(
        input_data_path=raw_data_path,
        output_data_path="s3://korean-llm-data/processed_corpus.jsonl",
        ray_cluster_address=ray_cluster_address,
        mlflow_tracking_uri=mlflow_tracking_uri
    )
    
    # DVC ì—°ë™ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
    preprocess_task.add_pod_annotation("dvc.stage", "preprocess")
    preprocess_task.add_pod_annotation("mlflow.experiment", "korean-llm-training")
    
    # 2ë‹¨ê³„: CPT í•™ìŠµ (Ray Train)
    cpt_task = cpt_training_with_ray(
        input_data_path=preprocess_task.output,
        model_output_path="s3://korean-llm-models/cpt_model",
        model_name=model_name,
        learning_rate=cpt_learning_rate,
        num_epochs=2,
        ray_cluster_address=ray_cluster_address,
        mlflow_tracking_uri=mlflow_tracking_uri
    )
    
    # ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ ì„¤ì •
    cpt_task.set_cpu_limit("32")
    cpt_task.set_memory_limit("128Gi")
    cpt_task.set_gpu_limit("4")
    cpt_task.add_pod_annotation("dvc.stage", "cpt_training")
    
    # 3ë‹¨ê³„: SFT í•™ìŠµ (ì¼ë°˜ ì»´í¬ë„ŒíŠ¸)
    sft_task = sft_training_component(
        base_model_path=cpt_task.output,
        instruction_data_path=instruction_data_path,
        model_output_path="s3://korean-llm-models/sft_model",
        learning_rate=sft_learning_rate,
        num_epochs=3
    )
    
    sft_task.set_cpu_limit("16")
    sft_task.set_memory_limit("64Gi")
    sft_task.set_gpu_limit("2")
    sft_task.add_pod_annotation("dvc.stage", "sft_training")
    
    # 4ë‹¨ê³„: RLHF í•™ìŠµ (Ray Train + Tune)
    rlhf_task = rlhf_training_with_ray(
        base_model_path=sft_task.output,
        preference_data_path=preference_data_path,
        model_output_path="s3://korean-llm-models/rlhf_model",
        ray_cluster_address=ray_cluster_address,
        mlflow_tracking_uri=mlflow_tracking_uri
    )
    
    rlhf_task.set_cpu_limit("64")
    rlhf_task.set_memory_limit("256Gi")
    rlhf_task.set_gpu_limit("8")
    rlhf_task.add_pod_annotation("dvc.stage", "rlhf_training")
    
    # 5ë‹¨ê³„: ëª¨ë¸ í‰ê°€
    evaluation_task = model_evaluation_component(
        model_path=rlhf_task.output,
        test_data_path="s3://korean-llm-data/test_dataset.jsonl"
    )
    
    evaluation_task.add_pod_annotation("dvc.stage", "evaluation")
    
    # 6ë‹¨ê³„: ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡
    registry_task = register_model_component(
        model_path=rlhf_task.output,
        model_name="korean-llm-v1",
        evaluation_metrics=evaluation_task.output,
        mlflow_tracking_uri=mlflow_tracking_uri
    )
    
    # ì¡°ê±´ë¶€ ë°°í¬ ì„¤ì •
    with dsl.Condition(evaluation_task.outputs["accuracy"] > 0.85):
        deploy_task = deploy_model_component(
            model_path=rlhf_task.output,
            model_name="korean-llm-v1"
        )

@dsl.component(
    base_image="python:3.9-slim",
    packages_to_install=["transformers", "datasets", "torch", "mlflow"]
)
def sft_training_component(
    base_model_path: str,
    instruction_data_path: str,
    model_output_path: str,
    learning_rate: float = 2e-5,
    num_epochs: int = 3
) -> str:
    """SFT í•™ìŠµ ì»´í¬ë„ŒíŠ¸ (ì¼ë°˜ Pod)"""
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
    from datasets import load_dataset
    from trl import SFTTrainer
    import mlflow
    
    mlflow.set_tracking_uri("http://mlflow-server:5000")
    
    with mlflow.start_run(run_name="sft-training"):
        # ëª¨ë¸ ë¡œë”©
        model = AutoModelForCausalLM.from_pretrained(base_model_path)
        tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        
        # ë°ì´í„°ì…‹ ë¡œë”©
        dataset = load_dataset("json", data_files=instruction_data_path, split="train")
        
        # í•™ìŠµ ì„¤ì •
        training_args = TrainingArguments(
            output_dir="/tmp/sft_output",
            num_train_epochs=num_epochs,
            per_device_train_batch_size=4,
            learning_rate=learning_rate,
            logging_steps=100,
            save_steps=1000,
            fp16=True,
        )
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì • ë° í•™ìŠµ
        trainer = SFTTrainer(
            model=model,
            tokenizer=tokenizer,
            args=training_args,
            train_dataset=dataset,
            dataset_text_field="text",
        )
        
        trainer.train()
        trainer.save_model(model_output_path)
        
        # ë©”íŠ¸ë¦­ ë¡œê¹…
        final_loss = trainer.state.log_history[-1]["train_loss"]
        mlflow.log_metrics({
            "final_loss": final_loss,
            "total_steps": trainer.state.global_step
        })
        
        return model_output_path

@dsl.component(
    base_image="python:3.9-slim",
    packages_to_install=["transformers", "datasets", "torch", "evaluate"]
)
def model_evaluation_component(
    model_path: str,
    test_data_path: str
) -> dict:
    """ëª¨ë¸ í‰ê°€ ì»´í¬ë„ŒíŠ¸"""
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from datasets import load_dataset
    import json
    
    # ëª¨ë¸ ë¡œë”©
    model = AutoModelForCausalLM.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©
    test_dataset = load_dataset("json", data_files=test_data_path, split="train")
    
    # í‰ê°€ ì‹¤í–‰
    correct_predictions = 0
    total_samples = min(100, len(test_dataset))
    
    for sample in test_dataset[:total_samples]:
        prompt = sample["prompt"]
        expected = sample["expected"]
        
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=128)
        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        if expected.lower() in prediction.lower():
            correct_predictions += 1
    
    accuracy = correct_predictions / total_samples
    
    results = {
        "accuracy": accuracy,
        "total_samples": total_samples,
        "correct_predictions": correct_predictions
    }
    
    return results
```

### 3.2 DVC ë° MLflow í†µí•©

```python
# mlops_integration.py
import dvc.api
import mlflow
from mlflow.tracking import MlflowClient
import boto3
from datetime import datetime

class MLOpsIntegration:
    def __init__(self, mlflow_uri, dvc_remote, s3_bucket):
        self.mlflow_client = MlflowClient(mlflow_uri)
        self.dvc_remote = dvc_remote
        self.s3_client = boto3.client('s3')
        self.s3_bucket = s3_bucket
        
    def setup_experiment_tracking(self, pipeline_run_id):
        """ì‹¤í—˜ ì¶”ì  ì„¤ì •"""
        # MLflow ì‹¤í—˜ ìƒì„±
        experiment_name = f"korean-llm-pipeline-{datetime.now().strftime('%Y%m%d')}"
        
        try:
            experiment = self.mlflow_client.get_experiment_by_name(experiment_name)
            experiment_id = experiment.experiment_id
        except:
            experiment_id = self.mlflow_client.create_experiment(experiment_name)
        
        # DVC íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        dvc_pipeline = {
            "stages": {
                "preprocess": {
                    "cmd": f"echo 'Preprocessing started for {pipeline_run_id}'",
                    "deps": ["s3://korean-llm-data/raw_corpus.jsonl"],
                    "outs": ["s3://korean-llm-data/processed_corpus.jsonl"]
                },
                "cpt_training": {
                    "cmd": f"echo 'CPT training started for {pipeline_run_id}'",
                    "deps": ["s3://korean-llm-data/processed_corpus.jsonl"],
                    "outs": ["s3://korean-llm-models/cpt_model"],
                    "metrics": ["metrics/cpt_metrics.json"]
                },
                "sft_training": {
                    "cmd": f"echo 'SFT training started for {pipeline_run_id}'",
                    "deps": ["s3://korean-llm-models/cpt_model"],
                    "outs": ["s3://korean-llm-models/sft_model"],
                    "metrics": ["metrics/sft_metrics.json"]
                },
                "rlhf_training": {
                    "cmd": f"echo 'RLHF training started for {pipeline_run_id}'",
                    "deps": ["s3://korean-llm-models/sft_model"],
                    "outs": ["s3://korean-llm-models/rlhf_model"],
                    "metrics": ["metrics/rlhf_metrics.json"]
                }
            }
        }
        
        return experiment_id, dvc_pipeline
    
    def log_stage_completion(self, stage_name, metrics, artifacts):
        """ìŠ¤í…Œì´ì§€ ì™„ë£Œ ë¡œê¹…"""
        with mlflow.start_run(run_name=f"{stage_name}-completion"):
            # ë©”íŠ¸ë¦­ ë¡œê¹…
            mlflow.log_metrics(metrics)
            
            # ì•„í‹°íŒ©íŠ¸ ë¡œê¹…
            for artifact_path in artifacts:
                if artifact_path.startswith("s3://"):
                    # S3ì—ì„œ ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œ í›„ MLflowì— ì—…ë¡œë“œ
                    local_path = self.download_s3_artifact(artifact_path)
                    mlflow.log_artifact(local_path)
                else:
                    mlflow.log_artifact(artifact_path)
            
            # DVC ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.update_dvc_metrics(stage_name, metrics)
    
    def download_s3_artifact(self, s3_path):
        """S3 ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ"""
        # s3://bucket/key í˜•íƒœì—ì„œ bucketê³¼ key ì¶”ì¶œ
        parts = s3_path.replace("s3://", "").split("/", 1)
        bucket = parts[0]
        key = parts[1]
        
        local_path = f"/tmp/{key.split('/')[-1]}"
        self.s3_client.download_file(bucket, key, local_path)
        return local_path
    
    def update_dvc_metrics(self, stage_name, metrics):
        """DVC ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        metrics_file = f"metrics/{stage_name}_metrics.json"
        
        # S3ì— ë©”íŠ¸ë¦­ íŒŒì¼ ì—…ë¡œë“œ
        import json
        import tempfile
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(metrics, f, indent=2)
            temp_path = f.name
        
        # S3ì— ì—…ë¡œë“œ
        s3_key = f"metrics/{stage_name}_metrics.json"
        self.s3_client.upload_file(temp_path, self.s3_bucket, s3_key)
    
    def register_final_model(self, model_path, model_name, evaluation_metrics):
        """ìµœì¢… ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡"""
        # MLflow ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
        model_uri = f"s3://{self.s3_bucket}/{model_path}"
        
        mlflow.register_model(
            model_uri=model_uri,
            name=model_name,
            tags={
                "accuracy": str(evaluation_metrics["accuracy"]),
                "pipeline_version": "v1.0",
                "training_framework": "ray+kubeflow"
            }
        )
        
        # ì„±ëŠ¥ ê¸°ì¤€ ì¶©ì¡± ì‹œ Production ìŠ¤í…Œì´ì§€ë¡œ ìŠ¹ê²©
        if evaluation_metrics["accuracy"] > 0.85:
            client = MlflowClient()
            client.transition_model_version_stage(
                name=model_name,
                version=1,
                stage="Production"
                         )
```

---

## 4. RayService ë° Argo Rollout ë°°í¬

### 4.1 RayService ì •ì˜

```yaml
# rayservice-korean-llm.yaml
apiVersion: ray.io/v1alpha1
kind: RayService
metadata:
  name: korean-llm-service
  namespace: kubeflow
spec:
  serviceUnhealthySecondThreshold: 300
  deploymentUnhealthySecondThreshold: 300
  
  rayClusterConfig:
    rayVersion: '2.8.0'
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
        num-cpus: '0'
      template:
        spec:
          containers:
          - name: ray-head
            image: rayproject/ray:2.8.0-py310-gpu
            ports:
            - containerPort: 6379
              name: gcs-server
            - containerPort: 8265
              name: dashboard
            - containerPort: 8000
              name: serve
            resources:
              limits:
                cpu: "4"
                memory: "16Gi"
              requests:
                cpu: "2"
                memory: "8Gi"
            env:
            - name: MODEL_PATH
              value: "s3://korean-llm-models/rlhf_model"
            volumeMounts:
            - name: model-cache
              mountPath: /model-cache
          volumes:
          - name: model-cache
            emptyDir: {}
    
    workerGroupSpecs:
    - replicas: 3
      minReplicas: 2
      maxReplicas: 10
      groupName: inference-workers
      rayStartParams:
        num-cpus: '8'
        num-gpus: '1'
      template:
        spec:
          containers:
          - name: ray-worker
            image: rayproject/ray:2.8.0-py310-gpu
            resources:
              limits:
                cpu: "16"
                memory: "32Gi"
                nvidia.com/gpu: "1"
              requests:
                cpu: "8"
                memory: "16Gi"
                nvidia.com/gpu: "1"
            volumeMounts:
            - name: model-cache
              mountPath: /model-cache
          volumes:
          - name: model-cache
            emptyDir: {}
  
  serveConfigV2: |
    applications:
    - name: korean-llm-app
      import_path: korean_llm_serve:app
      runtime_env:
        pip:
          - torch
          - transformers
          - unsloth @ git+https://github.com/unslothai/unsloth.git
          - fastapi
          - uvicorn
      deployments:
      - name: KoreanLLMDeployment
        num_replicas: 3
        autoscaling_config:
          min_replicas: 2
          max_replicas: 10
          target_num_ongoing_requests_per_replica: 2
        ray_actor_options:
          num_cpus: 4
          num_gpus: 1
          memory: 16000000000  # 16GB
        user_config:
          model_path: "s3://korean-llm-models/rlhf_model"
          max_batch_size: 8
          max_sequence_length: 2048
```

### 4.2 Ray Serve ì• í”Œë¦¬ì¼€ì´ì…˜

```python
# korean_llm_serve.py
from ray import serve
from ray.serve import deployment
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from unsloth import FastLanguageModel
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import asyncio
from typing import List, Optional
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(title="Korean LLM API", version="1.0.0")

class InferenceRequest(BaseModel):
    prompt: str
    max_length: int = 256
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 50
    do_sample: bool = True

class InferenceResponse(BaseModel):
    generated_text: str
    inference_time: float
    tokens_generated: int

@serve.deployment(
    num_replicas=3,
    autoscaling_config={
        "min_replicas": 2,
        "max_replicas": 10,
        "target_num_ongoing_requests_per_replica": 2,
        "metrics_interval_s": 10,
        "look_back_period_s": 30,
    },
    ray_actor_options={
        "num_cpus": 4,
        "num_gpus": 1,
        "memory": 16 * 1024 * 1024 * 1024,  # 16GB
    },
    health_check_period_s=10,
    health_check_timeout_s=30,
)
@serve.ingress(app)
class KoreanLLMDeployment:
    def __init__(self, model_path: str = "s3://korean-llm-models/rlhf_model"):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.device = None
        self._load_model()
    
    def _load_model(self):
        """ëª¨ë¸ ë¡œë”©"""
        logger.info(f"ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_path}")
        
        # GPU ë””ë°”ì´ìŠ¤ ì„¤ì •
        if torch.cuda.is_available():
            self.device = torch.device("cuda")
            logger.info(f"GPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©: {self.device}")
        else:
            self.device = torch.device("cpu")
            logger.info("CPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©")
        
        try:
            # Unsloth ëª¨ë¸ ë¡œë”©
            self.model, self.tokenizer = FastLanguageModel.from_pretrained(
                model_name=self.model_path,
                max_seq_length=2048,
                dtype=None,
                load_in_4bit=True,
            )
            
            # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •
            FastLanguageModel.for_inference(self.model)
            self.model = self.model.to(self.device)
            
            logger.info("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e
    
    @app.post("/generate", response_model=InferenceResponse)
    async def generate(self, request: InferenceRequest) -> InferenceResponse:
        """í…ìŠ¤íŠ¸ ìƒì„± API"""
        try:
            import time
            start_time = time.time()
            
            # ì…ë ¥ í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                request.prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1024
            ).to(self.device)
            
            # í…ìŠ¤íŠ¸ ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=request.max_length,
                    temperature=request.temperature,
                    top_p=request.top_p,
                    top_k=request.top_k,
                    do_sample=request.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            # ë””ì½”ë”©
            generated_text = self.tokenizer.decode(
                outputs[0],
                skip_special_tokens=True
            )
            
            # í”„ë¡¬í”„íŠ¸ ì œê±°
            response_text = generated_text[len(request.prompt):].strip()
            
            inference_time = time.time() - start_time
            tokens_generated = len(outputs[0]) - len(inputs.input_ids[0])
            
            return InferenceResponse(
                generated_text=response_text,
                inference_time=inference_time,
                tokens_generated=tokens_generated
            )
            
        except Exception as e:
            logger.error(f"ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.get("/health")
    async def health_check(self):
        """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
        if self.model is None:
            raise HTTPException(status_code=503, detail="Model not loaded")
        return {"status": "healthy", "model_path": self.model_path}
    
    @app.get("/metrics")
    async def get_metrics(self):
        """ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸"""
        return {
            "model_loaded": self.model is not None,
            "device": str(self.device),
            "model_path": self.model_path
        }

# Ray Serve ì•± ìƒì„±
app = KoreanLLMDeployment.bind()
```

### 4.3 Argo Rollout ì„¤ì •

```yaml
# argo-rollout-korean-llm.yaml
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: korean-llm-rollout
  namespace: kubeflow
spec:
  replicas: 6
  strategy:
    canary:
      maxSurge: "25%"
      maxUnavailable: 0
      steps:
      - setWeight: 20
      - pause: {duration: 2m}
      - setWeight: 40
      - pause: {duration: 2m}
      - setWeight: 60
      - pause: {duration: 2m}
      - setWeight: 80
      - pause: {duration: 2m}
      analysis:
        templates:
        - templateName: korean-llm-success-rate
        - templateName: korean-llm-latency
        args:
        - name: service-name
          value: korean-llm-service
        - name: canary-hash
          valueFrom:
            podTemplateHashValue: Latest
      trafficRouting:
        istio:
          virtualService:
            name: korean-llm-vs
          destinationRule:
            name: korean-llm-dr
            canarySubsetName: canary
            stableSubsetName: stable
  
  selector:
    matchLabels:
      app: korean-llm-inference
  
  template:
    metadata:
      labels:
        app: korean-llm-inference
    spec:
      containers:
      - name: korean-llm
        image: korean-llm-service:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            cpu: "4"
            memory: "16Gi"
            nvidia.com/gpu: "1"
          limits:
            cpu: "8"
            memory: "32Gi"
            nvidia.com/gpu: "1"
        env:
        - name: MODEL_VERSION
          value: "v1.0"
        - name: RAY_ADDRESS
          value: "ray://korean-llm-service-head:10001"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10

---
apiVersion: argoproj.io/v1alpha1
kind: AnalysisTemplate
metadata:
  name: korean-llm-success-rate
spec:
  args:
  - name: service-name
  - name: canary-hash
  metrics:
  - name: success-rate
    interval: 30s
    count: 5
    successCondition: result[0] >= 0.95
    failureLimit: 2
    provider:
      prometheus:
        address: http://prometheus:9090
        query: |
          sum(rate(http_requests_total{service="{{args.service-name}}",code=~"2.."}[2m])) /
          sum(rate(http_requests_total{service="{{args.service-name}}"}[2m]))

---
apiVersion: argoproj.io/v1alpha1
kind: AnalysisTemplate
metadata:
  name: korean-llm-latency
spec:
  args:
  - name: service-name
  metrics:
  - name: latency-p95
    interval: 30s
    count: 5
    successCondition: result[0] <= 2000  # 2ì´ˆ ì´í•˜
    failureLimit: 2
    provider:
      prometheus:
        address: http://prometheus:9090
        query: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket{service="{{args.service-name}}"}[2m])) by (le)
          ) * 1000
```

### 4.4 ë¡œë“œë°¸ëŸ°ì„œ ë° ëª¨ë‹ˆí„°ë§

```yaml
# loadbalancer-monitoring.yaml
apiVersion: v1
kind: Service
metadata:
  name: korean-llm-external-service
  namespace: kubeflow
spec:
  type: LoadBalancer
  selector:
    app: korean-llm-inference
  ports:
  - port: 80
    targetPort: 8000
    protocol: TCP
  sessionAffinity: None

---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: korean-llm-vs
  namespace: kubeflow
spec:
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: korean-llm-service
        subset: canary
  - route:
    - destination:
        host: korean-llm-service
        subset: stable

---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: korean-llm-dr
  namespace: kubeflow
spec:
  host: korean-llm-service
  subsets:
  - name: stable
    labels:
      app: korean-llm-inference
  - name: canary
    labels:
      app: korean-llm-inference

---
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: korean-llm-metrics
  namespace: kubeflow
spec:
  selector:
    matchLabels:
      app: korean-llm-inference
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
```

---

## 5. ì‹¤í–‰ ë° ìš´ì˜

### 5.1 íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```python
# run_integrated_pipeline.py
import kfp
from kfp import compiler
from korean_llm_pipeline import korean_llm_ray_pipeline
from mlops_integration import MLOpsIntegration
import asyncio

async def run_complete_mlops_pipeline():
    """ì™„ì „í•œ MLOps íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    
    # Kubeflow Pipelines í´ë¼ì´ì–¸íŠ¸
    kfp_client = kfp.Client(host="http://kubeflow-pipelines:8080")
    
    # MLOps í†µí•© í´ë˜ìŠ¤
    mlops = MLOpsIntegration(
        mlflow_uri="http://mlflow-server:5000",
        dvc_remote="s3://korean-llm-dvc",
        s3_bucket="korean-llm-artifacts"
    )
    
    # íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼
    compiler.Compiler().compile(
        pipeline_func=korean_llm_ray_pipeline,
        package_path="korean_llm_ray_pipeline.yaml"
    )
    
    # ì‹¤í—˜ ì„¤ì •
    experiment = kfp_client.create_experiment(
        name="Korean LLM Ray Integration",
        description="Complete MLOps pipeline with Kubeflow + Ray"
    )
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    run = kfp_client.run_pipeline(
        experiment_id=experiment.id,
        job_name="korean-llm-ray-training",
        pipeline_package_path="korean_llm_ray_pipeline.yaml",
        params={
            "raw_data_path": "s3://korean-llm-data/raw_corpus.jsonl",
            "instruction_data_path": "s3://korean-llm-data/instruction_dataset.jsonl",
            "preference_data_path": "s3://korean-llm-data/preference_dataset.jsonl",
            "model_name": "unsloth/Qwen2.5-7B-bnb-4bit",
            "cpt_learning_rate": 1e-5,
            "sft_learning_rate": 2e-5
        }
    )
    
    print(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘: {run.id}")
    
    # íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ëŒ€ê¸°
    kfp_client.wait_for_run_completion(run.id, timeout=86400)  # 24ì‹œê°„ íƒ€ì„ì•„ì›ƒ
    
    # ì‹¤í–‰ ê²°ê³¼ í™•ì¸
    run_detail = kfp_client.get_run(run.id)
    
    if run_detail.run.status == "Succeeded":
        print("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ!")
        
        # RayService ë°°í¬
        await deploy_rayservice()
        
        # Argo Rollout ì‹œì‘
        await start_argo_rollout()
        
    else:
        print(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {run_detail.run.status}")

async def deploy_rayservice():
    """RayService ë°°í¬"""
    import subprocess
    
    print("RayService ë°°í¬ ì‹œì‘...")
    
    # RayService ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì ìš©
    subprocess.run([
        "kubectl", "apply", "-f", "rayservice-korean-llm.yaml"
    ], check=True)
    
    # ì„œë¹„ìŠ¤ ì¤€ë¹„ ëŒ€ê¸°
    subprocess.run([
        "kubectl", "wait", "--for=condition=ready", 
        "rayservice/korean-llm-service", 
        "--timeout=600s", "-n", "kubeflow"
    ], check=True)
    
    print("RayService ë°°í¬ ì™„ë£Œ!")

async def start_argo_rollout():
    """Argo Rollout ì‹œì‘"""
    import subprocess
    
    print("Argo Rollout ì‹œì‘...")
    
    # Rollout ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì ìš©
    subprocess.run([
        "kubectl", "apply", "-f", "argo-rollout-korean-llm.yaml"
    ], check=True)
    
    # Rollout ìƒíƒœ í™•ì¸
    subprocess.run([
        "kubectl", "argo", "rollouts", "get", "rollout", 
        "korean-llm-rollout", "-n", "kubeflow"
    ], check=True)
    
    print("Argo Rollout ì‹œì‘ ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(run_complete_mlops_pipeline())
```

### 5.2 ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

```python
# monitoring_alerts.py
import asyncio
import aiohttp
import json
from datetime import datetime
import logging

class MLOpsMonitoring:
    def __init__(self):
        self.prometheus_url = "http://prometheus:9090"
        self.alertmanager_url = "http://alertmanager:9093"
        self.slack_webhook = "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
    
    async def check_pipeline_health(self):
        """íŒŒì´í”„ë¼ì¸ í—¬ìŠ¤ ì²´í¬"""
        checks = [
            self.check_kubeflow_pipeline_status(),
            self.check_ray_cluster_health(),
            self.check_rayservice_status(),
            self.check_model_performance()
        ]
        
        results = await asyncio.gather(*checks, return_exceptions=True)
        
        # ì•Œë¦¼ ë°œì†¡
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                await self.send_alert(f"Health check {i} failed: {result}")
    
    async def check_kubeflow_pipeline_status(self):
        """Kubeflow íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸"""
        async with aiohttp.ClientSession() as session:
            query = 'kubeflow_pipeline_runs_total{status="Failed"}'
            params = {"query": query}
            
            async with session.get(f"{self.prometheus_url}/api/v1/query", params=params) as resp:
                data = await resp.json()
                
                if data["data"]["result"]:
                    failed_runs = int(data["data"]["result"][0]["value"][1])
                    if failed_runs > 0:
                        raise Exception(f"{failed_runs} pipeline runs failed")
    
    async def check_ray_cluster_health(self):
        """Ray í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸"""
        async with aiohttp.ClientSession() as session:
            query = 'ray_cluster_nodes{state="alive"}'
            params = {"query": query}
            
            async with session.get(f"{self.prometheus_url}/api/v1/query", params=params) as resp:
                data = await resp.json()
                
                if not data["data"]["result"]:
                    raise Exception("No alive Ray cluster nodes found")
    
    async def check_rayservice_status(self):
        """RayService ìƒíƒœ í™•ì¸"""
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get("http://korean-llm-service:8000/health") as resp:
                    if resp.status != 200:
                        raise Exception(f"RayService health check failed: {resp.status}")
            except Exception as e:
                raise Exception(f"RayService unreachable: {e}")
    
    async def check_model_performance(self):
        """ëª¨ë¸ ì„±ëŠ¥ í™•ì¸"""
        async with aiohttp.ClientSession() as session:
            query = 'rate(http_requests_total{service="korean-llm-service",code=~"5.."}[5m])'
            params = {"query": query}
            
            async with session.get(f"{self.prometheus_url}/api/v1/query", params=params) as resp:
                data = await resp.json()
                
                if data["data"]["result"]:
                    error_rate = float(data["data"]["result"][0]["value"][1])
                    if error_rate > 0.05:  # 5% ì´ìƒ ì—ëŸ¬ìœ¨
                        raise Exception(f"High error rate detected: {error_rate:.2%}")
    
    async def send_alert(self, message):
        """Slack ì•Œë¦¼ ë°œì†¡"""
        alert_data = {
            "text": f"ğŸš¨ Korean LLM MLOps Alert",
            "attachments": [{
                "color": "danger",
                "fields": [{
                    "title": "Alert Message",
                    "value": message,
                    "short": False
                }, {
                    "title": "Timestamp",
                    "value": datetime.now().isoformat(),
                    "short": True
                }]
            }]
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(self.slack_webhook, json=alert_data) as resp:
                if resp.status != 200:
                    logging.error(f"Failed to send Slack alert: {resp.status}")

async def main():
    """ëª¨ë‹ˆí„°ë§ ë©”ì¸ ë£¨í”„"""
    monitoring = MLOpsMonitoring()
    
    while True:
        try:
            await monitoring.check_pipeline_health()
            print(f"Health check completed at {datetime.now()}")
        except Exception as e:
            print(f"Monitoring error: {e}")
        
        await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì²´í¬

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ê²°ë¡ 

ë³¸ ê°€ì´ë“œë¥¼ í†µí•´ Kubeflow Pipelinesë¥¼ ì›Œí¬í”Œë¡œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¡œ, Rayë¥¼ ë¶„ì‚° ì‹¤í–‰ ì—”ì§„ìœ¼ë¡œ í™œìš©í•˜ëŠ” ì™„ì „í•œ ì—”í„°í”„ë¼ì´ì¦ˆ MLOps ì‹œìŠ¤í…œì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

**ì£¼ìš” ì„±ê³¼**:

- ğŸ”„ **ëª…í™•í•œ ì—­í•  ë¶„ë¦¬**: KubeflowëŠ” ì›Œí¬í”Œë¡œ ê´€ë¦¬, RayëŠ” ë¶„ì‚° ì²˜ë¦¬
- âš¡ **ìµœì í™”ëœ ì„±ëŠ¥**: ê° ë‹¨ê³„ë³„ ë§ì¶¤í˜• ë¶„ì‚° ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
- ğŸ“Š **í†µí•© ê´€ë¦¬**: MLflow & DVCë¥¼ í†µí•œ ì‹¤í—˜ ì¶”ì  ë° ë²„ì „ ê´€ë¦¬
- ğŸš€ **ìë™í™”ëœ ë°°í¬**: RayService + Argo Rolloutìœ¼ë¡œ ì•ˆì „í•œ ë°°í¬

**ì—”í„°í”„ë¼ì´ì¦ˆ ê°€ì¹˜**:

- **í™•ì¥ì„±**: Rayì˜ ë™ì  ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì›Œí¬ë¡œë“œì— ë”°ë¥¸ ìë™ í™•ì¥
- **ì•ˆì •ì„±**: Kubeflowì˜ ì›Œí¬í”Œë¡œ ê´€ë¦¬ë¡œ ì¥ì•  ë³µêµ¬ ë° ì¬ì‹¤í–‰ ì§€ì›
- **íš¨ìœ¨ì„±**: ê° ìŠ¤í…ë³„ ìµœì í™”ëœ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ìœ¼ë¡œ ë¹„ìš© ì ˆê°
- **ê±°ë²„ë„ŒìŠ¤**: ì™„ì „í•œ ì¶”ì  ê°€ëŠ¥ì„±ê³¼ ì¬í˜„ ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸

**ì‹¤ë¬´ ì ìš© ì‹œë‚˜ë¦¬ì˜¤**:

- **ëŒ€ê·œëª¨ LLM í•™ìŠµ**: ìˆ˜ë°± GB ë°ì´í„°ì…‹ì˜ íš¨ìœ¨ì  ì²˜ë¦¬
- **ì‹¤ì‹œê°„ ì„œë¹™**: ê³ ì„±ëŠ¥ ì¶”ë¡  ì„œë¹„ìŠ¤ ì œê³µ
- **A/B í…ŒìŠ¤íŠ¸**: ì•ˆì „í•œ ëª¨ë¸ ë°°í¬ ë° ì„±ëŠ¥ ë¹„êµ
- **ì§€ì†ì  í•™ìŠµ**: ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ëª¨ë¸ ì§€ì† ê°œì„ 

ì´ ì‹œë¦¬ì¦ˆì˜ ë‹¤ë¥¸ ê¸€ ë³´ê¸°:

- [1í¸: Unslothë¥¼ í™œìš©í•œ í•œêµ­ì–´ íŠ¹í™” LLM í•™ìŠµ ì™„ì „ ê°€ì´ë“œ](#)
- [2í¸: ì¿ ë²„ë„¤í‹°ìŠ¤ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•](#)
- [3í¸: Kubeflow ë° MLOps í”„ë ˆì„ì›Œí¬ í™œìš©](#)
- [4í¸: Rayì™€ KubeRayë¥¼ í™œìš©í•œ ëŒ€ê·œëª¨ ë¶„ì‚° í•™ìŠµ](#)
- 5í¸: Kubeflow + Ray í†µí•© ì—”í„°í”„ë¼ì´ì¦ˆ MLOps ì‹œìŠ¤í…œ (í˜„ì¬ ê¸€)

Kubeflowì™€ Rayì˜ ê°•ë ¥í•œ ì¡°í•©ìœ¼ë¡œ í•œêµ­ì–´ íŠ¹í™” LLM ê°œë°œì˜ ìƒˆë¡œìš´ í‘œì¤€ì„ ê²½í—˜í•´ë³´ì„¸ìš”! ğŸš€

---

## ê°œìš”

ë³¸ ê°€ì´ë“œëŠ” [Unsloth+TRL í•œêµ­ì–´ LLM í•™ìŠµ ì‹œë¦¬ì¦ˆ](#)ì˜ 5í¸ìœ¼ë¡œ, Kubeflow Pipelinesë¥¼ ì›Œí¬í”Œë¡œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¡œ, Rayë¥¼ ìŠ¤í… ë‚´ë¶€ì˜ ë¶„ì‚° ì‹¤í–‰ ì—”ì§„ìœ¼ë¡œ í™œìš©í•˜ëŠ” ì™„ì „í•œ ì—”í„°í”„ë¼ì´ì¦ˆ MLOps ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

**ì•„í‚¤í…ì²˜ ê°œìš”**:

```
Kubeflow Pipeline DAG
 â”œâ”€ Preprocess ( RayJob â†’ ray.data )  â”€â”
 â”œâ”€ CPT ( RayJob â†’ ray.train )        â”‚  ëª¨ë‘ MLflow & DVC ì—°ë™
 â”œâ”€ SFT ( RayJob â†’ ray.train )        â”œâ”€ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ëŠ” S3 â†’ MLflow Registry
 â”œâ”€ RLHF ( RayJob â†’ ray.train/tune )  â”‚
 â””â”€ Evaluate ( ì¼ë°˜ Pod or RayJob )   â”˜
Serve ë‹¨ê³„: RayService + Argo Rollout (A/B) â†’ ì™¸ë¶€ LoadBalancer
```

**í•™ìŠµ ëª©í‘œ**:

- ğŸ”„ **Kubeflow Pipelines**: DAG ê¸°ë°˜ ì›Œí¬í”Œë¡œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- âš¡ **Ray Integration**: ê° ìŠ¤í…ì—ì„œ ë¶„ì‚° ì²˜ë¦¬ ì—”ì§„ìœ¼ë¡œ í™œìš©
- ğŸ“Š **MLflow & DVC**: ì‹¤í—˜ ì¶”ì  ë° ë°ì´í„° ë²„ì „ ê´€ë¦¬ í†µí•©
- ğŸš€ **RayService**: ê³ ì„±ëŠ¥ ëª¨ë¸ ì„œë¹™ ì¸í”„ë¼
- ğŸ”„ **Argo Rollout**: A/B í…ŒìŠ¤íŠ¸ ë° ì ì§„ì  ë°°í¬

---
