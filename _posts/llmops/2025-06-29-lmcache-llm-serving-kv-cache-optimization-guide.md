---
title: "LMCache: LLM ì„œë¹™ ì„±ëŠ¥ì„ 3-10ë°° í–¥ìƒì‹œí‚¤ëŠ” KV ìºì‹œ ìµœì í™” ì†”ë£¨ì…˜"
excerpt: "LMCacheëŠ” KV ìºì‹œ ì¬ì‚¬ìš©ì„ í†µí•´ LLM ì„œë¹™ ì„±ëŠ¥ì„ ëŒ€í­ ê°œì„ í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ì—”ì§„ì…ë‹ˆë‹¤. vLLMê³¼ í†µí•©í•˜ì—¬ TTFT ê°ì†Œì™€ ì²˜ë¦¬ëŸ‰ ì¦ëŒ€ë¥¼ ì‹¤í˜„í•©ë‹ˆë‹¤."
seo_title: "LMCache: LLM KV ìºì‹œ ìµœì í™”ë¡œ 3-10ë°° ì„±ëŠ¥ í–¥ìƒ ê°€ì´ë“œ - Thaki Cloud"
seo_description: "LMCacheë¡œ LLM ì„œë¹™ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ì„¸ìš”. KV ìºì‹œ ì¬ì‚¬ìš©, vLLM í†µí•©, RAG ìµœì í™”ë¥¼ í†µí•œ ì‹¤ì „ ê°€ì´ë“œì™€ êµ¬í˜„ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤."
date: 2025-06-29
last_modified_at: 2025-06-29
categories:
  - llmops
tags:
  - LMCache
  - vLLM
  - KV-Cache
  - LLM-Serving
  - Performance-Optimization
  - RAG
  - GPU-Optimization
  - TTFT
  - Throughput
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
header:
  teaser: "/assets/images/thumbnails/post-thumbnail.jpg"
  overlay_image: "/assets/images/headers/post-header.jpg"
  overlay_filter: 0.5
canonical_url: "https://thakicloud.github.io/llmops/lmcache-llm-serving-kv-cache-optimization-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 12ë¶„

## ì„œë¡ 

LLM(Large Language Model) ì„œë¹™ì—ì„œ ê°€ì¥ í° ë„ì „ ê³¼ì œ ì¤‘ í•˜ë‚˜ëŠ” **TTFT(Time To First Token) ì§€ì—°**ê³¼ **ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œì˜ ì„±ëŠ¥ ì €í•˜**ì…ë‹ˆë‹¤. íŠ¹íˆ RAG(Retrieval-Augmented Generation)ì´ë‚˜ ë©€í‹°ë¼ìš´ë“œ ëŒ€í™”ì™€ ê°™ì´ ë°˜ë³µì ì¸ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ” ë™ì¼í•œ KV ìºì‹œë¥¼ ë°˜ë³µ ê³„ì‚°í•˜ì—¬ GPU ìì›ì„ ë‚­ë¹„í•˜ê²Œ ë©ë‹ˆë‹¤.

**LMCache**ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°œë°œëœ í˜ì‹ ì ì¸ LLM ì„œë¹™ ì—”ì§„ í™•ì¥ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤. "Redis for LLMs"ë¼ëŠ” ìŠ¬ë¡œê±´ì²˜ëŸ¼, ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ì˜ KV ìºì‹œë¥¼ ë‹¤ì–‘í•œ ì €ì¥ ìœ„ì¹˜ì— ë³´ê´€í•˜ê³  ì¬í™œìš©í•¨ìœ¼ë¡œì¨ **3-10ë°°ì˜ ì„±ëŠ¥ í–¥ìƒ**ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

## LMCache í•µì‹¬ ê°œë…

### KV ìºì‹œ ì¬ì‚¬ìš©ì˜ í˜ì‹ 

ê¸°ì¡´ LLM ì„œë¹™ ì‹œìŠ¤í…œì˜ í•œê³„ì :
- **ë°˜ë³µ ê³„ì‚° ë‚­ë¹„**: ë™ì¼í•œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë§¤ë²ˆ KV ìºì‹œë¥¼ ìƒˆë¡œ ê³„ì‚°
- **ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì„±**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ìºì‹œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ  
- **í™•ì¥ì„± ë¶€ì¡±**: ì„œë¡œ ë‹¤ë¥¸ ì„œë¹™ ì¸ìŠ¤í„´ìŠ¤ ê°„ ìºì‹œ ê³µìœ  ë¶ˆê°€

LMCacheì˜ í˜ì‹ ì  ì ‘ê·¼:
- **ë‹¤ì¸µ ìºì‹œ ì €ì¥**: GPU, CPU DRAM, ë¡œì»¬ ë””ìŠ¤í¬ì— ê±¸ì¹œ ê³„ì¸µì  ì €ì¥
- **ë²”ìš© ì¬ì‚¬ìš©**: prefixë¿ë§Œ ì•„ë‹ˆë¼ ì„ì˜ì˜ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ ì¬ì‚¬ìš© ê°€ëŠ¥
- **ì¸ìŠ¤í„´ìŠ¤ ê°„ ê³µìœ **: ì„œë¡œ ë‹¤ë¥¸ ì„œë¹™ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ê°„ KV ìºì‹œ ê³µìœ 

### ì£¼ìš” ì•„í‚¤í…ì²˜ íŠ¹ì§•

```python
# LMCache ìºì‹œ ê³„ì¸µ êµ¬ì¡°
GPU Memory (ìµœê³ ì†)
    â†“
CPU DRAM (ê³ ì†)
    â†“  
Local Disk (ì €ì†, ëŒ€ìš©ëŸ‰)
    â†“
P2P Network Sharing (ë¶„ì‚° ìºì‹œ)
```

## ì£¼ìš” ê¸°ëŠ¥ ë° íŠ¹ì§•

### 1. ê³ ì„±ëŠ¥ CPU KV ìºì‹œ ì˜¤í”„ë¡œë”©

**CPU DRAM í™œìš©**:
- GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ CPU DRAMìœ¼ë¡œ ìºì‹œ ì˜¤í”„ë¡œë”©
- ì§€ëŠ¥ì ì¸ ìºì‹œ ê³„ì¸µ ê´€ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€ë¹„ ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”

**êµ¬í˜„ ì˜ˆì‹œ**:
```python
# LMCache CPU ì˜¤í”„ë¡œë”© ì„¤ì •
lmcache_config = {
    "cpu_offload_gb": 32,  # 32GB CPU DRAM ì‚¬ìš©
    "gpu_cache_size_gb": 8,  # 8GB GPU ìºì‹œ ìœ ì§€
    "offload_strategy": "lru"  # LRU ìºì‹œ êµì²´ ì •ì±…
}
```

### 2. ë¶„ë¦¬í˜• í”„ë¦¬í•„(Disaggregated Prefill)

**í”„ë¦¬í•„ ìµœì í™”**:
- í”„ë¦¬í•„ ë‹¨ê³„ì™€ ë””ì½”ë”© ë‹¨ê³„ì˜ ë¶„ë¦¬ ì²˜ë¦¬
- í”„ë¦¬í•„ ê²°ê³¼ì˜ íš¨ìœ¨ì ì¸ ìºì‹± ë° ì¬ì‚¬ìš©
- ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”ë¡œ ì²˜ë¦¬ëŸ‰ í–¥ìƒ

### 3. P2P KV ìºì‹œ ê³µìœ 

**ë¶„ì‚° ìºì‹œ ë„¤íŠ¸ì›Œí¬**:
- ì—¬ëŸ¬ ì„œë¹™ ì¸ìŠ¤í„´ìŠ¤ ê°„ ì‹¤ì‹œê°„ ìºì‹œ ê³µìœ 
- ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
- í´ëŸ¬ìŠ¤í„° ì „ì²´ì˜ ìºì‹œ íš¨ìœ¨ì„± ê·¹ëŒ€í™”

## ì„±ëŠ¥ ê°œì„  íš¨ê³¼

### ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

LMCache + vLLM ì¡°í•©ìœ¼ë¡œ ë‹¬ì„±í•œ ì„±ëŠ¥ ê°œì„ :

| ì‹œë‚˜ë¦¬ì˜¤ | TTFT ê°œì„  | ì²˜ë¦¬ëŸ‰ ê°œì„  | GPU ì‚¬ì´í´ ì ˆì•½ |
|---------|----------|-----------|---------------|
| **ë©€í‹°ë¼ìš´ë“œ QA** | 5-8ë°° ë‹¨ì¶• | 3-5ë°° ì¦ê°€ | 60-80% ì ˆì•½ |
| **RAG ì‹œìŠ¤í…œ** | 3-6ë°° ë‹¨ì¶• | 4-7ë°° ì¦ê°€ | 50-70% ì ˆì•½ |
| **ë¬¸ì„œ ìš”ì•½** | 4-10ë°° ë‹¨ì¶• | 2-4ë°° ì¦ê°€ | 40-60% ì ˆì•½ |
| **ì½”ë“œ ìƒì„±** | 2-5ë°° ë‹¨ì¶• | 3-6ë°° ì¦ê°€ | 45-65% ì ˆì•½ |

### ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€

**RAG ì‹œìŠ¤í…œ ìµœì í™”**:
```python
# ê¸°ì¡´: ë§¤ë²ˆ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒˆë¡œ ì²˜ë¦¬
context = retrieve_documents(query)
response = llm.generate(f"{context}\n\nQuestion: {query}")

# LMCache ì ìš©: ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ì¬ì‚¬ìš©
cached_context_id = lmcache.cache_context(context)
response = llm.generate_with_cache(cached_context_id, query)
```

## ì„¤ì¹˜ ë° ì„¤ì • ê°€ì´ë“œ

### ê¸°ë³¸ ì„¤ì¹˜

```bash
# PyPIë¥¼ í†µí•œ ì„¤ì¹˜
pip install lmcache

# ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜
git clone https://github.com/LMCache/LMCache.git
cd LMCache
pip install -e .
```

### vLLMê³¼ í†µí•©

```python
from lmcache import LMCacheEngine
from vllm import LLM

# LMCache ì„¤ì •
cache_config = {
    "backend": "redis",  # ë˜ëŠ” "local", "distributed"
    "host": "localhost",
    "port": 6379,
    "max_memory_gb": 16
}

# vLLM + LMCache ì´ˆê¸°í™”
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    tensor_parallel_size=2,
    cache_config=cache_config
)

# ìºì‹œ ì—”ì§„ ì—°ê²°
cache_engine = LMCacheEngine(llm, cache_config)
```

### Dockerë¥¼ í†µí•œ ë°°í¬

```bash
# ì‚¬ì „ ë¹Œë“œëœ Docker ì´ë¯¸ì§€ ì‚¬ìš©
docker pull lmcache/vllm:latest

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
    --gpus all \
    --name lmcache-vllm \
    -p 8000:8000 \
    -e CACHE_BACKEND=redis \
    -e REDIS_HOST=redis-server \
    lmcache/vllm:latest
```

## ê³ ê¸‰ ì„¤ì • ë° ìµœì í™”

### ìºì‹œ ì „ëµ ì„¤ì •

```python
# ê³ ê¸‰ ìºì‹œ ì„¤ì •
advanced_config = {
    "cache_policy": {
        "eviction_strategy": "lfu",  # LFU, LRU, FIFO
        "ttl_seconds": 3600,  # ìºì‹œ ë§Œë£Œ ì‹œê°„
        "compression": "zstd",  # ì••ì¶• ì•Œê³ ë¦¬ì¦˜
        "replication_factor": 2  # ë³µì œë³¸ ê°œìˆ˜
    },
    "performance": {
        "prefetch_enabled": True,
        "batch_size": 32,
        "worker_threads": 4
    }
}
```

### ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­

```python
# ìºì‹œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
metrics = cache_engine.get_metrics()
print(f"Cache hit rate: {metrics['hit_rate']:.2%}")
print(f"Memory usage: {metrics['memory_usage_gb']:.1f}GB")
print(f"Average latency: {metrics['avg_latency_ms']:.1f}ms")
```

## ì‹¤ì „ ì‚¬ìš© ì‚¬ë¡€

### 1. ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œ

```python
class ConversationSystem:
    def __init__(self):
        self.llm = LMCacheEngine(...)
        self.conversation_history = []
    
    def chat(self, user_message):
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ìºì‹±
        history_key = self.cache_conversation_context()
        
        # ìƒˆ ë©”ì‹œì§€ì™€ í•¨ê»˜ ìƒì„±
        response = self.llm.generate_with_cache(
            cache_key=history_key,
            new_input=user_message
        )
        
        self.conversation_history.append({
            "user": user_message,
            "assistant": response
        })
        
        return response
```

### 2. RAG ì‹œìŠ¤í…œ ìµœì í™”

```python
class OptimizedRAG:
    def __init__(self):
        self.retriever = DocumentRetriever()
        self.llm = LMCacheEngine(...)
        self.document_cache = {}
    
    def answer(self, question):
        # ë¬¸ì„œ ê²€ìƒ‰
        docs = self.retriever.retrieve(question)
        
        # ë¬¸ì„œ ë‚´ìš© ìºì‹±
        doc_cache_keys = []
        for doc in docs:
            if doc.id not in self.document_cache:
                cache_key = self.llm.cache_document(doc.content)
                self.document_cache[doc.id] = cache_key
            doc_cache_keys.append(self.document_cache[doc.id])
        
        # ìºì‹œëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë‹µë³€ ìƒì„±
        return self.llm.generate_with_context_cache(
            context_keys=doc_cache_keys,
            question=question
        )
```

### 3. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```python
class BatchProcessor:
    def __init__(self):
        self.llm = LMCacheEngine(...)
    
    def process_batch(self, requests):
        # ê³µí†µ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìºì‹±
        template_key = self.llm.cache_prompt_template(
            "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{text}\n\nìš”ì•½:"
        )
        
        # ë°°ì¹˜ ì²˜ë¦¬
        results = []
        for request in requests:
            result = self.llm.generate_with_template(
                template_key=template_key,
                variables={"text": request.text}
            )
            results.append(result)
        
        return results
```

## ì»¤ë®¤ë‹ˆí‹° ë° ê°œë°œ ì°¸ì—¬

### ì»¤ë®¤ë‹ˆí‹° ë¯¸íŒ…

**ì •ê¸° ë¯¸íŒ… ì¼ì •**:
- **í™”ìš”ì¼ ì˜¤ì „ 9:00 (PT)** - ê°œë°œì ì¤‘ì‹¬ ë¯¸íŒ…
- **í™”ìš”ì¼ ì˜¤í›„ 6:30 (PT)** - ì‚¬ìš©ì ì¤‘ì‹¬ ë¯¸íŒ…
- **ê²©ì£¼ êµëŒ€ ì§„í–‰** - ê¸€ë¡œë²Œ ì°¸ì—¬ì ë°°ë ¤

### ê¸°ì—¬ ë°©ë²•

```bash
# ê°œë°œ í™˜ê²½ ì„¤ì •
git clone https://github.com/LMCache/LMCache.git
cd LMCache

# ê°œë°œ ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements/dev.txt

# ì‚¬ì „ ì»¤ë°‹ í›… ì„¤ì •
pre-commit install

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/
```

### ì£¼ìš” ê¸°ì—¬ ì˜ì—­

1. **ì„±ëŠ¥ ìµœì í™”**: ìºì‹œ ì•Œê³ ë¦¬ì¦˜ ê°œì„ 
2. **ë°±ì—”ë“œ í™•ì¥**: ìƒˆë¡œìš´ ì €ì¥ì†Œ ë°±ì—”ë“œ ì¶”ê°€
3. **ëª¨ë‹ˆí„°ë§**: ë©”íŠ¸ë¦­ ë° ëŒ€ì‹œë³´ë“œ ê°œë°œ
4. **ë¬¸ì„œí™”**: ì‚¬ìš©ì ê°€ì´ë“œ ë° API ë¬¸ì„œ ì‘ì„±

## ë¡œë“œë§µ ë° í–¥í›„ ê³„íš

### ë‹¨ê¸° ê³„íš (2025 Q3-Q4)

- **Kubernetes í†µí•©**: í´ëŸ¬ìŠ¤í„° í™˜ê²½ì—ì„œì˜ ìë™ ìŠ¤ì¼€ì¼ë§
- **ë©€í‹° GPU ìµœì í™”**: GPU ê°„ ìºì‹œ ê³µìœ  ê°œì„ 
- **ì••ì¶• ì•Œê³ ë¦¬ì¦˜**: ë” íš¨ìœ¨ì ì¸ ìºì‹œ ì••ì¶• ë°©ì‹ ë„ì…

### ì¤‘ì¥ê¸° ê³„íš (2026)

- **ë¶„ì‚° ìºì‹œ í´ëŸ¬ìŠ¤í„°**: ëŒ€ê·œëª¨ ë¶„ì‚° í™˜ê²½ ì§€ì›
- **AI ê¸°ë°˜ ìºì‹œ ì˜ˆì¸¡**: ë¨¸ì‹ ëŸ¬ë‹ì„ í™œìš©í•œ ì§€ëŠ¥ì  ìºì‹œ ê´€ë¦¬
- **í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ**: ì£¼ìš” í´ë¼ìš°ë“œ í”Œë«í¼ê³¼ì˜ ë„¤ì´í‹°ë¸Œ í†µí•©

## ê´€ë ¨ ì—°êµ¬ ë° ë…¼ë¬¸

LMCacheëŠ” ë‹¤ìŒ ì—°êµ¬ ë…¼ë¬¸ë“¤ì— ê¸°ë°˜í•˜ì—¬ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤:

1. **CacheGen** (SIGCOMM 2024): KV ìºì‹œ ì••ì¶• ë° ìŠ¤íŠ¸ë¦¬ë° ê¸°ìˆ 
2. **Content Delivery Network for LLMs** (2024): LLMì„ ìœ„í•œ CDN ê°œë… ë„ì…
3. **CacheBlend** (EuroSys 2025): RAGë¥¼ ìœ„í•œ ìºì‹œëœ ì§€ì‹ ìœµí•© ê¸°ìˆ 

```bibtex
@inproceedings{liu2024cachegen,
  title={Cachegen: Kv cache compression and streaming for fast large language model serving},
  author={Liu, Yuhan and Li, Hanchen and others},
  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},
  year={2024}
}
```

## ê²°ë¡ 

LMCacheëŠ” LLM ì„œë¹™ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ê²Œì„ ì²´ì¸ì €ì…ë‹ˆë‹¤. KV ìºì‹œ ì¬ì‚¬ìš©ì´ë¼ëŠ” í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ í†µí•´ **ì‹¤ì§ˆì ì´ê³  ì¸¡ì • ê°€ëŠ¥í•œ ì„±ëŠ¥ ê°œì„ **ì„ ì œê³µí•˜ë©°, íŠ¹íˆ RAG, ë©€í‹°í„´ ëŒ€í™”, ë¬¸ì„œ ì²˜ë¦¬ì™€ ê°™ì€ ì‹¤ë¬´ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ íƒì›”í•œ íš¨ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤.

**ì£¼ìš” ì¥ì  ìš”ì•½**:
- âœ… **3-10ë°° ì„±ëŠ¥ í–¥ìƒ**: TTFT ë‹¨ì¶• ë° ì²˜ë¦¬ëŸ‰ ì¦ëŒ€
- âœ… **GPU ìì› ì ˆì•½**: 40-80%ì˜ GPU ì‚¬ì´í´ ì ˆì•½
- âœ… **ì‰¬ìš´ í†µí•©**: ê¸°ì¡´ vLLM í™˜ê²½ì— ê°„ë‹¨í•œ ì„¤ì •ìœ¼ë¡œ ì ìš©
- âœ… **í™•ì¥ì„±**: ë¶„ì‚° í™˜ê²½ì—ì„œì˜ ìºì‹œ ê³µìœ  ì§€ì›
- âœ… **í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°**: ì§€ì†ì ì¸ ê°œë°œ ë° ì§€ì›

LLM ì„œë¹™ ì„±ëŠ¥ ìµœì í™”ë¥¼ ê³ ë¯¼í•˜ê³  ìˆë‹¤ë©´, LMCacheëŠ” ë°˜ë“œì‹œ ê²€í† í•´ë³¼ ê°€ì¹˜ê°€ ìˆëŠ” ì†”ë£¨ì…˜ì…ë‹ˆë‹¤. íŠ¹íˆ ë°˜ë³µì ì¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë§ì€ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œëŠ” ì¦‰ì‹œ íš¨ê³¼ë¥¼ ì²´ê°í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

**ë‹¤ìŒ ë‹¨ê³„**: [LMCache ê³µì‹ ë¬¸ì„œ](https://github.com/LMCache/LMCache)ë¥¼ ì°¸ì¡°í•˜ì—¬ ì—¬ëŸ¬ë¶„ì˜ í™˜ê²½ì— ë§ëŠ” ì„¤ì •ì„ ì‹œì‘í•´ë³´ì„¸ìš”!

---

ğŸ“š **ì°¸ê³  ìë£Œ**:
- [LMCache GitHub Repository](https://github.com/LMCache/LMCache)
- [LMCache ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://lmcache.ai/)
- [vLLM Production Stack](https://github.com/LMCache/LMCache)
- [ì»¤ë®¤ë‹ˆí‹° Slack ì±„ë„](https://github.com/LMCache/LMCache) 