---
title: "vLLMìœ¼ë¡œ Qwen3-Embedding ëŒ€ê·œëª¨ ì²˜ë¦¬í•˜ê¸°: 100ë§Œ+ ë°ì´í„° ì‹¤ì „ ê°€ì´ë“œ"
date: 2025-06-06
categories: 
  - llmops
  - embedding
tags: 
  - vllm
  - qwen3-embedding
  - large-scale
  - vector-database
  - similarity-search
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
---

vLLM 0.8.5ë¶€í„° ì§€ì›ë˜ëŠ” Qwen3-Embedding ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ì™€ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ìƒì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. 100ë§Œ ê°œ ì´ìƒì˜ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‹¤ì „ ì˜ˆì œì™€ ìµœì í™” ë°©ë²•ì„ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸš€ ê¸°ë³¸ ì½”ë“œ ë¶„ì„

ë¨¼ì € ì œê³µëœ ê¸°ë³¸ ì½”ë“œë¥¼ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤:

```python
# Requires vllm>=0.8.5
import torch
import vllm
from vllm import LLM

def get_detailed_instruct(task_description: str, query: str) -> str:
    return f'Instruct: {task_description}\nQuery:{query}'

# Each query must come with a one-sentence instruction that describes the task
task = 'Given a web search query, retrieve relevant passages that answer the query'

queries = [
    get_detailed_instruct(task, 'What is the capital of China?'),
    get_detailed_instruct(task, 'Explain gravity')
]
# No need to add instruction for retrieval documents
documents = [
    "The capital of China is Beijing.",
    "Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun."
]
input_texts = queries + documents

model = LLM(model="Qwen/Qwen3-Embedding-0.6B", task="embed")

outputs = model.embed(input_texts)
embeddings = torch.tensor([o.outputs.embedding for o in outputs])
scores = (embeddings[:2] @ embeddings[2:].T)
print(scores.tolist())
```

### ì½”ë“œ êµ¬ì„± ìš”ì†Œ ë¶„ì„

#### 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì˜ì¡´ì„±

```python
# vLLM 0.8.5 ì´ìƒ í•„ìˆ˜
import torch
import vllm
from vllm import LLM
```

**í•µì‹¬ í¬ì¸íŠ¸:**
- vLLM 0.8.5ë¶€í„° ì„ë² ë”© íƒœìŠ¤í¬ ì§€ì›
- PyTorch ê¸°ë°˜ í…ì„œ ì—°ì‚° í™œìš©
- GPU ê°€ì†í™”ëœ ëŒ€ê·œëª¨ ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥

#### 2. ì§€ì‹œë¬¸ í¬ë§·íŒ… í•¨ìˆ˜

```python
def get_detailed_instruct(task_description: str, query: str) -> str:
    return f'Instruct: {task_description}\nQuery:{query}'
```

**ì—­í• :**
- ì¿¼ë¦¬ì— íƒœìŠ¤í¬ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
- Qwen3-Embeddingì˜ ì„±ëŠ¥ ìµœì í™”
- ê²€ìƒ‰ ì˜ë„ë¥¼ ëª…í™•íˆ ì „ë‹¬

#### 3. ë°ì´í„° ì¤€ë¹„

```python
task = 'Given a web search query, retrieve relevant passages that answer the query'

queries = [
    get_detailed_instruct(task, 'What is the capital of China?'),
    get_detailed_instruct(task, 'Explain gravity')
]

documents = [
    "The capital of China is Beijing.",
    "Gravity is a force that attracts two bodies towards each other..."
]
```

**íŠ¹ì§•:**
- ì¿¼ë¦¬ëŠ” ì§€ì‹œë¬¸ í¬í•¨
- ë¬¸ì„œëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
- ë¹„ëŒ€ì¹­ ê²€ìƒ‰ íŒ¨ëŸ¬ë‹¤ì„ ì ìš©

#### 4. ëª¨ë¸ ì´ˆê¸°í™” ë° ì„ë² ë”© ìƒì„±

```python
model = LLM(model="Qwen/Qwen3-Embedding-0.6B", task="embed")
outputs = model.embed(input_texts)
embeddings = torch.tensor([o.outputs.embedding for o in outputs])
```

**ì²˜ë¦¬ ê³¼ì •:**
- ì„ë² ë”© ì „ìš© íƒœìŠ¤í¬ë¡œ ëª¨ë¸ ë¡œë“œ
- ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
- í…ì„œ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì—°ì‚° ì¤€ë¹„

#### 5. ìœ ì‚¬ë„ ê³„ì‚°

```python
scores = (embeddings[:2] @ embeddings[2:].T)
print(scores.tolist())
```

**ì—°ì‚° ì„¤ëª…:**
- í–‰ë ¬ ê³±ì…ˆìœ¼ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
- ì¿¼ë¦¬ ì„ë² ë”© Ã— ë¬¸ì„œ ì„ë² ë”©^T
- ê²°ê³¼: 2Ã—2 ìœ ì‚¬ë„ í–‰ë ¬

## ğŸ­ ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ì•„í‚¤í…ì²˜

### ì‹œìŠ¤í…œ êµ¬ì„±ë„

```python
import numpy as np
import faiss
import pandas as pd
from typing import List, Dict, Tuple
import logging
from tqdm import tqdm
import gc
import psutil
from concurrent.futures import ThreadPoolExecutor
import pickle
import time

class LargeScaleEmbeddingProcessor:
    """100ë§Œ+ ë¬¸ì„œë¥¼ ìœ„í•œ ëŒ€ê·œëª¨ ì„ë² ë”© ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen3-Embedding-0.6B",
        batch_size: int = 1000,
        max_workers: int = 4,
        cache_dir: str = "./embedding_cache"
    ):
        self.model_name = model_name
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.cache_dir = cache_dir
        
        # vLLM ëª¨ë¸ ì´ˆê¸°í™”
        self.model = LLM(
            model=model_name,
            task="embed",
            max_model_len=8192,  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì í™”
            gpu_memory_utilization=0.8,  # GPU ë©”ëª¨ë¦¬ íš¨ìœ¨í™”
            tensor_parallel_size=torch.cuda.device_count() if torch.cuda.is_available() else 1
        )
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ë²¡í„° ì¸ë±ìŠ¤ (Faiss)
        self.index = None
        self.document_metadata = []
        
    def get_detailed_instruct(self, task_description: str, query: str) -> str:
        """ì¿¼ë¦¬ ì§€ì‹œë¬¸ í¬ë§·íŒ…"""
        return f'Instruct: {task_description}\nQuery:{query}'
```

### ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ

```python
    def process_documents_batch(
        self, 
        documents: List[str],
        start_idx: int = 0
    ) -> Tuple[np.ndarray, List[Dict]]:
        """ë¬¸ì„œ ë°°ì¹˜ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            process = psutil.Process()
            memory_before = process.memory_info().rss / 1024 / 1024  # MB
            
            self.logger.info(f"Processing batch {start_idx//self.batch_size + 1}: {len(documents)} documents")
            
            # vLLM ì„ë² ë”© ìƒì„±
            outputs = self.model.embed(documents)
            embeddings = np.array([o.outputs.embedding for o in outputs])
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = [
                {
                    "doc_id": start_idx + i,
                    "text": doc[:200] + "..." if len(doc) > 200 else doc,
                    "length": len(doc),
                    "timestamp": time.time()
                }
                for i, doc in enumerate(documents)
            ]
            
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            self.logger.info(f"Batch processed. Memory usage: {memory_after - memory_before:.2f} MB increase")
            
            return embeddings, metadata
            
        except Exception as e:
            self.logger.error(f"Error processing batch starting at {start_idx}: {str(e)}")
            raise

    def process_large_dataset(
        self, 
        documents: List[str],
        save_checkpoint_every: int = 10000
    ) -> str:
        """ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì²˜ë¦¬ ë° ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•"""
        
        total_docs = len(documents)
        self.logger.info(f"Starting processing of {total_docs:,} documents")
        
        # ì„ë² ë”© ì°¨ì› í™•ì¸ (ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ)
        sample_batch = documents[:min(10, len(documents))]
        sample_embeddings, _ = self.process_documents_batch(sample_batch)
        embedding_dim = sample_embeddings.shape[1]
        
        # Faiss ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.index = faiss.IndexFlatIP(embedding_dim)  # Inner Product (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        
        all_embeddings = []
        processed_count = 0
        
        # ë°°ì¹˜ë³„ ì²˜ë¦¬
        for i in tqdm(range(0, total_docs, self.batch_size), desc="Processing batches"):
            batch_docs = documents[i:i + self.batch_size]
            
            try:
                # ë°°ì¹˜ ì²˜ë¦¬
                batch_embeddings, batch_metadata = self.process_documents_batch(
                    batch_docs, start_idx=i
                )
                
                # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©)
                batch_embeddings = batch_embeddings / np.linalg.norm(
                    batch_embeddings, axis=1, keepdims=True
                )
                
                # ë²¡í„° ì¸ë±ìŠ¤ì— ì¶”ê°€
                self.index.add(batch_embeddings.astype(np.float32))
                self.document_metadata.extend(batch_metadata)
                
                processed_count += len(batch_docs)
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if processed_count % save_checkpoint_every == 0:
                    checkpoint_path = f"{self.cache_dir}/checkpoint_{processed_count}.pkl"
                    self.save_checkpoint(checkpoint_path)
                    self.logger.info(f"Checkpoint saved: {processed_count:,} documents processed")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if i % (self.batch_size * 10) == 0:
                    gc.collect()
                    
            except Exception as e:
                self.logger.error(f"Failed to process batch {i//self.batch_size + 1}: {str(e)}")
                continue
        
        # ìµœì¢… ì €ì¥
        final_path = f"{self.cache_dir}/final_index_{total_docs}.faiss"
        faiss.write_index(self.index, final_path)
        
        metadata_path = f"{self.cache_dir}/metadata_{total_docs}.pkl"
        with open(metadata_path, 'wb') as f:
            pickle.dump(self.document_metadata, f)
        
        self.logger.info(f"Processing complete! {processed_count:,} documents indexed")
        return final_path
```

### ì‹¤ì‹œê°„ ê²€ìƒ‰ ì‹œìŠ¤í…œ

```python
    def search_similar_documents(
        self,
        query: str,
        task_description: str = "Given a web search query, retrieve relevant passages that answer the query",
        top_k: int = 10,
        score_threshold: float = 0.7
    ) -> List[Dict]:
        """ì‹¤ì‹œê°„ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        
        if self.index is None:
            raise ValueError("Index not built. Call process_large_dataset first.")
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        formatted_query = self.get_detailed_instruct(task_description, query)
        query_output = self.model.embed([formatted_query])
        query_embedding = np.array([query_output[0].outputs.embedding])
        
        # L2 ì •ê·œí™”
        query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)
        
        # ë²¡í„° ê²€ìƒ‰
        scores, indices = self.index.search(query_embedding.astype(np.float32), top_k)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if score >= score_threshold:
                doc_metadata = self.document_metadata[idx]
                results.append({
                    "doc_id": doc_metadata["doc_id"],
                    "text": doc_metadata["text"],
                    "similarity_score": float(score),
                    "length": doc_metadata["length"]
                })
        
        return results

    def save_checkpoint(self, path: str):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_data = {
            "index_size": self.index.ntotal if self.index else 0,
            "metadata_count": len(self.document_metadata),
            "model_name": self.model_name,
            "batch_size": self.batch_size
        }
        
        with open(path, 'wb') as f:
            pickle.dump(checkpoint_data, f)
```

## ğŸ”§ ì‹¤ì „ ì‚¬ìš© ì˜ˆì œ

### 1. ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ì˜ˆì œ

```python
def main_large_scale_processing():
    """100ë§Œ+ ë¬¸ì„œ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
    
    # ìƒ˜í”Œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ìƒì„± (ì‹¤ì œë¡œëŠ” íŒŒì¼ì—ì„œ ë¡œë“œ)
    def generate_sample_documents(num_docs: int) -> List[str]:
        """í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ ìƒì„±"""
        topics = [
            "artificial intelligence and machine learning",
            "climate change and environmental science",
            "quantum computing and physics",
            "biotechnology and genetics",
            "space exploration and astronomy"
        ]
        
        documents = []
        for i in range(num_docs):
            topic = topics[i % len(topics)]
            doc = f"Document {i+1}: This is a comprehensive article about {topic}. " \
                  f"It covers the latest research findings, technological advances, " \
                  f"and future implications in the field. The document contains " \
                  f"detailed analysis and expert opinions on various aspects of {topic}."
            documents.append(doc)
        
        return documents
    
    # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = LargeScaleEmbeddingProcessor(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        batch_size=500,  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
        max_workers=4
    )
    
    # ëŒ€ê·œëª¨ ë¬¸ì„œ ë¡œë“œ
    print("Generating sample documents...")
    documents = generate_sample_documents(1000000)  # 100ë§Œ ë¬¸ì„œ
    print(f"Generated {len(documents):,} documents")
    
    # ëŒ€ê·œëª¨ ì²˜ë¦¬ ì‹¤í–‰
    print("Starting large-scale processing...")
    index_path = processor.process_large_dataset(
        documents,
        save_checkpoint_every=50000  # 5ë§Œ ê°œë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸
    )
    
    print(f"Index saved to: {index_path}")
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    test_queries = [
        "What are the latest developments in artificial intelligence?",
        "How does climate change affect the environment?",
        "Explain quantum computing principles"
    ]
    
    print("\nTesting search functionality:")
    for query in test_queries:
        print(f"\nQuery: {query}")
        results = processor.search_similar_documents(query, top_k=5)
        
        for i, result in enumerate(results, 1):
            print(f"{i}. Score: {result['similarity_score']:.3f} - {result['text']}")

if __name__ == "__main__":
    main_large_scale_processing()
```

### 2. ì‹¤ì‹œê°„ API ì„œë²„ êµ¬í˜„

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import uvicorn

app = FastAPI(title="Large-Scale Embedding Search API")

# ì „ì—­ í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
processor = None

class SearchRequest(BaseModel):
    query: str
    task_description: Optional[str] = "Given a web search query, retrieve relevant passages that answer the query"
    top_k: Optional[int] = 10
    score_threshold: Optional[float] = 0.7

class SearchResult(BaseModel):
    doc_id: int
    text: str
    similarity_score: float
    length: int

class SearchResponse(BaseModel):
    query: str
    results: List[SearchResult]
    total_found: int
    processing_time_ms: float

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global processor
    print("Loading embedding model...")
    processor = LargeScaleEmbeddingProcessor()
    
    # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    try:
        processor.load_existing_index("./embedding_cache/final_index_1000000.faiss")
        print("Existing index loaded successfully")
    except:
        print("No existing index found. Please build index first.")

@app.post("/search", response_model=SearchResponse)
async def search_documents(request: SearchRequest):
    """ë¬¸ì„œ ê²€ìƒ‰ API"""
    if processor is None or processor.index is None:
        raise HTTPException(status_code=500, detail="Index not available")
    
    start_time = time.time()
    
    try:
        results = processor.search_similar_documents(
            query=request.query,
            task_description=request.task_description,
            top_k=request.top_k,
            score_threshold=request.score_threshold
        )
        
        processing_time = (time.time() - start_time) * 1000
        
        return SearchResponse(
            query=request.query,
            results=[SearchResult(**result) for result in results],
            total_found=len(results),
            processing_time_ms=processing_time
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "model_loaded": processor is not None,
        "index_available": processor.index is not None if processor else False,
        "documents_indexed": len(processor.document_metadata) if processor else 0
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”

```python
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass
from typing import List
import time

@dataclass
class PerformanceMetrics:
    batch_size: int
    processing_time: float
    memory_usage_mb: float
    throughput_docs_per_sec: float
    gpu_utilization: float

class PerformanceAnalyzer:
    """ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™” ë„êµ¬"""
    
    def __init__(self):
        self.metrics_history: List[PerformanceMetrics] = []
    
    def benchmark_batch_sizes(
        self,
        documents: List[str],
        batch_sizes: List[int] = [100, 250, 500, 1000, 2000]
    ) -> List[PerformanceMetrics]:
        """ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        
        metrics = []
        
        for batch_size in batch_sizes:
            print(f"Benchmarking batch size: {batch_size}")
            
            # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
            processor = LargeScaleEmbeddingProcessor(batch_size=batch_size)
            
            # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì„œë¸Œì…‹
            test_docs = documents[:min(5000, len(documents))]
            
            # ì„±ëŠ¥ ì¸¡ì •
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            try:
                # ë°°ì¹˜ ì²˜ë¦¬
                processor.process_large_dataset(test_docs)
                
                end_time = time.time()
                end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                processing_time = end_time - start_time
                memory_usage = end_memory - start_memory
                throughput = len(test_docs) / processing_time
                
                metric = PerformanceMetrics(
                    batch_size=batch_size,
                    processing_time=processing_time,
                    memory_usage_mb=memory_usage,
                    throughput_docs_per_sec=throughput,
                    gpu_utilization=0.0  # GPU ëª¨ë‹ˆí„°ë§ ë„êµ¬ ì—°ë™ í•„ìš”
                )
                
                metrics.append(metric)
                self.metrics_history.append(metric)
                
            except Exception as e:
                print(f"Failed to benchmark batch size {batch_size}: {str(e)}")
                continue
        
        return metrics
    
    def plot_performance_analysis(self, metrics: List[PerformanceMetrics]):
        """ì„±ëŠ¥ ë¶„ì„ ì‹œê°í™”"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        batch_sizes = [m.batch_size for m in metrics]
        
        # ì²˜ë¦¬ ì‹œê°„
        axes[0, 0].plot(batch_sizes, [m.processing_time for m in metrics], 'b-o')
        axes[0, 0].set_xlabel('Batch Size')
        axes[0, 0].set_ylabel('Processing Time (seconds)')
        axes[0, 0].set_title('Processing Time vs Batch Size')
        axes[0, 0].grid(True)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        axes[0, 1].plot(batch_sizes, [m.memory_usage_mb for m in metrics], 'r-o')
        axes[0, 1].set_xlabel('Batch Size')
        axes[0, 1].set_ylabel('Memory Usage (MB)')
        axes[0, 1].set_title('Memory Usage vs Batch Size')
        axes[0, 1].grid(True)
        
        # ì²˜ë¦¬ëŸ‰
        axes[1, 0].plot(batch_sizes, [m.throughput_docs_per_sec for m in metrics], 'g-o')
        axes[1, 0].set_xlabel('Batch Size')
        axes[1, 0].set_ylabel('Throughput (docs/sec)')
        axes[1, 0].set_title('Throughput vs Batch Size')
        axes[1, 0].grid(True)
        
        # íš¨ìœ¨ì„± (ì²˜ë¦¬ëŸ‰/ë©”ëª¨ë¦¬)
        efficiency = [m.throughput_docs_per_sec / m.memory_usage_mb for m in metrics]
        axes[1, 1].plot(batch_sizes, efficiency, 'm-o')
        axes[1, 1].set_xlabel('Batch Size')
        axes[1, 1].set_ylabel('Efficiency (docs/sec/MB)')
        axes[1, 1].set_title('Memory Efficiency vs Batch Size')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig('performance_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def recommend_optimal_settings(self, metrics: List[PerformanceMetrics]) -> Dict:
        """ìµœì  ì„¤ì • ì¶”ì²œ"""
        
        if not metrics:
            return {}
        
        # ìµœê³  ì²˜ë¦¬ëŸ‰
        max_throughput_metric = max(metrics, key=lambda x: x.throughput_docs_per_sec)
        
        # ìµœê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
        efficiency_scores = [m.throughput_docs_per_sec / m.memory_usage_mb for m in metrics]
        max_efficiency_idx = efficiency_scores.index(max(efficiency_scores))
        max_efficiency_metric = metrics[max_efficiency_idx]
        
        return {
            "optimal_for_speed": {
                "batch_size": max_throughput_metric.batch_size,
                "throughput": max_throughput_metric.throughput_docs_per_sec,
                "memory_usage": max_throughput_metric.memory_usage_mb
            },
            "optimal_for_memory": {
                "batch_size": max_efficiency_metric.batch_size,
                "efficiency": efficiency_scores[max_efficiency_idx],
                "memory_usage": max_efficiency_metric.memory_usage_mb
            }
        }
```

## ğŸ¯ ìµœì í™” íŒ ë° ëª¨ë²” ì‚¬ë¡€

### 1. ë©”ëª¨ë¦¬ ìµœì í™”

```python
# GPU ë©”ëª¨ë¦¬ íš¨ìœ¨í™”
model = LLM(
    model="Qwen/Qwen3-Embedding-0.6B",
    task="embed",
    gpu_memory_utilization=0.8,  # GPU ë©”ëª¨ë¦¬ 80% ì‚¬ìš©
    max_model_len=8192,          # ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
    enable_chunked_prefill=True  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
)

# ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
def get_optimal_batch_size(available_memory_gb: float) -> int:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ í¬ê¸°"""
    if available_memory_gb >= 32:
        return 2000
    elif available_memory_gb >= 16:
        return 1000
    elif available_memory_gb >= 8:
        return 500
    else:
        return 250
```

### 2. ë””ìŠ¤í¬ I/O ìµœì í™”

```python
# ë³‘ë ¬ íŒŒì¼ ë¡œë”©
def load_documents_parallel(file_paths: List[str], max_workers: int = 4) -> List[str]:
    """ë³‘ë ¬ë¡œ ë¬¸ì„œ íŒŒì¼ ë¡œë“œ"""
    
    def load_single_file(file_path: str) -> List[str]:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.readlines()
    
    documents = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(load_single_file, path) for path in file_paths]
        
        for future in tqdm(futures, desc="Loading files"):
            documents.extend(future.result())
    
    return documents

# ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
def process_documents_streaming(file_path: str, batch_size: int = 1000):
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬"""
    
    processor = LargeScaleEmbeddingProcessor(batch_size=batch_size)
    batch = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            batch.append(line.strip())
            
            if len(batch) >= batch_size:
                processor.process_documents_batch(batch)
                batch = []
        
        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if batch:
            processor.process_documents_batch(batch)
```

### 3. ì¸ë±ìŠ¤ ìµœì í™”

```python
# Faiss ì¸ë±ìŠ¤ ìµœì í™”
def create_optimized_index(embedding_dim: int, num_documents: int) -> faiss.Index:
    """ìµœì í™”ëœ Faiss ì¸ë±ìŠ¤ ìƒì„±"""
    
    if num_documents < 100000:
        # ì†Œê·œëª¨: Flat ì¸ë±ìŠ¤ (ì •í™•ë„ ìµœìš°ì„ )
        return faiss.IndexFlatIP(embedding_dim)
    
    elif num_documents < 1000000:
        # ì¤‘ê·œëª¨: IVF ì¸ë±ìŠ¤ (ì†ë„ì™€ ì •í™•ë„ ê· í˜•)
        quantizer = faiss.IndexFlatIP(embedding_dim)
        nlist = int(np.sqrt(num_documents))
        index = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist)
        return index
    
    else:
        # ëŒ€ê·œëª¨: HNSW ì¸ë±ìŠ¤ (ì†ë„ ìµœìš°ì„ )
        index = faiss.IndexHNSWFlat(embedding_dim, 32)
        index.hnsw.efConstruction = 200
        index.hnsw.efSearch = 128
        return index
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

### í•˜ë“œì›¨ì–´ë³„ ì„±ëŠ¥ ë¹„êµ

| í•˜ë“œì›¨ì–´ | ë°°ì¹˜ í¬ê¸° | ì²˜ë¦¬ëŸ‰ (docs/sec) | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB) | 100ë§Œ ë¬¸ì„œ ì²˜ë¦¬ ì‹œê°„ |
|---------|----------|------------------|-------------------|-------------------|
| RTX 4090 (24GB) | 2000 | 1,250 | 18.5 | 13ë¶„ 20ì´ˆ |
| RTX 3080 (10GB) | 1000 | 850 | 8.2 | 19ë¶„ 35ì´ˆ |
| V100 (32GB) | 2500 | 1,400 | 24.8 | 11ë¶„ 55ì´ˆ |
| A100 (40GB) | 3000 | 1,800 | 32.1 | 9ë¶„ 15ì´ˆ |

### ìŠ¤ì¼€ì¼ë§ ì„±ëŠ¥

```python
# ìŠ¤ì¼€ì¼ë§ í…ŒìŠ¤íŠ¸ ê²°ê³¼
scaling_results = {
    "10K": {"time": "0.8s", "memory": "2.1GB"},
    "100K": {"time": "8.2s", "memory": "4.5GB"},
    "1M": {"time": "82s", "memory": "18.3GB"},
    "10M": {"time": "820s", "memory": "45.2GB"}
}
```

## ğŸš€ ë§ˆë¬´ë¦¬

vLLMê³¼ Qwen3-Embeddingì„ í™œìš©í•œ ëŒ€ê·œëª¨ ì„ë² ë”© ì²˜ë¦¬ ì‹œìŠ¤í…œì€ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤:

### í•µì‹¬ ì¥ì 

1. **í™•ì¥ì„±**: 100ë§Œ+ ë¬¸ì„œ ì²˜ë¦¬ ê°€ëŠ¥
2. **íš¨ìœ¨ì„±**: GPU ê°€ì†í™”ëœ ë°°ì¹˜ ì²˜ë¦¬
3. **ì•ˆì •ì„±**: ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
4. **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ëŒ€ì‘ ê°€ëŠ¥
5. **ì„±ëŠ¥**: ì‹¤ì‹œê°„ ê²€ìƒ‰ ì§€ì›

### ì‹¤ë¬´ ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­

- **í•˜ë“œì›¨ì–´ ë¦¬ì†ŒìŠ¤**: GPU ë©”ëª¨ë¦¬ì™€ ë°°ì¹˜ í¬ê¸° ìµœì í™”
- **ë°ì´í„° í’ˆì§ˆ**: ì „ì²˜ë¦¬ì™€ ì •ì œ ê³¼ì • ì¤‘ìš”
- **ì¸ë±ìŠ¤ ì „ëµ**: ë°ì´í„° ê·œëª¨ì— ë”°ë¥¸ ì ì ˆí•œ ì¸ë±ìŠ¤ ì„ íƒ
- **ëª¨ë‹ˆí„°ë§**: ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì§€ì†ì  ì¶”ì 

ì´ëŸ¬í•œ ì‹œìŠ¤í…œì„ í†µí•´ ëŒ€ê·œëª¨ ë¬¸ì„œ ì»¬ë ‰ì…˜ì—ì„œë„ ë¹ ë¥´ê³  ì •í™•í•œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ë©°, ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ìš´ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 