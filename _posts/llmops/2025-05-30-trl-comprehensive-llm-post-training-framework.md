---
title: "TRL: Hugging Faceì˜ ì°¨ì„¸ëŒ€ LLM í›„ì²˜ë¦¬ í”„ë ˆì„ì›Œí¬ ì™„ì „ ê°€ì´ë“œ"
excerpt: "SFT, DPO, GRPO, PPO ë“± ìµœì‹  ê°•í™”í•™ìŠµ ê¸°ë²•ìœ¼ë¡œ Transformer ëª¨ë¸ì„ í›„ì²˜ë¦¬í•˜ëŠ” í¬ê´„ì  ë¼ì´ë¸ŒëŸ¬ë¦¬. CLIë¶€í„° ë¶„ì‚° í•™ìŠµê¹Œì§€ ëª¨ë“  ê²ƒì„ ì§€ì›"
date: 2025-05-30
categories:
  - llmops
tags:
  - TRL
  - TransformerRL
  - DPO
  - GRPO
  - PPO
  - SFT
  - RLHF
  - HuggingFace
  - ê°•í™”í•™ìŠµ
  - í›„ì²˜ë¦¬
author_profile: true
toc: true
toc_label: "TRL ì™„ì „ ê°€ì´ë“œ"
---

> **TL;DR** [TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl)ì€ ğŸ¤—Hugging Faceì—ì„œ ê°œë°œí•œ **LLM í›„ì²˜ë¦¬ ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬**ë‹¤. SFT, DPO, GRPO, PPO, Reward Modeling ë“± **ìµœì‹  ê°•í™”í•™ìŠµ ê¸°ë²•**ì„ í†µí•© ì§€ì›í•˜ë©°, CLIë¶€í„° ë¶„ì‚° í•™ìŠµê¹Œì§€ **ëª¨ë“  ê·œëª¨ì˜ í”„ë¡œì íŠ¸**ì— ëŒ€ì‘í•œë‹¤. Llama 3, Qwen, DeepSeek-R1 ë“± ì£¼ìš” ëª¨ë¸ë“¤ì´ ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ í›„ì²˜ë¦¬ë˜ì—ˆë‹¤.

---

## TRLì´ë€?

[TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl)ì€ ğŸ¤—Hugging Face ìƒíƒœê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ëœ **ì°¨ì„¸ëŒ€ LLM í›„ì²˜ë¦¬(Post-Training) ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬**ë‹¤. **1ë§Œ 4ì²œ ê°œ ì´ìƒì˜ GitHub ìŠ¤íƒ€**ë¥¼ ë°›ìœ¼ë©° í˜„ì¬ ì—…ê³„ í‘œì¤€ìœ¼ë¡œ ìë¦¬ì¡ê³  ìˆë‹¤.

### í•µì‹¬ íŠ¹ì§•

- **í¬ê´„ì  í›„ì²˜ë¦¬ ê¸°ë²•**: SFT, DPO, GRPO, PPO, Reward Modeling ë“± ëª¨ë“  ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ ì§€ì›
- **í™•ì¥ì„±**: ë‹¨ì¼ GPUë¶€í„° ë©€í‹°ë…¸ë“œ í´ëŸ¬ìŠ¤í„°ê¹Œì§€ ì™„ë²½ ì§€ì›
- **íš¨ìœ¨ì„±**: ğŸ¤—PEFT, Unsloth í†µí•©ìœ¼ë¡œ ì œí•œëœ í•˜ë“œì›¨ì–´ì—ì„œë„ ëŒ€í˜• ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥
- **ì‚¬ìš© í¸ì˜ì„±**: ì½”ë“œ ì—†ì´ ì‚¬ìš© ê°€ëŠ¥í•œ CLI ì¸í„°í˜ì´ìŠ¤
- **ìƒíƒœê³„ í†µí•©**: Transformers, Accelerate, PEFTì™€ ì™„ë²½ í˜¸í™˜

## ì™œ TRLì„ ì¨ì•¼ í• ê¹Œ?

### ì—…ê³„ ê²€ì¦ëœ í”„ë ˆì„ì›Œí¬

ì£¼ìš” LLMë“¤ì´ TRLë¡œ í›„ì²˜ë¦¬ë˜ì—ˆë‹¤:

- **Llama 3**: Metaì˜ DPO í•™ìŠµì— TRL ì‚¬ìš©
- **DeepSeek-R1**: GRPO ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒ
- **Qwen ì‹œë¦¬ì¦ˆ**: Alibabaì˜ ë‹¤ì–‘í•œ í›„ì²˜ë¦¬ ì‹¤í—˜
- **Gemma**: Googleì˜ instruction tuning

### í•™ìˆ -ì‚°ì—… ê°„ê·¹ í•´ì†Œ

ì—°êµ¬ì—ì„œ ê²€ì¦ëœ ìµœì‹  ì•Œê³ ë¦¬ì¦˜ì„ **ì¦‰ì‹œ í”„ë¡œë•ì…˜**ì— ì ìš© ê°€ëŠ¥í•˜ë‹¤:

| ì•Œê³ ë¦¬ì¦˜ | ë…¼ë¬¸ ë°œí‘œ | TRL ì§€ì› | ì‹¤ì œ ì ìš© |
|---------|---------|---------|---------|
| DPO | 2023.05 | 2023.08 | Llama 3 |
| GRPO | 2024.02 | 2024.03 | DeepSeek-R1 |
| ORPO | 2024.03 | 2024.04 | ë‹¤ìˆ˜ ëª¨ë¸ |

## ì„¤ì¹˜ ë°©ë²•

### ê¸°ë³¸ ì„¤ì¹˜

```bash
pip install trl
```

### ìµœì‹  ê¸°ëŠ¥ ì‚¬ìš©

```bash
pip install git+https://github.com/huggingface/trl.git
```

### ê°œë°œ í™˜ê²½ ì„¤ì •

```bash
git clone https://github.com/huggingface/trl.git
cd trl/
pip install -e .[dev]
```

## í•µì‹¬ íŠ¸ë ˆì´ë„ˆ ìƒì„¸ ê°€ì´ë“œ

### SFTTrainer: ì§€ë„ íŒŒì¸íŠœë‹

**SFT(Supervised Fine-Tuning)**ëŠ” ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ì„ íŠ¹ì • íƒœìŠ¤í¬ë‚˜ ë„ë©”ì¸ì— ì ì‘ì‹œí‚¤ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•ì´ë‹¤.

#### í•µì‹¬ ê°œë…

- **ëª©ì **: ì¼ë°˜ì ì¸ ì–¸ì–´ ëª¨ë¸ì„ íŠ¹ì • í˜•ì‹(ì˜ˆ: ì±„íŒ…)ìœ¼ë¡œ í•™ìŠµ
- **ë°ì´í„°**: ì…ë ¥-ì¶œë ¥ ìŒìœ¼ë¡œ êµ¬ì„±ëœ ì§€ë„í•™ìŠµ ë°ì´í„°
- **ì†ì‹¤í•¨ìˆ˜**: í‘œì¤€ ì–¸ì–´ ëª¨ë¸ë§ ì†ì‹¤ (Cross-Entropy)

#### ì‹¤ìŠµ ì˜ˆì œ

```python
from trl import SFTTrainer
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B")

# ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("trl-lib/Capybara", split="train")

# SFT íŠ¸ë ˆì´ë„ˆ ì„¤ì •
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    max_seq_length=2048,
    dataset_text_field="text",  # í…ìŠ¤íŠ¸ í•„ë“œ ì§€ì •
    packing=True,  # íš¨ìœ¨ì ì¸ íŒ¨í‚¹ ì‚¬ìš©
)

trainer.train()
```

#### ê³ ê¸‰ ì„¤ì •

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./sft-model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    warmup_steps=100,
    learning_rate=2e-5,
    logging_steps=50,
    save_steps=500,
    eval_steps=500,
    evaluation_strategy="steps",
    save_total_limit=3,
    load_best_model_at_end=True,
)

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
)
```

### DPOTrainer: ì§ì ‘ ì„ í˜¸ ìµœì í™”

**DPO(Direct Preference Optimization)**ëŠ” ì¸ê°„ í”¼ë“œë°±ì„ ì§ì ‘ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ ê°œì„ í•˜ëŠ” í˜ì‹ ì ì¸ ë°©ë²•ì´ë‹¤.

#### í•µì‹¬ ê°œë…

- **ëª©ì **: ì¸ê°„ì˜ ì„ í˜¸ë„ë¥¼ ì§ì ‘ í•™ìŠµí•˜ì—¬ ë” ë‚˜ì€ ì‘ë‹µ ìƒì„±
- **ë°ì´í„°**: (í”„ë¡¬í”„íŠ¸, ì„ í˜¸ ì‘ë‹µ, ë¹„ì„ í˜¸ ì‘ë‹µ) ì‚¼ì¤‘ìŒ
- **ì¥ì **: PPO ëŒ€ë¹„ ì•ˆì •ì ì´ê³  êµ¬í˜„ì´ ê°„ë‹¨

#### ìˆ˜í•™ì  ì›ë¦¬

DPOëŠ” ë‹¤ìŒ ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì í™”í•œë‹¤:

```
L_DPO = -E[(x,y_w,y_l)~D][log Ïƒ(Î² log Ï€_Î¸(y_w|x)/Ï€_ref(y_w|x) - Î² log Ï€_Î¸(y_l|x)/Ï€_ref(y_l|x))]
```

ì—¬ê¸°ì„œ:
- `y_w`: ì„ í˜¸ë˜ëŠ” ì‘ë‹µ
- `y_l`: ì„ í˜¸ë˜ì§€ ì•ŠëŠ” ì‘ë‹µ  
- `Ï€_Î¸`: í•™ìŠµ ì¤‘ì¸ ëª¨ë¸
- `Ï€_ref`: ì°¸ì¡° ëª¨ë¸
- `Î²`: ì˜¨ë„ íŒŒë¼ë¯¸í„°

#### ì‹¤ìŠµ ì˜ˆì œ

```python
from trl import DPOTrainer, DPOConfig
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer

# ëª¨ë¸ ì¤€ë¹„
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")

# DPO ë°ì´í„°ì…‹ ë¡œë“œ (ì„ í˜¸ë„ ìŒ í¬í•¨)
dataset = load_dataset("trl-lib/ultrafeedback_binarized", split="train")

# DPO ì„¤ì •
training_args = DPOConfig(
    output_dir="./dpo-model",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=5e-7,  # DPOëŠ” ë‚®ì€ í•™ìŠµë¥  ì‚¬ìš©
    beta=0.1,  # DPO ë² íƒ€ íŒŒë¼ë¯¸í„°
    max_length=1024,
    max_prompt_length=512,
)

# DPO íŠ¸ë ˆì´ë„ˆ
trainer = DPOTrainer(
    model=model,
    ref_model=None,  # Noneì´ë©´ ìë™ìœ¼ë¡œ ì°¸ì¡° ëª¨ë¸ ìƒì„±
    args=training_args,
    train_dataset=dataset,
    processing_class=tokenizer,
)

trainer.train()
```

#### DPO ë°ì´í„° í¬ë§·

```python
# DPO ë°ì´í„°ì…‹ ì˜ˆì‹œ
{
    "prompt": "íŒŒì´ì¬ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
    "chosen": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ë ¤ë©´ `sort()` ë©”ì„œë“œë‚˜ `sorted()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤...",
    "rejected": "ì •ë ¬? ê·¸ëƒ¥ ì†ìœ¼ë¡œ í•˜ë‚˜ì”© ì˜®ê¸°ë©´ ë©ë‹ˆë‹¤..."
}
```

### GRPOTrainer: ê·¸ë£¹ ìƒëŒ€ ì •ì±… ìµœì í™”

**GRPO(Group Relative Policy Optimization)**ëŠ” PPOë³´ë‹¤ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìƒˆë¡œìš´ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.

#### í•µì‹¬ ê°œë…

- **ëª©ì **: PPOì˜ ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ë©´ì„œ ì„±ëŠ¥ ìœ ì§€
- **íŠ¹ì§•**: DeepSeek-R1ì˜ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì— ì‚¬ìš©ë¨
- **ì¥ì **: ê¸´ ì»¨í…ìŠ¤íŠ¸ì—ì„œë„ ì•ˆì •ì ì¸ í•™ìŠµ

#### ì‹¤ìŠµ ì˜ˆì œ

```python
from trl import GRPOTrainer
from datasets import load_dataset

# ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("trl-lib/tldr", split="train")

# ë³´ìƒ í•¨ìˆ˜ ì •ì˜ (ì˜ˆì‹œ: ê³ ìœ  ë¬¸ì ìˆ˜)
def reward_unique_chars(completions, **kwargs):
    return [len(set(completion)) for completion in completions]

# GRPO íŠ¸ë ˆì´ë„ˆ ì„¤ì •
trainer = GRPOTrainer(
    model="Qwen/Qwen2-0.5B-Instruct",
    reward_funcs=reward_unique_chars,  # ë³´ìƒ í•¨ìˆ˜ ë¦¬ìŠ¤íŠ¸
    train_dataset=dataset,
    num_train_epochs=1,
    per_device_train_batch_size=2,
    learning_rate=1e-6,
)

trainer.train()
```

#### ë³µí•© ë³´ìƒ í•¨ìˆ˜ í™œìš©

```python
def reward_length(completions, **kwargs):
    """ì ì ˆí•œ ê¸¸ì´ ë³´ìƒ"""
    return [min(len(c)/100, 1.0) for c in completions]

def reward_no_repetition(completions, **kwargs):
    """ë°˜ë³µ ë°©ì§€ ë³´ìƒ"""
    scores = []
    for c in completions:
        words = c.split()
        unique_ratio = len(set(words)) / max(len(words), 1)
        scores.append(unique_ratio)
    return scores

# ì—¬ëŸ¬ ë³´ìƒ í•¨ìˆ˜ ì¡°í•©
trainer = GRPOTrainer(
    model="Qwen/Qwen2-0.5B-Instruct",
    reward_funcs=[reward_unique_chars, reward_length, reward_no_repetition],
    reward_weights=[0.4, 0.3, 0.3],  # ê°€ì¤‘ì¹˜ ì„¤ì •
    train_dataset=dataset,
)
```

### RewardTrainer: ë³´ìƒ ëª¨ë¸ í•™ìŠµ

**ë³´ìƒ ëª¨ë¸(Reward Model)**ì€ ì¸ê°„ì˜ ì„ í˜¸ë„ë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ë¥¸ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì‹ í˜¸ë¥¼ ì œê³µí•œë‹¤.

#### í•µì‹¬ ê°œë…

- **ëª©ì **: ì¸ê°„ í”¼ë“œë°±ì„ ìˆ˜ì¹˜ ì ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸ í•™ìŠµ
- **êµ¬ì¡°**: ì¼ë°˜ì ìœ¼ë¡œ ë¶„ë¥˜ê¸° í˜•íƒœ (Binary ë˜ëŠ” Regression)
- **í™œìš©**: PPO, GRPO ë“±ì—ì„œ ë³´ìƒ ì‹ í˜¸ë¡œ ì‚¬ìš©

#### ì‹¤ìŠµ ì˜ˆì œ

```python
from trl import RewardTrainer, RewardConfig
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# ë³´ìƒ ëª¨ë¸ ì¤€ë¹„ (ë¶„ë¥˜ í—¤ë“œ ì¶”ê°€)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
model = AutoModelForSequenceClassification.from_pretrained(
    "Qwen/Qwen2.5-0.5B-Instruct", 
    num_labels=1  # ë‹¨ì¼ ìŠ¤ì½”ì–´ ì¶œë ¥
)
model.config.pad_token_id = tokenizer.pad_token_id

# ì„ í˜¸ë„ ë°ì´í„°ì…‹
dataset = load_dataset("trl-lib/ultrafeedback_binarized", split="train")

# ë³´ìƒ ëª¨ë¸ í•™ìŠµ ì„¤ì •
training_args = RewardConfig(
    output_dir="./reward-model",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=1e-5,
    max_length=1024,
    remove_unused_columns=False,
)

# ë³´ìƒ íŠ¸ë ˆì´ë„ˆ
trainer = RewardTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    processing_class=tokenizer,
)

trainer.train()
```

#### ë³´ìƒ ëª¨ë¸ ì‚¬ìš©

```python
# í•™ìŠµëœ ë³´ìƒ ëª¨ë¸ë¡œ ì ìˆ˜ ê³„ì‚°
def get_reward_score(text, model, tokenizer):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=1024)
    with torch.no_grad():
        outputs = model(**inputs)
        score = outputs.logits.item()
    return score

# ì˜ˆì‹œ ì‚¬ìš©
text = "ì´ê²ƒì€ í›Œë¥­í•œ ë‹µë³€ì…ë‹ˆë‹¤!"
score = get_reward_score(text, model, tokenizer)
print(f"ë³´ìƒ ì ìˆ˜: {score}")
```

## ê³ ê¸‰ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜

### PPO (Proximal Policy Optimization)

PPOëŠ” OpenAI GPT ì‹œë¦¬ì¦ˆì—ì„œ ì‚¬ìš©ëœ ì „í†µì ì¸ RLHF ë°©ë²•ì´ë‹¤.

#### íŠ¹ì§• ë° í•œê³„

**ì¥ì :**
- ì•ˆì •ì ì¸ í•™ìŠµ
- ì´ë¡ ì  ë³´ì¥
- ê´‘ë²”ìœ„í•œ ê²€ì¦

**ë‹¨ì :**
- ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- ë³µì¡í•œ êµ¬í˜„
- ê¸´ í•™ìŠµ ì‹œê°„

#### TRLì—ì„œì˜ PPO

```python
from trl import PPOTrainer, PPOConfig

# PPO ì„¤ì •
config = PPOConfig(
    model_name="Qwen/Qwen2.5-0.5B-Instruct",
    learning_rate=1.41e-5,
    batch_size=64,
    mini_batch_size=16,
    ppo_epochs=4,
)

# PPO íŠ¸ë ˆì´ë„ˆ (ë” ë³µì¡í•œ ì„¤ì • í•„ìš”)
trainer = PPOTrainer(
    config=config,
    model=model,
    ref_model=ref_model,
    tokenizer=tokenizer,
    dataset=dataset,
    data_collator=collator,
)
```

### ORPO (Odds Ratio Preference Optimization)

ORPOëŠ” SFTì™€ preference learningì„ ë™ì‹œì— ìˆ˜í–‰í•˜ëŠ” íš¨ìœ¨ì ì¸ ë°©ë²•ì´ë‹¤.

#### í•µì‹¬ ì•„ì´ë””ì–´

- **í†µí•© í•™ìŠµ**: SFT + DPOë¥¼ í•œ ë²ˆì— ìˆ˜í–‰
- **íš¨ìœ¨ì„±**: ë³„ë„ì˜ SFT ë‹¨ê³„ ë¶ˆí•„ìš”
- **ì„±ëŠ¥**: DPOì™€ ìœ ì‚¬í•œ ì„±ëŠ¥, ë” ë¹ ë¥¸ ìˆ˜ë ´

### KTO (Kahneman-Tversky Optimization)

KTOëŠ” ì¸ê°„ì˜ ì¸ì§€ í¸í–¥ì„ ê³ ë ¤í•œ ìƒˆë¡œìš´ ìµœì í™” ë°©ë²•ì´ë‹¤.

#### íŠ¹ì§•

- **ì¸ì§€ê³¼í•™ ê¸°ë°˜**: ì¸ê°„ì˜ ì†ì‹¤ íšŒí”¼ ì„±í–¥ ë°˜ì˜
- **ë°ì´í„° íš¨ìœ¨ì„±**: ì ì€ ì„ í˜¸ë„ ë°ì´í„°ë¡œë„ íš¨ê³¼ì 
- **ì•ˆì •ì„±**: DPOë³´ë‹¤ ë” ì•ˆì •ì ì¸ í•™ìŠµ

## CLI ì‚¬ìš©ë²•

TRLì€ ì½”ë“œ ì‘ì„± ì—†ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ CLIë¥¼ ì œê³µí•œë‹¤.

### SFT ëª…ë ¹ì–´

```bash
# ê¸°ë³¸ SFT
trl sft --model_name_or_path Qwen/Qwen2.5-0.5B \
        --dataset_name trl-lib/Capybara \
        --output_dir ./sft-output

# ê³ ê¸‰ ì˜µì…˜
trl sft --model_name_or_path Qwen/Qwen2.5-0.5B \
        --dataset_name trl-lib/Capybara \
        --output_dir ./sft-output \
        --num_train_epochs 3 \
        --per_device_train_batch_size 4 \
        --learning_rate 2e-5 \
        --max_seq_length 2048 \
        --packing True
```

### DPO ëª…ë ¹ì–´

```bash
# ê¸°ë³¸ DPO
trl dpo --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
        --dataset_name argilla/Capybara-Preferences \
        --output_dir ./dpo-output

# ë² íƒ€ íŒŒë¼ë¯¸í„° ì¡°ì •
trl dpo --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
        --dataset_name argilla/Capybara-Preferences \
        --output_dir ./dpo-output \
        --beta 0.1 \
        --learning_rate 5e-7
```

### ë„ì›€ë§ í™•ì¸

```bash
# ì „ì²´ ëª…ë ¹ì–´ í™•ì¸
trl --help

# íŠ¹ì • ëª…ë ¹ì–´ ë„ì›€ë§
trl sft --help
trl dpo --help
```

## ë¶„ì‚° í•™ìŠµ ë° ìµœì í™”

### ğŸ¤—Accelerate í†µí•©

```python
from accelerate import Accelerator
from trl import DPOTrainer

# Accelerator ì´ˆê¸°í™”
accelerator = Accelerator()

# ìë™ìœ¼ë¡œ ë¶„ì‚° í™˜ê²½ ê°ì§€ ë° ì„¤ì •
trainer = DPOTrainer(
    model=model,
    train_dataset=dataset,
    # acceleratorê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬
)
```

### DeepSpeed ì„¤ì •

```json
// deepspeed_config.json
{
    "fp16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu"
        }
    },
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4
}
```

```bash
# DeepSpeedë¡œ í•™ìŠµ ì‹¤í–‰
deepspeed train_script.py --deepspeed deepspeed_config.json
```

### Unsloth í†µí•©

TRLì€ Unslothì™€ ì™„ë²½ í†µí•©ë˜ì–´ **2ë°° ë¹ ë¥¸ í•™ìŠµ**ì„ ì§€ì›í•œë‹¤:

```python
from unsloth import FastLanguageModel
from trl import SFTTrainer

# Unslothë¡œ ëª¨ë¸ ë¡œë“œ
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3-8b-bnb-4bit",
    max_seq_length=2048,
    load_in_4bit=True,
)

# TRL SFTTrainerì™€ í•¨ê»˜ ì‚¬ìš©
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    # Unsloth ìµœì í™”ê°€ ìë™ ì ìš©ë¨
)
```

## ì‹¤ì „ í™œìš© íŒ

### ë°ì´í„°ì…‹ ì¤€ë¹„

#### SFT ë°ì´í„° í˜•ì‹

```python
# ëŒ€í™”í˜• ë°ì´í„°
sft_data = [
    {
        "text": "<|user|>ì•ˆë…•í•˜ì„¸ìš”<|assistant|>ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?<|end|>"
    },
    {
        "text": "<|user|>íŒŒì´ì¬ ë¬¸ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”<|assistant|>íŒŒì´ì¬ì€ ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ ë¬¸ë²•ì„ ê°€ì§„ ì–¸ì–´ì…ë‹ˆë‹¤...<|end|>"
    }
]
```

#### DPO ë°ì´í„° í˜•ì‹

```python
# ì„ í˜¸ë„ ìŒ ë°ì´í„°
dpo_data = [
    {
        "prompt": "ì¢‹ì€ í”„ë¡œê·¸ë˜ë° ìŠµê´€ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "chosen": "1. ì½”ë“œ ê°€ë…ì„±ì„ ìœ„í•´ ëª…í™•í•œ ë³€ìˆ˜ëª… ì‚¬ìš©\n2. ì£¼ì„ ì‘ì„±\n3. í•¨ìˆ˜ë¥¼ ì‘ê²Œ ë‚˜ëˆ„ê¸°...",
        "rejected": "ê·¸ëƒ¥ ëŒì•„ê°€ê²Œë§Œ í•˜ë©´ ë©ë‹ˆë‹¤."
    }
]
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

#### SFT í•˜ì´í¼íŒŒë¼ë¯¸í„°

```python
# ë³´ìˆ˜ì  ì„¤ì • (ì•ˆì •ì )
conservative_args = TrainingArguments(
    learning_rate=1e-5,
    num_train_epochs=1,
    warmup_ratio=0.1,
    weight_decay=0.01,
)

# ê³µê²©ì  ì„¤ì • (ë¹ ë¥¸ ì ì‘)
aggressive_args = TrainingArguments(
    learning_rate=5e-5,
    num_train_epochs=3,
    warmup_ratio=0.05,
    weight_decay=0.1,
)
```

#### DPO í•˜ì´í¼íŒŒë¼ë¯¸í„°

```python
# ë² íƒ€ ê°’ ì¡°ì •
# ë†’ì€ ë² íƒ€ (0.5): ê°•í•œ ì œì•½, ì•ˆì „í•œ í•™ìŠµ
# ë‚®ì€ ë² íƒ€ (0.01): ì•½í•œ ì œì•½, ë” í° ë³€í™”

dpo_config = DPOConfig(
    beta=0.1,  # ì¼ë°˜ì ìœ¼ë¡œ 0.01-0.5 ì‚¬ì´
    learning_rate=5e-7,  # SFTë³´ë‹¤ ë‚®ê²Œ
    max_length=1024,
    max_prompt_length=512,
)
```

### í‰ê°€ ë° ëª¨ë‹ˆí„°ë§

#### WandB í†µí•©

```python
import wandb

# WandB ì´ˆê¸°í™”
wandb.init(project="llm-post-training")

# íŠ¸ë ˆì´ë„ˆì— ìë™ ë¡œê¹…
trainer = DPOTrainer(
    model=model,
    train_dataset=dataset,
    report_to="wandb",  # ìë™ ë¡œê¹…
)
```

#### ì»¤ìŠ¤í…€ í‰ê°€ ë©”íŠ¸ë¦­

```python
def compute_metrics(eval_pred):
    """ì»¤ìŠ¤í…€ í‰ê°€ í•¨ìˆ˜"""
    predictions, labels = eval_pred
    # ì—¬ê¸°ì— í‰ê°€ ë¡œì§ êµ¬í˜„
    return {"custom_metric": score}

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)
```

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ë©”ëª¨ë¦¬ ë¶€ì¡± í•´ê²°

```python
# ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”
training_args = TrainingArguments(
    gradient_checkpointing=True,
    dataloader_pin_memory=False,
    per_device_train_batch_size=1,  # ë°°ì¹˜ í¬ê¸° ê°ì†Œ
    gradient_accumulation_steps=8,  # ëˆ„ì ìœ¼ë¡œ íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° ì¦ê°€
)
```

### í•™ìŠµ ë¶ˆì•ˆì •ì„± í•´ê²°

```python
# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¡°ì •
from transformers import get_cosine_schedule_with_warmup

# ë” ë¶€ë“œëŸ¬ìš´ í•™ìŠµë¥  ê°ì†Œ
scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,
    num_training_steps=1000,
)
```

### DPO ìˆ˜ë ´ ë¬¸ì œ

```python
# ì°¸ì¡° ëª¨ë¸ ê³ ì • í™•ì¸
trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,  # ëª…ì‹œì ìœ¼ë¡œ ì°¸ì¡° ëª¨ë¸ ì œê³µ
    beta=0.1,  # ë² íƒ€ ê°’ ì¡°ì •
    train_dataset=dataset,
)
```

## ìµœì‹  ë™í–¥ ë° ë¡œë“œë§µ

### 2025ë…„ ì£¼ìš” ì—…ë°ì´íŠ¸

- **CLI ê°œì„ **: ë” ì§ê´€ì ì¸ ëª…ë ¹ì–´ ì¸í„°í˜ì´ìŠ¤
- **ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜**: ORPO, KTO, SimPO ë“± ì¶”ê°€
- **ì„±ëŠ¥ ìµœì í™”**: Unsloth ì™„ì „ í†µí•©
- **ë©€í‹°ëª¨ë‹¬ ì§€ì›**: ë¹„ì „-ì–¸ì–´ ëª¨ë¸ í›„ì²˜ë¦¬

### ì•ìœ¼ë¡œì˜ ë°œì „ ë°©í–¥

- **Self-supervised RL**: ì™¸ë¶€ ë³´ìƒ ì—†ëŠ” ìê¸°ì§€ë„ í•™ìŠµ
- **Constitutional AI**: ì›ì¹™ ê¸°ë°˜ AI ì •ë ¬
- **Federated RLHF**: ë¶„ì‚° í™˜ê²½ì—ì„œì˜ ì¸ê°„ í”¼ë“œë°± í•™ìŠµ

## ì»¤ë®¤ë‹ˆí‹°ì™€ ìë£Œ

### ê³µì‹ ìë£Œ

- **GitHub**: [https://github.com/huggingface/trl](https://github.com/huggingface/trl)
- **ë¬¸ì„œ**: [https://hf.co/docs/trl](https://hf.co/docs/trl)
- **ë…¼ë¬¸ ëª¨ìŒ**: [TRL Paper Collection](https://huggingface.co/collections/trl-lib)

### í•™ìŠµ ìë£Œ

- **ê³µì‹ ì˜ˆì œ**: [TRL Examples](https://github.com/huggingface/trl/tree/main/examples)
- **ë…¸íŠ¸ë¶ ëª¨ìŒ**: Colabê³¼ Kaggle ë…¸íŠ¸ë¶ ì œê³µ
- **ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸**: HF ë¸”ë¡œê·¸ì˜ ì‹¬í™” ê°€ì´ë“œ

### ì»¤ë®¤ë‹ˆí‹° ì§€ì›

- **Discord**: HuggingFace ê³µì‹ ë””ìŠ¤ì½”ë“œ
- **Forum**: HuggingFace Forumì˜ TRL ì„¹ì…˜
- **GitHub Issues**: ë²„ê·¸ ë¦¬í¬íŠ¸ ë° ê¸°ëŠ¥ ìš”ì²­

## ë§ˆë¬´ë¦¬

TRLì€ í˜„ì¬ ê°€ì¥ ì™„ì„±ë„ ë†’ì€ LLM í›„ì²˜ë¦¬ í”„ë ˆì„ì›Œí¬ë‹¤. **í•™ìˆ  ì—°êµ¬ì—ì„œ ê²€ì¦ëœ ìµœì‹  ì•Œê³ ë¦¬ì¦˜**ë“¤ì„ **í”„ë¡œë•ì…˜ ë ˆë²¨ì˜ ì•ˆì •ì„±**ìœ¼ë¡œ ì œê³µí•˜ë©°, **ì´ˆë³´ìë¶€í„° ì „ë¬¸ê°€ê¹Œì§€** ëª¨ë“  ìˆ˜ì¤€ì˜ ì‚¬ìš©ìë¥¼ ì§€ì›í•œë‹¤.

íŠ¹íˆ **CLIë¥¼ í†µí•œ ì½”ë“œ ì—†ëŠ” ì‚¬ìš©**ë¶€í„° **ë¶„ì‚° í•™ìŠµì„ í†µí•œ ëŒ€ê·œëª¨ ë°°í¬**ê¹Œì§€ ë‹¤ì–‘í•œ ìš”êµ¬ì‚¬í•­ì— ëŒ€ì‘í•  ìˆ˜ ìˆì–´, LLM í›„ì²˜ë¦¬ì˜ **ì‚¬ì‹¤ìƒ í‘œì¤€**ìœ¼ë¡œ ìë¦¬ì¡ì•˜ë‹¤.

Llama 3, DeepSeek-R1, Qwen ë“± ì£¼ìš” ëª¨ë¸ë“¤ì´ ì´ë¯¸ TRLë¡œ í›„ì²˜ë¦¬ë˜ê³  ìˆìœ¼ë‹ˆ, LLM ê°œë°œì— ê´€ì‹¬ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ìµí˜€ë‘˜ ë§Œí•œ ë„êµ¬ë‹¤.

---

*ì´ ê¸€ì´ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´, TRL GitHubì— â­ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!* 