---
title: "Transformer를 넘어: LLM의 미래를 바꿀 3가지 혁신 아키텍처 심층 분석"
excerpt: "리퀴드 뉴럴넷, 맘바(Mamba), 디퓨전 기반 언어 모델 등 Transformer를 대체할 차세대 LLM 아키텍처들의 핵심 원리와 성능 비교 분석"
date: 2025-05-27
categories:
  - llmops
tags:
  - MLOps
  - LLMOps 
  - Transformer
  - Liquid Neural Networks
  - Mamba
  - Diffusion Models
  - Thaki Cloud
author_profile: true # 회사 계정 또는 HR 담당자로 설정 가능
---

대규모 언어 모델(LLM) 분야는 Transformer 아키텍처의 등장 이후 눈부신 발전을 거듭해 왔습니다. 그러나 Transformer의 이차적 연산량 증가, 긴 시퀀스 처리의 어려움, 막대한 에너지 소비 등의 한계는 새로운 아키텍처 탐색의 필요성을 야기했습니다. 본 포스트에서는 Transformer를 대체하거나 강력한 경쟁자로 부상하고 있는 세 가지 혁신적인 아키텍처인 리퀴드 뉴럴넷(Liquid Neural Networks, LNNs), 맘바(Mamba, 선택적 상태 공간 모델), 그리고 디퓨전 기반 언어 모델(Diffusion-based Language Models)에 대해 심층적으로 분석하고 미래 전망을 논의하고자 합니다. 이들은 공통적으로 메모리, 지연 시간, 에너지 비용을 절감하면서도 Transformer와 동등하거나 그 이상의 성능 달성을 목표로 합니다.

## 1. 리퀴드 뉴럴넷 (Liquid Neural Networks, LNNs): 연속 시간의 힘

리퀴드 뉴럴넷은 시간에 따라 동적으로 변화하는 시스템을 모델링하는 데 탁월한 능력을 보여주는 아키텍처입니다.

### 1.1 핵심 아이디어

LNN은 Liquid-Time-Constant (LTC) RNN에서 발전된 개념으로, 각 뉴런을 1차 상미분방정식(ODE)으로 표현합니다. 핵심적인 특징은 뉴런의 시간 상수 $\tau$가 고정된 값이 아니라 입력에 따라 동적으로 변화한다는 점입니다. 이를 통해 각 뉴런은 실시간으로 입력 데이터의 특성에 맞춰 자신의 내부 동역학을 조정할 수 있으며, MIT 연구진은 이를 *"학습 중 실행 (learning on the job)"*이라고 명명했습니다. 이러한 특성은 예측 불가능하거나 변화가 잦은 시계열 데이터 처리에 강점을 보입니다.

### 1.2 수식 표현

LNN 뉴런의 동역학은 다음 미분방정식으로 표현됩니다:

$$
\dot{h}_t = -\frac{1}{\tau(x_t,h_t)}h_t + f_\theta(x_t,h_t)
$$

여기서 $h_t$는 뉴런의 상태, $x_t$는 입력, $\tau(x_t,h_t)$는 입력과 현재 상태에 따라 변하는 유효 시간 상수, $f_\theta(x_t,h_t)$는 학습 가능한 비선형 함수를 나타냅니다. 이 미분방정식은 미분 가능한 적분기(solver)를 통해 풀리므로, 긴 시퀀스에 대해서도 안정적인 그래디언트 흐름을 확보할 수 있습니다.

### 1.3 LLM 규모로의 최근 확장

LNN은 최근 LLM 규모로 확장되며 그 가능성을 입증하고 있습니다:

* **Liquid Language Model (LLM-Liq, 23억 매개변수, 2024년 12월):** WikiText-103 데이터셋에서 비슷한 크기의 Transformer 모델 대비 5% 이내의 perplexity를 달성하면서도 계산량은 30%, 피크 메모리는 25% 절감했습니다.
* **Liquid Foundation Models (2024-2025년):** 사기 탐지, 자율주행과 같은 시계열 데이터 분석 작업에서 기존 모델 대비 에너지 사용량을 2~4배 감소시키는 성과를 보였습니다.
* **2025년 5월 엣지-AI 벤치마크:** 1,100만 개의 매개변수를 가진 LNN이 6,000만 개의 매개변수를 가진 Transformer 모델을 드론 내비게이션 작업에서 지연 시간 및 강건성 측면에서 능가했습니다.

### 1.4 강점과 한계

| 항목          | 강점                                                                  | 한계                                                                      |
| :------------ | :-------------------------------------------------------------------- | :------------------------------------------------------------------------ |
| **적응성** | ODE 구조로 인해 Out-of-Distribution(OOD) 속도 변동 및 하드웨어 클럭 드리프트에 강함 | 강직(stiff) ODE 솔버 필요 시, GPU에 최적화된 행렬 곱셈(mat-mul)보다 느릴 수 있음 |
| **메모리·에너지** | 가변 스텝(variable-step) 적분기로 쉬운 구간에서 FLOP 절감 가능                     | Flash-attention과 같은 최적화된 커널 부재; 시퀀스 길이(L) > 8k에서 메모리 부담 증가 |
| **하드웨어** | 뉴로모픽 칩 및 FPGA 친화적, 8-bit 고정소수점 연산에서도 양호한 성능 유지           | 희소(sparse) 수치 라이브러리 미성숙                                           |

## 2. 맘바 (Mamba Net): 선택적 상태 공간 모델 (SSM)의 혁신

맘바는 기존의 상태 공간 모델(SSM)을 언어 모델링에 효과적으로 적용하기 위해 고안된 아키텍처로, 어텐션 메커니즘을 대체하는 새로운 접근 방식을 제시합니다.

### 2.1 왜 상태 공간 모델인가?

SSM은 시퀀스 데이터를 다음과 같은 선형 상태 공간 방정식으로 모델링합니다:

$$
s_{t+1}=A(x_t)s_t+B(x_t)
$$
$$
y_t=C s_t
$$

여기서 $s_t$는 잠재 상태, $x_t$는 입력, $y_t$는 출력입니다. 이 구조는 시퀀스 길이에 대해 선형적인 $O(L)$ 시간 및 메모리 복잡도를 달성할 수 있게 해줍니다. 이전의 S4/S4D 모델은 $A, B, C$ 매개변수가 입력에 독립적(time-invariant)이라 언어와 같이 내용에 따라 동적으로 변하는 패턴을 포착하는 데 한계가 있었습니다. 맘바는 이러한 매개변수, 특히 $A$와 $B$를 현재 입력 토큰 $x_t$에 따라 동적으로 생성함으로써 내용 기반 추론(content-aware reasoning) 능력을 크게 향상시켰습니다.

### 2.2 아키텍처 특징

* **토큰 의존적 SSM 커널:** 각 토큰마다 작은 MLP(Multi-Layer Perceptron)가 컨볼루션 필터 역할을 하는 SSM의 매개변수($A, B$)를 동적으로 생성합니다. 이를 통해 입력 내용에 따라 선택적으로 정보를 압축하고 전파할 수 있습니다.
* **스트리밍 모드:** 전용 CUDA 커널을 사용하여 상태(state)를 효율적으로 롤링(rolling) 유지합니다. 이를 통해 컨텍스트 길이에 구애받지 않는 일정한 지연 시간과 높은 처리량을 달성합니다. (예: A100 GPU에서 동급 Transformer 대비 5배 처리량)
* **미니멀 블록 구조:** 기존 Transformer의 어텐션 및 FFN(Feed-Forward Network) 레이어 없이, SSM과 게이팅 메커니즘만으로 블록을 구성하여 파라미터 수를 약 20% 절감합니다.

### 2.3 실험 결과

맘바는 다양한 벤치마크에서 뛰어난 성능을 입증했습니다:

| 모델                   | 파라미터 | OpenWebText-2 PPL ↓ | 토큰/s ↑  |
| :--------------------- | :------- | :------------------ | :-------- |
| LLaMA-2 3B (flash-attn) | 3.0 B    | 6.72                | 30 k      |
| **Mamba-3B** | **3.0 B**| **6.30** | **1.18 M**|
| Transformer 6B         | 6.0 B    | 6.20                | 300 k     |

정보 검색(IR) 및 컴퓨터 비전 분야에서도 유사한 이점을 보여주었습니다. 예를 들어, RankMamba는 ColBERT-v2와 유사한 품질을 30% 적은 GPU 시간으로 달성했으며, Mamba를 사용한 SAM(Segment Anything Model) 변형은 4k x 4k 이미지 분할에서 2배의 속도 향상을 보였습니다.

### 2.4 생태계 및 변형

* **MoE-Mamba:** 전문가 혼합(Mixture-of-Experts) 기법을 적용하여 동일 FLOPs에서 Perplexity를 5% 추가 감소시켰습니다.
* **Mixture-of-Mamba:** 128k 토큰의 긴 컨텍스트를 14GB VRAM이라는 제한된 환경에서 처리 가능함을 보였습니다.
* **오픈소스 구현:** `state-spaces/mamba` GitHub 저장소를 통해 활발히 개발 및 공유되고 있습니다.

### 2.5 주의 사항

맘바는 추론 속도가 매우 빠르지만, 훈련 처리량은 FlashAttention으로 최적화된 Transformer보다 낮을 수 있습니다. 이는 선택적 SSM 커널이 기존의 고도로 최적화된 행렬 곱셈 융합(mat-mul fusion) 경로를 깨뜨리기 때문입니다. 또한, 시퀀스 길이가 65k 이상으로 매우 길어질 경우 그래디언트 체크포인팅 시 메모리가 급증하는 현상이 보고된 바 있습니다.

## 3. 디퓨전 기반 언어 모델 ("Diffusion Net"): 잡음에서 텍스트 생성

디퓨전 모델은 본래 이미지 생성 분야에서 큰 성공을 거둔 후, 텍스트 생성 분야로 확장되고 있는 아키텍처입니다.

### 3.1 이미지에서 토큰으로의 전환

디퓨전 모델(Denoising Diffusion Probabilistic Models, DDPMs)은 원본 데이터에 점진적으로 잡음(noise)을 추가하는 정방향 프로세스와, 잡음으로부터 원본 데이터를 점진적으로 복원하는 역방향 프로세스를 학습합니다. 이를 텍스트에 적용하기 위해서는 몇 가지 수정이 필요합니다:
* **잡음 유형:** 이미지의 가우시안 잡음 대신, 텍스트에서는 토큰을 마스킹하거나 다른 토큰으로 치환하는 방식의 범주형 잡음(categorical noise)을 사용합니다.
* **손실 함수:** 연속적인 픽셀 값 대신 이산적인 어휘(vocabulary)를 다루므로, 일반적으로 교차 엔트로피(Cross-Entropy) 손실 함수가 사용됩니다.

### 3.2 두 가지 주요 연구 흐름

* **Masked-Diffuse LM (Soft-Masked Noise, 2023년):**
    * "소프트 마스크(soft-mask)"라는 개념을 도입하여 토큰을 부분적으로 치환하는 방식으로 잡음을 주입합니다.
    * 이를 통해 기존 방식 대비 훈련 비용을 절반으로 줄이면서도 BLEU/ROUGE와 같은 번역 및 요약 성능 지표를 향상시켰습니다.

* **Large Language Diffusion Models (LLaDA, 2025년 2월):**
    * 80억 개의 매개변수를 가진 대규모 디퓨전 언어 모델입니다.
    * 정방향 프로세스에서 약 15%의 토큰을 마스킹하고, 역방향 프로세스에서는 15단계 이하의 비교적 적은 스텝으로 원본 텍스트를 복원합니다.
    * The Pile 데이터셋에서 LLaMA-3 8B 모델과 동등한 perplexity를 달성했으며, 특히 긴 컨텍스트 질의응답(long-context QA)에서는 더 나은 성능을 보였습니다. 추론 시 메모리 사용량은 1/3 수준으로 감소했습니다.
    * **LLaDA-V (2025년 5월):** 시각 인코더를 결합하여 멀티모달 비자기회귀적(non-autoregressive) 추론이 가능하도록 확장되었습니다.

2023-2024년 관련 연구 서베이에 따르면, 디퓨전 기반 언어 모델은 생성 텍스트의 다양성(distinct-n)과 제어 가능성(controllability)을 향상시키는 잠재력을 보이지만, 생성 지연 시간이 자기회귀 모델 대비 2~3배 더 길다는 단점이 있습니다.

### 3.3 실무적 고려 사항

* **스텝 축소:** 생성 속도 향상을 위해, 로그 우도(likelihood) 훈련과 스케줄러 증류(scheduler distillation) 같은 기법을 통해 복원 스텝 수를 30회에서 8회 정도로 크게 줄이는 연구가 진행 중입니다.
* **하이브리드 모델:** Diffusion-Transformer (DiT, 2024년)와 같이 Transformer의 UNet 구조를 디퓨전 모델의 잡음 제거 네트워크로 활용하여 잠재 공간(latent space)에서 전역적인 계획(global planning) 능력을 유지하려는 시도도 있습니다.

## 4. 아키텍처 비교 분석

세 가지 아키텍처의 주요 특징을 비교하면 다음과 같습니다.

| 기준                 | Liquid NN                                  | Mamba SSM                                      | Diffusion Net                                  |
| :------------------- | :----------------------------------------- | :--------------------------------------------- | :--------------------------------------------- |
| **확장성 비용** | $O(L)$, 가변 스텝                           | $O(L)$, 커널 융합                              | $\geq k \cdot O(L)$ (k = 복원 스텝 수)            |
| **스트리밍 추론** | 네이티브 (연속 시간 모델)                      | 네이티브 (순환 상태 유지)                          | 네이티브 아님 (전체 시퀀스 반복 복원)                |
| **파라미터 효율** | 높음 (동적 뉴런 활용)                        | 중간 (Transformer 대비 약 20% 적음)              | Transformer 백본 사용 시 유사                   |
| **훈련 성숙도** | JAX-diffrax ODE 라이브러리 성숙              | PyTorch/CUDA 안정적, 오픈소스 생태계 활발         | 이산형 디퓨전 연산 라이브러리 부족                |
| **주요 강점** | OOD 강건성, 뉴로모픽 하드웨어 적합           | 5배 높은 처리량, 긴 시퀀스 효율적 처리             | 생성 다양성, 전역적 계획, 제어 가능성             |
| **주요 약점** | ODE 솔버 오버헤드, 특정 커널 최적화 부족       | 역전파 시 훈련 처리량 저하, 긴 시퀀스 메모리 문제  | 다단계 생성으로 인한 지연 시간                  |

## 5. 전망 및 결론

Transformer를 넘어선 차세대 LLM 아키텍처들은 각자의 독특한 접근 방식으로 특정 문제들을 해결하며 빠르게 발전하고 있습니다. 앞으로의 전망은 다음과 같습니다:

* **아이디어 융합:** State-Space Diffusion (SSM과 디퓨전의 결합), Liquid-Mamba (LNN과 Mamba의 장점 통합) 등 다양한 아키텍처의 핵심 아이디어를 융합하려는 시도가 등장할 것입니다.
* **하드웨어 공동 설계:** 전용 커널, 순환 연산 유닛, 적응형 ODE 솔버 등을 통합한 ASIC(주문형 반도체) 개발이 예고되고 있어, Transformer 전용 가속기와의 하드웨어적 격차도 점차 줄어들 것으로 기대됩니다.
* **생태계 확장:** Hugging Face의 "Alternative-Backbones" 프로젝트(2025년 3분기 예정) 등을 통해 ONNX 변환, 양자화 레시피 등이 제공되면, 이러한 새로운 아키텍처들의 채택 장벽이 크게 낮아질 것입니다.

**결론적으로,** 현재 LLM 작업에 가장 손쉽게 적용 가능한 대안으로는 **맘바(Mamba)**가 주목받고 있습니다. **리퀴드 뉴럴넷(LNN)**은 자원 제약이 심하거나 안전이 필수적인 환경(예: 엣지 AI, 실시간 제어)에서 강점을 보일 것입니다. **디퓨전 기반 언어 모델**은 실시간 지연 시간보다 생성물의 다양성, 전역적 일관성, 멀티모달 융합 능력이 더 중요한 애플리케이션에서 유망합니다.

이러한 혁신적인 아키텍처들의 발전은 LLM 분야의 지평을 넓히고, 더욱 효율적이고 강력하며 다양한 응용이 가능한 인공지능 시대를 앞당길 것으로 기대됩니다.

---

**주요 참고 문헌:**

* MIT CSAIL Liquid NN 보도자료
* Hasani, R., Lechner, M., Amini, A., Liebenwein, L., Ray, A., Moreno, M. G., ... & Rus, D. (2020). "Liquid Time-Constant Networks."
* Gu, A., & Dao, T. (2023). "Mamba: Linear-Time Sequence Modeling with Selective State Spaces."
* Mamba GitHub Repository: [state-spaces/mamba](https://github.com/state-spaces/mamba)
* RankMamba 연구 (2024)
* Large Language Diffusion Models (LLaDA) (2025)
* Soft-Masked Diffusion LM (2023)
* Diffusion LM likelihood 개선 연구
* Liquid AI 스타트업 관련 Wired 보도 (2024)
* 디퓨전 텍스트 생성 관련 서베이 (2024)
