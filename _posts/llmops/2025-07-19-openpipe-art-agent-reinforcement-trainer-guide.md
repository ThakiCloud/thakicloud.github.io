---
title: "OpenPipe ART: ì‹¤ì „ ì—ì´ì „íŠ¸ ê°•í™”í•™ìŠµ í”„ë ˆì„ì›Œí¬ ì™„ë²½ ê°€ì´ë“œ"
excerpt: "GRPOë¥¼ í™œìš©í•œ ë©€í‹°ìŠ¤í… ì—ì´ì „íŠ¸ í›ˆë ¨ë¶€í„° ì‹¤ì œ ì—…ë¬´ íƒœìŠ¤í¬ ì ìš©ê¹Œì§€. Qwen, Llama, Kimi ëª¨ë¸ë¡œ ì‹¤ë¬´í˜• AI ì—ì´ì „íŠ¸ ê°œë°œí•˜ê¸°"
seo_title: "OpenPipe ART ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ ê°€ì´ë“œ - GRPO ë©€í‹°ìŠ¤í… í›ˆë ¨ - Thaki Cloud"
seo_description: "OpenPipe ARTë¡œ ì‹¤ì „ AI ì—ì´ì „íŠ¸ ê°•í™”í•™ìŠµí•˜ê¸°. GRPO ì•Œê³ ë¦¬ì¦˜, ë©€í‹°ìŠ¤í… í›ˆë ¨, RULER ë³´ìƒí•¨ìˆ˜, vLLM í†µí•©ê¹Œì§€ ì™„ë²½ ê°€ì´ë“œ"
date: 2025-07-19
last_modified_at: 2025-07-19
categories:
  - llmops
tags:
  - OpenPipe
  - ART
  - ê°•í™”í•™ìŠµ
  - GRPO
  - ì—ì´ì „íŠ¸
  - ë©€í‹°ìŠ¤í…
  - Qwen
  - Llama
  - vLLM
  - RULER
  - ì‹¤ë¬´AI
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/llmops/openpipe-art-agent-reinforcement-trainer-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 30ë¶„

## ì„œë¡ 

**OpenPipe ART (Agent Reinforcement Trainer)**ëŠ” ì‹¤ì œ ì—…ë¬´ í™˜ê²½ì—ì„œ ë©€í‹°ìŠ¤í… AI ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨í•  ìˆ˜ ìˆëŠ” í˜ì‹ ì ì¸ ê°•í™”í•™ìŠµ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. **GRPO (Group Relative Policy Optimization)** ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•˜ì—¬ Qwen2.5, Qwen3, Llama, Kimi ë“± ë‹¤ì–‘í•œ ì–¸ì–´ëª¨ë¸ë¡œ ì‹¤ë¬´í˜• ì—ì´ì „íŠ¸ë¥¼ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ARTì˜ í•µì‹¬ íŠ¹ì§•

- ğŸš€ **GRPO ì•Œê³ ë¦¬ì¦˜**: ê·¸ë£¹ ìƒëŒ€ ì •ì±… ìµœì í™”ë¡œ ì•ˆì •ì ì¸ ë©€í‹°ìŠ¤í… í•™ìŠµ
- ğŸ¯ **ì‹¤ë¬´ ì¤‘ì‹¬**: ì´ë©”ì¼ ê²€ìƒ‰, ê²Œì„, ì½”ë”© ë“± ì‹¤ì œ íƒœìŠ¤í¬ì—ì„œ í›ˆë ¨
- ğŸ”§ **RULER í†µí•©**: ë³´ìƒí•¨ìˆ˜ ì—”ì§€ë‹ˆì–´ë§ ì—†ì´ ìë™ í‰ê°€ ì‹œìŠ¤í…œ
- ğŸŒ **í´ë¼ì´ì–¸íŠ¸-ì„œë²„**: ë¶„ì‚° í™˜ê²½ì—ì„œ í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜
- ğŸ“Š **ëª¨ë‹ˆí„°ë§**: W&B, Langfuse, OpenPipe ì™„ì „ í†µí•©

### ì§€ì› ëª¨ë¸

| ëª¨ë¸ íŒ¨ë°€ë¦¬ | ì§€ì› ë²„ì „ | íŠ¹ì§• |
|------------|----------|------|
| **Qwen** | 2.5, 3.0 | ì¤‘êµ­ì–´/ì˜ì–´ ë©€í‹°ëª¨ë‹¬ ì§€ì› |
| **Llama** | 3.1, 3.2 | Metaì˜ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ |
| **Kimi** | Latest | ë¡± ì»¨í…ìŠ¤íŠ¸ íŠ¹í™” |
| **HuggingFace** | ëª¨ë“  í˜¸í™˜ ëª¨ë¸ | ì»¤ìŠ¤í…€ ëª¨ë¸ ì§€ì› |

## í™˜ê²½ ì„¤ì • ë° ì„¤ì¹˜

### 1. ê¸°ë³¸ ìš”êµ¬ì‚¬í•­

```bash
# Python 3.8+ í•„ìš”
python --version

# CUDA ì§€ì› í™•ì¸ (ì„ íƒì‚¬í•­)
nvidia-smi

# Git ì„¤ì¹˜ í™•ì¸
git --version
```

### 2. OpenPipe ART ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/OpenPipe/ART.git
cd ART

# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python -m venv venv_art
source venv_art/bin/activate  # macOS/Linux
# venv_art\Scripts\activate  # Windows

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ê°œë°œìš© ì„¤ì¹˜ (ì„ íƒì‚¬í•­)
pip install -e .
```

### 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
cat > .env << 'EOF'
# ëª¨ë¸ ì„¤ì •
OPENAI_API_KEY=your_openai_api_key
OPENPIPE_API_KEY=your_openpipe_api_key

# ëª¨ë‹ˆí„°ë§ ì„¤ì •
WANDB_API_KEY=your_wandb_api_key
LANGFUSE_SECRET_KEY=your_langfuse_secret_key
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key
LANGFUSE_HOST=https://cloud.langfuse.com

# í›ˆë ¨ ì„¤ì •
ART_SERVER_PORT=8000
ART_CLIENT_WORKERS=4
EOF

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
source .env
```

## GRPO ì•Œê³ ë¦¬ì¦˜ ì´í•´

### GRPOë€?

**Group Relative Policy Optimization**ì€ ARTì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë©€í‹°ìŠ¤í… ì—ì´ì „íŠ¸ í›ˆë ¨ì— íŠ¹í™”ëœ ê°•í™”í•™ìŠµ ê¸°ë²•ì…ë‹ˆë‹¤.

```python
# GRPO í•µì‹¬ ê°œë… ì½”ë“œ ì˜ˆì‹œ
import torch
import torch.nn.functional as F

class GRPOTrainer:
    def __init__(self, model, reference_model, beta=0.1):
        self.model = model
        self.reference_model = reference_model
        self.beta = beta  # KL divergence ê°€ì¤‘ì¹˜
    
    def compute_grpo_loss(self, states, actions, rewards, group_baselines):
        """GRPO ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°"""
        
        # í˜„ì¬ ì •ì±…ì˜ ë¡œê·¸ í™•ë¥ 
        current_logprobs = self.model.get_log_probs(states, actions)
        
        # ì°¸ì¡° ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥ 
        with torch.no_grad():
            reference_logprobs = self.reference_model.get_log_probs(states, actions)
        
        # KL divergence í˜ë„í‹°
        kl_penalty = self.beta * (current_logprobs - reference_logprobs)
        
        # ê·¸ë£¹ ìƒëŒ€ì  ì´ì  ê³„ì‚°
        group_advantages = rewards - group_baselines
        
        # GRPO ì†ì‹¤
        policy_loss = -(group_advantages * current_logprobs).mean()
        kl_loss = kl_penalty.mean()
        
        total_loss = policy_loss + kl_loss
        
        return {
            'total_loss': total_loss,
            'policy_loss': policy_loss,
            'kl_loss': kl_loss,
            'advantages': group_advantages.mean()
        }
```

### GRPO vs PPO ë¹„êµ

| íŠ¹ì§• | GRPO | PPO |
|------|------|-----|
| **ë©€í‹°ìŠ¤í… í•™ìŠµ** | íŠ¹í™”ë¨ | ë‹¨ì¼ ìŠ¤í… ì¤‘ì‹¬ |
| **ê·¸ë£¹ ê¸°ë°˜ í‰ê°€** | ì§€ì› | ê°œë³„ í‰ê°€ |
| **ì•ˆì •ì„±** | ë†’ìŒ | ë³´í†µ |
| **ìˆ˜ë ´ ì†ë„** | ë¹ ë¦„ | ë³´í†µ |

## ì‹¤ì „ ì˜ˆì œ: ì´ë©”ì¼ ê²€ìƒ‰ ì—ì´ì „íŠ¸

### 1. ARTâ€¢E [RULER] ì˜ˆì œ ë¶„ì„

ê°€ì¥ ì‹¤ìš©ì ì¸ ì˜ˆì œì¸ ì´ë©”ì¼ ê²€ìƒ‰ ì—ì´ì „íŠ¸ë¥¼ êµ¬í˜„í•´ë´…ì‹œë‹¤.

```python
# examples/email_search/email_agent.py
import asyncio
from typing import List, Dict, Any
from art.environment import Environment
from art.agent import Agent
from art.trainer import GRPOTrainer

class EmailSearchEnvironment(Environment):
    def __init__(self, email_database: List[Dict]):
        super().__init__()
        self.email_db = email_database
        self.current_query = None
        self.search_history = []
        
    async def reset(self) -> Dict[str, Any]:
        """í™˜ê²½ ì´ˆê¸°í™”"""
        self.current_query = self.sample_query()
        self.search_history = []
        
        return {
            'query': self.current_query,
            'available_actions': ['search', 'filter', 'sort', 'select'],
            'search_results': [],
            'step': 0
        }
    
    async def step(self, action: Dict[str, Any]) -> tuple:
        """ì•¡ì…˜ ì‹¤í–‰ ë° ìƒíƒœ ì—…ë°ì´íŠ¸"""
        state = await self.get_state()
        
        if action['type'] == 'search':
            results = self.search_emails(action['query'])
            reward = self.calculate_search_reward(results)
            
        elif action['type'] == 'filter':
            results = self.filter_emails(state['search_results'], action['filters'])
            reward = self.calculate_filter_reward(results)
            
        elif action['type'] == 'select':
            selected_email = self.select_email(action['email_id'])
            reward = self.calculate_selection_reward(selected_email)
            done = True
        
        self.search_history.append(action)
        
        new_state = await self.get_state()
        return new_state, reward, done, {}
    
    def search_emails(self, query: str) -> List[Dict]:
        """ì´ë©”ì¼ ê²€ìƒ‰ ë¡œì§"""
        results = []
        query_terms = query.lower().split()
        
        for email in self.email_db:
            score = 0
            content = f"{email['subject']} {email['body']} {email['sender']}".lower()
            
            for term in query_terms:
                if term in content:
                    score += content.count(term)
            
            if score > 0:
                email_copy = email.copy()
                email_copy['relevance_score'] = score
                results.append(email_copy)
        
        return sorted(results, key=lambda x: x['relevance_score'], reverse=True)
    
    def calculate_search_reward(self, results: List[Dict]) -> float:
        """ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë³´ìƒ ê³„ì‚°"""
        if not results:
            return -0.1  # ê²°ê³¼ ì—†ìŒ í˜ë„í‹°
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ ë³´ìƒ
        relevance_scores = [r['relevance_score'] for r in results[:5]]
        avg_relevance = sum(relevance_scores) / len(relevance_scores)
        
        # ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
        diversity_bonus = len(set(r['sender'] for r in results[:10])) * 0.01
        
        return min(avg_relevance * 0.1 + diversity_bonus, 1.0)

class EmailSearchAgent(Agent):
    def __init__(self, model_name: str = "Qwen/Qwen2.5-7B-Instruct"):
        super().__init__(model_name)
        self.action_history = []
        
    async def select_action(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒíƒœ ê¸°ë°˜ ì•¡ì…˜ ì„ íƒ"""
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = """
        ë‹¹ì‹ ì€ ì´ë©”ì¼ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ë§ëŠ” ì´ë©”ì¼ì„ ì°¾ê¸° ìœ„í•´
        ë‹¤ìŒ ì•¡ì…˜ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
        
        1. search: í‚¤ì›Œë“œë¡œ ì´ë©”ì¼ ê²€ìƒ‰
        2. filter: ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•„í„°ë§ (ë‚ ì§œ, ë°œì‹ ì ë“±)
        3. sort: ê²°ê³¼ ì •ë ¬ (ê´€ë ¨ì„±, ë‚ ì§œ ë“±)
        4. select: ìµœì¢… ì´ë©”ì¼ ì„ íƒ
        
        ê° ë‹¨ê³„ì—ì„œ ìµœì ì˜ ì•¡ì…˜ì„ ì„ íƒí•˜ì—¬ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì´ë©”ì¼ì„ ì°¾ì•„ì£¼ì„¸ìš”.
        """
        
        # í˜„ì¬ ìƒíƒœ ì„¤ëª…
        state_description = f"""
        í˜„ì¬ ìƒí™©:
        - ê²€ìƒ‰ ì¿¼ë¦¬: {state['query']}
        - í˜„ì¬ ë‹¨ê³„: {state['step']}
        - ì‚¬ìš© ê°€ëŠ¥í•œ ì•¡ì…˜: {state['available_actions']}
        - ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(state.get('search_results', []))}
        """
        
        # ëª¨ë¸ì—ê²Œ ì•¡ì…˜ ìš”ì²­
        response = await self.generate_response(
            system_prompt + state_description,
            max_tokens=150,
            temperature=0.7
        )
        
        # ì‘ë‹µ íŒŒì‹±
        action = self.parse_action(response, state)
        self.action_history.append(action)
        
        return action
    
    def parse_action(self, response: str, state: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ ì‘ë‹µì„ ì•¡ì…˜ìœ¼ë¡œ íŒŒì‹±"""
        response_lower = response.lower()
        
        if 'search' in response_lower and state['step'] == 0:
            # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.extract_keywords(response, state['query'])
            return {
                'type': 'search',
                'query': keywords,
                'reasoning': response
            }
        
        elif 'filter' in response_lower and len(state.get('search_results', [])) > 0:
            filters = self.extract_filters(response)
            return {
                'type': 'filter',
                'filters': filters,
                'reasoning': response
            }
        
        elif 'select' in response_lower:
            email_id = self.extract_email_id(response, state.get('search_results', []))
            return {
                'type': 'select',
                'email_id': email_id,
                'reasoning': response
            }
        
        # ê¸°ë³¸ ì•¡ì…˜
        return {
            'type': 'search',
            'query': state['query'],
            'reasoning': 'Default search action'
        }
```

### 2. í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

```python
# train_email_agent.py
import asyncio
import json
from art.trainer import GRPOTrainer
from art.environment import Environment
from email_agent import EmailSearchEnvironment, EmailSearchAgent

async def train_email_agent():
    """ì´ë©”ì¼ ê²€ìƒ‰ ì—ì´ì „íŠ¸ í›ˆë ¨"""
    
    # ì´ë©”ì¼ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
    with open('email_database.json', 'r') as f:
        email_db = json.load(f)
    
    # í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    env = EmailSearchEnvironment(email_db)
    agent = EmailSearchAgent("Qwen/Qwen2.5-7B-Instruct")
    
    # í›ˆë ¨ ì„¤ì •
    trainer = GRPOTrainer(
        agent=agent,
        environment=env,
        learning_rate=1e-5,
        batch_size=8,
        episodes_per_batch=32,
        max_episode_length=10,
        kl_penalty=0.1
    )
    
    # í›ˆë ¨ ì‹¤í–‰
    for epoch in range(100):
        print(f"Epoch {epoch + 1}/100")
        
        # ì—í”¼ì†Œë“œ ìˆ˜ì§‘
        episodes = await trainer.collect_episodes()
        
        # ëª¨ë¸ ì—…ë°ì´íŠ¸
        loss_info = await trainer.update_policy(episodes)
        
        # ë¡œê¹…
        if epoch % 10 == 0:
            print(f"Policy Loss: {loss_info['policy_loss']:.4f}")
            print(f"KL Divergence: {loss_info['kl_loss']:.4f}")
            print(f"Average Reward: {loss_info['avg_reward']:.4f}")
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            await trainer.save_checkpoint(f"checkpoint_epoch_{epoch}.pt")
    
    print("Training completed!")

if __name__ == "__main__":
    asyncio.run(train_email_agent())
```

### 3. í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

```python
# evaluate_email_agent.py
import asyncio
import json
from email_agent import EmailSearchEnvironment, EmailSearchAgent

async def evaluate_agent():
    """í›ˆë ¨ëœ ì—ì´ì „íŠ¸ í‰ê°€"""
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    with open('test_queries.json', 'r') as f:
        test_queries = json.load(f)
    
    with open('email_database.json', 'r') as f:
        email_db = json.load(f)
    
    # í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    env = EmailSearchEnvironment(email_db)
    agent = EmailSearchAgent("Qwen/Qwen2.5-7B-Instruct")
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    agent.load_checkpoint("best_checkpoint.pt")
    
    total_success = 0
    total_queries = len(test_queries)
    
    for i, query_data in enumerate(test_queries):
        print(f"Testing query {i+1}/{total_queries}: {query_data['query']}")
        
        # í™˜ê²½ ì´ˆê¸°í™”
        state = await env.reset()
        state['query'] = query_data['query']
        
        done = False
        step_count = 0
        max_steps = 10
        
        while not done and step_count < max_steps:
            # ì—ì´ì „íŠ¸ ì•¡ì…˜ ì„ íƒ
            action = await agent.select_action(state)
            
            # í™˜ê²½ì—ì„œ ì•¡ì…˜ ì‹¤í–‰
            state, reward, done, info = await env.step(action)
            step_count += 1
            
            print(f"  Step {step_count}: {action['type']} - Reward: {reward:.3f}")
        
        # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
        if done and reward > 0.5:  # ì„±ê³µ ì„ê³„ê°’
            total_success += 1
            print(f"  âœ… Success!")
        else:
            print(f"  âŒ Failed")
    
    success_rate = total_success / total_queries
    print(f"\nOverall Success Rate: {success_rate:.2%}")

if __name__ == "__main__":
    asyncio.run(evaluate_agent())
```

## í´ë¼ì´ì–¸íŠ¸-ì„œë²„ ì•„í‚¤í…ì²˜

### 1. ì„œë²„ ì„¤ì •

```python
# art_server.py
from fastapi import FastAPI, WebSocket
from fastapi.responses import HTMLResponse
import asyncio
import json
from typing import Dict, List
import uvicorn

app = FastAPI(title="ART Training Server")

class TrainingServer:
    def __init__(self):
        self.active_clients: Dict[str, WebSocket] = {}
        self.training_queue: List[Dict] = []
        self.results_queue: List[Dict] = []
        
    async def register_client(self, client_id: str, websocket: WebSocket):
        """í´ë¼ì´ì–¸íŠ¸ ë“±ë¡"""
        self.active_clients[client_id] = websocket
        print(f"Client {client_id} connected")
    
    async def distribute_episodes(self, episodes: List[Dict]):
        """ì—í”¼ì†Œë“œë¥¼ í´ë¼ì´ì–¸íŠ¸ë“¤ì—ê²Œ ë¶„ì‚°"""
        if not self.active_clients:
            return
        
        clients = list(self.active_clients.keys())
        episodes_per_client = len(episodes) // len(clients)
        
        for i, client_id in enumerate(clients):
            start_idx = i * episodes_per_client
            end_idx = start_idx + episodes_per_client if i < len(clients) - 1 else len(episodes)
            
            client_episodes = episodes[start_idx:end_idx]
            
            await self.send_to_client(client_id, {
                'type': 'train_episodes',
                'episodes': client_episodes,
                'client_id': client_id
            })
    
    async def send_to_client(self, client_id: str, message: Dict):
        """íŠ¹ì • í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë©”ì‹œì§€ ì „ì†¡"""
        if client_id in self.active_clients:
            try:
                await self.active_clients[client_id].send_text(json.dumps(message))
            except Exception as e:
                print(f"Failed to send to client {client_id}: {e}")
                del self.active_clients[client_id]

server = TrainingServer()

@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    await websocket.accept()
    await server.register_client(client_id, websocket)
    
    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            if message['type'] == 'episode_result':
                server.results_queue.append(message)
            elif message['type'] == 'status_update':
                print(f"Client {client_id}: {message['status']}")
                
    except Exception as e:
        print(f"Client {client_id} disconnected: {e}")
        if client_id in server.active_clients:
            del server.active_clients[client_id]

@app.get("/")
async def get_dashboard():
    """í›ˆë ¨ ëŒ€ì‹œë³´ë“œ"""
    return HTMLResponse(content="""
    <!DOCTYPE html>
    <html>
    <head>
        <title>ART Training Dashboard</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            .status { padding: 20px; background: #f0f0f0; border-radius: 5px; }
            .client { margin: 10px 0; padding: 10px; background: #e8f4f8; }
        </style>
    </head>
    <body>
        <h1>ART Training Dashboard</h1>
        <div class="status">
            <h3>Server Status: Running</h3>
            <p>Active Clients: <span id="client-count">0</span></p>
            <p>Episodes in Queue: <span id="episode-count">0</span></p>
        </div>
        
        <div id="clients">
            <h3>Connected Clients</h3>
        </div>
        
        <script>
            // ì‹¤ì‹œê°„ ìƒíƒœ ì—…ë°ì´íŠ¸ ë¡œì§
            function updateStatus() {
                fetch('/status')
                    .then(response => response.json())
                    .then(data => {
                        document.getElementById('client-count').textContent = data.active_clients;
                        document.getElementById('episode-count').textContent = data.episodes_queued;
                    });
            }
            
            setInterval(updateStatus, 2000);
        </script>
    </body>
    </html>
    """)

@app.get("/status")
async def get_status():
    """ì„œë²„ ìƒíƒœ API"""
    return {
        "active_clients": len(server.active_clients),
        "episodes_queued": len(server.training_queue),
        "results_available": len(server.results_queue)
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2. í´ë¼ì´ì–¸íŠ¸ ì„¤ì •

```python
# art_client.py
import asyncio
import websockets
import json
import torch
from art.agent import Agent
from art.environment import Environment

class TrainingClient:
    def __init__(self, client_id: str, server_url: str = "ws://localhost:8000"):
        self.client_id = client_id
        self.server_url = f"{server_url}/ws/{client_id}"
        self.agent = None
        self.environment = None
        
    async def connect(self):
        """ì„œë²„ì— ì—°ê²°"""
        self.websocket = await websockets.connect(self.server_url)
        print(f"Client {self.client_id} connected to server")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ ì „ì†¡
        await self.send_message({
            'type': 'status_update',
            'status': 'connected',
            'client_id': self.client_id
        })
    
    async def send_message(self, message: dict):
        """ì„œë²„ì— ë©”ì‹œì§€ ì „ì†¡"""
        await self.websocket.send(json.dumps(message))
    
    async def handle_server_messages(self):
        """ì„œë²„ ë©”ì‹œì§€ ì²˜ë¦¬"""
        async for message in self.websocket:
            data = json.loads(message)
            
            if data['type'] == 'train_episodes':
                await self.process_episodes(data['episodes'])
            elif data['type'] == 'update_model':
                await self.update_model(data['model_state'])
            elif data['type'] == 'shutdown':
                break
    
    async def process_episodes(self, episodes: list):
        """ì—í”¼ì†Œë“œ ì²˜ë¦¬ ë° í›ˆë ¨"""
        print(f"Processing {len(episodes)} episodes")
        
        results = []
        for episode in episodes:
            # ì—í”¼ì†Œë“œ ì‹¤í–‰
            result = await self.run_episode(episode)
            results.append(result)
        
        # ê²°ê³¼ë¥¼ ì„œë²„ë¡œ ì „ì†¡
        await self.send_message({
            'type': 'episode_result',
            'results': results,
            'client_id': self.client_id
        })
    
    async def run_episode(self, episode_config: dict):
        """ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰"""
        if not self.agent or not self.environment:
            # ì—ì´ì „íŠ¸ì™€ í™˜ê²½ ì´ˆê¸°í™”
            self.agent = Agent(episode_config['model_name'])
            self.environment = Environment(episode_config['env_config'])
        
        state = await self.environment.reset()
        total_reward = 0
        steps = 0
        max_steps = episode_config.get('max_steps', 50)
        
        episode_data = {
            'states': [],
            'actions': [],
            'rewards': [],
            'done': False
        }
        
        while steps < max_steps:
            # ì—ì´ì „íŠ¸ ì•¡ì…˜ ì„ íƒ
            action = await self.agent.select_action(state)
            
            # í™˜ê²½ì—ì„œ ì•¡ì…˜ ì‹¤í–‰
            next_state, reward, done, info = await self.environment.step(action)
            
            # ë°ì´í„° ìˆ˜ì§‘
            episode_data['states'].append(state)
            episode_data['actions'].append(action)
            episode_data['rewards'].append(reward)
            
            total_reward += reward
            steps += 1
            state = next_state
            
            if done:
                episode_data['done'] = True
                break
        
        return {
            'episode_id': episode_config['episode_id'],
            'total_reward': total_reward,
            'steps': steps,
            'data': episode_data
        }

async def start_client(client_id: str):
    """í´ë¼ì´ì–¸íŠ¸ ì‹œì‘"""
    client = TrainingClient(client_id)
    
    try:
        await client.connect()
        await client.handle_server_messages()
    except Exception as e:
        print(f"Client error: {e}")
    finally:
        if hasattr(client, 'websocket'):
            await client.websocket.close()

if __name__ == "__main__":
    import sys
    client_id = sys.argv[1] if len(sys.argv) > 1 else "client_1"
    asyncio.run(start_client(client_id))
```

## RULER í†µí•© ë³´ìƒ ì‹œìŠ¤í…œ

### 1. RULER ê¸°ë°˜ ìë™ í‰ê°€

```python
# ruler_evaluator.py
from typing import Dict, List, Any
import json
import re

class RULERRewardSystem:
    """RULER ê¸°ë°˜ ìë™ ë³´ìƒ ê³„ì‚° ì‹œìŠ¤í…œ"""
    
    def __init__(self, task_config: Dict[str, Any]):
        self.task_config = task_config
        self.evaluation_criteria = self.load_criteria()
        
    def load_criteria(self) -> Dict[str, Any]:
        """í‰ê°€ ê¸°ì¤€ ë¡œë“œ"""
        return {
            'accuracy': {
                'weight': 0.4,
                'description': 'ì‘ì—… ì •í™•ë„'
            },
            'efficiency': {
                'weight': 0.3,
                'description': 'ì‹¤í–‰ íš¨ìœ¨ì„± (ë‹¨ê³„ ìˆ˜)'
            },
            'completeness': {
                'weight': 0.2,
                'description': 'ì‘ì—… ì™„ì„±ë„'
            },
            'user_satisfaction': {
                'weight': 0.1,
                'description': 'ì‚¬ìš©ì ë§Œì¡±ë„'
            }
        }
    
    def evaluate_episode(self, episode_data: Dict[str, Any]) -> float:
        """ì—í”¼ì†Œë“œ ì „ì²´ í‰ê°€"""
        
        total_score = 0.0
        
        for criterion, config in self.evaluation_criteria.items():
            if criterion == 'accuracy':
                score = self.evaluate_accuracy(episode_data)
            elif criterion == 'efficiency':
                score = self.evaluate_efficiency(episode_data)
            elif criterion == 'completeness':
                score = self.evaluate_completeness(episode_data)
            elif criterion == 'user_satisfaction':
                score = self.evaluate_user_satisfaction(episode_data)
            else:
                score = 0.0
            
            weighted_score = score * config['weight']
            total_score += weighted_score
            
            print(f"{criterion}: {score:.3f} (weight: {config['weight']}) = {weighted_score:.3f}")
        
        return min(max(total_score, 0.0), 1.0)  # 0-1 ë²”ìœ„ë¡œ í´ë¨í•‘
    
    def evaluate_accuracy(self, episode_data: Dict[str, Any]) -> float:
        """ì •í™•ë„ í‰ê°€"""
        if not episode_data.get('final_result'):
            return 0.0
        
        expected_result = episode_data.get('expected_result')
        actual_result = episode_data.get('final_result')
        
        if not expected_result:
            # íœ´ë¦¬ìŠ¤í‹± í‰ê°€
            return self.heuristic_accuracy_evaluation(actual_result, episode_data)
        
        # ì§ì ‘ ë¹„êµ
        if isinstance(expected_result, str) and isinstance(actual_result, str):
            return self.string_similarity(expected_result, actual_result)
        elif isinstance(expected_result, dict) and isinstance(actual_result, dict):
            return self.dict_similarity(expected_result, actual_result)
        else:
            return 1.0 if expected_result == actual_result else 0.0
    
    def evaluate_efficiency(self, episode_data: Dict[str, Any]) -> float:
        """íš¨ìœ¨ì„± í‰ê°€ (ë‹¨ê³„ ìˆ˜ ê¸°ë°˜)"""
        actual_steps = len(episode_data.get('actions', []))
        optimal_steps = episode_data.get('optimal_steps', actual_steps)
        
        if actual_steps <= optimal_steps:
            return 1.0
        else:
            # ë‹¨ê³„ ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ì ìˆ˜ ê°ì†Œ
            penalty = (actual_steps - optimal_steps) / optimal_steps
            return max(0.0, 1.0 - penalty * 0.5)
    
    def evaluate_completeness(self, episode_data: Dict[str, Any]) -> float:
        """ì™„ì„±ë„ í‰ê°€"""
        required_actions = episode_data.get('required_actions', [])
        performed_actions = episode_data.get('actions', [])
        
        if not required_actions:
            return 1.0 if episode_data.get('done', False) else 0.5
        
        # í•„ìˆ˜ ì•¡ì…˜ ìˆ˜í–‰ ì—¬ë¶€ í™•ì¸
        performed_action_types = [action.get('type') for action in performed_actions]
        completed_requirements = 0
        
        for req_action in required_actions:
            if req_action in performed_action_types:
                completed_requirements += 1
        
        return completed_requirements / len(required_actions)
    
    def evaluate_user_satisfaction(self, episode_data: Dict[str, Any]) -> float:
        """ì‚¬ìš©ì ë§Œì¡±ë„ í‰ê°€ (íœ´ë¦¬ìŠ¤í‹±)"""
        # ì—ëŸ¬ ë°œìƒ ì—¬ë¶€
        error_penalty = len(episode_data.get('errors', [])) * 0.1
        
        # ì‘ë‹µ ì‹œê°„ ê³ ë ¤
        response_time = episode_data.get('response_time', 0)
        time_penalty = max(0, (response_time - 30) / 60) * 0.2  # 30ì´ˆ ì´ˆê³¼ì‹œ í˜ë„í‹°
        
        # ê¸°ë³¸ ì ìˆ˜ì—ì„œ í˜ë„í‹° ì°¨ê°
        base_score = 1.0
        final_score = base_score - error_penalty - time_penalty
        
        return max(0.0, final_score)
    
    def string_similarity(self, expected: str, actual: str) -> float:
        """ë¬¸ìì—´ ìœ ì‚¬ë„ ê³„ì‚°"""
        from difflib import SequenceMatcher
        return SequenceMatcher(None, expected.lower(), actual.lower()).ratio()
    
    def dict_similarity(self, expected: dict, actual: dict) -> float:
        """ë”•ì…”ë„ˆë¦¬ ìœ ì‚¬ë„ ê³„ì‚°"""
        expected_keys = set(expected.keys())
        actual_keys = set(actual.keys())
        
        # í‚¤ ì¼ì¹˜ìœ¨
        key_match = len(expected_keys & actual_keys) / len(expected_keys | actual_keys)
        
        # ê°’ ì¼ì¹˜ìœ¨
        matching_keys = expected_keys & actual_keys
        value_matches = 0
        
        for key in matching_keys:
            if expected[key] == actual[key]:
                value_matches += 1
        
        value_match = value_matches / len(matching_keys) if matching_keys else 0
        
        return (key_match + value_match) / 2
    
    def heuristic_accuracy_evaluation(self, result: Any, episode_data: Dict[str, Any]) -> float:
        """íœ´ë¦¬ìŠ¤í‹± ì •í™•ë„ í‰ê°€"""
        # ì‘ì—… ìœ í˜•ë³„ íœ´ë¦¬ìŠ¤í‹± í‰ê°€
        task_type = episode_data.get('task_type', 'general')
        
        if task_type == 'email_search':
            return self.evaluate_email_search_accuracy(result, episode_data)
        elif task_type == 'code_generation':
            return self.evaluate_code_accuracy(result, episode_data)
        elif task_type == 'question_answering':
            return self.evaluate_qa_accuracy(result, episode_data)
        else:
            # ì¼ë°˜ì ì¸ íœ´ë¦¬ìŠ¤í‹±
            return 0.7 if episode_data.get('done', False) else 0.3
    
    def evaluate_email_search_accuracy(self, result: Any, episode_data: Dict[str, Any]) -> float:
        """ì´ë©”ì¼ ê²€ìƒ‰ ì •í™•ë„ í‰ê°€"""
        if not result or not isinstance(result, dict):
            return 0.0
        
        # ì´ë©”ì¼ì´ ì„ íƒë˜ì—ˆëŠ”ì§€ í™•ì¸
        if 'selected_email' not in result:
            return 0.2
        
        email = result['selected_email']
        query = episode_data.get('original_query', '').lower()
        
        # ì´ë©”ì¼ ë‚´ìš©ê³¼ ì¿¼ë¦¬ì˜ ê´€ë ¨ì„± í™•ì¸
        content = f"{email.get('subject', '')} {email.get('body', '')}".lower()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        query_words = query.split()
        matches = sum(1 for word in query_words if word in content)
        keyword_score = matches / len(query_words) if query_words else 0
        
        # ë°œì‹ ì ê´€ë ¨ì„± (ìˆë‹¤ë©´)
        sender_score = 0
        if 'sender' in email and any(word in email['sender'].lower() for word in query_words):
            sender_score = 0.2
        
        return min(1.0, keyword_score * 0.7 + sender_score + 0.1)  # ê¸°ë³¸ ì ìˆ˜ 0.1

# ì‚¬ìš© ì˜ˆì‹œ
ruler_system = RULERRewardSystem({
    'task_type': 'email_search',
    'optimal_steps': 5,
    'required_actions': ['search', 'filter', 'select']
})

# ì—í”¼ì†Œë“œ ë°ì´í„° ì˜ˆì‹œ
episode_data = {
    'actions': [
        {'type': 'search', 'query': 'meeting schedule'},
        {'type': 'filter', 'filters': {'sender': 'boss@company.com'}},
        {'type': 'select', 'email_id': 'email_123'}
    ],
    'final_result': {
        'selected_email': {
            'id': 'email_123',
            'subject': 'Team Meeting Schedule for Next Week',
            'sender': 'boss@company.com',
            'body': 'Hi team, here is the schedule for next week...'
        }
    },
    'original_query': 'meeting schedule boss',
    'done': True,
    'task_type': 'email_search',
    'response_time': 25,
    'errors': []
}

reward = ruler_system.evaluate_episode(episode_data)
print(f"Episode Reward: {reward:.3f}")
```

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹… ì‹œìŠ¤í…œ

### 1. Weights & Biases í†µí•©

```python
# wandb_integration.py
import wandb
import json
from datetime import datetime
from typing import Dict, List, Any

class ARTWandbLogger:
    def __init__(self, project_name: str = "art-training", experiment_name: str = None):
        self.project_name = project_name
        self.experiment_name = experiment_name or f"art_experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.run = None
        
    def initialize_run(self, config: Dict[str, Any]):
        """W&B ì‹¤í–‰ ì´ˆê¸°í™”"""
        self.run = wandb.init(
            project=self.project_name,
            name=self.experiment_name,
            config=config,
            tags=["ART", "GRPO", config.get("model_name", "unknown")],
            notes=f"ART training with {config.get('agent_type', 'unknown')} agent"
        )
        
        # ì•„í‹°íŒ©íŠ¸ ì¶”ì ì„ ìœ„í•œ í…Œì´ë¸” ìƒì„±
        self.episode_table = wandb.Table(columns=[
            "episode_id", "total_reward", "steps", "success", "accuracy", "efficiency"
        ])
        
        self.action_table = wandb.Table(columns=[
            "episode_id", "step", "action_type", "reward", "state_description"
        ])
    
    def log_training_metrics(self, epoch: int, metrics: Dict[str, float]):
        """í›ˆë ¨ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        if not self.run:
            return
            
        wandb.log({
            "epoch": epoch,
            "policy_loss": metrics.get("policy_loss", 0),
            "value_loss": metrics.get("value_loss", 0),
            "kl_divergence": metrics.get("kl_divergence", 0),
            "entropy": metrics.get("entropy", 0),
            "learning_rate": metrics.get("learning_rate", 0),
            "average_reward": metrics.get("average_reward", 0),
            "success_rate": metrics.get("success_rate", 0),
            "average_episode_length": metrics.get("average_episode_length", 0)
        })
    
    def log_episode(self, episode_data: Dict[str, Any]):
        """ì—í”¼ì†Œë“œ ë°ì´í„° ë¡œê¹…"""
        if not self.run:
            return
            
        episode_id = episode_data.get("episode_id", "unknown")
        total_reward = episode_data.get("total_reward", 0)
        steps = episode_data.get("steps", 0)
        success = episode_data.get("success", False)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        accuracy = episode_data.get("accuracy_score", 0)
        efficiency = 1.0 - (steps / episode_data.get("max_steps", steps)) if steps > 0 else 0
        
        # ì—í”¼ì†Œë“œ í…Œì´ë¸”ì— ì¶”ê°€
        self.episode_table.add_data(
            episode_id, total_reward, steps, success, accuracy, efficiency
        )
        
        # ì•¡ì…˜ë³„ ì„¸ë¶€ ë¡œê¹…
        for i, action in enumerate(episode_data.get("actions", [])):
            self.action_table.add_data(
                episode_id,
                i,
                action.get("type", "unknown"),
                action.get("reward", 0),
                action.get("state_description", "")[:100]  # ì²˜ìŒ 100ìë§Œ
            )
    
    def log_model_artifacts(self, model_path: str, epoch: int):
        """ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ë¡œê¹…"""
        if not self.run:
            return
            
        artifact = wandb.Artifact(
            name=f"model_epoch_{epoch}",
            type="model",
            description=f"ART model checkpoint at epoch {epoch}"
        )
        artifact.add_file(model_path)
        self.run.log_artifact(artifact)
    
    def finish_run(self):
        """ì‹¤í–‰ ì¢…ë£Œ ë° í…Œì´ë¸” ì—…ë¡œë“œ"""
        if not self.run:
            return
            
        # í…Œì´ë¸” ì—…ë¡œë“œ
        self.run.log({"episodes": self.episode_table})
        self.run.log({"actions": self.action_table})
        
        # ì‹¤í–‰ ì¢…ë£Œ
        wandb.finish()

# ì‚¬ìš© ì˜ˆì‹œ
logger = ARTWandbLogger("email-agent-training", "qwen2.5_email_search_v1")

# ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰ ì´ˆê¸°í™”
config = {
    "model_name": "Qwen/Qwen2.5-7B-Instruct",
    "agent_type": "email_search",
    "learning_rate": 1e-5,
    "batch_size": 8,
    "max_episodes": 1000,
    "max_steps_per_episode": 10
}

logger.initialize_run(config)

# í›ˆë ¨ ì¤‘ ë©”íŠ¸ë¦­ ë¡œê¹…
for epoch in range(100):
    # ... í›ˆë ¨ ë¡œì§ ...
    
    metrics = {
        "policy_loss": 0.123,
        "kl_divergence": 0.045,
        "average_reward": 0.67,
        "success_rate": 0.82
    }
    
    logger.log_training_metrics(epoch, metrics)

logger.finish_run()
```

### 2. Langfuse í†µí•©

```python
# langfuse_integration.py
from langfuse import Langfuse
from langfuse.decorators import observe, langfuse_context
from typing import Dict, List, Any
import json

class ARTLangfuseTracker:
    def __init__(self):
        self.langfuse = Langfuse()
        
    @observe()
    def track_episode(self, episode_data: Dict[str, Any]):
        """ì—í”¼ì†Œë“œ ì¶”ì """
        
        # ì—í”¼ì†Œë“œë¥¼ í•˜ë‚˜ì˜ íŠ¸ë ˆì´ìŠ¤ë¡œ ìƒì„±
        trace = self.langfuse.trace(
            name=f"episode_{episode_data.get('episode_id')}",
            input={
                "initial_state": episode_data.get("initial_state"),
                "task_description": episode_data.get("task_description")
            },
            output={
                "final_result": episode_data.get("final_result"),
                "total_reward": episode_data.get("total_reward"),
                "success": episode_data.get("success")
            },
            metadata={
                "agent_type": episode_data.get("agent_type"),
                "model_name": episode_data.get("model_name"),
                "environment": episode_data.get("environment_type")
            }
        )
        
        # ê° ì•¡ì…˜ì„ ê°œë³„ ìŠ¤íŒ¬ìœ¼ë¡œ ì¶”ì 
        for i, action in enumerate(episode_data.get("actions", [])):
            span = trace.span(
                name=f"action_{i}_{action.get('type')}",
                input={
                    "state": action.get("state"),
                    "action_type": action.get("type"),
                    "parameters": action.get("parameters")
                },
                output={
                    "result": action.get("result"),
                    "reward": action.get("reward"),
                    "next_state": action.get("next_state")
                },
                metadata={
                    "step_number": i,
                    "reasoning": action.get("reasoning"),
                    "execution_time": action.get("execution_time")
                }
            )
        
        return trace
    
    @observe()
    def track_model_generation(self, prompt: str, response: str, metadata: Dict[str, Any]):
        """ëª¨ë¸ ìƒì„± ì¶”ì """
        
        generation = self.langfuse.generation(
            name="agent_action_generation",
            model=metadata.get("model_name", "unknown"),
            input=prompt,
            output=response,
            metadata={
                "temperature": metadata.get("temperature", 0.7),
                "max_tokens": metadata.get("max_tokens", 150),
                "token_count": metadata.get("token_count", 0),
                "latency_ms": metadata.get("latency_ms", 0)
            }
        )
        
        return generation
    
    def track_training_session(self, session_data: Dict[str, Any]):
        """í›ˆë ¨ ì„¸ì…˜ ì¶”ì """
        
        session = self.langfuse.trace(
            name=f"training_session_{session_data.get('session_id')}",
            input={
                "config": session_data.get("config"),
                "start_time": session_data.get("start_time")
            },
            output={
                "final_metrics": session_data.get("final_metrics"),
                "total_episodes": session_data.get("total_episodes"),
                "training_time": session_data.get("training_time")
            },
            metadata={
                "experiment_name": session_data.get("experiment_name"),
                "model_checkpoints": session_data.get("model_checkpoints"),
                "hardware_info": session_data.get("hardware_info")
            }
        )
        
        return session
    
    def get_performance_analytics(self, trace_ids: List[str]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¶„ì„ ë°ì´í„° ì¶”ì¶œ"""
        
        analytics = {
            "total_traces": len(trace_ids),
            "success_rate": 0,
            "average_reward": 0,
            "average_steps": 0,
            "action_distribution": {},
            "error_patterns": []
        }
        
        successful_episodes = 0
        total_reward = 0
        total_steps = 0
        action_counts = {}
        
        for trace_id in trace_ids:
            trace = self.langfuse.get_trace(trace_id)
            
            # ì„±ê³µë¥  ê³„ì‚°
            if trace.output.get("success", False):
                successful_episodes += 1
            
            # ë³´ìƒ ëˆ„ì 
            reward = trace.output.get("total_reward", 0)
            total_reward += reward
            
            # ìŠ¤í… ìˆ˜ ê³„ì‚°
            spans = trace.observations
            steps = len([s for s in spans if s.type == "SPAN"])
            total_steps += steps
            
            # ì•¡ì…˜ ë¶„í¬ ê³„ì‚°
            for span in spans:
                if span.type == "SPAN" and span.name.startswith("action_"):
                    action_type = span.name.split("_")[-1]
                    action_counts[action_type] = action_counts.get(action_type, 0) + 1
        
        # ë¶„ì„ ê²°ê³¼ ê³„ì‚°
        analytics["success_rate"] = successful_episodes / len(trace_ids) if trace_ids else 0
        analytics["average_reward"] = total_reward / len(trace_ids) if trace_ids else 0
        analytics["average_steps"] = total_steps / len(trace_ids) if trace_ids else 0
        analytics["action_distribution"] = action_counts
        
        return analytics

# ì‚¬ìš© ì˜ˆì‹œ
tracker = ARTLangfuseTracker()

# ì—í”¼ì†Œë“œ ì¶”ì 
episode_data = {
    "episode_id": "ep_001",
    "agent_type": "email_search",
    "model_name": "Qwen/Qwen2.5-7B-Instruct",
    "initial_state": {"query": "find meeting emails"},
    "actions": [
        {
            "type": "search",
            "state": {"query": "find meeting emails"},
            "parameters": {"keywords": ["meeting", "schedule"]},
            "result": {"found_emails": 15},
            "reward": 0.3,
            "reasoning": "Initial search for meeting-related emails"
        }
    ],
    "final_result": {"selected_email": "email_123"},
    "total_reward": 0.85,
    "success": True
}

trace = tracker.track_episode(episode_data)
print(f"Episode tracked: {trace.id}")
```

## ì‹¤ì „ í”„ë¡œì íŠ¸: ê²Œì„ AI ì—ì´ì „íŠ¸

### 1. 2048 ê²Œì„ ì—ì´ì „íŠ¸

```python
# games/game_2048.py
import numpy as np
import random
from typing import Tuple, List, Dict, Any
from art.environment import Environment

class Game2048Environment(Environment):
    """2048 ê²Œì„ í™˜ê²½"""
    
    def __init__(self, board_size: int = 4):
        super().__init__()
        self.board_size = board_size
        self.board = None
        self.score = 0
        self.moves_count = 0
        
    async def reset(self) -> Dict[str, Any]:
        """ê²Œì„ ì´ˆê¸°í™”"""
        self.board = np.zeros((self.board_size, self.board_size), dtype=int)
        self.score = 0
        self.moves_count = 0
        
        # ì´ˆê¸° 2ê°œ ìˆ«ì ë°°ì¹˜
        self.add_random_tile()
        self.add_random_tile()
        
        return await self.get_state()
    
    async def step(self, action: Dict[str, Any]) -> Tuple[Dict, float, bool, Dict]:
        """ì•¡ì…˜ ì‹¤í–‰"""
        direction = action.get('direction')  # 'up', 'down', 'left', 'right'
        
        prev_board = self.board.copy()
        prev_score = self.score
        
        # ì´ë™ ì‹¤í–‰
        moved = self.move(direction)
        
        if moved:
            self.add_random_tile()
            self.moves_count += 1
        
        # ë³´ìƒ ê³„ì‚°
        reward = self.calculate_reward(prev_board, prev_score, moved)
        
        # ê²Œì„ ì¢…ë£Œ í™•ì¸
        done = self.is_game_over()
        
        state = await self.get_state()
        info = {
            'moved': moved,
            'score_increase': self.score - prev_score,
            'max_tile': np.max(self.board)
        }
        
        return state, reward, done, info
    
    def move(self, direction: str) -> bool:
        """ë³´ë“œ ì´ë™ ë° í•©ì¹˜ê¸°"""
        if direction == 'left':
            return self.move_left()
        elif direction == 'right':
            return self.move_right()
        elif direction == 'up':
            return self.move_up()
        elif direction == 'down':
            return self.move_down()
        return False
    
    def move_left(self) -> bool:
        """ì™¼ìª½ìœ¼ë¡œ ì´ë™"""
        moved = False
        for i in range(self.board_size):
            row = self.board[i, :]
            new_row, row_moved = self.merge_line(row)
            self.board[i, :] = new_row
            if row_moved:
                moved = True
        return moved
    
    def move_right(self) -> bool:
        """ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™"""
        moved = False
        for i in range(self.board_size):
            row = self.board[i, ::-1]  # ë’¤ì§‘ê¸°
            new_row, row_moved = self.merge_line(row)
            self.board[i, :] = new_row[::-1]  # ë‹¤ì‹œ ë’¤ì§‘ê¸°
            if row_moved:
                moved = True
        return moved
    
    def move_up(self) -> bool:
        """ìœ„ë¡œ ì´ë™"""
        moved = False
        for j in range(self.board_size):
            col = self.board[:, j]
            new_col, col_moved = self.merge_line(col)
            self.board[:, j] = new_col
            if col_moved:
                moved = True
        return moved
    
    def move_down(self) -> bool:
        """ì•„ë˜ë¡œ ì´ë™"""
        moved = False
        for j in range(self.board_size):
            col = self.board[::-1, j]  # ë’¤ì§‘ê¸°
            new_col, col_moved = self.merge_line(col)
            self.board[:, j] = new_col[::-1]  # ë‹¤ì‹œ ë’¤ì§‘ê¸°
            if col_moved:
                moved = True
        return moved
    
    def merge_line(self, line: np.ndarray) -> Tuple[np.ndarray, bool]:
        """í•œ ì¤„ í•©ì¹˜ê¸° ë¡œì§"""
        # 0ì´ ì•„ë‹Œ ìš”ì†Œë“¤ì„ ì™¼ìª½ìœ¼ë¡œ ì´ë™
        non_zero = line[line != 0]
        
        # ì¸ì ‘í•œ ê°™ì€ ìˆ«ì í•©ì¹˜ê¸°
        merged = []
        i = 0
        while i < len(non_zero):
            if i < len(non_zero) - 1 and non_zero[i] == non_zero[i + 1]:
                # í•©ì¹˜ê¸°
                merged_value = non_zero[i] * 2
                merged.append(merged_value)
                self.score += merged_value
                i += 2
            else:
                merged.append(non_zero[i])
                i += 1
        
        # ê²°ê³¼ ë°°ì—´ ìƒì„±
        result = np.zeros(self.board_size, dtype=int)
        result[:len(merged)] = merged
        
        # ë³€í™” ì—¬ë¶€ í™•ì¸
        moved = not np.array_equal(line, result)
        
        return result, moved
    
    def add_random_tile(self):
        """ë¬´ì‘ìœ„ ìœ„ì¹˜ì— 2 ë˜ëŠ” 4 ì¶”ê°€"""
        empty_cells = [(i, j) for i in range(self.board_size) 
                      for j in range(self.board_size) if self.board[i, j] == 0]
        
        if empty_cells:
            i, j = random.choice(empty_cells)
            self.board[i, j] = 2 if random.random() < 0.9 else 4
    
    def is_game_over(self) -> bool:
        """ê²Œì„ ì¢…ë£Œ í™•ì¸"""
        # ë¹ˆ ì¹¸ì´ ìˆìœ¼ë©´ ê³„ì† ê°€ëŠ¥
        if np.any(self.board == 0):
            return False
        
        # í•©ì¹  ìˆ˜ ìˆëŠ” ì¸ì ‘í•œ íƒ€ì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        for i in range(self.board_size):
            for j in range(self.board_size):
                current = self.board[i, j]
                
                # ì˜¤ë¥¸ìª½ í™•ì¸
                if j < self.board_size - 1 and self.board[i, j + 1] == current:
                    return False
                
                # ì•„ë˜ìª½ í™•ì¸
                if i < self.board_size - 1 and self.board[i + 1, j] == current:
                    return False
        
        return True
    
    def calculate_reward(self, prev_board: np.ndarray, prev_score: int, moved: bool) -> float:
        """ë³´ìƒ ê³„ì‚°"""
        if not moved:
            return -0.1  # ë¬´íš¨í•œ ì´ë™ í˜ë„í‹°
        
        # ì ìˆ˜ ì¦ê°€ ë³´ìƒ
        score_reward = (self.score - prev_score) / 100.0
        
        # ë¹ˆ ì¹¸ ë³´ìƒ
        empty_cells = np.sum(self.board == 0)
        empty_reward = empty_cells * 0.01
        
        # ìµœëŒ€ íƒ€ì¼ ë³´ìƒ
        max_tile = np.max(self.board)
        max_tile_reward = np.log2(max_tile) * 0.1 if max_tile > 0 else 0
        
        # ë³´ë“œ í‰í™œì„± ë³´ìƒ (ì¸ì ‘í•œ íƒ€ì¼ë“¤ì˜ ì°¨ì´ê°€ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
        smoothness = self.calculate_smoothness()
        smoothness_reward = smoothness * 0.05
        
        total_reward = score_reward + empty_reward + max_tile_reward + smoothness_reward
        
        return total_reward
    
    def calculate_smoothness(self) -> float:
        """ë³´ë“œ í‰í™œì„± ê³„ì‚°"""
        smoothness = 0
        
        for i in range(self.board_size):
            for j in range(self.board_size):
                if self.board[i, j] != 0:
                    value = np.log2(self.board[i, j])
                    
                    # ì˜¤ë¥¸ìª½ê³¼ ë¹„êµ
                    if j < self.board_size - 1 and self.board[i, j + 1] != 0:
                        target_value = np.log2(self.board[i, j + 1])
                        smoothness -= abs(value - target_value)
                    
                    # ì•„ë˜ìª½ê³¼ ë¹„êµ
                    if i < self.board_size - 1 and self.board[i + 1, j] != 0:
                        target_value = np.log2(self.board[i + 1, j])
                        smoothness -= abs(value - target_value)
        
        return smoothness
    
    async def get_state(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜"""
        return {
            'board': self.board.tolist(),
            'score': self.score,
            'moves_count': self.moves_count,
            'max_tile': int(np.max(self.board)),
            'empty_cells': int(np.sum(self.board == 0)),
            'available_moves': self.get_available_moves()
        }
    
    def get_available_moves(self) -> List[str]:
        """ê°€ëŠ¥í•œ ì´ë™ ë°©í–¥ ë°˜í™˜"""
        moves = []
        
        for direction in ['up', 'down', 'left', 'right']:
            # ì„ì‹œë¡œ ì´ë™í•´ë³´ê³  ë³€í™”ê°€ ìˆëŠ”ì§€ í™•ì¸
            temp_board = self.board.copy()
            temp_env = Game2048Environment(self.board_size)
            temp_env.board = temp_board
            
            if temp_env.move(direction):
                moves.append(direction)
        
        return moves

class Game2048Agent(Agent):
    """2048 ê²Œì„ ì—ì´ì „íŠ¸"""
    
    def __init__(self, model_name: str = "Qwen/Qwen2.5-7B-Instruct"):
        super().__init__(model_name)
        self.strategy = "minimax"  # ë˜ëŠ” "deep_learning"
        
    async def select_action(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ì•¡ì…˜ ì„ íƒ"""
        
        if self.strategy == "minimax":
            return await self.minimax_action(state)
        else:
            return await self.ml_based_action(state)
    
    async def minimax_action(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ë¯¸ë‹ˆë§¥ìŠ¤ ê¸°ë°˜ ì•¡ì…˜ ì„ íƒ"""
        board = np.array(state['board'])
        available_moves = state['available_moves']
        
        if not available_moves:
            return {'direction': 'up', 'reasoning': 'No moves available'}
        
        best_move = None
        best_score = -float('inf')
        
        for move in available_moves:
            # ê° ì´ë™ì— ëŒ€í•´ ì ìˆ˜ ê³„ì‚°
            score = self.evaluate_move(board, move, depth=3)
            
            if score > best_score:
                best_score = score
                best_move = move
        
        return {
            'direction': best_move,
            'reasoning': f'Minimax evaluation: {best_move} scored {best_score:.2f}',
            'expected_score': best_score
        }
    
    def evaluate_move(self, board: np.ndarray, direction: str, depth: int) -> float:
        """ì´ë™ í‰ê°€ (ë¯¸ë‹ˆë§¥ìŠ¤)"""
        if depth == 0:
            return self.evaluate_board(board)
        
        # ì„ì‹œ í™˜ê²½ì—ì„œ ì´ë™ ì‹œë®¬ë ˆì´ì…˜
        temp_env = Game2048Environment(board.shape[0])
        temp_env.board = board.copy()
        
        if not temp_env.move(direction):
            return -1000  # ë¬´íš¨í•œ ì´ë™
        
        # ìƒˆë¡œìš´ íƒ€ì¼ì´ ì¶”ê°€ë  ëª¨ë“  ê°€ëŠ¥í•œ ìœ„ì¹˜ì— ëŒ€í•´ í‰ê°€
        empty_cells = [(i, j) for i in range(board.shape[0]) 
                      for j in range(board.shape[0]) if temp_env.board[i, j] == 0]
        
        if not empty_cells:
            return self.evaluate_board(temp_env.board)
        
        total_score = 0
        for i, j in empty_cells:
            for value in [2, 4]:
                prob = 0.9 if value == 2 else 0.1
                
                # ìƒˆ íƒ€ì¼ ì¶”ê°€
                temp_env.board[i, j] = value
                
                # ë‹¤ìŒ ë ˆë²¨ í‰ê°€
                next_score = max([
                    self.evaluate_move(temp_env.board, next_dir, depth - 1)
                    for next_dir in ['up', 'down', 'left', 'right']
                ])
                
                total_score += prob * next_score
                
                # íƒ€ì¼ ì œê±°
                temp_env.board[i, j] = 0
        
        return total_score / len(empty_cells)
    
    def evaluate_board(self, board: np.ndarray) -> float:
        """ë³´ë“œ í‰ê°€ í•¨ìˆ˜"""
        # ë¹ˆ ì¹¸ ì ìˆ˜
        empty_score = np.sum(board == 0) * 10
        
        # ìµœëŒ€ íƒ€ì¼ ì ìˆ˜
        max_tile = np.max(board)
        max_score = np.log2(max_tile) * 100 if max_tile > 0 else 0
        
        # í‰í™œì„± ì ìˆ˜
        smoothness = self.calculate_smoothness(board) * 50
        
        # ë‹¨ì¡°ì„± ì ìˆ˜ (í° ìˆ˜ê°€ í•œìª½ìœ¼ë¡œ ëª°ë ¤ìˆìœ¼ë©´ ì¢‹ìŒ)
        monotonicity = self.calculate_monotonicity(board) * 100
        
        return empty_score + max_score + smoothness + monotonicity
    
    def calculate_smoothness(self, board: np.ndarray) -> float:
        """í‰í™œì„± ê³„ì‚°"""
        smoothness = 0
        size = board.shape[0]
        
        for i in range(size):
            for j in range(size):
                if board[i, j] != 0:
                    value = np.log2(board[i, j])
                    
                    for di, dj in [(0, 1), (1, 0)]:  # ì˜¤ë¥¸ìª½, ì•„ë˜
                        ni, nj = i + di, j + dj
                        if ni < size and nj < size and board[ni, nj] != 0:
                            target_value = np.log2(board[ni, nj])
                            smoothness -= abs(value - target_value)
        
        return smoothness
    
    def calculate_monotonicity(self, board: np.ndarray) -> float:
        """ë‹¨ì¡°ì„± ê³„ì‚°"""
        totals = [0, 0, 0, 0]  # up, down, left, right
        
        for i in range(board.shape[0]):
            current = 0
            next_val = 1
            
            # ìˆ˜í‰ ë‹¨ì¡°ì„±
            while next_val < board.shape[1]:
                while next_val < board.shape[1] and board[i, next_val] == 0:
                    next_val += 1
                
                if next_val >= board.shape[1]:
                    next_val -= 1
                
                current_value = np.log2(board[i, current]) if board[i, current] != 0 else 0
                next_value = np.log2(board[i, next_val]) if board[i, next_val] != 0 else 0
                
                if current_value > next_value:
                    totals[0] += next_value - current_value
                elif next_value > current_value:
                    totals[1] += current_value - next_value
                
                current = next_val
                next_val += 1
        
        # ìˆ˜ì§ ë‹¨ì¡°ì„±
        for j in range(board.shape[1]):
            current = 0
            next_val = 1
            
            while next_val < board.shape[0]:
                while next_val < board.shape[0] and board[next_val, j] == 0:
                    next_val += 1
                
                if next_val >= board.shape[0]:
                    next_val -= 1
                
                current_value = np.log2(board[current, j]) if board[current, j] != 0 else 0
                next_value = np.log2(board[next_val, j]) if board[next_val, j] != 0 else 0
                
                if current_value > next_value:
                    totals[2] += next_value - current_value
                elif next_value > current_value:
                    totals[3] += current_value - next_value
                
                current = next_val
                next_val += 1
        
        return max(totals[0], totals[1]) + max(totals[2], totals[3])

# í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
async def train_2048_agent():
    """2048 ì—ì´ì „íŠ¸ í›ˆë ¨"""
    
    env = Game2048Environment()
    agent = Game2048Agent()
    
    trainer = GRPOTrainer(
        agent=agent,
        environment=env,
        learning_rate=5e-6,
        batch_size=16,
        episodes_per_batch=64,
        max_episode_length=200
    )
    
    best_score = 0
    
    for epoch in range(500):
        print(f"Epoch {epoch + 1}/500")
        
        episodes = await trainer.collect_episodes()
        
        # ìµœê³  ì ìˆ˜ ì¶”ì 
        epoch_best = max([ep['total_reward'] for ep in episodes])
        if epoch_best > best_score:
            best_score = epoch_best
            await trainer.save_checkpoint(f"2048_best_model.pt")
        
        loss_info = await trainer.update_policy(episodes)
        
        if epoch % 50 == 0:
            print(f"Best Score: {best_score}")
            print(f"Average Reward: {loss_info['avg_reward']:.3f}")
            print(f"Policy Loss: {loss_info['policy_loss']:.6f}")

if __name__ == "__main__":
    asyncio.run(train_2048_agent())
```

## ê²°ë¡ 

**OpenPipe ART (Agent Reinforcement Trainer)**ëŠ” ì‹¤ì œ ì—…ë¬´ í™˜ê²½ì—ì„œ ë©€í‹°ìŠ¤í… AI ì—ì´ì „íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í›ˆë ¨í•  ìˆ˜ ìˆëŠ” í˜ì‹ ì ì¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

### ğŸ¯ í•µì‹¬ ì„±ê³¼

1. **GRPO ì•Œê³ ë¦¬ì¦˜**: ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ê°•í™”í•™ìŠµ í›ˆë ¨
2. **ì‹¤ë¬´ ì ìš©ì„±**: ì´ë©”ì¼ ê²€ìƒ‰, ê²Œì„, ì½”ë”© ë“± ë‹¤ì–‘í•œ ì‹¤ì œ íƒœìŠ¤í¬ ì§€ì›
3. **í™•ì¥ì„±**: í´ë¼ì´ì–¸íŠ¸-ì„œë²„ ì•„í‚¤í…ì²˜ë¡œ ëŒ€ê·œëª¨ ë¶„ì‚° í›ˆë ¨ ê°€ëŠ¥
4. **ìë™í™”**: RULER í†µí•©ìœ¼ë¡œ ë³´ìƒí•¨ìˆ˜ ì—”ì§€ë‹ˆì–´ë§ ìƒëµ

### ğŸš€ ì‹¤ë¬´ ì ìš© í¬ì¸íŠ¸

- **ìŠ¤íƒ€íŠ¸ì—…**: ì œí•œëœ ìì›ìœ¼ë¡œ ì‹¤ìš©ì ì¸ AI ì—ì´ì „íŠ¸ ê°œë°œ
- **ê¸°ì—…**: ì—…ë¬´ ìë™í™”ë¥¼ ìœ„í•œ íŠ¹í™” ì—ì´ì „íŠ¸ í›ˆë ¨
- **ì—°êµ¬ê¸°ê´€**: ë©€í‹°ìŠ¤í… ê°•í™”í•™ìŠµ ì—°êµ¬ í”Œë«í¼
- **ê°œë°œì**: ê²Œì„, íˆ´, ì„œë¹„ìŠ¤ì— í†µí•© ê°€ëŠ¥í•œ ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸

### ğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„

1. **ê¸°ë³¸ ì˜ˆì œ**: ì´ë©”ì¼ ê²€ìƒ‰ ì—ì´ì „íŠ¸ë¡œ ì‹œì‘
2. **ì»¤ìŠ¤í…€ í™˜ê²½**: ìì‹ ë§Œì˜ íƒœìŠ¤í¬ í™˜ê²½ êµ¬í˜„
3. **í”„ë¡œë•ì…˜ ë°°í¬**: ì‹¤ì œ ì„œë¹„ìŠ¤ì— ì—ì´ì „íŠ¸ í†µí•©
4. **ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬**: [OpenPipe ART GitHub](https://github.com/OpenPipe/ART)ì— ê°œì„ ì‚¬í•­ ê¸°ì—¬

OpenPipe ARTë¥¼ í†µí•´ ì°¨ì„¸ëŒ€ ì‹¤ë¬´í˜• AI ì—ì´ì „íŠ¸ë¥¼ ê°œë°œí•´ë³´ì„¸ìš”! ğŸš€

---

**ì°¸ê³  ìë£Œ**:
- [OpenPipe ART GitHub](https://github.com/OpenPipe/ART)
- [GRPO ë…¼ë¬¸](https://arxiv.org/abs/2402.03300)
- [OpenPipe ê³µì‹ ë¬¸ì„œ](https://docs.openpipe.ai/)
- [Langfuse í†µí•© ê°€ì´ë“œ](https://langfuse.com/docs) 