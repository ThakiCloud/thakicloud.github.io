---
title: "Unslothë¥¼ í™œìš©í•œ í•œêµ­ì–´ íŠ¹í™” LLM í•™ìŠµ ì™„ì „ ê°€ì´ë“œ"
excerpt: "Unslothë¡œ ë†’ì€ ìˆ˜ì¤€ì˜ í•œêµ­ì–´ íŠ¹í™” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë‹¨ê³„ë³„ ì‹¤ë¬´ ê°€ì´ë“œ"
date: 2025-06-17
categories:
  - llmops
tags:
  - Unsloth
  - Korean LLM
  - Fine-tuning
  - LoRA
  - SFT
  - RLHF
  - Language Model Training
  - Korean NLP
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
---

## ê°œìš”

ë³¸ ê°€ì´ë“œëŠ” Unslothë¥¼ í™œìš©í•˜ì—¬ ë†’ì€ ìˆ˜ì¤€ì˜ í•œêµ­ì–´ íŠ¹í™” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤. í™˜ê²½ ì„¤ì •ë¶€í„° ë°°í¬ê¹Œì§€ ì „ ê³¼ì •ì„ ì‹¤ë¬´ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤.

**í•™ìŠµ ëª©í‘œ**:
- ğŸš€ **Unsloth ê¸°ë°˜ íš¨ìœ¨ì  í•™ìŠµ**: ë©”ëª¨ë¦¬ ìµœì í™”ëœ í•œêµ­ì–´ LLM í›ˆë ¨
- ğŸ‡°ğŸ‡· **í•œêµ­ì–´ íŠ¹í™” ìµœì í™”**: í† í¬ë‚˜ì´ì €ë¶€í„° ë°ì´í„°ì…‹ê¹Œì§€ í•œêµ­ì–´ ìµœì í™”
- ğŸ“Š **3ë‹¨ê³„ í•™ìŠµ íŒŒì´í”„ë¼ì¸**: CPT â†’ SFT â†’ RLHF ì™„ì „ êµ¬í˜„
- âš¡ **ì‹¤ë¬´ ë°°í¬**: ì˜¨í”„ë ˆë¯¸ìŠ¤ ë° í´ë¼ìš°ë“œ ë°°í¬ ê°€ì´ë“œ

---

## 1. í™˜ê²½ ì„¤ì • ë° ì¤€ë¹„

### 1.1 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
# Unsloth ì„¤ì¹˜ (ìµœì‹  ë²„ì „)
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install --no-deps xformers trl peft accelerate bitsandbytes

# ì¶”ê°€ ì˜ì¡´ì„±
pip install datasets transformers tokenizers sentencepiece
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# í•œêµ­ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install konlpy kss soynlp
```

### 1.2 ê¸°ë³¸ import ë° ì„¤ì •

```python
import torch
from unsloth import FastLanguageModel
from datasets import Dataset, load_dataset
from transformers import TrainingArguments, AutoTokenizer
from trl import SFTTrainer, PPOTrainer, PPOConfig
import json
import pandas as pd
from typing import List, Dict
import warnings
warnings.filterwarnings("ignore")

# GPU ë©”ëª¨ë¦¬ ì„¤ì •
torch.cuda.empty_cache()
print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
```

### 1.3 í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­

**ê¶Œì¥ ì‚¬ì–‘**:
- **GPU**: RTX 4090 (24GB) ë˜ëŠ” A100 (40GB) ì´ìƒ
- **RAM**: 64GB ì´ìƒ
- **ì €ì¥ê³µê°„**: 500GB ì´ìƒ (ëª¨ë¸ + ë°ì´í„°ì…‹)

**ìµœì†Œ ì‚¬ì–‘**:
- **GPU**: RTX 3080 (10GB) + 4bit ì–‘ìí™”
- **RAM**: 32GB
- **ì €ì¥ê³µê°„**: 200GB

---

## 2. í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì¤€ë¹„ ë° ì ì‘

### 2.1 í•œêµ­ì–´ íŠ¹í™” í† í¬ë‚˜ì´ì € êµ¬ì„±

```python
from sentencepiece import SentencePieceProcessor, SentencePieceTrainer
import re

class KoreanTokenizerBuilder:
    def __init__(self, vocab_size=32000):
        self.vocab_size = vocab_size
        
    def prepare_korean_corpus(self, text_files):
        """í•œêµ­ì–´ ì½”í¼ìŠ¤ ì „ì²˜ë¦¬"""
        processed_texts = []
        
        for file_path in text_files:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
                
                # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ê·œí™”
                text = self.normalize_korean_text(text)
                processed_texts.append(text)
        
        return processed_texts
    
    def normalize_korean_text(self, text):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # í•œêµ­ì–´ íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
        text = text.replace('ã†', 'Â·')
        text = text.replace('ï½', '~')
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        return text.strip()
    
    def train_korean_tokenizer(self, corpus_files):
        """í•œêµ­ì–´ íŠ¹í™” í† í¬ë‚˜ì´ì € í•™ìŠµ"""
        SentencePieceTrainer.train(
            input=','.join(corpus_files),
            model_prefix='korean_tokenizer',
            vocab_size=self.vocab_size,
            character_coverage=0.9995,
            model_type='bpe',
            # í•œêµ­ì–´ íŠ¹ìˆ˜ ì²˜ë¦¬
            normalization_rule_name='nmt_nfkc_cf',
            remove_extra_whitespaces=True,
            input_sentence_size=10000000,
            shuffle_input_sentence=True,
            # í•œêµ­ì–´ íŠ¹ìˆ˜ í† í°
            user_defined_symbols=['<í•œêµ­ì–´>', '<ì˜ì–´>', '<ì½”ë“œ>'],
            unk_surface=' â‡ '
        )
        
        return 'korean_tokenizer.model'

# ì‚¬ìš© ì˜ˆì‹œ
tokenizer_builder = KoreanTokenizerBuilder(vocab_size=32000)
korean_corpus_files = ['korean_wiki.txt', 'korean_news.txt', 'korean_books.txt']
tokenizer_path = tokenizer_builder.train_korean_tokenizer(korean_corpus_files)
```

### 2.2 ëª¨ë¸ ë¡œë”© ë° í† í¬ë‚˜ì´ì € ì ìš©

```python
def load_model_with_korean_optimization(model_name="unsloth/Qwen2.5-7B-bnb-4bit"):
    """í•œêµ­ì–´ ìµœì í™”ëœ ëª¨ë¸ ë¡œë”©"""
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_name,
        max_seq_length=131072,  # ë†’ì€ê³¼ ë™ì¼í•œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
        dtype=None,  # ìë™ ê°ì§€ (bfloat16 ê¶Œì¥)
        load_in_4bit=True,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
        trust_remote_code=True
    )
    
    # í•œêµ­ì–´ íŠ¹ìˆ˜ í† í° ì¶”ê°€
    special_tokens = {
        "pad_token": "<pad>",
        "eos_token": "</s>",
        "bos_token": "<s>",
        "unk_token": "<unk>",
        "additional_special_tokens": ["<í•œêµ­ì–´>", "<ì˜ì–´>", "<ì½”ë“œ>", "<ìˆ˜í•™>"]
    }
    
    tokenizer.add_special_tokens(special_tokens)
    model.resize_token_embeddings(len(tokenizer))
    
    # LoRA ì–´ëŒ‘í„° ì„¤ì • (í•œêµ­ì–´ ìµœì í™”)
    model = FastLanguageModel.get_peft_model(
        model,
        r=64,  # LoRA rank (ë†’ì„ìˆ˜ë¡ ì„±ëŠ¥ í–¥ìƒ, ë©”ëª¨ë¦¬ ì¦ê°€)
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        lora_alpha=16,
        lora_dropout=0.1,
        bias="none",
        use_gradient_checkpointing="unsloth",  # Unsloth ìµœì í™”
        random_state=3407,
        use_rslora=False,  # Rank Stabilized LoRA
        loftq_config=None,  # LoftQ ì–‘ìí™”
    )
    
    return model, tokenizer

# ëª¨ë¸ ë¡œë”©
model, tokenizer = load_model_with_korean_optimization()
print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,}")
print(f"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters(only_trainable=True):,}")
```

---

## 3. í•œêµ­ì–´ ë°ì´í„°ì…‹ êµ¬ì„±

### 3.1 ê³ í’ˆì§ˆ í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘

```python
class KoreanDatasetBuilder:
    def __init__(self):
        self.datasets = []
        
    def collect_web_data(self):
        """ì›¹ì—ì„œ ê³ í’ˆì§ˆ í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘"""
        web_sources = {
            "korean_wikipedia": {
                "url": "https://dumps.wikimedia.org/kowiki/",
                "quality": "high",
                "size": "1.2GB"
            },
            "korean_news": {
                "sources": ["ì—°í•©ë‰´ìŠ¤", "ì¡°ì„ ì¼ë³´", "í•œê²¨ë ˆ"],
                "quality": "high", 
                "size": "800MB"
            },
            "korean_blogs": {
                "platforms": ["ë„¤ì´ë²„ ë¸”ë¡œê·¸", "í‹°ìŠ¤í† ë¦¬"],
                "quality": "medium",
                "size": "2.1GB"
            }
        }
        return web_sources
    
    def collect_academic_data(self):
        """í•™ìˆ  ìë£Œ ìˆ˜ì§‘"""
        academic_sources = {
            "korean_papers": "í•œêµ­ í•™ìˆ  ë…¼ë¬¸ (KISS, DBpia)",
            "textbooks": "í•œêµ­ì–´ êµê³¼ì„œ ë° ì „ë¬¸ì„œì ",
            "legal_documents": "ë²•ë¥  ë¬¸ì„œ (êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°)"
        }
        return academic_sources
    
    def generate_synthetic_data(self, base_model):
        """í•©ì„± ë°ì´í„° ìƒì„±"""
        synthetic_templates = {
            "qa_pairs": self.generate_korean_qa_pairs(base_model),
            "conversations": self.generate_korean_conversations(base_model),
            "instructions": self.generate_korean_instructions(base_model)
        }
        return synthetic_templates
    
    def create_balanced_dataset(self):
        """ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ êµ¬ì„±"""
        # ë†’ì€ ì–¸ì–´ ë¹„ìœ¨ ì°¸ê³ : í•œêµ­ì–´ 42%, ì˜ì–´ 51%, ê¸°íƒ€ 7%
        dataset_composition = {
            "korean": {
                "ratio": 0.42,
                "sources": ["wiki", "news", "books", "academic", "synthetic"],
                "total_tokens": "420B"
            },
            "english": {
                "ratio": 0.51,
                "sources": ["common_crawl", "books", "academic", "code"],
                "total_tokens": "510B"
            },
            "others": {
                "ratio": 0.07,
                "sources": ["code", "math", "multilingual"],
                "total_tokens": "70B"
            }
        }
        
        return dataset_composition

# ë°ì´í„°ì…‹ ë¹Œë” ì´ˆê¸°í™”
dataset_builder = KoreanDatasetBuilder()
dataset_composition = dataset_builder.create_balanced_dataset()
```

### 3.2 ë°ì´í„° ì „ì²˜ë¦¬ ë° í¬ë§·íŒ…

```python
def preprocess_korean_dataset(raw_data_path, output_path):
    """í•œêµ­ì–´ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬"""
    import json
    from tqdm import tqdm
    
    processed_data = []
    
    with open(raw_data_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f, desc="ë°ì´í„° ì „ì²˜ë¦¬"):
            try:
                item = json.loads(line)
                
                # í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì‚¬
                if is_high_quality_korean_text(item['text']):
                    processed_item = {
                        "text": normalize_korean_text(item['text']),
                        "source": item.get('source', 'unknown'),
                        "length": len(item['text']),
                        "language": "korean"
                    }
                    processed_data.append(processed_item)
                    
            except Exception as e:
                continue
    
    # ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in processed_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_data):,}ê°œ ìƒ˜í”Œ")
    return processed_data

def is_high_quality_korean_text(text):
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì‚¬"""
    import re
    
    # ìµœì†Œ ê¸¸ì´ ê²€ì‚¬
    if len(text) < 50:
        return False
    
    # í•œêµ­ì–´ ë¹„ìœ¨ ê²€ì‚¬
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    total_chars = len(text)
    korean_ratio = korean_chars / total_chars if total_chars > 0 else 0
    
    if korean_ratio < 0.3:  # í•œêµ­ì–´ ë¹„ìœ¨ 30% ì´ìƒ
        return False
    
    # ë°˜ë³µ íŒ¨í„´ ê²€ì‚¬
    if has_repetitive_patterns(text):
        return False
    
    return True

def format_instruction_dataset(raw_instructions):
    """Instruction ë°ì´í„°ì…‹ í¬ë§·íŒ…"""
    formatted_data = []
    
    for item in raw_instructions:
        # Alpaca ìŠ¤íƒ€ì¼ í¬ë§·
        if item.get('input'):
            formatted_text = f"""### ì§€ì‹œì‚¬í•­:
{item['instruction']}

### ì…ë ¥:
{item['input']}

### ì‘ë‹µ:
{item['output']}"""
        else:
            formatted_text = f"""### ì§€ì‹œì‚¬í•­:
{item['instruction']}

### ì‘ë‹µ:
{item['output']}"""
        
        formatted_data.append({
            "text": formatted_text,
            "type": "instruction"
        })
    
    return formatted_data
```

---

## 4. ë‹¨ê³„ë³„ í•™ìŠµ ê³¼ì •

### 4.1 1ë‹¨ê³„: ì§€ì†ì  ì‚¬ì „í•™ìŠµ (CPT)

```python
def continuous_pretraining(model, tokenizer, korean_corpus_path):
    """ì§€ì†ì  ì‚¬ì „í•™ìŠµ ë‹¨ê³„"""
    
    print("=== 1ë‹¨ê³„: ì§€ì†ì  ì‚¬ì „í•™ìŠµ ì‹œì‘ ===")
    
    # ëŒ€ìš©ëŸ‰ í•œêµ­ì–´ ì½”í¼ìŠ¤ ë¡œë”©
    dataset = load_dataset("json", data_files=korean_corpus_path, split="train")
    
    def tokenize_function(examples):
        # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        return tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=4096,  # CPTì—ì„œëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì§§ì€ ì‹œí€€ìŠ¤ ì‚¬ìš©
            return_overflowing_tokens=True,
        )
    
    # í† í¬ë‚˜ì´ì§• (ë°°ì¹˜ ì²˜ë¦¬)
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        batch_size=1000,
        num_proc=4,
        remove_columns=dataset.column_names,
    )
    
    # CPT í•™ìŠµ ì„¤ì •
    cpt_args = TrainingArguments(
        output_dir="./korean_cpt_output",
        overwrite_output_dir=True,
        num_train_epochs=2,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=32,
        learning_rate=1e-5,  # CPTì—ì„œëŠ” ë‚®ì€ í•™ìŠµë¥ 
        weight_decay=0.01,
        logging_steps=100,
        save_steps=2000,
        save_total_limit=3,
        warmup_steps=1000,
        fp16=True,
        gradient_checkpointing=True,
        dataloader_pin_memory=False,
        remove_unused_columns=False,
        report_to="none",  # wandb ì‚¬ìš© ì‹œ "wandb"
    )
    
    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        args=cpt_args,
        train_dataset=tokenized_dataset,
        dataset_text_field="text",
        max_seq_length=4096,
        packing=True,  # íš¨ìœ¨ì ì¸ ë°°ì¹˜ íŒ¨í‚¹
    )
    
    # í•™ìŠµ ì‹¤í–‰
    trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    trainer.save_model("./korean_cpt_final")
    print("=== CPT ì™„ë£Œ ===")
    
    return model

# CPT ì‹¤í–‰
model = continuous_pretraining(model, tokenizer, "korean_corpus.jsonl")
```

### 4.2 2ë‹¨ê³„: ì§€ë„ ë¯¸ì„¸ì¡°ì • (SFT)

```python
def supervised_fine_tuning(model, tokenizer, instruction_dataset_path):
    """ì§€ë„ ë¯¸ì„¸ì¡°ì • ë‹¨ê³„"""
    
    print("=== 2ë‹¨ê³„: ì§€ë„ ë¯¸ì„¸ì¡°ì • ì‹œì‘ ===")
    
    # Instruction ë°ì´í„°ì…‹ ë¡œë”©
    sft_dataset = load_dataset("json", data_files=instruction_dataset_path, split="train")
    
    def formatting_prompts_func(examples):
        """í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… í•¨ìˆ˜"""
        texts = []
        for i in range(len(examples["instruction"])):
            instruction = examples["instruction"][i]
            input_text = examples["input"][i] if examples["input"][i] else ""
            output = examples["output"][i]
            
            if input_text:
                text = f"""### ì§€ì‹œì‚¬í•­:
{instruction}

### ì…ë ¥:
{input_text}

### ì‘ë‹µ:
{output}<|endoftext|>"""
            else:
                text = f"""### ì§€ì‹œì‚¬í•­:
{instruction}

### ì‘ë‹µ:
{output}<|endoftext|>"""
            
            texts.append(text)
        
        return {"text": texts}
    
    # ë°ì´í„°ì…‹ í¬ë§·íŒ…
    sft_dataset = sft_dataset.map(
        formatting_prompts_func,
        batched=True,
        num_proc=4,
    )
    
    # SFT í•™ìŠµ ì„¤ì •
    sft_args = TrainingArguments(
        output_dir="./korean_sft_output",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,
        learning_rate=2e-5,  # SFTì—ì„œëŠ” ë†’ì€ í•™ìŠµë¥ 
        weight_decay=0.01,
        logging_steps=50,
        save_steps=1000,
        warmup_steps=100,
        fp16=True,
        optim="adamw_8bit",
        lr_scheduler_type="cosine",
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="loss",
        greater_is_better=False,
    )
    
    # SFT íŠ¸ë ˆì´ë„ˆ
    sft_trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=sft_dataset,
        dataset_text_field="text",
        max_seq_length=2048,
        args=sft_args,
        packing=False,  # SFTì—ì„œëŠ” íŒ¨í‚¹ ë¹„í™œì„±í™”
    )
    
    # í•™ìŠµ ì‹¤í–‰
    sft_trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    sft_trainer.save_model("./korean_sft_final")
    print("=== SFT ì™„ë£Œ ===")
    
    return model

# SFT ì‹¤í–‰
model = supervised_fine_tuning(model, tokenizer, "korean_instructions.jsonl")
```

### 4.3 3ë‹¨ê³„: ì¸ê°„ í”¼ë“œë°± ê°•í™”í•™ìŠµ (RLHF)

```python
def reinforcement_learning_from_human_feedback(model, tokenizer, preference_data_path):
    """RLHF ë‹¨ê³„"""
    
    print("=== 3ë‹¨ê³„: RLHF ì‹œì‘ ===")
    
    # ì°¸ì¡° ëª¨ë¸ (frozen)
    ref_model, _ = FastLanguageModel.from_pretrained(
        "korean_sft_final",
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )
    ref_model = FastLanguageModel.for_inference(ref_model)
    
    # PPO ì„¤ì •
    ppo_config = PPOConfig(
        model_name="korean_rlhf_model",
        learning_rate=1e-6,  # RLHFì—ì„œëŠ” ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ 
        batch_size=16,
        mini_batch_size=4,
        gradient_accumulation_steps=4,
        optimize_cuda_cache=True,
        early_stopping=True,
        target_kl=0.1,
        ppo_epochs=4,
        seed=42,
        log_with="tensorboard",
    )
    
    # ë³´ìƒ ëª¨ë¸ ë¡œë”© (ì‚¬ì „ í•™ìŠµëœ í•œêµ­ì–´ ë³´ìƒ ëª¨ë¸)
    reward_model = load_korean_reward_model()
    
    # PPO íŠ¸ë ˆì´ë„ˆ
    ppo_trainer = PPOTrainer(
        config=ppo_config,
        model=model,
        ref_model=ref_model,
        tokenizer=tokenizer,
        reward_model=reward_model,
    )
    
    # ê°•í™”í•™ìŠµ ë°ì´í„° ë¡œë”©
    rl_dataset = load_dataset("json", data_files=preference_data_path, split="train")
    
    # ê°•í™”í•™ìŠµ ì‹¤í–‰
    for epoch in range(3):
        for batch in rl_dataset.iter(batch_size=16):
            # ì¿¼ë¦¬ ì¤€ë¹„
            query_tensors = [
                tokenizer.encode(query, return_tensors="pt")[0] 
                for query in batch["query"]
            ]
            
            # ì‘ë‹µ ìƒì„±
            response_tensors = ppo_trainer.generate(
                query_tensors,
                return_prompt=False,
                max_new_tokens=256,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
            )
            
            # ë³´ìƒ ê³„ì‚°
            rewards = compute_rewards(batch["query"], response_tensors, reward_model)
            
            # PPO ì—…ë°ì´íŠ¸
            stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
            
            if epoch % 100 == 0:
                print(f"Epoch {epoch}, Reward: {torch.mean(rewards):.4f}")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    ppo_trainer.save_model("./korean_rlhf_final")
    print("=== RLHF ì™„ë£Œ ===")
    
    return model

def load_korean_reward_model():
    """í•œêµ­ì–´ ë³´ìƒ ëª¨ë¸ ë¡œë”©"""
    # ì‹¤ì œë¡œëŠ” ì‚¬ì „ì— í•™ìŠµëœ í•œêµ­ì–´ ë³´ìƒ ëª¨ë¸ì„ ë¡œë”©
    # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ë”ë¯¸ ëª¨ë¸ ë°˜í™˜
    class DummyRewardModel:
        def __call__(self, queries, responses):
            # ì‹¤ì œë¡œëŠ” ì‘ë‹µì˜ í’ˆì§ˆ, ìœ ìš©ì„±, ì•ˆì „ì„± ë“±ì„ í‰ê°€
            return torch.randn(len(responses))
    
    return DummyRewardModel()

def compute_rewards(queries, responses, reward_model):
    """ë³´ìƒ ê³„ì‚°"""
    rewards = reward_model(queries, responses)
    return rewards
```

---

## 5. í‰ê°€ ë° ë²¤ì¹˜ë§ˆí¬

### 5.1 í•œêµ­ì–´ ëŠ¥ë ¥ ì¢…í•© í‰ê°€

```python
def evaluate_korean_capabilities(model, tokenizer):
    """í•œêµ­ì–´ ëŠ¥ë ¥ ì¢…í•© í‰ê°€"""
    
    print("=== í•œêµ­ì–´ ëŠ¥ë ¥ í‰ê°€ ì‹œì‘ ===")
    
    benchmarks = {
        "KMMLU": evaluate_kmmlu,
        "CLIcK": evaluate_click,
        "Ko-IFEval": evaluate_ko_ifeval,
        "Korean-QA": evaluate_korean_qa,
        "Korean-Math": evaluate_korean_math
    }
    
    results = {}
    for benchmark_name, eval_func in benchmarks.items():
        print(f"\n{benchmark_name} í‰ê°€ ì¤‘...")
        score = eval_func(model, tokenizer)
        results[benchmark_name] = score
        print(f"{benchmark_name}: {score:.1f}%")
    
    # ì¢…í•© ì ìˆ˜ ê³„ì‚°
    overall_score = sum(results.values()) / len(results)
    print(f"\n=== ì¢…í•© ì ìˆ˜: {overall_score:.1f}% ===")
    
    return results

def evaluate_kmmlu(model, tokenizer):
    """KMMLU ë²¤ì¹˜ë§ˆí¬ í‰ê°€"""
    # í•œêµ­ì–´ ì „ë¬¸ ì§€ì‹ ì´í•´ë„ í‰ê°€
    kmmlu_dataset = load_dataset("HAERAE-HUB/KMMLU", split="test")
    
    correct = 0
    total = 0
    
    for item in kmmlu_dataset:
        prompt = format_kmmlu_prompt(item)
        response = generate_response(model, tokenizer, prompt)
        
        if check_answer_correctness(response, item["answer"]):
            correct += 1
        total += 1
        
        if total % 100 == 0:
            print(f"ì§„í–‰ë¥ : {total}/{len(kmmlu_dataset)}")
    
    accuracy = (correct / total) * 100
    return accuracy

def format_kmmlu_prompt(item):
    """KMMLU í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
    choices = '\n'.join([f"{chr(65+i)}. {choice}" for i, choice in enumerate(item["choices"])])
    
    prompt = f"""ë‹¤ìŒ ë¬¸ì œë¥¼ ì½ê³  ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”.

ë¬¸ì œ: {item["question"]}

ì„ íƒì§€:
{choices}

ì •ë‹µ:"""
    
    return prompt

def generate_response(model, tokenizer, prompt, max_length=512):
    """ëª¨ë¸ ì‘ë‹µ ìƒì„±"""
    inputs = tokenizer(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            temperature=0.1,  # í‰ê°€ì—ì„œëŠ” ë‚®ì€ temperature
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(prompt):].strip()
```

### 5.2 í† í° íš¨ìœ¨ì„± ì¸¡ì •

```python
def measure_token_efficiency(tokenizer, test_texts):
    """í† í° íš¨ìœ¨ì„± ì¸¡ì •"""
    
    print("=== í† í° íš¨ìœ¨ì„± ì¸¡ì • ===")
    
    korean_stats = {"chars": 0, "tokens": 0}
    english_stats = {"chars": 0, "tokens": 0}
    
    for text in test_texts:
        char_count = len(text)
        token_count = len(tokenizer.encode(text))
        
        if is_korean_text(text):
            korean_stats["chars"] += char_count
            korean_stats["tokens"] += token_count
        else:
            english_stats["chars"] += char_count
            english_stats["tokens"] += token_count
    
    # ë¬¸ì/í† í° ë¹„ìœ¨ ê³„ì‚°
    korean_ratio = korean_stats["chars"] / korean_stats["tokens"]
    english_ratio = english_stats["chars"] / english_stats["tokens"]
    
    print(f"í•œêµ­ì–´ ë¬¸ì/í† í° ë¹„ìœ¨: {korean_ratio:.2f}")
    print(f"ì˜ì–´ ë¬¸ì/í† í° ë¹„ìœ¨: {english_ratio:.2f}")
    
    # GPT-4oì™€ ë¹„êµ (ì°¸ê³ ê°’)
    gpt4o_korean_ratio = 1.5
    efficiency_improvement = korean_ratio / gpt4o_korean_ratio
    
    print(f"GPT-4o ëŒ€ë¹„ í•œêµ­ì–´ íš¨ìœ¨ì„±: {efficiency_improvement:.2f}x")
    
    return korean_ratio, efficiency_improvement

def is_korean_text(text):
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ íŒë³„"""
    import re
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    return korean_chars / len(text) > 0.5 if text else False
```

---

## 6. ëª¨ë¸ ë°°í¬ ë° ìµœì í™”

### 6.1 ëª¨ë¸ ì €ì¥ ë° ë³€í™˜

```python
def save_and_optimize_model(model, tokenizer, output_dir="./ax4_korean_final"):
    """ëª¨ë¸ ì €ì¥ ë° ìµœì í™”"""
    
    print("=== ëª¨ë¸ ì €ì¥ ë° ìµœì í™” ì‹œì‘ ===")
    
    # 1. Unsloth ìµœì í™”ëœ ëª¨ë¸ ì €ì¥
    model.save_pretrained_merged(
        output_dir,
        tokenizer,
        save_method="merged_16bit"  # 16bit ì •ë°€ë„ë¡œ ì €ì¥
    )
    print("âœ“ 16bit ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    
    # 2. 4bit ì–‘ìí™” ëª¨ë¸ ì €ì¥ (ê²½ëŸ‰í™”)
    model.save_pretrained_merged(
        output_dir + "_4bit",
        tokenizer,
        save_method="merged_4bit"
    )
    print("âœ“ 4bit ì–‘ìí™” ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    
    # 3. GGUF í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (CPU ì¶”ë¡ ìš©)
    model.save_pretrained_gguf(
        output_dir + "_gguf",
        tokenizer,
        quantization_method="q4_k_m"  # 4bit ì–‘ìí™”
    )
    print("âœ“ GGUF ëª¨ë¸ ë³€í™˜ ì™„ë£Œ")
    
    # 4. vLLM í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ì €ì¥
    try:
        model.save_pretrained_vllm(
            output_dir + "_vllm",
            tokenizer
        )
        print("âœ“ vLLM í˜¸í™˜ ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    except:
        print("âš  vLLM ë³€í™˜ ì‹¤íŒ¨ (ìˆ˜ë™ ë³€í™˜ í•„ìš”)")
    
    # 5. ëª¨ë¸ ì •ë³´ ì €ì¥
    model_info = {
        "model_name": "AX4.0-Korean",
        "base_model": "Qwen2.5-7B",
        "training_stages": ["CPT", "SFT", "RLHF"],
        "context_length": 131072,
        "vocab_size": len(tokenizer),
        "parameters": model.num_parameters(),
        "trainable_parameters": model.num_parameters(only_trainable=True)
    }
    
    with open(f"{output_dir}/model_info.json", "w", encoding="utf-8") as f:
        json.dump(model_info, f, indent=2, ensure_ascii=False)
    
    print("=== ëª¨ë¸ ì €ì¥ ì™„ë£Œ ===")
    return output_dir

# ëª¨ë¸ ì €ì¥ ì‹¤í–‰
final_model_path = save_and_optimize_model(model, tokenizer)
```

### 6.2 ë°°í¬ ì¤€ë¹„

```python
def prepare_deployment_configs():
    """ë°°í¬ìš© ì„¤ì • íŒŒì¼ ìƒì„±"""
    
    # vLLM ì„œë¹™ ì„¤ì •
    vllm_config = {
        "model": "./ax4_korean_final_vllm",
        "host": "0.0.0.0",
        "port": 8000,
        "max_model_len": 131072,
        "tensor_parallel_size": 2,
        "gpu_memory_utilization": 0.9,
        "enable_chunked_prefill": True,
        "max_num_batched_tokens": 8192
    }
    
    # Docker ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
    docker_script = f"""
# Dockerfile
FROM nvidia/cuda:11.8-devel-ubuntu20.04

# ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y python3 python3-pip

# Python ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
RUN pip3 install vllm transformers torch

# ëª¨ë¸ ë³µì‚¬
COPY ./ax4_korean_final_vllm /app/model

# ì„œë¹™ ìŠ¤í¬ë¦½íŠ¸
COPY serve.py /app/serve.py

WORKDIR /app
EXPOSE 8000

CMD ["python3", "serve.py"]
"""
    
    # Kubernetes ë°°í¬ YAML
    k8s_config = """
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ax4-korean-llm
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ax4-korean-llm
  template:
    metadata:
      labels:
        app: ax4-korean-llm
    spec:
      containers:
      - name: llm-server
        image: ax4-korean:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            nvidia.com/gpu: 1
          limits:
            nvidia.com/gpu: 1
---
apiVersion: v1
kind: Service
metadata:
  name: ax4-korean-service
spec:
  selector:
    app: ax4-korean-llm
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
"""
    
    return vllm_config, docker_script, k8s_config

# ë°°í¬ ì„¤ì • ìƒì„±
vllm_config, docker_script, k8s_config = prepare_deployment_configs()

# ì„¤ì • íŒŒì¼ ì €ì¥
with open("vllm_config.json", "w") as f:
    json.dump(vllm_config, f, indent=2)

with open("Dockerfile", "w") as f:
    f.write(docker_script)

with open("k8s-deployment.yaml", "w") as f:
    f.write(k8s_config)

print("ë°°í¬ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ")
```

---

## 7. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ê°œì„ 

### 7.1 ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

```python
def setup_monitoring_system():
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•"""
    
    monitoring_metrics = {
        "response_quality": {
            "description": "ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜",
            "threshold": 0.8,
            "measurement": "ìë™ í‰ê°€ + ì‚¬ìš©ì í”¼ë“œë°±"
        },
        "token_efficiency": {
            "description": "í† í° íš¨ìœ¨ì„±",
            "threshold": 1.5,  # í•œêµ­ì–´ ë¬¸ì/í† í° ë¹„ìœ¨
            "measurement": "ì‹¤ì‹œê°„ í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„"
        },
        "korean_accuracy": {
            "description": "í•œêµ­ì–´ ì •í™•ë„",
            "threshold": 0.85,
            "measurement": "í•œêµ­ì–´ ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜"
        },
        "safety_compliance": {
            "description": "ì•ˆì „ì„± ì¤€ìˆ˜ìœ¨",
            "threshold": 0.95,
            "measurement": "ìœ í•´ ì½˜í…ì¸  í•„í„°ë§ ì •í™•ë„"
        }
    }
    
    return monitoring_metrics

def continuous_improvement_pipeline():
    """ì§€ì†ì  ê°œì„  íŒŒì´í”„ë¼ì¸"""
    
    improvement_pipeline = {
        "data_collection": {
            "user_feedback": "ì‚¬ìš©ì í‰ê°€ ë° í”¼ë“œë°± ìˆ˜ì§‘",
            "usage_patterns": "ì‚¬ìš© íŒ¨í„´ ë¶„ì„",
            "error_cases": "ì˜¤ë¥˜ ì‚¬ë¡€ ìˆ˜ì§‘"
        },
        "analysis": {
            "performance_analysis": "ì„±ëŠ¥ ì €í•˜ êµ¬ê°„ ë¶„ì„",
            "failure_analysis": "ì‹¤íŒ¨ ì‚¬ë¡€ ì›ì¸ ë¶„ì„",
            "improvement_opportunities": "ê°œì„  ê¸°íšŒ ì‹ë³„"
        },
        "improvement": {
            "data_augmentation": "ì¶”ê°€ í•™ìŠµ ë°ì´í„° ìƒì„±",
            "model_updates": "ëª¨ë¸ ì—…ë°ì´íŠ¸",
            "deployment": "ê°œì„ ëœ ëª¨ë¸ ë°°í¬"
        }
    }
    
    return improvement_pipeline

# ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
monitoring_system = setup_monitoring_system()
improvement_pipeline = continuous_improvement_pipeline()

print("ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ")
```

---

## ì£¼ìš” ê³ ë ¤ì‚¬í•­ ë° íŒ

### ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬

**ê³ í’ˆì§ˆ ë°ì´í„° í™•ë³´**:
- ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ ê· í˜•ì¡íŒ ë°ì´í„° ìˆ˜ì§‘
- ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§ í•„ìˆ˜
- í•©ì„± ë°ì´í„° í™œìš©ìœ¼ë¡œ ë°ì´í„° ë¶€ì¡± í•´ê²°

**ë°ì´í„° ì „ì²˜ë¦¬**:
```python
# ë°ì´í„° í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸
quality_checklist = {
    "minimum_length": 50,      # ìµœì†Œ ë¬¸ì ìˆ˜
    "korean_ratio": 0.3,       # í•œêµ­ì–´ ë¹„ìœ¨ 30% ì´ìƒ
    "repetition_check": True,   # ë°˜ë³µ íŒ¨í„´ ê²€ì‚¬
    "encoding_check": True,     # ì¸ì½”ë”© ì˜¤ë¥˜ ê²€ì‚¬
    "profanity_filter": True    # ìš•ì„¤/ìœ í•´ ì½˜í…ì¸  í•„í„°
}
```

### ë¦¬ì†ŒìŠ¤ ìµœì í™”

**ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**:
- Unslothì˜ ë©”ëª¨ë¦¬ ìµœì í™” ê¸°ëŠ¥ ì ê·¹ í™œìš©
- LoRA/QLoRAë¡œ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ì¡°ì ˆ
- Gradient checkpointingìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½

**í•™ìŠµ ì†ë„ í–¥ìƒ**:
```python
# ìµœì í™” ì„¤ì •
optimization_settings = {
    "use_gradient_checkpointing": True,
    "fp16": True,  # ë˜ëŠ” bf16
    "dataloader_num_workers": 4,
    "pin_memory": True,
    "pack_sequences": True
}
```

### í‰ê°€ ë° ê²€ì¦

**ë‹¤ê°ë„ í‰ê°€**:
- ì •ëŸ‰ì  í‰ê°€: KMMLU, CLIcK ë“± í‘œì¤€ ë²¤ì¹˜ë§ˆí¬
- ì •ì„±ì  í‰ê°€: ì‹¤ì œ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸
- ì•ˆì „ì„± í‰ê°€: ìœ í•´ ì½˜í…ì¸  ìƒì„± ì—¬ë¶€ ê²€ì‚¬

**ì§€ì†ì  ëª¨ë‹ˆí„°ë§**:
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¶”ì 
- ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘
- ì •ê¸°ì ì¸ ë²¤ì¹˜ë§ˆí¬ ì¬í‰ê°€

---

## H100 GPUë³„ í•œêµ­ì–´ íŠ¹í™” LLM í•™ìŠµ ì‹œê°„ ì¶”ì •

í•œêµ­ì–´ íŠ¹í™” LLM í•™ìŠµì— í•„ìš”í•œ ì‹œê°„ì„ H100 GPU ê°œìˆ˜ë³„ë¡œ ì •ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

### 7B ëª¨ë¸ (ê²½ëŸ‰ ë²„ì „)

| GPU êµ¬ì„± | CPT | SFT | RLHF | ì´ í•™ìŠµì‹œê°„ |
|---------|-----|-----|------|---------|
| **H100 1ì¥** | 5-7ì¼ | 12-18ì‹œê°„ | 8-12ì‹œê°„ | **ì•½ 6-8ì¼** |
| **H100 2ì¥** | 3-4ì¼ | 6-9ì‹œê°„ | 4-6ì‹œê°„ | **ì•½ 3.5-4.5ì¼** |
| **H100 4ì¥** | 1.5-2ì¼ | 3-4ì‹œê°„ | 2-3ì‹œê°„ | **ì•½ 2-2.5ì¼** |
| **H100 8ì¥** | 18-24ì‹œê°„ | 1.5-2ì‹œê°„ | 1-1.5ì‹œê°„ | **ì•½ 1-1.5ì¼** |

### 72B ëª¨ë¸ (í‘œì¤€ ë²„ì „)

| GPU êµ¬ì„± | CPT | SFT | RLHF | ì´ í•™ìŠµì‹œê°„ |
|---------|-----|-----|------|---------|
| **H100 1ì¥** | ë¶ˆê°€ëŠ¥* | - | - | **ë©”ëª¨ë¦¬ ë¶€ì¡±** |
| **H100 2ì¥** | 4-6ì£¼ | 3-5ì¼ | 2-3ì¼ | **ì•½ 5-7ì£¼** |
| **H100 4ì¥** | 2-3ì£¼ | 1.5-2ì¼ | 1-1.5ì¼ | **ì•½ 2.5-3.5ì£¼** |
| **H100 8ì¥** | 1-1.5ì£¼ | 12-18ì‹œê°„ | 8-12ì‹œê°„ | **ì•½ 1.5-2ì£¼** |

*72B ëª¨ë¸ì€ H100 1ì¥ìœ¼ë¡œëŠ” ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ í•™ìŠµ ë¶ˆê°€ëŠ¥

### ì£¼ìš” ì˜í–¥ ìš”ì¸

#### 1. í•™ìŠµ ë‹¨ê³„ë³„ ì‹œê°„ ë¹„ì¤‘
- **CPT (ì§€ì†ì  ì‚¬ì „í•™ìŠµ)**: ì „ì²´ ì‹œê°„ì˜ 80-85%
- **SFT (ì§€ë„ ë¯¸ì„¸ì¡°ì •)**: ì „ì²´ ì‹œê°„ì˜ 10-15%  
- **RLHF (ê°•í™”í•™ìŠµ)**: ì „ì²´ ì‹œê°„ì˜ 5-10%

#### 2. ë°ì´í„° ê·œëª¨
- **í•œêµ­ì–´ ì½”í¼ìŠ¤**: 500GB-1TB
- **ì´ í† í° ìˆ˜**: 1-2ì¡° í† í°
- **í•™ìŠµ ì—í­**: CPT 3-5íšŒ, SFT 3-5íšŒ

#### 3. ìµœì í™” ê¸°ë²• ì ìš© ì‹œ
```python
# Unsloth + QLoRA ì ìš© ì‹œ ì‹œê°„ ë‹¨ì¶•
ê¸°ë³¸ í•™ìŠµ ì‹œê°„ Ã— 0.6-0.7 = ìµœì í™” í›„ ì‹œê°„

# ì˜ˆì‹œ: 7B ëª¨ë¸, H100 4ì¥
ê¸°ë³¸: 2-2.5ì¼ â†’ ìµœì í™” í›„: 1.2-1.8ì¼
```

### ì‹¤ì œ ë¹„ìš© ì¶”ì •

#### í´ë¼ìš°ë“œ ë¹„ìš© (H100 ì‹œê°„ë‹¹ ì•½ $4-6)
- **7B ëª¨ë¸**: 
  - H100 4ì¥: $1,200-2,400
  - H100 8ì¥: $800-1,440
  
- **72B ëª¨ë¸**:
  - H100 4ì¥: $6,000-12,600
  - H100 8ì¥: $4,000-8,640

### ê¶Œì¥ êµ¬ì„±

#### ê°œë°œ/ì‹¤í—˜ í™˜ê²½
- **7B ëª¨ë¸ + H100 4ì¥**: ë¹„ìš© íš¨ìœ¨ì ì´ë©° 2-3ì¼ ë‚´ ì™„ë£Œ

#### í”„ë¡œë•ì…˜ í™˜ê²½  
- **72B ëª¨ë¸ + H100 8ì¥**: Llama 3 70B ìˆ˜ì¤€ì˜ ëª¨ë¸ì´ 6.4ë°±ë§Œ H100 GPU-ì‹œê°„ì´ í•„ìš”í•˜ì§€ë§Œ, í•œêµ­ì–´ íŠ¹í™” í•™ìŠµì€ ê¸°ì¡´ ëª¨ë¸ ê¸°ë°˜ì´ë¯€ë¡œ í›¨ì”¬ ë‹¨ì¶•ë©ë‹ˆë‹¤

#### ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
- **7B ëª¨ë¸**: H100 80GB 1ì¥ìœ¼ë¡œ ì¶©ë¶„
- **72B ëª¨ë¸**: ìµœì†Œ H100 2ì¥ í•„ìš” (í…ì„œ ë³‘ë ¬í™”)

### í•™ìŠµ íš¨ìœ¨ì„± íŒ

1. **Gradient Checkpointing**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ì ˆì•½
2. **Mixed Precision (FP16/BF16)**: í•™ìŠµ ì†ë„ 1.5-2ë°° í–¥ìƒ
3. **DeepSpeed ZeRO**: ëŒ€ìš©ëŸ‰ ëª¨ë¸ í•™ìŠµ ì‹œ í•„ìˆ˜
4. **Unsloth ìµœì í™”**: ë©”ëª¨ë¦¬ì™€ ì†ë„ ëª¨ë‘ 30-50% ê°œì„ 

ì´ëŸ¬í•œ ì¶”ì •ì¹˜ëŠ” ë°ì´í„° í’ˆì§ˆ, í•˜ë“œì›¨ì–´ êµ¬ì„±, ìµœì í™” ìˆ˜ì¤€ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ, ì‹¤ì œ íŒŒì¼ëŸ¿ í…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ì •í™•í•œ ì‹œê°„ì„ ì¸¡ì •í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. 

---

## ê²°ë¡ 

ë³¸ ê°€ì´ë“œë¥¼ í†µí•´ Unslothë¥¼ í™œìš©í•œ í•œêµ­ì–´ íŠ¹í™” LLM êµ¬ì¶•ì˜ ì „ ê³¼ì •ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. ì£¼ìš” ì„±ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

**ê¸°ìˆ ì  ì„±ê³¼**:
- **íš¨ìœ¨ì ì¸ í•™ìŠµ**: Unslothë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ì ˆì•½
- **í•œêµ­ì–´ ìµœì í™”**: í† í¬ë‚˜ì´ì €ë¶€í„° ë°ì´í„°ê¹Œì§€ í•œêµ­ì–´ íŠ¹í™”
- **3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸**: CPT â†’ SFT â†’ RLHF ì™„ì „ êµ¬í˜„
- **ë°°í¬ ì¤€ë¹„**: ë‹¤ì–‘í•œ í˜•íƒœì˜ ëª¨ë¸ ë³€í™˜ ë° ë°°í¬ ì„¤ì •

**ì‹¤ë¬´ì  ê°€ì¹˜**:
- **ë¹„ìš© íš¨ìœ¨ì„±**: ê¸°ì¡´ ëŒ€ë¹„ 50% ì´ìƒ ë¦¬ì†ŒìŠ¤ ì ˆì•½
- **ì„±ëŠ¥ í–¥ìƒ**: ë†’ì€ ìˆ˜ì¤€ì˜ í•œêµ­ì–´ ì„±ëŠ¥ ë‹¬ì„±
- **í™•ì¥ ê°€ëŠ¥ì„±**: ë‹¤ì–‘í•œ ë„ë©”ì¸ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥
- **ìœ ì§€ë³´ìˆ˜ì„±**: ì²´ê³„ì ì¸ ëª¨ë‹ˆí„°ë§ ë° ê°œì„  ì‹œìŠ¤í…œ

ì´ëŸ¬í•œ ê³¼ì •ì„ í†µí•´ í•œêµ­ì–´ íŠ¹í™” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•í•˜ê³  ì‹¤ë¬´ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§€ì†ì ì¸ ê°œì„ ê³¼ ëª¨ë‹ˆí„°ë§ì„ í†µí•´ ë”ìš± í–¥ìƒëœ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.



