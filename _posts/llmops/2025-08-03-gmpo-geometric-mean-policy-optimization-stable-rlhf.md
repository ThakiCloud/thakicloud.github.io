---
title: "GMPO: ê¸°í•˜ í‰ê·  ê¸°ë°˜ ì •ì±… ìµœì í™”ë¡œ í•´ê²°í•˜ëŠ” RLHF ì•ˆì •ì„± ë¬¸ì œ"
excerpt: "GRPOì˜ ì´ìƒì¹˜ ë¯¼ê°ì„±ì„ í•´ê²°í•œ Geometric-Mean Policy Optimizationìœ¼ë¡œ ìˆ˜í•™ì  ì¶”ë¡  4.1%, ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  1.4% ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í•œ í˜ì‹ ì  LLMOps ê¸°ë²•ì„ ë¶„ì„í•©ë‹ˆë‹¤."
seo_title: "GMPO ê¸°í•˜ í‰ê·  ì •ì±… ìµœì í™” ì™„ì „ ê°€ì´ë“œ - RLHF ì•ˆì •ì„± ê°œì„  - Thaki Cloud"
seo_description: "GRPO í•œê³„ë¥¼ ê·¹ë³µí•œ GMPOì˜ ê¸°í•˜ í‰ê·  ê¸°ë°˜ í† í° ë ˆë²¨ ë³´ìƒ ìµœì í™” ê¸°ë²•ê³¼ ì‹¤ì œ êµ¬í˜„ ë°©ë²•, LLMOps ê´€ì ì—ì„œì˜ í™œìš© ì „ëµì„ ìƒì„¸ ë¶„ì„í•©ë‹ˆë‹¤."
date: 2025-08-03
last_modified_at: 2025-08-03
categories:
  - llmops
  - research
tags:
  - GMPO
  - Geometric-Mean-Policy-Optimization
  - GRPO
  - RLHF
  - Reinforcement-Learning
  - Policy-Optimization
  - LLM-Training
  - Importance-Sampling
  - Token-Level-Rewards
  - Mathematical-Reasoning
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/llmops/gmpo-geometric-mean-policy-optimization-stable-rlhf/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 10ë¶„

## ì„œë¡ 

2025ë…„ 7ì›”, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ê°•í™”í•™ìŠµ í›ˆë ¨ì— ìƒˆë¡œìš´ í˜ì‹ ì´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤. **Geometric-Mean Policy Optimization (GMPO)**ì€ ê¸°ì¡´ GRPO(Group Relative Policy Optimization)ì˜ ì•ˆì •ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , **ìˆ˜í•™ì  ì¶”ë¡ ì—ì„œ 4.1%, ë©€í‹°ëª¨ë‹¬ ì¶”ë¡ ì—ì„œ 1.4%ì˜ ì„±ëŠ¥ í–¥ìƒ**ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

ì´ ì—°êµ¬ëŠ” [Hugging Face Papers](https://huggingface.co/papers/2507.20673)ì— ë°œí‘œë˜ì—ˆìœ¼ë©°, **í† í° ë ˆë²¨ ë³´ìƒì˜ ê¸°í•˜ í‰ê· ì„ ìµœëŒ€í™”**í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ í†µí•´ LLMOps í™˜ê²½ì—ì„œì˜ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ëª¨ë¸ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

## GRPOì˜ í•œê³„ì  ë¶„ì„

### GRPOì˜ ì‘ë™ ì›ë¦¬

**Group Relative Policy Optimization (GRPO)**ì€ í† í° ë ˆë²¨ ë³´ìƒì˜ **ì‚°ìˆ  í‰ê· ì„ ìµœì í™”**í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤:

```python
# GRPOì˜ ê¸°ë³¸ ê°œë…
class GRPO:
    def optimize_policy(self, token_rewards):
        """ì‚°ìˆ  í‰ê·  ê¸°ë°˜ ì •ì±… ìµœì í™”"""
        # ì‚°ìˆ  í‰ê·  ê³„ì‚°
        arithmetic_mean = sum(token_rewards) / len(token_rewards)
        
        # Importance sampling ratio ê³„ì‚°
        importance_ratios = []
        for token in tokens:
            ratio = current_policy(token) / old_policy(token)
            importance_ratios.append(ratio)
            
        return self.update_policy(arithmetic_mean, importance_ratios)
```

### í•µì‹¬ ë¬¸ì œì ë“¤

#### 1. ì´ìƒì¹˜ ë¯¼ê°ì„± (Outlier Sensitivity)
```
ë¬¸ì œ ìƒí™©:
- íŠ¹ì • í† í°ì—ì„œ ë§¤ìš° ë†’ê±°ë‚˜ ë‚®ì€ ë³´ìƒ ë°œìƒ
- ì‚°ìˆ  í‰ê· ì´ ì´ìƒì¹˜ì— í¬ê²Œ ì˜í–¥ë°›ìŒ
- ì „ì²´ í•™ìŠµ ê³¼ì •ì˜ ë¶ˆì•ˆì •ì„± ì•¼ê¸°

ì˜ˆì‹œ:
í† í° ë³´ìƒ: [0.1, 0.2, 0.1, 10.0, 0.1]
ì‚°ìˆ  í‰ê· : 2.1 (ì´ìƒì¹˜ 10.0ì— ì˜í•´ í¬ê²Œ ì™œê³¡ë¨)
```

#### 2. ê·¹ë‹¨ì  Importance Sampling Ratio
```python
# GRPOì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œ
def problematic_importance_ratio():
    """ê·¹ë‹¨ì  importance sampling ratio ë¬¸ì œ"""
    
    # í˜„ì¬ ì •ì±…ê³¼ ì´ì „ ì •ì±… ê°„ í™•ë¥  ë¹„ìœ¨
    current_prob = 0.001   # í˜„ì¬ ì •ì±…ì—ì„œ ë‚®ì€ í™•ë¥ 
    old_prob = 0.9         # ì´ì „ ì •ì±…ì—ì„œ ë†’ì€ í™•ë¥ 
    
    importance_ratio = current_prob / old_prob  # 0.0011
    
    # ë˜ëŠ” ë°˜ëŒ€ì˜ ê²½ìš°
    current_prob = 0.9
    old_prob = 0.001
    
    importance_ratio = current_prob / old_prob  # 900.0
    
    # ê·¹ë‹¨ì  ë¹„ìœ¨ë¡œ ì¸í•œ í›ˆë ¨ ë¶ˆì•ˆì •ì„±
    return importance_ratio
```

#### 3. ì •ì±… ì—…ë°ì´íŠ¸ ë¶ˆì•ˆì •ì„±
```
ë¶ˆì•ˆì •ì„±ì˜ ì›ì¸:
â”œâ”€â”€ ì´ìƒì¹˜ë¡œ ì¸í•œ ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ
â”œâ”€â”€ ê·¹ë‹¨ì  importance sampling ratio
â”œâ”€â”€ ë¹„ì¼ê´€ì ì¸ í•™ìŠµ ì‹ í˜¸
â””â”€â”€ ìˆ˜ë ´ ì†ë„ ì €í•˜

ê²°ê³¼:
â”œâ”€â”€ í›ˆë ¨ ì¤‘ ì„±ëŠ¥ ê¸‰ë½
â”œâ”€â”€ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ëª¨ë¸ ë™ì‘
â”œâ”€â”€ ì¬í˜„ì„± ë¶€ì¡±
â””â”€â”€ í”„ë¡œë•ì…˜ ë°°í¬ ìœ„í—˜ì„± ì¦ê°€
```

## GMPO: í˜ì‹ ì  í•´ê²°ì±…

### ê¸°í•˜ í‰ê· ì˜ í•µì‹¬ ì•„ì´ë””ì–´

GMPOëŠ” **ì‚°ìˆ  í‰ê·  ëŒ€ì‹  ê¸°í•˜ í‰ê· **ì„ ì‚¬ìš©í•˜ì—¬ í† í° ë ˆë²¨ ë³´ìƒì„ ìµœì í™”í•©ë‹ˆë‹¤:

```python
import numpy as np

class GMPO:
    def optimize_policy(self, token_rewards):
        """ê¸°í•˜ í‰ê·  ê¸°ë°˜ ì •ì±… ìµœì í™”"""
        
        # ê¸°í•˜ í‰ê·  ê³„ì‚°
        geometric_mean = np.exp(np.mean(np.log(token_rewards + epsilon)))
        
        # ì•ˆì •ì ì¸ importance sampling ratio
        stable_ratios = self.compute_stable_ratios(token_rewards)
        
        return self.update_policy(geometric_mean, stable_ratios)
    
    def compute_stable_ratios(self, rewards):
        """ì•ˆì •ì ì¸ importance sampling ratio ê³„ì‚°"""
        # ê¸°í•˜ í‰ê·  ê¸°ë°˜ìœ¼ë¡œ ë” ì•ˆì •ì ì¸ ë¹„ìœ¨ ìƒì„±
        ratios = []
        for reward in rewards:
            # ë¡œê·¸ ê³µê°„ì—ì„œì˜ ì•ˆì •ì  ê³„ì‚°
            log_ratio = np.log(reward + epsilon) - np.log(self.baseline + epsilon)
            ratio = np.exp(np.clip(log_ratio, -2.0, 2.0))  # ì•ˆì •ì„±ì„ ìœ„í•œ í´ë¦¬í•‘
            ratios.append(ratio)
        return ratios
```

### ê¸°í•˜ í‰ê·  vs ì‚°ìˆ  í‰ê·  ë¹„êµ

#### ì´ìƒì¹˜ ì €í•­ì„± ë¶„ì„
```python
# ì´ìƒì¹˜ì— ëŒ€í•œ ë¯¼ê°ì„± ë¹„êµ
def compare_sensitivity():
    """ì‚°ìˆ  í‰ê· ê³¼ ê¸°í•˜ í‰ê· ì˜ ì´ìƒì¹˜ ë¯¼ê°ì„± ë¹„êµ"""
    
    # ì •ìƒì ì¸ í† í° ë³´ìƒ
    normal_rewards = [0.8, 0.9, 0.7, 0.8, 0.9]
    
    # ì´ìƒì¹˜ê°€ í¬í•¨ëœ í† í° ë³´ìƒ
    outlier_rewards = [0.8, 0.9, 0.7, 15.0, 0.9]  # 15.0ì´ ì´ìƒì¹˜
    
    # ì‚°ìˆ  í‰ê· 
    normal_arithmetic = sum(normal_rewards) / len(normal_rewards)  # 0.82
    outlier_arithmetic = sum(outlier_rewards) / len(outlier_rewards)  # 3.66
    
    arithmetic_change = (outlier_arithmetic - normal_arithmetic) / normal_arithmetic * 100  # 346% ì¦ê°€
    
    # ê¸°í•˜ í‰ê· 
    normal_geometric = np.exp(np.mean(np.log(normal_rewards)))  # 0.82
    outlier_geometric = np.exp(np.mean(np.log(outlier_rewards)))  # 1.26
    
    geometric_change = (outlier_geometric - normal_geometric) / normal_geometric * 100  # 54% ì¦ê°€
    
    print(f"ì‚°ìˆ  í‰ê·  ë³€í™”ìœ¨: {arithmetic_change:.1f}%")
    print(f"ê¸°í•˜ í‰ê·  ë³€í™”ìœ¨: {geometric_change:.1f}%")
    
    return {
        'arithmetic_sensitivity': arithmetic_change,
        'geometric_sensitivity': geometric_change,
        'stability_improvement': arithmetic_change / geometric_change  # 6.4ë°° ê°œì„ 
    }
```

#### ìˆ˜í•™ì  ì•ˆì •ì„± ì¦ëª…
```
ê¸°í•˜ í‰ê· ì˜ ì•ˆì •ì„± íŠ¹ì„±:

1. ì´ìƒì¹˜ ì €í•­ì„±:
   G = (xâ‚ Ã— xâ‚‚ Ã— ... Ã— xâ‚™)^(1/n)
   - í•˜ë‚˜ì˜ í° ê°’ì´ ì „ì²´ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì œí•œ
   - ê³±ì…ˆ êµ¬ì¡°ë¡œ ì¸í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì •ê·œí™”

2. ë¡œê·¸ ê³µê°„ ì„ í˜•ì„±:
   log(G) = (1/n) Ã— Î£log(xáµ¢)
   - ë¡œê·¸ ë³€í™˜ìœ¼ë¡œ ê³±ì…ˆ â†’ ë§ì…ˆ
   - ë” ì•ˆì •ì ì¸ ìˆ˜ì¹˜ ê³„ì‚°

3. Importance Sampling Ratio ì•ˆì •ì„±:
   ratio = exp(log(current) - log(old))
   - ë¡œê·¸ ê³µê°„ì—ì„œì˜ ì°¨ì´ ê³„ì‚°
   - ìì—°ìŠ¤ëŸ¬ìš´ í´ë¦¬í•‘ íš¨ê³¼
```

### GMPO êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

#### ì•ˆì „í•œ ê¸°í•˜ í‰ê·  ê³„ì‚°
```python
class SafeGeometricMean:
    def __init__(self, epsilon=1e-8, clip_range=(-10, 10)):
        self.epsilon = epsilon
        self.clip_range = clip_range
    
    def compute(self, values):
        """ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì „í•œ ê¸°í•˜ í‰ê·  ê³„ì‚°"""
        
        # 1. ì–‘ìˆ˜ ë³´ì¥
        safe_values = np.maximum(values, self.epsilon)
        
        # 2. ë¡œê·¸ ë³€í™˜
        log_values = np.log(safe_values)
        
        # 3. ì•ˆì •ì„±ì„ ìœ„í•œ í´ë¦¬í•‘
        clipped_logs = np.clip(log_values, self.clip_range[0], self.clip_range[1])
        
        # 4. í‰ê·  ë° ì§€ìˆ˜ ë³€í™˜
        mean_log = np.mean(clipped_logs)
        geometric_mean = np.exp(mean_log)
        
        return geometric_mean
    
    def compute_with_weights(self, values, weights):
        """ê°€ì¤‘ ê¸°í•˜ í‰ê·  ê³„ì‚°"""
        safe_values = np.maximum(values, self.epsilon)
        log_values = np.log(safe_values)
        
        # ê°€ì¤‘ í‰ê· 
        weighted_mean_log = np.average(log_values, weights=weights)
        
        return np.exp(weighted_mean_log)
```

#### ì •ì±… ì—…ë°ì´íŠ¸ ì•Œê³ ë¦¬ì¦˜
```python
class GMPOOptimizer:
    def __init__(self, learning_rate=1e-4, beta1=0.9, beta2=0.999):
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.geometric_mean_calculator = SafeGeometricMean()
        
    def update_policy(self, states, actions, rewards, old_log_probs):
        """GMPO ê¸°ë°˜ ì •ì±… ì—…ë°ì´íŠ¸"""
        
        # 1. í˜„ì¬ ì •ì±…ìœ¼ë¡œ ë¡œê·¸ í™•ë¥  ê³„ì‚°
        current_log_probs = self.policy.log_prob(states, actions)
        
        # 2. í† í°ë³„ ê¸°í•˜ í‰ê·  ë³´ìƒ ê³„ì‚°
        geometric_rewards = self.geometric_mean_calculator.compute(rewards)
        
        # 3. ì•ˆì •ì ì¸ importance ratio ê³„ì‚°
        log_ratios = current_log_probs - old_log_probs
        importance_ratios = torch.exp(torch.clamp(log_ratios, -2.0, 2.0))
        
        # 4. GMPO ì†ì‹¤ í•¨ìˆ˜
        gmpo_loss = self.compute_gmpo_loss(
            importance_ratios, 
            geometric_rewards, 
            log_ratios
        )
        
        # 5. ì •ì±… ì—…ë°ì´íŠ¸
        self.optimizer.zero_grad()
        gmpo_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=1.0)
        self.optimizer.step()
        
        return gmpo_loss.item()
    
    def compute_gmpo_loss(self, ratios, rewards, log_ratios):
        """GMPO ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°"""
        
        # ê¸°í•˜ í‰ê·  ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€
        advantages = rewards - self.baseline
        
        # PPO ìŠ¤íƒ€ì¼ í´ë¦¬í•‘ with ê¸°í•˜ í‰ê· 
        clipped_ratios = torch.clamp(ratios, 0.8, 1.2)
        
        loss1 = ratios * advantages
        loss2 = clipped_ratios * advantages
        
        policy_loss = -torch.min(loss1, loss2).mean()
        
        # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤
        entropy_bonus = self.entropy_coef * self.policy.entropy().mean()
        
        return policy_loss - entropy_bonus
```

## ì„±ëŠ¥ ê°œì„  ê²°ê³¼

### ìˆ˜í•™ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬

GMPOëŠ” ë‹¤ì–‘í•œ ìˆ˜í•™ì  ì¶”ë¡  íƒœìŠ¤í¬ì—ì„œ **í‰ê·  4.1%ì˜ ì„±ëŠ¥ í–¥ìƒ**ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤:

| ë²¤ì¹˜ë§ˆí¬ | GRPO ì„±ëŠ¥ | GMPO ì„±ëŠ¥ | ê°œì„ ìœ¨ |
|----------|-----------|-----------|--------|
| **AIME24** | 76.2% | 79.8% | **+3.6%** |
| **AMC** | 82.1% | 86.7% | **+4.6%** |
| **MATH500** | 68.9% | 72.4% | **+3.5%** |
| **OlympiadBench** | 71.3% | 75.9% | **+4.6%** |
| **Minerva** | 79.6% | 82.8% | **+3.2%** |
| **í‰ê· ** | 75.6% | 79.5% | **+4.1%** |

### ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ì„±ëŠ¥

| ë²¤ì¹˜ë§ˆí¬ | GRPO ì„±ëŠ¥ | GMPO ì„±ëŠ¥ | ê°œì„ ìœ¨ |
|----------|-----------|-----------|--------|
| **Geometry3K** | 84.7% | 86.1% | **+1.4%** |

### í›ˆë ¨ ì•ˆì •ì„± ì§€í‘œ

```python
# í›ˆë ¨ ì•ˆì •ì„± ë¹„êµ ë¶„ì„
training_metrics = {
    'GRPO': {
        'loss_variance': 0.024,
        'gradient_norm_std': 2.34,
        'importance_ratio_range': (0.001, 847.3),
        'convergence_epochs': 45,
        'training_crashes': 3
    },
    'GMPO': {
        'loss_variance': 0.008,          # 67% ê°ì†Œ
        'gradient_norm_std': 0.97,       # 59% ê°ì†Œ  
        'importance_ratio_range': (0.12, 8.4),  # ê·¹ë‹¨ê°’ ì œê±°
        'convergence_epochs': 32,        # 29% ë¹ ë¥¸ ìˆ˜ë ´
        'training_crashes': 0            # ì•ˆì •ì  í›ˆë ¨
    }
}

stability_improvement = {
    'loss_stability': 1 - (0.008 / 0.024),      # 67% ê°œì„ 
    'gradient_stability': 1 - (0.97 / 2.34),    # 59% ê°œì„ 
    'ratio_stability': 1 - (8.4 / 847.3),       # 99% ê°œì„ 
    'convergence_speed': 1 - (32 / 45)          # 29% ê°œì„ 
}
```

## LLMOps ê´€ì ì—ì„œì˜ í™œìš© ì „ëµ

### 1. í”„ë¡œë•ì…˜ í™˜ê²½ ë°°í¬

#### ì•ˆì •ì ì¸ ëª¨ë¸ ì„œë¹™
```python
class GMPOModelServer:
    def __init__(self, model_path, config):
        self.model = self.load_gmpo_model(model_path)
        self.config = config
        self.stability_monitor = StabilityMonitor()
        
    def serve_request(self, input_data):
        """ì•ˆì •ì ì¸ ì¶”ë¡  ì„œë¹„ìŠ¤"""
        
        # 1. ì…ë ¥ ê²€ì¦
        validated_input = self.validate_input(input_data)
        
        # 2. GMPOë¡œ í›ˆë ¨ëœ ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            output = self.model.generate(
                validated_input,
                max_length=512,
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )
        
        # 3. ì¶œë ¥ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
        quality_score = self.stability_monitor.evaluate_output(output)
        
        if quality_score < self.config.min_quality_threshold:
            # í’ˆì§ˆì´ ë‚®ìœ¼ë©´ ëŒ€ì•ˆ ì „ëµ ì‚¬ìš©
            output = self.fallback_generation(validated_input)
            
        return {
            'response': output,
            'quality_score': quality_score,
            'model_version': 'GMPO-7B',
            'stability_metric': self.stability_monitor.get_current_metrics()
        }
```

#### ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ
```python
class GMPOMonitoring:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alert_manager = AlertManager()
        
    def monitor_model_performance(self):
        """ì‹¤ì‹œê°„ ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        
        while True:
            # í•µì‹¬ ì§€í‘œ ìˆ˜ì§‘
            metrics = {
                'response_quality': self.measure_response_quality(),
                'inference_latency': self.measure_latency(),
                'importance_ratio_distribution': self.check_ratio_stability(),
                'gradient_norms': self.monitor_gradients(),
                'memory_usage': self.check_memory_usage()
            }
            
            # ì´ìƒ ì§•í›„ íƒì§€
            anomalies = self.detect_anomalies(metrics)
            
            if anomalies:
                self.alert_manager.send_alert({
                    'severity': 'WARNING',
                    'message': f'GMPO ëª¨ë¸ ì´ìƒ ê°ì§€: {anomalies}',
                    'metrics': metrics,
                    'timestamp': datetime.now(),
                    'recommended_action': self.get_recommended_action(anomalies)
                })
            
            time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
```

### 2. ì§€ì†ì  í•™ìŠµ íŒŒì´í”„ë¼ì¸

#### ì•ˆì „í•œ ëª¨ë¸ ì—…ë°ì´íŠ¸
```python
class GMPOContinualLearning:
    def __init__(self, base_model, safety_config):
        self.base_model = base_model
        self.safety_config = safety_config
        self.gmpo_optimizer = GMPOOptimizer()
        
    def safe_model_update(self, new_training_data):
        """ì•ˆì „í•œ GMPO ê¸°ë°˜ ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        
        # 1. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        validated_data = self.validate_training_data(new_training_data)
        
        # 2. ì‘ì€ ë°°ì¹˜ë¡œ ì•ˆì „ì„± í…ŒìŠ¤íŠ¸
        safety_test_results = self.run_safety_tests(validated_data[:100])
        
        if not safety_test_results['is_safe']:
            return {
                'success': False,
                'reason': 'Safety test failed',
                'details': safety_test_results
            }
        
        # 3. GMPOë¡œ ì ì§„ì  ì—…ë°ì´íŠ¸
        update_results = self.incremental_gmpo_update(validated_data)
        
        # 4. ì„±ëŠ¥ ê²€ì¦
        performance_check = self.validate_updated_model()
        
        if performance_check['performance_degradation'] > 0.05:
            # ì„±ëŠ¥ì´ 5% ì´ìƒ ë–¨ì–´ì§€ë©´ ë¡¤ë°±
            self.rollback_model()
            return {
                'success': False,
                'reason': 'Performance degradation detected',
                'details': performance_check
            }
        
        return {
            'success': True,
            'new_model_version': update_results['version'],
            'performance_improvement': performance_check['improvement'],
            'stability_metrics': update_results['stability']
        }
```

### 3. A/B í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬

#### GMPO vs ê¸°ì¡´ ëª¨ë¸ ë¹„êµ
```python
class GMPOABTesting:
    def __init__(self):
        self.grpo_model = self.load_model('grpo-7b')
        self.gmpo_model = self.load_model('gmpo-7b')
        self.traffic_splitter = TrafficSplitter()
        
    def run_ab_test(self, test_duration_hours=24):
        """GMPOì™€ GRPO ëª¨ë¸ A/B í…ŒìŠ¤íŠ¸"""
        
        test_results = {
            'grpo': {'requests': 0, 'success_rate': 0, 'avg_latency': 0, 'quality_scores': []},
            'gmpo': {'requests': 0, 'success_rate': 0, 'avg_latency': 0, 'quality_scores': []}
        }
        
        start_time = time.time()
        
        while (time.time() - start_time) < (test_duration_hours * 3600):
            # íŠ¸ë˜í”½ ë¶„í•  (50:50)
            user_request = self.get_next_request()
            model_choice = self.traffic_splitter.assign_model()
            
            if model_choice == 'grpo':
                result = self.test_grpo_model(user_request)
                test_results['grpo'] = self.update_metrics(test_results['grpo'], result)
            else:
                result = self.test_gmpo_model(user_request)
                test_results['gmpo'] = self.update_metrics(test_results['gmpo'], result)
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
        significance_test = self.statistical_significance_test(test_results)
        
        return {
            'test_results': test_results,
            'statistical_significance': significance_test,
            'recommendation': self.generate_recommendation(test_results, significance_test)
        }
```

## ì‹¤ì œ êµ¬í˜„ ê°€ì´ë“œ

### í™˜ê²½ ì„¤ì •

```bash
# GMPO í™˜ê²½ ì„¤ì •
git clone https://github.com/callsys/GMPO.git
cd GMPO

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install torch>=2.0.0 transformers>=4.30.0 accelerate datasets
pip install wandb tensorboard numpy scipy

# GMPO ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install -e .
```

### ê¸°ë³¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

```python
import torch
from gmpo import GMPOTrainer, GMPOConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

def train_gmpo_model():
    """GMPO ê¸°ë°˜ ëª¨ë¸ í›ˆë ¨"""
    
    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model_name = "microsoft/DialoGPT-medium"
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # 2. GMPO ì„¤ì •
    gmpo_config = GMPOConfig(
        learning_rate=1e-5,
        batch_size=16,
        max_length=512,
        geometric_mean_epsilon=1e-8,
        importance_ratio_clip=2.0,
        gradient_clip_norm=1.0,
        entropy_coefficient=0.01
    )
    
    # 3. GMPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = GMPOTrainer(
        model=model,
        tokenizer=tokenizer,
        config=gmpo_config,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset
    )
    
    # 4. í›ˆë ¨ ì‹¤í–‰
    training_results = trainer.train(
        num_epochs=5,
        save_strategy="epoch",
        evaluation_strategy="steps",
        eval_steps=500,
        logging_steps=100
    )
    
    return training_results

# í›ˆë ¨ ì‹¤í–‰
if __name__ == "__main__":
    results = train_gmpo_model()
    print(f"í›ˆë ¨ ì™„ë£Œ: {results}")
```

### ì¶”ë¡  ìµœì í™”

```python
class GMPOInferenceOptimizer:
    def __init__(self, model_path):
        self.model = self.load_optimized_model(model_path)
        self.geometric_mean_calculator = SafeGeometricMean()
        
    def optimized_generation(self, prompt, **kwargs):
        """GMPO ê¸°ë°˜ ìµœì í™”ëœ í…ìŠ¤íŠ¸ ìƒì„±"""
        
        # 1. ì…ë ¥ í† í°í™”
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        # 2. ê¸°í•˜ í‰ê·  ê¸°ë°˜ í™•ë¥  ì¡°ì •
        with torch.no_grad():
            # ì²« ë²ˆì§¸ forward passë¡œ ê¸°ë³¸ ë¡œì§“ ê³„ì‚°
            outputs = self.model(**inputs)
            base_logits = outputs.logits
            
            # ê¸°í•˜ í‰ê·  ê¸°ë°˜ í™•ë¥  ì¡°ì •
            adjusted_probs = self.adjust_probabilities_geometric(base_logits)
            
            # ì¡°ì •ëœ í™•ë¥ ë¡œ ìƒ˜í”Œë§
            generated_tokens = self.sample_with_adjusted_probs(
                adjusted_probs, 
                max_length=kwargs.get('max_length', 256)
            )
        
        return self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    def adjust_probabilities_geometric(self, logits):
        """ê¸°í•˜ í‰ê·  ê¸°ë°˜ í™•ë¥  ì¡°ì •"""
        
        # ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥  ë³€í™˜
        probs = torch.softmax(logits, dim=-1)
        
        # ê¸°í•˜ í‰ê·  ê³„ì‚° (ì•ˆì •ì„± ìœ„í•´ ë¡œê·¸ ê³µê°„ì—ì„œ)
        log_probs = torch.log(probs + 1e-8)
        geometric_log_mean = torch.mean(log_probs, dim=-1, keepdim=True)
        
        # ê¸°í•˜ í‰ê·  ê¸°ë°˜ ì¡°ì •
        adjusted_log_probs = log_probs - geometric_log_mean
        adjusted_probs = torch.softmax(adjusted_log_probs, dim=-1)
        
        return adjusted_probs
```

## ì„±ëŠ¥ ìµœì í™” ë° íŠœë‹

### í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

```python
import optuna

def optimize_gmpo_hyperparameters(trial):
    """Optunaë¥¼ ì‚¬ìš©í•œ GMPO í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ ì •ì˜
    params = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-6, 1e-3),
        'geometric_epsilon': trial.suggest_loguniform('geometric_epsilon', 1e-10, 1e-6),
        'importance_clip': trial.suggest_uniform('importance_clip', 1.5, 3.0),
        'entropy_coef': trial.suggest_uniform('entropy_coef', 0.001, 0.1),
        'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32, 64])
    }
    
    # GMPO í›ˆë ¨ ì‹¤í–‰
    trainer = GMPOTrainer(**params)
    results = trainer.train(epochs=3)  # ë¹ ë¥¸ í‰ê°€ë¥¼ ìœ„í•´ 3 ì—í¬í¬ë§Œ
    
    # ëª©ì  í•¨ìˆ˜: ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì˜ ì¡°í•©
    stability_score = 1.0 / (1.0 + results['loss_variance'])
    performance_score = results['eval_accuracy']
    
    objective = 0.7 * performance_score + 0.3 * stability_score
    
    return objective

# ìµœì í™” ì‹¤í–‰
study = optuna.create_study(direction='maximize')
study.optimize(optimize_gmpo_hyperparameters, n_trials=50)

best_params = study.best_params
print(f"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params}")
```

### ë¶„ì‚° í›ˆë ¨ ì„¤ì •

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

class DistributedGMPOTrainer:
    def __init__(self, rank, world_size):
        self.rank = rank
        self.world_size = world_size
        self.setup_distributed()
        
    def setup_distributed(self):
        """ë¶„ì‚° í›ˆë ¨ í™˜ê²½ ì„¤ì •"""
        dist.init_process_group(
            backend='nccl',
            rank=self.rank,
            world_size=self.world_size
        )
        torch.cuda.set_device(self.rank)
        
    def train_distributed_gmpo(self, model, dataset):
        """ë¶„ì‚° GMPO í›ˆë ¨"""
        
        # 1. ëª¨ë¸ì„ DDPë¡œ ë˜í•‘
        model = DDP(model, device_ids=[self.rank])
        
        # 2. ë°ì´í„° ë¶„ì‚° ìƒ˜í”ŒëŸ¬
        sampler = torch.utils.data.distributed.DistributedSampler(
            dataset, 
            num_replicas=self.world_size, 
            rank=self.rank
        )
        
        dataloader = torch.utils.data.DataLoader(
            dataset, 
            batch_size=16, 
            sampler=sampler
        )
        
        # 3. GMPO ì˜µí‹°ë§ˆì´ì €
        optimizer = GMPOOptimizer(model.parameters())
        
        # 4. ë¶„ì‚° í›ˆë ¨ ë£¨í”„
        for epoch in range(num_epochs):
            sampler.set_epoch(epoch)
            
            for batch in dataloader:
                # ê¸°í•˜ í‰ê·  ê¸°ë°˜ ì†ì‹¤ ê³„ì‚°
                loss = self.compute_distributed_gmpo_loss(model, batch)
                
                # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë° ë™ê¸°í™”
                optimizer.zero_grad()
                loss.backward()
                
                # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ê°„ ê·¸ë˜ë””ì–¸íŠ¸ í‰ê· í™”
                dist.all_reduce(loss, op=dist.ReduceOp.SUM)
                loss /= self.world_size
                
                optimizer.step()
                
                if self.rank == 0:  # ë§ˆìŠ¤í„° í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ë¡œê¹…
                    print(f"Epoch {epoch}, Loss: {loss.item():.4f}")
```

## ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

### ì‹¤ì‹œê°„ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ

```python
import streamlit as st
import plotly.graph_objects as go
from plotly.subplots import make_subplots

class GMPODashboard:
    def __init__(self):
        self.metrics_history = []
        self.current_session = GMPOMonitoringSession()
        
    def create_dashboard(self):
        """ì‹¤ì‹œê°„ GMPO ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ"""
        
        st.set_page_config(page_title="GMPO ëª¨ë¸ ëª¨ë‹ˆí„°ë§", layout="wide")
        st.title("ğŸš€ GMPO ëª¨ë¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")
        
        # 1. í•µì‹¬ ì§€í‘œ ìš”ì•½
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            current_accuracy = self.get_current_accuracy()
            st.metric("ì •í™•ë„", f"{current_accuracy:.2f}%", 
                     delta=f"{self.get_accuracy_delta():.2f}%")
        
        with col2:
            avg_latency = self.get_average_latency()
            st.metric("í‰ê·  ì‘ë‹µì‹œê°„", f"{avg_latency:.0f}ms",
                     delta=f"{self.get_latency_delta():.0f}ms")
        
        with col3:
            stability_score = self.get_stability_score()
            st.metric("ì•ˆì •ì„± ì ìˆ˜", f"{stability_score:.3f}",
                     delta=f"{self.get_stability_delta():.3f}")
        
        with col4:
            throughput = self.get_throughput()
            st.metric("ì²˜ë¦¬ëŸ‰", f"{throughput:.0f} req/sec",
                     delta=f"{self.get_throughput_delta():.0f}")
        
        # 2. ìƒì„¸ ì„±ëŠ¥ ì°¨íŠ¸
        self.render_performance_charts()
        
        # 3. ì‹¤ì‹œê°„ ë¡œê·¸
        self.render_real_time_logs()
        
    def render_performance_charts(self):
        """ì„±ëŠ¥ ì°¨íŠ¸ ë Œë”ë§"""
        
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('ì •í™•ë„ ì¶”ì„¸', 'Importance Ratio ë¶„í¬', 
                          'ì‘ë‹µ ì‹œê°„ ë¶„í¬', 'ê¸°í•˜ í‰ê·  ì•ˆì •ì„±'),
            specs=[[{"secondary_y": True}, {"type": "histogram"}],
                   [{"type": "box"}, {"type": "scatter"}]]
        )
        
        # ì •í™•ë„ ì¶”ì„¸
        timestamps, accuracies = self.get_accuracy_history()
        fig.add_trace(
            go.Scatter(x=timestamps, y=accuracies, name="ì •í™•ë„"),
            row=1, col=1
        )
        
        # Importance Ratio ë¶„í¬
        ratios = self.get_importance_ratios()
        fig.add_trace(
            go.Histogram(x=ratios, name="Importance Ratios"),
            row=1, col=2
        )
        
        # ì‘ë‹µ ì‹œê°„ ë°•ìŠ¤ í”Œë¡¯
        latencies = self.get_latency_distribution()
        fig.add_trace(
            go.Box(y=latencies, name="ì‘ë‹µ ì‹œê°„"),
            row=2, col=1
        )
        
        # ê¸°í•˜ í‰ê·  ì•ˆì •ì„±
        geometric_means = self.get_geometric_mean_history()
        fig.add_trace(
            go.Scatter(y=geometric_means, mode='lines+markers', name="ê¸°í•˜ í‰ê· "),
            row=2, col=2
        )
        
        st.plotly_chart(fig, use_container_width=True)
```

### ì´ìƒ ì§•í›„ íƒì§€

```python
class GMPOAnomalyDetector:
    def __init__(self):
        self.baseline_metrics = self.load_baseline_metrics()
        self.detector_models = self.initialize_detectors()
        
    def detect_anomalies(self, current_metrics):
        """ì‹¤ì‹œê°„ ì´ìƒ ì§•í›„ íƒì§€"""
        
        anomalies = []
        
        # 1. í†µê³„ì  ì´ìƒì¹˜ íƒì§€
        statistical_anomalies = self.statistical_outlier_detection(current_metrics)
        
        # 2. ê¸°í•˜ í‰ê·  íŒ¨í„´ ì´ìƒ íƒì§€
        geometric_anomalies = self.geometric_pattern_detection(current_metrics)
        
        # 3. Importance ratio ë¶„í¬ ì´ìƒ íƒì§€
        ratio_anomalies = self.importance_ratio_analysis(current_metrics)
        
        # 4. ì„±ëŠ¥ ê¸‰ë½ íƒì§€
        performance_anomalies = self.performance_degradation_detection(current_metrics)
        
        all_anomalies = {
            'statistical': statistical_anomalies,
            'geometric_pattern': geometric_anomalies,
            'importance_ratio': ratio_anomalies,
            'performance': performance_anomalies
        }
        
        # 5. ì‹¬ê°ë„ í‰ê°€ ë° ì•Œë¦¼
        severity = self.evaluate_severity(all_anomalies)
        
        if severity >= 0.7:  # ë†’ì€ ì‹¬ê°ë„
            self.send_critical_alert(all_anomalies, current_metrics)
        elif severity >= 0.4:  # ì¤‘ê°„ ì‹¬ê°ë„
            self.send_warning_alert(all_anomalies, current_metrics)
            
        return {
            'anomalies': all_anomalies,
            'severity': severity,
            'recommendations': self.generate_recommendations(all_anomalies)
        }
```

## ë¯¸ë˜ ë°œì „ ë°©í–¥

### 1. ë©€í‹°ëª¨ë‹¬ GMPO í™•ì¥

```python
class MultimodalGMPO:
    def __init__(self):
        self.vision_encoder = VisionEncoder()
        self.text_encoder = TextEncoder()
        self.multimodal_fusion = CrossModalFusion()
        
    def compute_multimodal_geometric_mean(self, vision_rewards, text_rewards):
        """ë©€í‹°ëª¨ë‹¬ í™˜ê²½ì—ì„œì˜ ê¸°í•˜ í‰ê·  ê³„ì‚°"""
        
        # ëª¨ë‹¬ë¦¬í‹°ë³„ ê¸°í•˜ í‰ê· 
        vision_geometric = self.geometric_mean(vision_rewards)
        text_geometric = self.geometric_mean(text_rewards)
        
        # í¬ë¡œìŠ¤ ëª¨ë‹¬ ê°€ì¤‘ ê¸°í•˜ í‰ê· 
        cross_modal_weights = self.compute_cross_modal_weights(
            vision_rewards, text_rewards
        )
        
        unified_geometric_mean = (
            vision_geometric ** cross_modal_weights['vision'] *
            text_geometric ** cross_modal_weights['text']
        )
        
        return unified_geometric_mean
```

### 2. ì ì‘ì  ê¸°í•˜ í‰ê·  íŒŒë¼ë¯¸í„°

```python
class AdaptiveGMPO:
    def __init__(self):
        self.epsilon_scheduler = EpsilonScheduler()
        self.clip_scheduler = ClipScheduler()
        
    def adaptive_geometric_optimization(self, epoch, batch_idx, metrics):
        """ì ì‘ì  GMPO íŒŒë¼ë¯¸í„° ì¡°ì •"""
        
        # í›ˆë ¨ ì§„í–‰ë„ì— ë”°ë¥¸ epsilon ì¡°ì •
        current_epsilon = self.epsilon_scheduler.get_epsilon(
            epoch=epoch,
            stability_score=metrics['stability'],
            convergence_rate=metrics['convergence']
        )
        
        # ë™ì  í´ë¦¬í•‘ ë²”ìœ„ ì¡°ì •
        current_clip_range = self.clip_scheduler.get_clip_range(
            importance_ratio_distribution=metrics['ratio_dist'],
            loss_variance=metrics['loss_variance']
        )
        
        return {
            'epsilon': current_epsilon,
            'clip_range': current_clip_range,
            'learning_rate_adjustment': self.compute_lr_adjustment(metrics)
        }
```

### 3. ì—°í•© í•™ìŠµì—ì„œì˜ GMPO

```python
class FederatedGMPO:
    def __init__(self, num_clients):
        self.num_clients = num_clients
        self.global_geometric_aggregator = GlobalGeometricAggregator()
        
    def federated_geometric_optimization(self, client_updates):
        """ì—°í•© í•™ìŠµì—ì„œì˜ ê¸°í•˜ í‰ê·  ê¸°ë°˜ ëª¨ë¸ ì§‘ê³„"""
        
        # í´ë¼ì´ì–¸íŠ¸ë³„ ê¸°í•˜ í‰ê·  ê³„ì‚°
        client_geometric_means = []
        for client_id, update in client_updates.items():
            client_geometric = self.compute_client_geometric_mean(update)
            client_geometric_means.append((client_id, client_geometric))
        
        # ê¸€ë¡œë²Œ ê¸°í•˜ í‰ê·  ì§‘ê³„
        global_geometric_mean = self.global_geometric_aggregator.aggregate(
            client_geometric_means
        )
        
        # ì•ˆì „ì„± ê²€ì¦
        safety_check = self.verify_global_model_safety(global_geometric_mean)
        
        if safety_check['is_safe']:
            return global_geometric_mean
        else:
            return self.fallback_aggregation(client_updates)
```

## ê²°ë¡ 

**Geometric-Mean Policy Optimization (GMPO)**ì€ ê¸°ì¡´ GRPOì˜ í•µì‹¬ í•œê³„ì ë“¤ì„ í•´ê²°í•˜ë©° LLMOps í™˜ê²½ì—ì„œì˜ **ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ëª¨ë¸ í›ˆë ¨**ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” í˜ì‹ ì  ê¸°ë²•ì…ë‹ˆë‹¤.

### í•µì‹¬ ì„±ê³¼

1. **ì•ˆì •ì„± í˜ì‹ **: ê¸°í•˜ í‰ê·  ì‚¬ìš©ìœ¼ë¡œ ì´ìƒì¹˜ ë¯¼ê°ì„± 67% ê°ì†Œ
2. **ì„±ëŠ¥ í–¥ìƒ**: ìˆ˜í•™ì  ì¶”ë¡  4.1%, ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  1.4% ê°œì„ 
3. **í›ˆë ¨ íš¨ìœ¨ì„±**: ìˆ˜ë ´ ì†ë„ 29% í–¥ìƒ, í›ˆë ¨ í¬ë˜ì‹œ ì™„ì „ ì œê±°
4. **ì‹¤ìš©ì„±**: í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì•ˆì •ì  ì•„í‚¤í…ì²˜

### LLMOps í™˜ê²½ì—ì„œì˜ ê°€ì¹˜

- **ì•ˆì •ì  ë°°í¬**: ì˜ˆì¸¡ ê°€ëŠ¥í•œ ëª¨ë¸ ë™ì‘ìœ¼ë¡œ í”„ë¡œë•ì…˜ ìœ„í—˜ ìµœì†Œí™”
- **íš¨ìœ¨ì  ìš´ì˜**: ì ì€ ë¦¬ì†ŒìŠ¤ë¡œ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±
- **ì§€ì†ì  ê°œì„ **: ì•ˆì „í•œ ì˜¨ë¼ì¸ í•™ìŠµ ë° ëª¨ë¸ ì—…ë°ì´íŠ¸
- **í™•ì¥ì„±**: ë¶„ì‚° í™˜ê²½ê³¼ ì—°í•© í•™ìŠµì—ì„œì˜ ìš°ìˆ˜í•œ í™•ì¥ì„±

GMPOëŠ” ë‹¨ìˆœí•œ ì•Œê³ ë¦¬ì¦˜ ê°œì„ ì„ ë„˜ì–´ì„œ **LLM ìš´ì˜ì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ë°”ê¾¸ëŠ” í˜ì‹ **ì…ë‹ˆë‹¤. ê¸°í•˜ í‰ê· ì˜ ìˆ˜í•™ì  íŠ¹ì„±ì„ í™œìš©í•œ ì´ ì ‘ê·¼ë²•ì€ **ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì˜ ì™„ë²½í•œ ê· í˜•**ì„ ì œê³µí•˜ë©°, ì‹¤ì œ ì‚°ì—… í™˜ê²½ì—ì„œì˜ AI ì‹œìŠ¤í…œ ìš´ì˜ì„ í•œ ë‹¨ê³„ ë°œì „ì‹œí‚¬ ê²ƒì…ë‹ˆë‹¤.

---

**ì°¸ê³  ìë£Œ:**
- [GMPO ë…¼ë¬¸ (Hugging Face Papers)](https://huggingface.co/papers/2507.20673)
- [GitHub ë¦¬í¬ì§€í† ë¦¬](https://github.com/callsys/GMPO)
- **ì €ì**: Yuzhong Zhao, Yue Liu, Junpeng Liu ì™¸ 9ëª…
- **ë°œí‘œ**: 2025ë…„ 7ì›” 28ì¼

**ê´€ë ¨ í‚¤ì›Œë“œ:** `#GMPO` `#GeometricMean` `#PolicyOptimization` `#RLHF` `#LLMOps` `#StableTraining`