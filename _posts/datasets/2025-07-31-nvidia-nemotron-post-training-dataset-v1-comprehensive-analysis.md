---
title: "NVIDIA Nemotron Post-Training Dataset v1 - LLM ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ëŒ€ê·œëª¨ í•©ì„± ë°ì´í„°ì…‹ ì™„ì „ ë¶„ì„"
excerpt: "NVIDIAê°€ ê³µê°œí•œ 2,560ë§Œ ê°œ ìƒ˜í”Œì˜ ëŒ€ê·œëª¨ í•©ì„± ë°ì´í„°ì…‹ìœ¼ë¡œ, ìˆ˜í•™, ì½”ë”©, STEM, ì¶”ë¡ , ë„êµ¬ í˜¸ì¶œ ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•œ ê³ í’ˆì§ˆ í›ˆë ¨ ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
seo_title: "NVIDIA Nemotron Post-Training Dataset v1 ì™„ì „ ë¶„ì„ - Thaki Cloud"
seo_description: "NVIDIA Nemotron Post-Training Dataset v1ì˜ êµ¬ì¡°, ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ë¶„í¬, ì‚¬ìš©ë²•ì„ ìƒì„¸íˆ ë¶„ì„í•œ ê°€ì´ë“œ. 25.6M ìƒ˜í”Œì˜ ê³ í’ˆì§ˆ í•©ì„± ë°ì´í„°ì…‹ í™œìš© ë°©ë²•ê¹Œì§€."
date: 2025-07-31
last_modified_at: 2025-07-31
categories:
  - datasets
  - llmops
tags:
  - NVIDIA
  - Nemotron
  - ë°ì´í„°ì…‹
  - í•©ì„±ë°ì´í„°
  - LLMí›ˆë ¨
  - íŒŒì¸íŠœë‹
  - HuggingFace
  - ë¨¸ì‹ ëŸ¬ë‹
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "database"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/datasets/nvidia-nemotron-post-training-dataset-v1-comprehensive-analysis/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 12ë¶„

## ì„œë¡ 

NVIDIAì—ì„œ 2025ë…„ 7ì›” 31ì¼ ê³µê°œí•œ **Nemotron Post-Training Dataset v1**ì€ LLM ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ëŒ€ê·œëª¨ í•©ì„± ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. ì´ **2,566ë§Œ ê°œ**ì˜ ê³ í’ˆì§ˆ ìƒ˜í”Œë¡œ êµ¬ì„±ëœ ì´ ë°ì´í„°ì…‹ì€ ìˆ˜í•™, ì½”ë”©, STEM, ì¼ë°˜ ì¶”ë¡ , ë„êµ¬ í˜¸ì¶œ ëŠ¥ë ¥ì„ íšê¸°ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆëŠ” í›ˆë ¨ ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì´ ë°ì´í„°ì…‹ì€ **Llama-3.3-Nemotron-Super-49B-v1.5** ëª¨ë¸ì˜ í›ˆë ¨ì— ì‚¬ìš©ë˜ì—ˆìœ¼ë©°, ì™„ì „í•œ íˆ¬ëª…ì„±ê³¼ ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•´ ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¥¼ ê³µê°œí•œ ê²ƒì€ ì—…ê³„ì—ì„œ ë§¤ìš° ì˜ë¯¸ ìˆëŠ” ì›€ì§ì„ì…ë‹ˆë‹¤.

## ë°ì´í„°ì…‹ ê°œìš” ë° í•µì‹¬ íŠ¹ì§•

### ê¸°ë³¸ ì •ë³´

| í•­ëª© | ìƒì„¸ ë‚´ìš© |
|------|-----------|
| **ë°ì´í„°ì…‹ ëª…** | NVIDIA Nemotron Post-Training Dataset v1 |
| **ê³µê°œì¼** | 2025ë…„ 7ì›” 31ì¼ |
| **ì „ì²´ ìƒ˜í”Œ ìˆ˜** | 25,659,642ê°œ |
| **íŒŒì¼ í¬ê¸°** | 203GB (Parquet í˜•ì‹) |
| **ë¼ì´ì„ ìŠ¤** | CC BY 4.0 (ìƒì—…ì /ë¹„ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥) |
| **í”Œë«í¼** | Hugging Face Datasets |

### ë°ì´í„° ìƒì„± ë°©ì‹

ì´ ë°ì´í„°ì…‹ì˜ ê°€ì¥ í° íŠ¹ì§•ì€ **100% í•©ì„± ë°ì´í„°**ë¼ëŠ” ì ì…ë‹ˆë‹¤:

- **í”„ë¡¬í”„íŠ¸**: ê³µê°œ ì½”í¼ìŠ¤ì—ì„œ ì¶”ì¶œ ë˜ëŠ” í•©ì„± ìƒì„±
- **ì‘ë‹µ**: ê³ ì„±ëŠ¥ AI ëª¨ë¸ì„ í†µí•´ í•©ì„± ìƒì„±
- **í’ˆì§ˆ í•„í„°ë§**: ì¼ê´€ì„± ë¶€ì¡±, ì‰¬ìš´ ë‹µë³€, ì˜ëª»ëœ ë¬¸ë²• ë“±ì„ ì œê±°

## ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„

### ì „ì²´ ë°ì´í„° ë¶„í¬

| ì¹´í…Œê³ ë¦¬ | ìƒ˜í”Œ ìˆ˜ | ë¹„ìœ¨ | ì£¼ìš” ìš©ë„ |
|----------|---------|------|----------|
| **stem** | 20,662,167 | 80.5% | ê³¼í•™, ê³µí•™, ìˆ˜í•™ ì¶”ë¡  |
| **code** | 1,896,395 | 7.4% | í”„ë¡œê·¸ë˜ë° ëŠ¥ë ¥ í–¥ìƒ |
| **math** | 2,044,407 | 8.0% | ìˆ˜í•™ì  ì¶”ë¡  ë° ê³„ì‚° |
| **chat** | 746,622 | 2.9% | ëŒ€í™”í˜• ìƒí˜¸ì‘ìš© |
| **tool_calling** | 310,051 | 1.2% | ë„êµ¬ í˜¸ì¶œ ë° API ì‚¬ìš© |
| **ì „ì²´** | **25,659,642** | **100%** | - |

### 1. STEM ì¹´í…Œê³ ë¦¬ (20.7M ìƒ˜í”Œ)

ì „ì²´ ë°ì´í„°ì˜ **80.5%**ë¥¼ ì°¨ì§€í•˜ëŠ” í•µì‹¬ ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.

#### í¬í•¨ ì˜ì—­
- **ê³¼í•™**: ë¬¼ë¦¬í•™, í™”í•™, ìƒë¬¼í•™
- **ê³µí•™**: ë‹¤ì–‘í•œ ê³µí•™ ë¶„ì•¼
- **ìˆ˜í•™**: ê³ ê¸‰ ìˆ˜í•™ ë¬¸ì œ
- **ì¸ë¬¸í•™**: ì¼ë°˜ì ì¸ ì¶”ë¡  ë¬¸ì œ

#### ê¶Œì¥ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
```text
Read the following problem carefully and provide a detailed, step-by-step answer.
{problem}
```

#### í™œìš© ë°©ì•ˆ
- ê³¼í•™ì  ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒ
- ë³µì¡í•œ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ ê°œë°œ
- í•™ìˆ ì  ê¸€ì“°ê¸° ëŠ¥ë ¥ í–¥ìƒ

### 2. ìˆ˜í•™ ì¹´í…Œê³ ë¦¬ (2.0M ìƒ˜í”Œ)

**ë‹¨ê³„ë³„ ìˆ˜í•™ ë¬¸ì œ í•´ê²°** ëŠ¥ë ¥ì„ ì§‘ì¤‘ í›ˆë ¨í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…ë‹ˆë‹¤.

#### íŠ¹ì§•
- ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œ í¬í•¨
- ë‹¨ê³„ë³„ í’€ì´ ê³¼ì • ì œê³µ
- ìµœì¢… ë‹µì•ˆì„ `\boxed{% raw %}{}{% endraw %}` í˜•ì‹ìœ¼ë¡œ ëª…ì‹œ

#### ê¶Œì¥ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
```text
Solve the following math problem. Explain your reasoning and put the final answer in \boxed{% raw %}{}{% endraw %}.
{problem}
```

#### í›ˆë ¨ íš¨ê³¼
- ë…¼ë¦¬ì  ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒ
- ìˆ˜í•™ì  í‘œê¸°ë²• ì´í•´ë„ ì¦ì§„
- ì²´ê³„ì  ë¬¸ì œ í•´ê²° ë°©ë²•ë¡  ìŠµë“

### 3. ì½”ë”© ì¹´í…Œê³ ë¦¬ (1.9M ìƒ˜í”Œ)

**í”„ë¡œê·¸ë˜ë° ëŠ¥ë ¥ í–¥ìƒ**ì„ ìœ„í•œ ê³ í’ˆì§ˆ ì½”ë“œ ìƒì„± ë°ì´í„°ì…ë‹ˆë‹¤.

#### ë°ì´í„° êµ¬ì„±
- í”„ë¡œê·¸ë˜ë° ì±Œë¦°ì§€
- ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œ
- ì½”ë“œ ì„¤ëª… ë° ìµœì í™”
- ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ì§€ì›

#### ê¶Œì¥ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
```text
Write a solution for the following programming challenge. Provide a brief explanation of your approach, followed by the complete code.
{problem}
```

#### ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤
ì¼ë¶€ í”„ë¡¬í”„íŠ¸ëŠ” **OpenCodeReasoning** ë“± ì™¸ë¶€ ì†ŒìŠ¤ì—ì„œ ê°€ì ¸ì™”ìœ¼ë©°, í•´ë‹¹ ë°ì´í„°ëŠ” ì›ë³¸ ì†ŒìŠ¤ì—ì„œ ë³„ë„ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.

### 4. ëŒ€í™” ì¹´í…Œê³ ë¦¬ (747K ìƒ˜í”Œ)

**ëŒ€í™”í˜• AI** ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ë°ì´í„°ì…ë‹ˆë‹¤.

#### íŠ¹ì§•
- ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„
- ë‹¤ì–‘í•œ ì£¼ì œì™€ ìƒí™©
- ë„ì›€ì´ ë˜ê³  ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ìŠ¤íƒ€ì¼

#### ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
```text
You are a helpful and friendly AI assistant.
```

#### ì™¸ë¶€ ë°ì´í„° ì—°ë™
ì¼ë¶€ ë°ì´í„°ëŠ” **lmsys-chat-1m** ë°ì´í„°ì…‹ì—ì„œ ê°€ì ¸ì™”ìœ¼ë©°, `input` í•„ë“œê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì›ë³¸ ì†ŒìŠ¤ì—ì„œ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.

### 5. ë„êµ¬ í˜¸ì¶œ ì¹´í…Œê³ ë¦¬ (310K ìƒ˜í”Œ)

**AI ì—ì´ì „íŠ¸** ë° **ë„êµ¬ í†µí•©** ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•œ ë°ì´í„°ì…ë‹ˆë‹¤.

#### ì§€ì› ì‹œë‚˜ë¦¬ì˜¤
- **ë‹¨ì¼ í„´**: í•œ ë²ˆì˜ ë„êµ¬ í˜¸ì¶œ
- **ë©€í‹° í„´**: ì—¬ëŸ¬ ë²ˆì˜ ëŒ€í™”ë¥¼ í†µí•œ ë„êµ¬ í˜¸ì¶œ
- **ë©€í‹° ìŠ¤í…**: ë³µì¡í•œ ë‹¤ë‹¨ê³„ ë„êµ¬ í˜¸ì¶œ

#### ë©”íƒ€ë°ì´í„° êµ¬ì¡°
- `tools`: ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì •ì˜
- `tool_calls`: ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ë„êµ¬ í˜¸ì¶œ ë‚´ì—­

## ë°ì´í„° ìƒì„± ëª¨ë¸ ë¶„ì„

### ì‚¬ìš©ëœ ëª¨ë¸

| ëª¨ë¸ | ìƒì„± ìƒ˜í”Œ ìˆ˜ | ë¹„ìœ¨ | íŠ¹ì§• |
|------|-------------|------|------|
| **DeepSeek-R1-0528** | 24,602,969 | 95.9% | ì£¼ë ¥ ìƒì„± ëª¨ë¸ |
| **Qwen3-235B-A22B** | 1,056,673 | 4.1% | ë³´ì¡° ìƒì„± ëª¨ë¸ |

### ìƒì„± í’ˆì§ˆ ë³´ì¥

1. **ë‹¤ì–‘ì„± í™•ë³´**: ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
2. **ì¶”ë¡  ëª¨ë“œ êµ¬ë¶„**: reasoning on/off ëª¨ë“œë³„ ì‘ë‹µ ìƒì„±
3. **í’ˆì§ˆ í•„í„°ë§**: ì¼ê´€ì„± ê²€ì¦ ë° ì˜¤ë¥˜ ì œê±°

## ì‹¤ì œ ì‚¬ìš© ë°©ë²•

### Hugging Face Datasetsë¥¼ í†µí•œ ë°ì´í„° ë¡œë“œ

```python
from datasets import load_dataset

# ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("nvidia/Nemotron-Post-Training-Dataset-v1")

# íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ ë¡œë“œ
code_math_dataset = load_dataset(
    "nvidia/Nemotron-Post-Training-Dataset-v1", 
    split=["code", "math"]
)

# ê°œë³„ ì¹´í…Œê³ ë¦¬ ë¡œë“œ
stem_data = load_dataset(
    "nvidia/Nemotron-Post-Training-Dataset-v1", 
    split="stem"
)
```

### ë°ì´í„° êµ¬ì¡° ì´í•´

```python
# ë°ì´í„° ìƒ˜í”Œ í™•ì¸
sample = dataset['train'][0]
print("UUID:", sample['uuid'])
print("Category:", sample['category'])
print("License:", sample['license'])
print("Messages:", sample['messages'])
print("Metadata:", sample['metadata'])
```

### íŒŒì¸íŠœë‹ ë°ì´í„° ì¤€ë¹„

```python
def format_chat_sample(sample):
    """ëŒ€í™” ë°ì´í„° í¬ë§·íŒ…"""
    messages = sample['messages']
    formatted = f"<s>[INST] {messages[0]['content']} [/INST] {messages[1]['content']}</s>"
    return {"text": formatted}

def format_math_sample(sample):
    """ìˆ˜í•™ ë¬¸ì œ í¬ë§·íŒ…"""
    problem = sample['messages'][0]['content']
    solution = sample['messages'][1]['content']
    formatted = f"Solve the following math problem. Explain your reasoning and put the final answer in \\boxed{% raw %}{{}}{% endraw %}.\n{problem}\n\n{solution}"
    return {"text": formatted}

# ë°ì´í„° ë³€í™˜ ì ìš©
chat_formatted = dataset['chat'].map(format_chat_sample)
math_formatted = dataset['math'].map(format_math_sample)
```

## í’ˆì§ˆ í‰ê°€ ë° ë²¤ì¹˜ë§ˆí¬

### ë°ì´í„° í’ˆì§ˆ ì§€í‘œ

1. **ì¼ê´€ì„±**: í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µ ê°„ì˜ ë…¼ë¦¬ì  ì—°ê²°ì„±
2. **ì •í™•ì„±**: ìˆ˜í•™, ê³¼í•™ ë¬¸ì œì˜ í•´ë‹µ ì •í™•ë„
3. **ë³µì¡ì„±**: ì ì ˆí•œ ë‚œì´ë„ ìˆ˜ì¤€ ìœ ì§€
4. **ë‹¤ì–‘ì„±**: ì£¼ì œì™€ í˜•ì‹ì˜ ë‹¤ì–‘ì„±

### ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ê²°ê³¼

**Llama-3.3-Nemotron-Super-49B-v1.5**ëŠ” ì´ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ë˜ì–´:
- ì›ë³¸ Llama-3.3-70B ëŒ€ë¹„ ë” íš¨ìœ¨ì ì¸ ì¶”ë¡ 
- 128K ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì§€ì›
- ì •í™•ë„-íš¨ìœ¨ì„± íŠ¸ë ˆì´ë“œì˜¤í”„ ìµœì í™”

## ë¼ì´ì„ ìŠ¤ ë° ì‚¬ìš© ì œí•œì‚¬í•­

### ë¼ì´ì„ ìŠ¤ ì •ë³´

- **ë¼ì´ì„ ìŠ¤**: Creative Commons Attribution 4.0 International (CC BY 4.0)
- **ìƒì—…ì  ì‚¬ìš©**: í—ˆìš©
- **ì¬ë°°í¬**: í—ˆìš© (ì¶œì²˜ ëª…ì‹œ í•„ìš”)
- **ìˆ˜ì •**: í—ˆìš©

### ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­

1. **ê°œì¸ì •ë³´ ë³´í˜¸**: PII ë°ì´í„° ë¯¸í¬í•¨ í™•ì¸
2. **ì €ì‘ê¶Œ ê²€í† **: ë²•ì  ê²€í†  ì™„ë£Œ
3. **í¸í–¥ì„± ìµœì†Œí™”**: ë‹¤ì–‘í•œ ê´€ì  ë°˜ì˜
4. **ì•ˆì „ì„±**: ìœ í•´ ì½˜í…ì¸  í•„í„°ë§

### ë°ì´í„° ì˜µíŠ¸ì•„ì›ƒ

ë¬¸ì œê°€ ë°œê²¬ë  ê²½ìš° `ln-dataset@nvidia.com`ìœ¼ë¡œ ì—°ë½ ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ì‹¤ì œ í™œìš© ì‚¬ë¡€

### 1. ìˆ˜í•™ êµìœ¡ AI ê°œë°œ

```python
# ìˆ˜í•™ íŠœí„° AI í›ˆë ¨ìš© ë°ì´í„° ì¤€ë¹„
math_subset = load_dataset(
    "nvidia/Nemotron-Post-Training-Dataset-v1", 
    split="math"
)

def create_math_tutor_format(sample):
    problem = sample['messages'][0]['content']
    solution = sample['messages'][1]['content']
    
    return {
        "instruction": "ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í•´ê²°í•´ì£¼ì„¸ìš”.",
        "input": problem,
        "output": solution
    }

math_tutor_data = math_subset.map(create_math_tutor_format)
```

### 2. ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ ê°œë°œ

```python
# ì½”ë”© ë„ìš°ë¯¸ AI í›ˆë ¨ìš© ë°ì´í„° ì¤€ë¹„
code_subset = load_dataset(
    "nvidia/Nemotron-Post-Training-Dataset-v1", 
    split="code"
)

def create_coding_assistant_format(sample):
    problem = sample['messages'][0]['content']
    solution = sample['messages'][1]['content']
    
    return {
        "instruction": "ë‹¤ìŒ í”„ë¡œê·¸ë˜ë° ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "input": problem,
        "output": solution
    }

coding_assistant_data = code_subset.map(create_coding_assistant_format)
```

### 3. ê³¼í•™ ì—°êµ¬ ë„ìš°ë¯¸ ê°œë°œ

```python
# STEM ë¶„ì•¼ ì—°êµ¬ ë„ìš°ë¯¸ AI í›ˆë ¨ìš© ë°ì´í„°
stem_subset = load_dataset(
    "nvidia/Nemotron-Post-Training-Dataset-v1", 
    split="stem"
)

def create_research_assistant_format(sample):
    query = sample['messages'][0]['content']
    response = sample['messages'][1]['content']
    
    return {
        "instruction": "ê³¼í•™ì  ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
        "input": query,
        "output": response
    }

research_assistant_data = stem_subset.map(create_research_assistant_format)
```

## ê¸°ìˆ ì  êµ¬í˜„ ê°€ì´ë“œ

### GPU ë©”ëª¨ë¦¬ ìµœì í™”

```python
from datasets import load_dataset
import torch

# ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
def process_in_batches(dataset, batch_size=1000):
    for i in range(0, len(dataset), batch_size):
        batch = dataset[i:i+batch_size]
        # ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§
        yield batch

# í° ë°ì´í„°ì…‹ì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ë¡œë“œ
dataset_stream = load_dataset(
    "nvidia/Nemotron-Post-Training-Dataset-v1",
    split="stem",
    streaming=True
)
```

### ë¶„ì‚° ì²˜ë¦¬ ì„¤ì •

```python
from datasets import load_dataset
from torch.utils.data import DataLoader
from torch.nn.parallel import DistributedDataParallel

# ë¶„ì‚° í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
def setup_distributed_data(rank, world_size):
    dataset = load_dataset(
        "nvidia/Nemotron-Post-Training-Dataset-v1",
        split="train"
    )
    
    # ê° GPUë³„ë¡œ ë°ì´í„° ë¶„í• 
    shard_size = len(dataset) // world_size
    start_idx = rank * shard_size
    end_idx = start_idx + shard_size if rank < world_size - 1 else len(dataset)
    
    return dataset[start_idx:end_idx]
```

### í›ˆë ¨ íŒŒì´í”„ë¼ì¸ êµ¬ì„±

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer

def setup_training_pipeline():
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model_name = "meta-llama/Llama-3.3-70B-Instruct"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬
    dataset = load_dataset("nvidia/Nemotron-Post-Training-Dataset-v1")
    
    def tokenize_function(examples):
        return tokenizer(
            examples['text'], 
            truncation=True, 
            padding=True, 
            max_length=2048
        )
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    
    # í›ˆë ¨ ì„¤ì •
    training_args = TrainingArguments(
        output_dir="./nemotron-finetuned",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=100,
        save_steps=1000,
        eval_steps=500,
        evaluation_strategy="steps",
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        dataloader_num_workers=4,
        fp16=True,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ Mixed Precision
    )
    
    return model, tokenizer, tokenized_dataset, training_args
```

## ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²˜ë¦¬

```python
# ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì œë„ˆë ˆì´í„° ì‚¬ìš©
def memory_efficient_data_loader(dataset_name, split, chunk_size=10000):
    dataset = load_dataset(dataset_name, split=split, streaming=True)
    
    chunk = []
    for sample in dataset:
        chunk.append(sample)
        if len(chunk) >= chunk_size:
            yield chunk
            chunk = []
    
    if chunk:  # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        yield chunk

# ì‚¬ìš© ì˜ˆì‹œ
for chunk in memory_efficient_data_loader(
    "nvidia/Nemotron-Post-Training-Dataset-v1", 
    "stem", 
    chunk_size=5000
):
    # ì²­í¬ë³„ ì²˜ë¦¬ ë¡œì§
    process_chunk(chunk)
```

### 2. í† í°í™” ìµœì í™”

```python
from transformers import AutoTokenizer
import multiprocessing as mp

def parallel_tokenization(dataset, tokenizer, num_processes=8):
    def tokenize_batch(batch):
        return tokenizer(
            batch['text'],
            truncation=True,
            padding=True,
            max_length=2048,
            return_tensors='pt'
        )
    
    # ë©€í‹°í”„ë¡œì„¸ì‹±ì„ í†µí•œ ë³‘ë ¬ í† í°í™”
    with mp.Pool(num_processes) as pool:
        tokenized_batches = pool.map(tokenize_batch, dataset)
    
    return tokenized_batches
```

## í’ˆì§ˆ ê²€ì¦ ë° ëª¨ë‹ˆí„°ë§

### ë°ì´í„° í’ˆì§ˆ ì²´í¬ ìŠ¤í¬ë¦½íŠ¸

```python
def validate_dataset_quality(dataset):
    """ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦"""
    quality_metrics = {
        'total_samples': len(dataset),
        'avg_input_length': 0,
        'avg_output_length': 0,
        'empty_samples': 0,
        'malformed_samples': 0
    }
    
    input_lengths = []
    output_lengths = []
    
    for sample in dataset:
        try:
            messages = sample['messages']
            if len(messages) != 2:
                quality_metrics['malformed_samples'] += 1
                continue
                
            input_text = messages[0]['content']
            output_text = messages[1]['content']
            
            if not input_text or not output_text:
                quality_metrics['empty_samples'] += 1
                continue
                
            input_lengths.append(len(input_text))
            output_lengths.append(len(output_text))
            
        except Exception as e:
            quality_metrics['malformed_samples'] += 1
            continue
    
    quality_metrics['avg_input_length'] = sum(input_lengths) / len(input_lengths)
    quality_metrics['avg_output_length'] = sum(output_lengths) / len(output_lengths)
    
    return quality_metrics

# ì‚¬ìš© ì˜ˆì‹œ
dataset = load_dataset("nvidia/Nemotron-Post-Training-Dataset-v1", split="math")
metrics = validate_dataset_quality(dataset)
print("ë°ì´í„°ì…‹ í’ˆì§ˆ ë©”íŠ¸ë¦­:", metrics)
```

## ì¸ìš© ë° ì°¸ì¡°

ì´ ë°ì´í„°ì…‹ì„ ì—°êµ¬ë‚˜ ê°œë°œì— ì‚¬ìš©í•  ê²½ìš° ë‹¤ìŒê³¼ ê°™ì´ ì¸ìš©í•´ì£¼ì„¸ìš”:

```bibtex
@misc{bercovich2025llamanemotronefficientreasoningmodels,
      title={Llama-Nemotron: Efficient Reasoning Models}, 
      author={Akhiad Bercovich and Itay Levy and Izik Golan and Mohammad Dabbah and Ran El-Yaniv and Omri Puny and Ido Galil and Zach Moshe and Tomer Ronen and Najeeb Nabwani and Ido Shahaf and Oren Tropp and Ehud Karpas and Ran Zilberstein and Jiaqi Zeng and Soumye Singhal and Alexander Bukharin and Yian Zhang and Tugrul Konuk and Gerald Shen and Ameya Sunil Mahabaleshwarkar and Bilal Kartal and Yoshi Suhara and Olivier Delalleau and Zijia Chen and Zhilin Wang and David Mosallanezhad and Adi Renduchintala and Haifeng Qian and Dima Rekesh and Fei Jia and Somshubra Majumdar and Vahid Noroozi and Wasi Uddin Ahmad and Sean Narenthiran and Aleksander Ficek and Mehrzad Samadi and Jocelyn Huang and Siddhartha Jain and Igor Gitman and Ivan Moshkov and Wei Du and Shubham Toshniwal and George Armstrong and Branislav Kisacanin and Matvei Novikov and Daria Gitman and Evelina Bakhturina and Jane Polak Scowcroft and John Kamalu and Dan Su and Kezhi Kong and Markus Kliegl and Rabeeh Karimi and Ying Lin and Sanjeev Satheesh and Jupinder Parmar and Pritam Gundecha and Brandon Norick and Joseph Jennings and Shrimai Prabhumoye and Syeda Nahida Akter and Mostofa Patwary and Abhinav Khattar and Deepak Narayanan and Roger Waleffe and Jimmy Zhang and Bor-Yiing Su and Guyue Huang and Terry Kong and Parth Chadha and Sahil Jain and Christine Harvey and Elad Segal and Jining Huang and Sergey Kashirsky and Robert McQueen and Izzy Putterman and George Lam and Arun Venkatesan and Sherry Wu and Vinh Nguyen and Manoj Kilaru and Andrew Wang and Anna Warno and Abhilash Somasamudramath and Sandip Bhaskar and Maka Dong and Nave Assaf and Shahar Mor and Omer Ullman Argov and Scot Junkin and Oleksandr Romanenko and Pedro Larroy and Monika Katariya and Marco Rovinelli and Viji Balas and Nicholas Edelman and Anahita Bhiwandiwalla and Muthu Subramaniam and Smita Ithape and Karthik Ramamoorthy and Yuting Wu and Suguna Varshini Velury and Omri Almog and Joyjit Daw and Denys Fridman and Erick Galinkin and Michael Evans and Katherine Luna and Leon Derczynski and Nikki Pope and Eileen Long and Seth Schneider and Guillermo Siman and Tomasz Grzegorzek and Pablo Ribalta and Monika Katariya and Joey Conway and Trisha Saar and Ann Guan and Krzysztof Pawelec and Shyamala Prayaga and Oleksii Kuchaiev and Boris Ginsburg and Oluwatobi Olabiyi and Kari Briski and Jonathan Cohen and Bryan Catanzaro and Jonah Alben and Yonatan Geifman and Eric Chung and Chris Alexiuk},
      year={2025},
      eprint={2505.00949},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.00949}, 
}
```

## ê²°ë¡ 

NVIDIA Nemotron Post-Training Dataset v1ì€ LLM ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ **ê²Œì„ ì²´ì¸ì €**ê°€ ë  ìˆ˜ ìˆëŠ” ëŒ€ê·œëª¨ ê³ í’ˆì§ˆ í•©ì„± ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. íŠ¹íˆ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤:

### ì£¼ìš” ì¥ì 

1. **ëŒ€ê·œëª¨**: 2,566ë§Œ ê°œì˜ ë°©ëŒ€í•œ ìƒ˜í”Œ
2. **ê³ í’ˆì§ˆ**: ì—„ê²©í•œ í•„í„°ë§ê³¼ ê²€ì¦ ê³¼ì •
3. **ë‹¤ì–‘ì„±**: 5ê°œ ì£¼ìš” ì¹´í…Œê³ ë¦¬ì˜ ê· í˜•ì¡íŒ êµ¬ì„±
4. **íˆ¬ëª…ì„±**: ì™„ì „í•œ ë°ì´í„°ì™€ ìƒì„± ê³¼ì • ê³µê°œ
5. **ìƒì—…ì  í™œìš© ê°€ëŠ¥**: CC BY 4.0 ë¼ì´ì„ ìŠ¤

### í™œìš© ë¶„ì•¼

- **êµìœ¡ AI**: ìˆ˜í•™, ê³¼í•™ íŠœí„°ë§ ì‹œìŠ¤í…œ
- **ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸**: í”„ë¡œê·¸ë˜ë° ë„ìš°ë¯¸ AI
- **ì—°êµ¬ ë„êµ¬**: ê³¼í•™ ì—°êµ¬ ì§€ì› ì‹œìŠ¤í…œ
- **ë²”ìš© AI**: ì¶”ë¡  ëŠ¥ë ¥ ê°•í™”ëœ ë²”ìš© ëª¨ë¸

### í–¥í›„ ì „ë§

ì´ ë°ì´í„°ì…‹ì˜ ê³µê°œëŠ” AI ì—…ê³„ì˜ **íˆ¬ëª…ì„±ê³¼ í˜‘ì—…**ì„ í•œ ë‹¨ê³„ ëŒì–´ì˜¬ë¦¬ëŠ” ì˜ë¯¸ ìˆëŠ” ì‹œë„ì…ë‹ˆë‹¤. ê°œë°œìë“¤ì€ ì´ ë°ì´í„°ë¥¼ í™œìš©í•´ ë” ê°•ë ¥í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

íŠ¹íˆ í•œêµ­ì˜ AI ì—°êµ¬ì§„ê³¼ ê°œë°œìë“¤ì—ê²ŒëŠ” **í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸** ê°œë°œì˜ ê¸°ë°˜ ë°ì´í„°ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì¢‹ì€ ê¸°íšŒê°€ ë  ê²ƒì…ë‹ˆë‹¤.

---

### ì°¸ê³  ë§í¬

- [NVIDIA Nemotron Dataset ê³µì‹ í˜ì´ì§€](https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1)
- [ArXiv ë…¼ë¬¸](https://arxiv.org/abs/2505.00949)
- [Hugging Face Datasets ë¬¸ì„œ](https://huggingface.co/docs/datasets/)
- [CC BY 4.0 ë¼ì´ì„ ìŠ¤](https://creativecommons.org/licenses/by/4.0/legalcode)

ì´ ë°ì´í„°ì…‹ì„ í™œìš©í•œ í”„ë¡œì íŠ¸ë‚˜ ì—°êµ¬ ê²°ê³¼ê°€ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ê³µìœ í•´ì£¼ì„¸ìš”! ğŸš€