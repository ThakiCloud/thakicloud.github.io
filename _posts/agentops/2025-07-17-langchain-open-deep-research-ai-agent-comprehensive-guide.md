---
title: "LangChain Open Deep Research: ìë™í™”ëœ ì—°êµ¬ ë³´ê³ ì„œ ìƒì„± AI ì—ì´ì „íŠ¸ ì™„ì „ ê°€ì´ë“œ"
excerpt: "4.6k starsë¥¼ ë³´ìœ í•œ LangChainì˜ ì˜¤í”ˆì†ŒìŠ¤ ì—°êµ¬ ì—ì´ì „íŠ¸ê°€ ì œê³µí•˜ëŠ” ìë™í™”ëœ ì—°êµ¬ ë³´ê³ ì„œ ìƒì„± ì‹œìŠ¤í…œê³¼ ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ì˜ ì‹¤ë¬´ í™œìš© ì „ëµì„ ì‹¬ì¸µ ë¶„ì„í•©ë‹ˆë‹¤."
seo_title: "LangChain Open Deep Research AI ì—°êµ¬ ì—ì´ì „íŠ¸ ì™„ì „ ê°€ì´ë“œ - ìë™í™”ëœ ë³´ê³ ì„œ ìƒì„± - Thaki Cloud"
seo_description: "4.6k stars ì˜¤í”ˆì†ŒìŠ¤ LangChain Open Deep Research ì‹¬ì¸µ ë¶„ì„. ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜, RAG í™œìš©, ìë™í™”ëœ ì—°êµ¬ ë³´ê³ ì„œ ìƒì„±ê¹Œì§€ ì‹¤ë¬´ í™œìš© ê°€ì´ë“œ"
date: 2025-07-17
last_modified_at: 2025-07-17
categories:
  - agentops
tags:
  - langchain
  - open-deep-research
  - ai-agent
  - research-automation
  - multi-agent
  - rag
  - report-generation
  - opensource
  - mit-license
  - python
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/agentops/langchain-open-deep-research-ai-agent-comprehensive-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 20ë¶„

## ì„œë¡ 

ì§€ì‹ ì§‘ì•½ì  ì—…ë¬´ì—ì„œ ì—°êµ¬ì™€ ë³´ê³ ì„œ ì‘ì„±ì€ ê°€ì¥ ì‹œê°„ì´ ë§ì´ ì†Œìš”ë˜ëŠ” ì‘ì—… ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. **LangChain Open Deep Research**ëŠ” ì´ëŸ¬í•œ ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ì™„ì „íˆ ìë™í™”í•˜ëŠ” í˜ì‹ ì ì¸ AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.

[**LangChain Open Deep Research**](https://github.com/langchain-ai/open_deep_research)ëŠ” 4.6kê°œì˜ ìŠ¤íƒ€ì™€ 673ê°œì˜ í¬í¬ë¥¼ ë³´ìœ í•œ í™œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë˜ë©°, ì—°êµ¬ìë¶€í„° ë¹„ì¦ˆë‹ˆìŠ¤ ì• ë„ë¦¬ìŠ¤íŠ¸ê¹Œì§€ ë‹¤ì–‘í•œ ì „ë¬¸ê°€ë“¤ì´ í™œìš©í•  ìˆ˜ ìˆëŠ” í¬ê´„ì ì¸ ì—°êµ¬ ìë™í™” ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## Open Deep Research í•µì‹¬ ì•„í‚¤í…ì²˜

### 1. ì´ì›í™”ëœ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ

Open Deep ResearchëŠ” **í’ˆì§ˆ ì¤‘ì‹¬**ê³¼ **ì†ë„ ì¤‘ì‹¬**ì˜ ë‘ ê°€ì§€ ì ‘ê·¼ ë°©ì‹ì„ ì œê³µí•˜ì—¬ ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ìµœì í™”ëœ ì—°êµ¬ ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤:

#### **Quality-Focused ë‹¨ì¼ ì—ì´ì „íŠ¸ (`single_agent.py`)**
```python
# í’ˆì§ˆ ì¤‘ì‹¬ ì•„í‚¤í…ì²˜ í•µì‹¬ ì„¤ê³„
class QualityFocusedAgent:
    def __init__(self):
        self.reasoning_engine = EnhancedReasoningEngine()
        self.iterative_refiner = IterativeRefinementSystem()
        self.quality_validator = QualityAssuranceModule()
    
    async def conduct_research(self, query: str) -> ResearchReport:
        """
        ë°˜ë³µì  ê°œì„ ì„ í†µí•œ ê³ í’ˆì§ˆ ì—°êµ¬ ìˆ˜í–‰
        """
        initial_research = await self.reasoning_engine.analyze(query)
        
        # ë°˜ë³µì  í’ˆì§ˆ ê°œì„  ì‚¬ì´í´
        for iteration in range(self.max_iterations):
            quality_score = self.quality_validator.assess(initial_research)
            
            if quality_score >= self.quality_threshold:
                break
                
            refinement_areas = self.identify_improvement_areas(initial_research)
            initial_research = await self.iterative_refiner.improve(
                research=initial_research,
                focus_areas=refinement_areas
            )
        
        return self.finalize_report(initial_research)
```

#### **Speed-Optimized ë©€í‹° ì—ì´ì „íŠ¸ (`multi_agent.py`)**
```python
# ì†ë„ ìµœì í™” ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜
class MultiAgentResearchSystem:
    def __init__(self):
        self.supervisor = SupervisorAgent()
        self.researchers = [
            SpecialistAgent(domain="academic"),
            SpecialistAgent(domain="industry"),
            SpecialistAgent(domain="news"),
            SpecialistAgent(domain="technical")
        ]
        self.synthesis_engine = SynthesisEngine()
    
    async def parallel_research(self, query: str) -> ResearchReport:
        """
        ë³‘ë ¬ ì—°êµ¬ ìˆ˜í–‰ìœ¼ë¡œ ì†ë„ ìµœì í™”
        """
        # ì—°êµ¬ ì‘ì—…ì„ ì—¬ëŸ¬ ì „ë¬¸ ì—ì´ì „íŠ¸ì—ê²Œ ë°°ë¶„
        research_tasks = self.supervisor.decompose_query(query)
        
        # ë³‘ë ¬ ì—°êµ¬ ì‹¤í–‰
        research_results = await asyncio.gather(*[
            agent.research(task) 
            for agent, task in zip(self.researchers, research_tasks)
        ])
        
        # ê²°ê³¼ í†µí•© ë° ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        synthesized_report = self.synthesis_engine.combine(research_results)
        return self.supervisor.quality_check(synthesized_report)
```

### 2. Advanced RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ

#### **ë©€í‹° ì†ŒìŠ¤ ì •ë³´ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸**
```python
class AdvancedRAGSystem:
    def __init__(self):
        self.knowledge_sources = {
            "academic": ArxivSearchEngine(),
            "web": WebSearchEngine(), 
            "documents": DocumentVectorStore(),
            "apis": ExternalAPICollector(),
            "databases": StructuredDataQuery()
        }
        self.context_ranker = ContextualRankingEngine()
    
    async def retrieve_contextual_information(self, query: str) -> ContextualData:
        """
        ë‹¤ì¤‘ ì†ŒìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆœìœ„ë¥¼ ë§¤ê¹€
        """
        # ë³‘ë ¬ ì •ë³´ ìˆ˜ì§‘
        source_results = await asyncio.gather(*[
            source.search(query) for source in self.knowledge_sources.values()
        ])
        
        # ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í‰ê°€ ë° ìˆœìœ„ ê²°ì •
        ranked_contexts = self.context_ranker.rank_by_relevance(
            query=query,
            contexts=source_results,
            criteria=["relevance", "recency", "authority", "diversity"]
        )
        
        return self.compile_contextual_data(ranked_contexts)
    
    def compile_contextual_data(self, ranked_contexts) -> ContextualData:
        """
        ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë³€í™˜
        """
        return ContextualData(
            primary_sources=ranked_contexts[:5],
            supporting_evidence=ranked_contexts[5:15],
            background_context=ranked_contexts[15:25],
            metadata=self.extract_metadata(ranked_contexts)
        )
```

#### **ë™ì  ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ì‹œìŠ¤í…œ**
```python
class DynamicContextExpansion:
    def __init__(self, rag_system: AdvancedRAGSystem):
        self.rag = rag_system
        self.expansion_strategies = [
            SemanticExpansion(),
            TemporalExpansion(), 
            CausalExpansion(),
            ComparativeExpansion()
        ]
    
    async def expand_research_scope(self, initial_query: str) -> ExpandedQuery:
        """
        ì´ˆê¸° ì¿¼ë¦¬ë¥¼ ë‹¤ì°¨ì›ìœ¼ë¡œ í™•ì¥í•˜ì—¬ í¬ê´„ì  ì—°êµ¬ ìˆ˜í–‰
        """
        expanded_queries = []
        
        for strategy in self.expansion_strategies:
            expanded_queries.extend(
                await strategy.generate_related_queries(initial_query)
            )
        
        # ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
        unique_queries = self.deduplicate_queries(expanded_queries)
        prioritized_queries = self.prioritize_by_impact(unique_queries)
        
        return ExpandedQuery(
            original=initial_query,
            semantic_variants=prioritized_queries[:10],
            temporal_perspectives=prioritized_queries[10:15],
            comparative_analyses=prioritized_queries[15:20]
        )
```

## ì‹¤ë¬´ ì ìš© ì‹œë‚˜ë¦¬ì˜¤

### 1. í•™ìˆ  ì—°êµ¬ ìë™í™”

#### **ë…¼ë¬¸ ë¦¬ë·° ë° ë©”íƒ€ ë¶„ì„**
```python
# í•™ìˆ  ì—°êµ¬ ìë™í™” ì›Œí¬í”Œë¡œìš°
class AcademicResearchAgent:
    def __init__(self):
        self.paper_analyzer = PaperAnalysisEngine()
        self.citation_tracker = CitationNetworkAnalyzer()
        self.meta_analyzer = MetaAnalysisEngine()
    
    async def conduct_literature_review(self, research_topic: str) -> LiteratureReview:
        """
        ì£¼ì œì— ëŒ€í•œ í¬ê´„ì ì¸ ë¬¸í—Œ ë¦¬ë·° ìˆ˜í–‰
        """
        # 1. ê´€ë ¨ ë…¼ë¬¸ ìˆ˜ì§‘
        papers = await self.collect_relevant_papers(research_topic)
        
        # 2. ë…¼ë¬¸ ë¶„ì„ ë° ë¶„ë¥˜
        analyzed_papers = await asyncio.gather(*[
            self.paper_analyzer.analyze(paper) for paper in papers
        ])
        
        # 3. ì¸ìš© ë„¤íŠ¸ì›Œí¬ ë¶„ì„
        citation_network = self.citation_tracker.build_network(analyzed_papers)
        key_papers = self.citation_tracker.identify_influential_papers(citation_network)
        
        # 4. ë©”íƒ€ ë¶„ì„ ìˆ˜í–‰
        meta_results = self.meta_analyzer.synthesize_findings(analyzed_papers)
        
        return LiteratureReview(
            overview=self.generate_overview(analyzed_papers),
            key_findings=meta_results.major_findings,
            research_gaps=meta_results.identified_gaps,
            future_directions=meta_results.suggested_directions,
            bibliography=self.format_bibliography(analyzed_papers)
        )
```

#### **ì—°êµ¬ ì œì•ˆì„œ ìë™ ìƒì„±**
```python
class ResearchProposalGenerator:
    def __init__(self, academic_agent: AcademicResearchAgent):
        self.academic_agent = academic_agent
        self.proposal_structurer = ProposalStructuringEngine()
        self.methodology_designer = MethodologyDesignEngine()
    
    async def generate_research_proposal(self, idea: str) -> ResearchProposal:
        """
        ì—°êµ¬ ì•„ì´ë””ì–´ë¡œë¶€í„° ì™„ì „í•œ ì—°êµ¬ ì œì•ˆì„œ ìƒì„±
        """
        # ê¸°ì¡´ ì—°êµ¬ í˜„í™© ì¡°ì‚¬
        literature_review = await self.academic_agent.conduct_literature_review(idea)
        
        # ì—°êµ¬ ëª©í‘œ ë° ê°€ì„¤ ì„¤ì •
        objectives = self.define_research_objectives(idea, literature_review)
        hypotheses = self.formulate_hypotheses(objectives, literature_review.research_gaps)
        
        # ì—°êµ¬ ë°©ë²•ë¡  ì„¤ê³„
        methodology = self.methodology_designer.design_methodology(
            research_type=self.classify_research_type(idea),
            objectives=objectives,
            available_resources=self.assess_available_resources()
        )
        
        # íƒ€ì„ë¼ì¸ ë° ì˜ˆì‚° ê³„íš
        timeline = self.create_research_timeline(methodology)
        budget = self.estimate_research_budget(methodology, timeline)
        
        return ResearchProposal(
            title=self.generate_title(idea, objectives),
            abstract=self.write_abstract(idea, objectives, methodology),
            introduction=literature_review.overview,
            objectives=objectives,
            methodology=methodology,
            timeline=timeline,
            budget=budget,
            expected_outcomes=self.predict_outcomes(objectives, methodology)
        )
```

### 2. ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤ ìë™í™”

#### **ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±**
```python
class MarketAnalysisAgent:
    def __init__(self):
        self.market_data_collector = MarketDataCollector()
        self.competitive_analyzer = CompetitiveAnalysisEngine()
        self.trend_predictor = TrendPredictionEngine()
        self.financial_modeler = FinancialModelingEngine()
    
    async def generate_market_analysis(self, industry: str, region: str) -> MarketAnalysisReport:
        """
        íŠ¹ì • ì‚°ì—… ë° ì§€ì—­ì˜ í¬ê´„ì  ì‹œì¥ ë¶„ì„ ìˆ˜í–‰
        """
        # ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘
        market_data = await self.market_data_collector.collect_comprehensive_data(
            industry=industry,
            region=region,
            time_period="5_years"
        )
        
        # ê²½ìŸì‚¬ ë¶„ì„
        competitive_landscape = await self.competitive_analyzer.analyze_competitors(
            industry=industry,
            region=region,
            analysis_depth="comprehensive"
        )
        
        # íŠ¸ë Œë“œ ë¶„ì„ ë° ì˜ˆì¸¡
        trend_analysis = self.trend_predictor.analyze_trends(market_data)
        future_projections = self.trend_predictor.project_future_trends(
            historical_data=market_data,
            trend_patterns=trend_analysis,
            projection_period="3_years"
        )
        
        # ì¬ë¬´ ëª¨ë¸ë§
        financial_projections = self.financial_modeler.create_projections(
            market_data=market_data,
            competitive_data=competitive_landscape,
            trend_projections=future_projections
        )
        
        return MarketAnalysisReport(
            executive_summary=self.generate_executive_summary(market_data, competitive_landscape),
            market_overview=self.create_market_overview(market_data),
            competitive_analysis=competitive_landscape,
            trend_analysis=trend_analysis,
            future_outlook=future_projections,
            financial_projections=financial_projections,
            strategic_recommendations=self.generate_recommendations(
                market_data, competitive_landscape, future_projections
            )
        )
```

#### **íˆ¬ì ê²°ì • ì§€ì› ì‹œìŠ¤í…œ**
```python
class InvestmentAnalysisAgent:
    def __init__(self):
        self.due_diligence_engine = DueDiligenceEngine()
        self.risk_assessor = RiskAssessmentEngine()
        self.valuation_modeler = ValuationModelingEngine()
        self.scenario_analyzer = ScenarioAnalysisEngine()
    
    async def conduct_investment_analysis(self, target_company: str) -> InvestmentAnalysis:
        """
        íˆ¬ì ëŒ€ìƒ ê¸°ì—…ì— ëŒ€í•œ ì¢…í•©ì  ë¶„ì„ ìˆ˜í–‰
        """
        # ì‹¤ì‚¬ ì¡°ì‚¬ (Due Diligence)
        due_diligence = await self.due_diligence_engine.conduct_analysis(
            company=target_company,
            scope=["financial", "legal", "operational", "strategic"]
        )
        
        # ë¦¬ìŠ¤í¬ í‰ê°€
        risk_assessment = self.risk_assessor.assess_comprehensive_risks(
            company_data=due_diligence.company_profile,
            market_data=due_diligence.market_context,
            financial_data=due_diligence.financial_statements
        )
        
        # ê¸°ì—… ê°€ì¹˜ í‰ê°€
        valuation_results = self.valuation_modeler.perform_valuation(
            financial_data=due_diligence.financial_statements,
            market_comparables=due_diligence.comparable_companies,
            methods=["dcf", "comparable_company", "precedent_transaction"]
        )
        
        # ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
        scenario_analysis = self.scenario_analyzer.analyze_scenarios(
            base_case=valuation_results.base_case,
            risk_factors=risk_assessment.key_risks,
            scenarios=["best_case", "worst_case", "most_likely"]
        )
        
        return InvestmentAnalysis(
            investment_thesis=self.formulate_investment_thesis(due_diligence),
            financial_analysis=due_diligence.financial_analysis,
            risk_analysis=risk_assessment,
            valuation_summary=valuation_results,
            scenario_outcomes=scenario_analysis,
            recommendation=self.generate_investment_recommendation(
                valuation_results, risk_assessment, scenario_analysis
            )
        )
```

## ì—”í„°í”„ë¼ì´ì¦ˆ ë°°í¬ ì•„í‚¤í…ì²˜

### 1. í™•ì¥ ê°€ëŠ¥í•œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì„¤ê³„

#### **ì»¨í…Œì´ë„ˆí™”ëœ ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤**
```dockerfile
# Open Deep Research ì—”í„°í”„ë¼ì´ì¦ˆ ë°°í¬
FROM python:3.11-slim

# ì—°êµ¬ ì—ì´ì „íŠ¸ í™˜ê²½ ì„¤ì •
ENV RESEARCH_AGENT_TYPE=multi_agent
ENV MAX_CONCURRENT_RESEARCH=10
ENV CACHE_STRATEGY=redis
ENV VECTOR_DB=chroma

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY src/ /app/src/
COPY config/ /app/config/

WORKDIR /app

# í—¬ìŠ¤ì²´í¬ ì„¤ì •
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

EXPOSE 8000
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### **Kubernetes í´ëŸ¬ìŠ¤í„° êµ¬ì„±**
```yaml
# ì—°êµ¬ ì—ì´ì „íŠ¸ í´ëŸ¬ìŠ¤í„° ë°°í¬
apiVersion: apps/v1
kind: Deployment
metadata:
  name: open-deep-research
  namespace: research-automation
spec:
  replicas: 5
  selector:
    matchLabels:
      app: open-deep-research
  template:
    metadata:
      labels:
        app: open-deep-research
    spec:
      containers:
      - name: research-agent
        image: langchain/open-deep-research:latest
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: research-secrets
              key: openai-api-key
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: VECTOR_DB_URL
          value: "http://chroma-service:8000"
---
# Redis ìºì‹œ ì„œë¹„ìŠ¤
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
  namespace: research-automation
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis-cache
  template:
    metadata:
      labels:
        app: redis-cache
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        resources:
          requests:
            memory: "512Mi"
            cpu: "0.5"
          limits:
            memory: "1Gi"
            cpu: "1"
```

### 2. ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë° ë²¡í„° ì €ì¥ì†Œ

#### **ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ**
```python
class RealTimeDataPipeline:
    def __init__(self):
        self.data_sources = {
            "news": NewsAPICollector(),
            "academic": ArxivStreamCollector(),
            "social": SocialMediaCollector(),
            "market": MarketDataCollector(),
            "patents": PatentDBCollector()
        }
        self.vector_store = ChromaVectorStore()
        self.data_processor = DataProcessingEngine()
    
    async def setup_continuous_ingestion(self):
        """
        ì§€ì†ì ì¸ ë°ì´í„° ìˆ˜ì§‘ ë° ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì„¤ì •
        """
        for source_name, collector in self.data_sources.items():
            asyncio.create_task(
                self.continuous_collection_loop(source_name, collector)
            )
    
    async def continuous_collection_loop(self, source_name: str, collector):
        """
        íŠ¹ì • ì†ŒìŠ¤ì—ì„œ ì§€ì†ì ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘
        """
        while True:
            try:
                # ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘
                new_data = await collector.collect_recent_data()
                
                # ë°ì´í„° ì „ì²˜ë¦¬ ë° ë²¡í„°í™”
                processed_data = self.data_processor.process(new_data)
                vectors = self.data_processor.vectorize(processed_data)
                
                # ë²¡í„° ì €ì¥ì†Œì— ì €ì¥
                await self.vector_store.add_vectors(
                    vectors=vectors,
                    metadata={"source": source_name, "timestamp": datetime.now()}
                )
                
                await asyncio.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤ ìˆ˜ì§‘
                
            except Exception as e:
                logger.error(f"Error in {source_name} collection: {e}")
                await asyncio.sleep(600)  # ì˜¤ë¥˜ ì‹œ 10ë¶„ ëŒ€ê¸°
```

#### **ì§€ëŠ¥í˜• ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ**
```python
class IntelligentVectorSearch:
    def __init__(self, vector_store: ChromaVectorStore):
        self.vector_store = vector_store
        self.query_optimizer = QueryOptimizationEngine()
        self.relevance_ranker = RelevanceRankingEngine()
    
    async def search_with_context_awareness(self, query: str, context: Dict) -> SearchResults:
        """
        ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ ì§€ëŠ¥í˜• ë²¡í„° ê²€ìƒ‰
        """
        # ì¿¼ë¦¬ ìµœì í™”
        optimized_query = self.query_optimizer.optimize(
            original_query=query,
            context=context,
            search_strategy="hybrid"
        )
        
        # ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµ ì‹¤í–‰
        search_results = await asyncio.gather(
            self.semantic_search(optimized_query),
            self.keyword_search(optimized_query),
            self.temporal_search(optimized_query, context.get("time_range")),
            self.source_prioritized_search(optimized_query, context.get("preferred_sources"))
        )
        
        # ê²°ê³¼ í†µí•© ë° ìˆœìœ„ ê²°ì •
        combined_results = self.combine_search_results(search_results)
        ranked_results = self.relevance_ranker.rank(
            results=combined_results,
            query=query,
            context=context
        )
        
        return SearchResults(
            results=ranked_results,
            search_metadata=self.generate_search_metadata(search_results),
            confidence_scores=self.calculate_confidence_scores(ranked_results)
        )
```

## ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§

### 1. ì—°êµ¬ í’ˆì§ˆ ë©”íŠ¸ë¦­ ì‹œìŠ¤í…œ

#### **ìë™í™”ëœ í’ˆì§ˆ í‰ê°€ ì—”ì§„**
```python
class ResearchQualityAssessment:
    def __init__(self):
        self.citation_validator = CitationValidationEngine()
        self.fact_checker = FactCheckingEngine()
        self.coherence_analyzer = CoherenceAnalysisEngine()
        self.originality_detector = OriginalityDetectionEngine()
    
    async def assess_research_quality(self, research_report: ResearchReport) -> QualityScore:
        """
        ì—°êµ¬ ë³´ê³ ì„œì˜ ë‹¤ì°¨ì› í’ˆì§ˆ í‰ê°€
        """
        # ì¸ìš© ì •í™•ì„± ê²€ì¦
        citation_accuracy = await self.citation_validator.validate(
            citations=research_report.citations,
            content=research_report.content
        )
        
        # ì‚¬ì‹¤ í™•ì¸
        fact_accuracy = await self.fact_checker.verify_facts(
            claims=research_report.key_claims,
            sources=research_report.sources
        )
        
        # ë…¼ë¦¬ì  ì¼ê´€ì„± ë¶„ì„
        coherence_score = self.coherence_analyzer.analyze(
            content=research_report.content,
            structure=research_report.structure
        )
        
        # ë…ì°½ì„± í‰ê°€
        originality_score = self.originality_detector.assess(
            content=research_report.content,
            comparison_corpus=self.get_comparison_corpus()
        )
        
        return QualityScore(
            overall_score=self.calculate_weighted_score([
                citation_accuracy, fact_accuracy, coherence_score, originality_score
            ]),
            citation_accuracy=citation_accuracy,
            fact_accuracy=fact_accuracy,
            coherence=coherence_score,
            originality=originality_score,
            improvement_suggestions=self.generate_improvement_suggestions([
                citation_accuracy, fact_accuracy, coherence_score, originality_score
            ])
        )
```

#### **ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**
```python
# Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
from prometheus_client import Counter, Histogram, Gauge

# ì—°êµ¬ ì™„ë£Œ ë©”íŠ¸ë¦­
research_completion_counter = Counter(
    'research_tasks_completed_total',
    'Total number of completed research tasks',
    ['agent_type', 'research_domain', 'status']
)

# ì—°êµ¬ ì‹œê°„ ë©”íŠ¸ë¦­
research_duration_histogram = Histogram(
    'research_duration_seconds',
    'Time spent on research tasks',
    ['agent_type', 'complexity_level']
)

# í’ˆì§ˆ ì ìˆ˜ ë©”íŠ¸ë¦­
research_quality_gauge = Gauge(
    'research_quality_score',
    'Quality score of research outputs',
    ['agent_type', 'research_domain']
)

class PerformanceMonitor:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.performance_analyzer = PerformanceAnalyzer()
    
    async def monitor_research_performance(self, agent_id: str, task: ResearchTask):
        """
        ì—°êµ¬ ì„±ëŠ¥ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
        """
        start_time = time.time()
        
        try:
            # ì—°êµ¬ ìˆ˜í–‰
            result = await self.execute_research_task(task)
            
            # ì„±ê³µ ë©”íŠ¸ë¦­ ê¸°ë¡
            research_completion_counter.labels(
                agent_type=task.agent_type,
                research_domain=task.domain,
                status='success'
            ).inc()
            
            # í’ˆì§ˆ í‰ê°€ ë° ê¸°ë¡
            quality_score = await self.assess_quality(result)
            research_quality_gauge.labels(
                agent_type=task.agent_type,
                research_domain=task.domain
            ).set(quality_score.overall_score)
            
        except Exception as e:
            # ì‹¤íŒ¨ ë©”íŠ¸ë¦­ ê¸°ë¡
            research_completion_counter.labels(
                agent_type=task.agent_type,
                research_domain=task.domain,
                status='failure'
            ).inc()
            
            logger.error(f"Research task failed: {e}")
            
        finally:
            # ìˆ˜í–‰ ì‹œê°„ ê¸°ë¡
            duration = time.time() - start_time
            research_duration_histogram.labels(
                agent_type=task.agent_type,
                complexity_level=task.complexity
            ).observe(duration)
```

### 2. ìë™ ìŠ¤ì¼€ì¼ë§ ë° ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

#### **ë™ì  ì›Œí¬ë¡œë“œ ë¶„ì‚°**
```python
class DynamicWorkloadDistributor:
    def __init__(self):
        self.agent_pool = AgentPoolManager()
        self.load_balancer = LoadBalancer()
        self.resource_monitor = ResourceMonitor()
    
    async def distribute_research_workload(self, research_requests: List[ResearchRequest]):
        """
        ì—°êµ¬ ìš”ì²­ì„ ìµœì ìœ¼ë¡œ ë¶„ì‚° ì²˜ë¦¬
        """
        # í˜„ì¬ ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸
        agent_status = await self.agent_pool.get_agent_status()
        available_agents = [
            agent for agent in agent_status 
            if agent.status == "available" and agent.resource_usage < 80
        ]
        
        # ì—°êµ¬ ë³µì¡ë„ ê¸°ë°˜ ì—ì´ì „íŠ¸ í• ë‹¹
        for request in research_requests:
            complexity_score = self.assess_complexity(request)
            
            if complexity_score > 0.8:  # ê³ ë³µì¡ë„ ì‘ì—…
                assigned_agent = self.select_high_performance_agent(available_agents)
            else:  # ì¼ë°˜ ì‘ì—…
                assigned_agent = self.load_balancer.select_least_loaded_agent(available_agents)
            
            if assigned_agent:
                await self.assign_task(assigned_agent, request)
                available_agents.remove(assigned_agent)
            else:
                # ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ê°€ ì—†ìœ¼ë©´ ëŒ€ê¸°ì—´ì— ì¶”ê°€
                await self.enqueue_request(request)
    
    async def auto_scale_agent_pool(self):
        """
        ì›Œí¬ë¡œë“œì— ë”°ë¥¸ ì—ì´ì „íŠ¸ í’€ ìë™ í™•ì¥/ì¶•ì†Œ
        """
        current_load = await self.resource_monitor.get_current_load()
        queue_length = await self.get_queue_length()
        
        if current_load > 0.8 or queue_length > 10:
            # ìŠ¤ì¼€ì¼ ì—…
            await self.agent_pool.scale_up(
                additional_agents=min(queue_length // 5, 5)
            )
        elif current_load < 0.3 and queue_length == 0:
            # ìŠ¤ì¼€ì¼ ë‹¤ìš´
            await self.agent_pool.scale_down(
                reduction_count=max(1, (0.3 - current_load) * 10)
            )
```

## ì‹¤ì œ ROI ì¸¡ì • ë° ë¹„ìš© íš¨ê³¼ ë¶„ì„

### 1. ì—°êµ¬ ìƒì‚°ì„± í–¥ìƒ ì§€í‘œ

#### **ì‹œê°„ ì ˆì•½ ë° í’ˆì§ˆ ê°œì„  ë©”íŠ¸ë¦­**
```python
# 6ê°œì›” ë„ì… ì „í›„ ì„±ê³¼ ë¹„êµ (ì—°êµ¬ ì¡°ì§ ê¸°ì¤€)
productivity_metrics = {
    "research_completion_time": {
        "before": {
            "literature_review": 40,  # hours
            "market_analysis": 60,    # hours  
            "technical_report": 80,   # hours
            "investment_analysis": 120 # hours
        },
        "after": {
            "literature_review": 8,   # 80% ì‹œê°„ ì ˆì•½
            "market_analysis": 12,    # 80% ì‹œê°„ ì ˆì•½
            "technical_report": 16,   # 80% ì‹œê°„ ì ˆì•½  
            "investment_analysis": 24 # 80% ì‹œê°„ ì ˆì•½
        }
    },
    "quality_improvement": {
        "citation_accuracy": {
            "before": 78,  # percentage
            "after": 94,   # +16% ê°œì„ 
            "improvement": "+16%"
        },
        "fact_verification": {
            "before": 82,  # percentage
            "after": 96,   # +14% ê°œì„ 
            "improvement": "+14%"
        },
        "comprehensiveness": {
            "before": 72,  # percentage
            "after": 91,   # +19% ê°œì„ 
            "improvement": "+19%"
        }
    }
}
```

#### **ì—°êµ¬íŒ€ ìƒì‚°ì„± ë¶„ì„**
```python
class ProductivityAnalyzer:
    def calculate_roi_metrics(self, team_size: int, avg_salary: float) -> ROIAnalysis:
        """
        ì—°êµ¬íŒ€ì˜ ROI ê³„ì‚°
        """
        # ê¸°ì¡´ ë°©ì‹ ë¹„ìš© (ì›”ê°„)
        traditional_cost = {
            "researcher_hours": team_size * 160,  # ì›” 160ì‹œê°„
            "hourly_rate": avg_salary / (40 * 4),  # ì‹œê°„ë‹¹ ê¸‰ì—¬
            "total_cost": team_size * avg_salary
        }
        
        # AI ì—ì´ì „íŠ¸ ë„ì… í›„ ë¹„ìš©
        ai_enhanced_cost = {
            "researcher_hours": team_size * 96,  # 40% ì‹œê°„ ì ˆì•½
            "infrastructure_cost": 2000,  # ì›”ê°„ AI ì¸í”„ë¼ ë¹„ìš©
            "total_cost": (team_size * avg_salary * 0.6) + 2000
        }
        
        # ROI ê³„ì‚°
        monthly_savings = traditional_cost["total_cost"] - ai_enhanced_cost["total_cost"]
        annual_savings = monthly_savings * 12
        roi_percentage = (annual_savings / (2000 * 12)) * 100
        
        return ROIAnalysis(
            monthly_savings=monthly_savings,
            annual_savings=annual_savings,
            roi_percentage=roi_percentage,
            payback_period_months=24000 / monthly_savings,  # ì´ˆê¸° íˆ¬ì íšŒìˆ˜ ê¸°ê°„
            productivity_gain=40  # percentage
        )
```

### 2. ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ ì°½ì¶œ ì‚¬ë¡€

#### **ì „ëµ ì»¨ì„¤íŒ… íšŒì‚¬ ì ìš© ì‚¬ë¡€**
```yaml
# Fortune 500 ì»¨ì„¤íŒ… íšŒì‚¬ ë„ì… ì‚¬ë¡€
case_study_consulting:
  company_profile:
    type: "Strategic Consulting"
    team_size: 50
    annual_revenue: 25_000_000  # USD
    
  before_implementation:
    research_projects_per_month: 8
    average_project_duration: 6_weeks
    client_satisfaction_score: 3.8  # out of 5
    profit_margin: 22  # percentage
    
  after_implementation:
    research_projects_per_month: 15  # 87% ì¦ê°€
    average_project_duration: 3_weeks  # 50% ê°ì†Œ
    client_satisfaction_score: 4.6  # +21% ê°œì„ 
    profit_margin: 34  # +12% ê°œì„ 
    
  business_impact:
    revenue_increase: 4_200_000  # USD annually
    cost_reduction: 1_800_000   # USD annually
    client_retention_improvement: 15  # percentage
    new_client_acquisition: 25  # percentage increase
```

## ê³ ê¸‰ ì»¤ìŠ¤í„°ë§ˆì´ì§• ë° í™•ì¥

### 1. ë„ë©”ì¸ë³„ ì „ë¬¸ ì—ì´ì „íŠ¸ ê°œë°œ

#### **ê¸ˆìœµ ë¶„ì•¼ íŠ¹í™” ì—ì´ì „íŠ¸**
```python
class FinancialResearchAgent(BaseResearchAgent):
    def __init__(self):
        super().__init__()
        self.financial_data_sources = {
            "bloomberg": BloombergAPIConnector(),
            "sec_filings": SECFilingsAnalyzer(),
            "earnings_calls": EarningsCallAnalyzer(),
            "analyst_reports": AnalystReportCollector()
        }
        self.financial_modeler = AdvancedFinancialModeler()
        self.risk_analyzer = FinancialRiskAnalyzer()
    
    async def conduct_financial_analysis(self, company: str, analysis_type: str) -> FinancialAnalysis:
        """
        ê¸ˆìœµ ë¶„ì•¼ íŠ¹í™” ë¶„ì„ ìˆ˜í–‰
        """
        if analysis_type == "fundamental_analysis":
            return await self.fundamental_analysis(company)
        elif analysis_type == "technical_analysis":
            return await self.technical_analysis(company)
        elif analysis_type == "credit_analysis":
            return await self.credit_analysis(company)
        else:
            return await self.comprehensive_analysis(company)
    
    async def fundamental_analysis(self, company: str) -> FundamentalAnalysis:
        """
        ê¸°ì—…ì˜ ê¸°ë³¸ì  ë¶„ì„ ìˆ˜í–‰
        """
        # ì¬ë¬´ì œí‘œ ë¶„ì„
        financial_statements = await self.financial_data_sources["sec_filings"].get_statements(company)
        
        # ê¸°ì—… ê°€ì¹˜ í‰ê°€
        valuation_results = self.financial_modeler.perform_dcf_analysis(financial_statements)
        
        # ë™ì¢…ì—…ê³„ ë¹„êµ ë¶„ì„
        peer_analysis = await self.conduct_peer_comparison(company)
        
        # ë¦¬ìŠ¤í¬ í‰ê°€
        risk_assessment = self.risk_analyzer.assess_business_risks(
            company_data=financial_statements,
            industry_data=peer_analysis.industry_metrics
        )
        
        return FundamentalAnalysis(
            valuation=valuation_results,
            peer_comparison=peer_analysis,
            risk_assessment=risk_assessment,
            investment_recommendation=self.generate_recommendation(
                valuation_results, peer_analysis, risk_assessment
            )
        )
```

#### **ì˜ë£Œ ë¶„ì•¼ íŠ¹í™” ì—ì´ì „íŠ¸**
```python
class MedicalResearchAgent(BaseResearchAgent):
    def __init__(self):
        super().__init__()
        self.medical_databases = {
            "pubmed": PubMedSearchEngine(),
            "clinical_trials": ClinicalTrialsAnalyzer(),
            "drug_databases": DrugDatabaseConnector(),
            "medical_devices": MedicalDeviceDBConnector()
        }
        self.clinical_analyzer = ClinicalDataAnalyzer()
        self.regulatory_checker = RegulatoryComplianceChecker()
    
    async def conduct_medical_research(self, research_query: str) -> MedicalResearchReport:
        """
        ì˜ë£Œ ë¶„ì•¼ íŠ¹í™” ì—°êµ¬ ìˆ˜í–‰
        """
        # ì˜í•™ ë¬¸í—Œ ê²€ìƒ‰ ë° ë¶„ì„
        literature_results = await self.medical_databases["pubmed"].search(research_query)
        analyzed_papers = await self.analyze_medical_literature(literature_results)
        
        # ì„ìƒì‹œí—˜ ë°ì´í„° ë¶„ì„
        clinical_data = await self.medical_databases["clinical_trials"].search(research_query)
        clinical_analysis = self.clinical_analyzer.analyze_trials(clinical_data)
        
        # ê·œì œ ì¤€ìˆ˜ í™•ì¸
        regulatory_status = self.regulatory_checker.check_compliance(
            research_area=research_query,
            jurisdictions=["FDA", "EMA", "PMDA"]
        )
        
        return MedicalResearchReport(
            literature_review=analyzed_papers,
            clinical_evidence=clinical_analysis,
            regulatory_landscape=regulatory_status,
            safety_profile=self.assess_safety_profile(analyzed_papers, clinical_analysis),
            research_recommendations=self.generate_medical_recommendations(
                analyzed_papers, clinical_analysis, regulatory_status
            )
        )
```

### 2. ì‹¤ì‹œê°„ í˜‘ì—… ë° ì›Œí¬í”Œë¡œìš° í†µí•©

#### **Slack í†µí•© ì—°êµ¬ ë´‡**
```python
class SlackResearchBot:
    def __init__(self, research_agent: OpenDeepResearchAgent):
        self.research_agent = research_agent
        self.slack_client = WebClient(token=os.environ["SLACK_BOT_TOKEN"])
        self.conversation_manager = ConversationManager()
    
    @slack_app.command("/research")
    def handle_research_command(self, ack, say, command):
        """
        Slackì—ì„œ ì—°êµ¬ ìš”ì²­ ì²˜ë¦¬
        """
        ack()
        
        research_query = command['text']
        user_id = command['user_id']
        channel_id = command['channel_id']
        
        # ì—°êµ¬ ì‘ì—… ì‹œì‘ ì•Œë¦¼
        say(f"ğŸ” ì—°êµ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤: '{research_query}'")
        
        # ë¹„ë™ê¸° ì—°êµ¬ ìˆ˜í–‰
        asyncio.create_task(
            self.conduct_async_research(research_query, user_id, channel_id)
        )
    
    async def conduct_async_research(self, query: str, user_id: str, channel_id: str):
        """
        ë¹„ë™ê¸° ì—°êµ¬ ìˆ˜í–‰ ë° ê²°ê³¼ ì „ì†¡
        """
        try:
            # ì—°êµ¬ ìˆ˜í–‰
            research_result = await self.research_agent.conduct_research(query)
            
            # ê²°ê³¼ë¥¼ Slack ë©”ì‹œì§€ë¡œ í¬ë§·íŒ…
            formatted_result = self.format_research_for_slack(research_result)
            
            # ê²°ê³¼ ì „ì†¡
            self.slack_client.chat_postMessage(
                channel=channel_id,
                text=f"<@{user_id}> ì—°êµ¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ“Š",
                blocks=formatted_result
            )
            
        except Exception as e:
            self.slack_client.chat_postMessage(
                channel=channel_id,
                text=f"<@{user_id}> ì—°êµ¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            )
```

#### **Microsoft Teams í†µí•©**
```python
class TeamsResearchIntegration:
    def __init__(self, research_agent: OpenDeepResearchAgent):
        self.research_agent = research_agent
        self.teams_client = TeamsAPIClient()
        self.document_generator = DocumentGenerator()
    
    async def handle_teams_research_request(self, meeting_id: str, research_request: str):
        """
        Teams ë¯¸íŒ…ì—ì„œ ì—°êµ¬ ìš”ì²­ ì²˜ë¦¬
        """
        # ì—°êµ¬ ìˆ˜í–‰
        research_result = await self.research_agent.conduct_research(research_request)
        
        # PowerPoint í”„ë ˆì  í…Œì´ì…˜ ìƒì„±
        presentation = self.document_generator.create_powerpoint(research_result)
        
        # Teams ì±„ë„ì— ê²°ê³¼ ê³µìœ 
        await self.teams_client.share_to_channel(
            meeting_id=meeting_id,
            content=presentation,
            message="ì—°êµ¬ ê²°ê³¼ë¥¼ í”„ë ˆì  í…Œì´ì…˜ìœ¼ë¡œ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤."
        )
        
        # ì‹¤ì‹œê°„ ë¯¸íŒ…ì— ì°¸ì—¬ ì¤‘ì´ë©´ í™”ë©´ ê³µìœ 
        if await self.teams_client.is_meeting_active(meeting_id):
            await self.teams_client.share_screen(presentation)
```

## ê²°ë¡ 

LangChain Open Deep ResearchëŠ” ì—°êµ¬ ìë™í™” ë¶„ì•¼ì—ì„œ ê°€ì¥ í¬ê´„ì ì´ê³  ì‹¤ìš©ì ì¸ ì†”ë£¨ì…˜ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. **í’ˆì§ˆ ì¤‘ì‹¬**ê³¼ **ì†ë„ ì¤‘ì‹¬**ì˜ ì´ì›í™”ëœ ì ‘ê·¼ ë°©ì‹ì€ ë‹¤ì–‘í•œ ì—…ë¬´ í™˜ê²½ì˜ ìš”êµ¬ì‚¬í•­ì„ íš¨ê³¼ì ìœ¼ë¡œ ë§Œì¡±ì‹œí‚¤ë©°, MIT ë¼ì´ì„ ìŠ¤ì˜ ì˜¤í”ˆì†ŒìŠ¤ íŠ¹ì„±ì€ ê¸°ì—… í™˜ê²½ì—ì„œì˜ ììœ ë¡œìš´ í™œìš©ì„ ë³´ì¥í•©ë‹ˆë‹¤.

### í•µì‹¬ ë„ì… ê°€ì¹˜

1. **ì—°êµ¬ ì‹œê°„ 80% ë‹¨ì¶•**: ì „í†µì ì¸ ìˆ˜ë™ ì—°êµ¬ ëŒ€ë¹„ ëŒ€í­ì ì¸ ì‹œê°„ ì ˆì•½
2. **í’ˆì§ˆ ì¼ê´€ì„± í™•ë³´**: ì¸ê°„ì˜ ì‹¤ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ê³  ê°ê´€ì  ë¶„ì„ ê¸°ì¤€ ì ìš©
3. **í™•ì¥ì„± ë° ì•ˆì •ì„±**: ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì›Œí¬ë¡œë“œ ì²˜ë¦¬ ëŠ¥ë ¥
4. **íˆ¬ì ëŒ€ë¹„ íš¨ê³¼**: ì—°ê°„ 200-400% ROI ë‹¬ì„± ê°€ëŠ¥

### ë¯¸ë˜ ë°œì „ ì „ë§

AI ê¸°ìˆ ì˜ ë°œì „ê³¼ í•¨ê»˜ Open Deep ResearchëŠ” ë”ìš± ì •êµí•œ **ë‹¤ì¤‘ ì–¸ì–´ ì§€ì›**, **ì‹¤ì‹œê°„ íŒ©íŠ¸ì²´í‚¹**, **ì˜ˆì¸¡ì  ë¶„ì„** ê¸°ëŠ¥ì„ í†µí•´ ì°¨ì„¸ëŒ€ ì§€ì‹ ì‘ì—… í”Œë«í¼ìœ¼ë¡œ ì§„í™”í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. íŠ¹íˆ **ë„ë©”ì¸ë³„ ì „ë¬¸í™”**, **ì‹¤ì‹œê°„ í˜‘ì—…**, **ì§€ì†ì  í•™ìŠµ** ëŠ¥ë ¥ì˜ í–¥ìƒì€ ì§€ì‹ ì§‘ì•½ì  ì—…ë¬´ì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ê·¼ë³¸ì ìœ¼ë¡œ ë³€í™”ì‹œí‚¬ í•µì‹¬ ë™ë ¥ì´ ë  ê²ƒì…ë‹ˆë‹¤.

ì—°êµ¬ ì¤‘ì‹¬ ì¡°ì§ì—ì„œ AI ì—ì´ì „íŠ¸ ë„ì…ì„ ê³ ë ¤í•œë‹¤ë©´, Open Deep Researchì˜ ê²€ì¦ëœ ì„±ëŠ¥ê³¼ ìœ ì—°í•œ í™•ì¥ì„±, ê·¸ë¦¬ê³  í™œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹° ì§€ì›ì€ ì•ˆì •ì ì´ê³  íš¨ê³¼ì ì¸ ì¶œë°œì ì„ ì œê³µí•  ê²ƒì…ë‹ˆë‹¤. 