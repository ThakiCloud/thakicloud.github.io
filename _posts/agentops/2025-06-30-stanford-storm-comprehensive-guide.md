---
title: "Stanford STORM: LLM ê¸°ë°˜ ì§€ì‹ íë ˆì´ì…˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì™„ë²½ ê°€ì´ë“œ"
excerpt: "Stanfordì—ì„œ ê°œë°œí•œ STORMê³¼ Co-STORM ì™„ë²½ ë¶„ì„. ìë™ ë¦¬ì„œì¹˜, ë³´ê³ ì„œ ìƒì„±, ì¸ìš© ì‹œìŠ¤í…œ, í˜‘ì—… AI ì—ì´ì „íŠ¸ê¹Œì§€ ì‹¤ì „ í™œìš© ê°€ì´ë“œ."
seo_title: "Stanford STORM ì™„ë²½ ê°€ì´ë“œ - LLM ì§€ì‹ íë ˆì´ì…˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ - Thaki Cloud"
seo_description: "Stanford STORM í”„ë¡œì íŠ¸ ì™„ë²½ ë¶„ì„. ìë™ ë¦¬ì„œì¹˜, Wikipedia ìŠ¤íƒ€ì¼ ë³´ê³ ì„œ ìƒì„±, í˜‘ì—… AI ì—ì´ì „íŠ¸, ì‹¤ì „ ë°°í¬ ê°€ì´ë“œ. 25.4k ìŠ¤íƒ€ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ í™œìš©ë²•."
date: 2025-06-30
last_modified_at: 2025-06-30
categories:
  - agentops
tags:
  - stanford-storm
  - knowledge-curation
  - llm-agents
  - report-generation
  - collaborative-ai
  - retrieval-augmented-generation
  - agentic-rag
  - research-automation
  - wikipedia-generation
  - multi-agent-system
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
header:
  teaser: "/assets/images/thumbnails/post-thumbnail.jpg"
  overlay_image: "/assets/images/headers/post-header.jpg"
  overlay_filter: 0.5
canonical_url: "https://thakicloud.github.io/agentops/stanford-storm-comprehensive-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 10ë¶„

## ì„œë¡ 

Stanfordì—ì„œ ê°œë°œí•œ **STORM(Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking)**ì€ LLM ê¸°ë°˜ ì§€ì‹ íë ˆì´ì…˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ğŸŒŸ **25.4k GitHub ìŠ¤íƒ€**ë¥¼ ë°›ì€ ì´ í”„ë¡œì íŠ¸ëŠ” ì£¼ì œë¥¼ ìë™ìœ¼ë¡œ ì—°êµ¬í•˜ê³  ì¸ìš©ê³¼ í•¨ê»˜ ì „ì²´ ê¸¸ì´ì˜ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” í˜ì‹ ì ì¸ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**STORM**ì€ ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ìƒì„±ê¸°ê°€ ì•„ë‹™ë‹ˆë‹¤. ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì§ˆë¬¸ì„ ì œê¸°í•˜ê³ , ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ë©°, ì²´ê³„ì ìœ¼ë¡œ ì¡°ì§í™”í•˜ì—¬ Wikipedia ìˆ˜ì¤€ì˜ ê³ í’ˆì§ˆ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” **ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œëŠ” STORMê³¼ Co-STORMì˜ í•µì‹¬ ê¸°ëŠ¥, ì‹¤ì „ í™œìš©ë²•, ê·¸ë¦¬ê³  ì»¤ìŠ¤í„°ë§ˆì´ì§• ë°©ë²•ê¹Œì§€ ì™„ë²½ ì •ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

## STORM ì‹œìŠ¤í…œ ê°œìš”

### í•µì‹¬ íŠ¹ì§•

**STORM**ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜ì‹ ì  íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:

- **ìë™ ë¦¬ì„œì¹˜**: ì£¼ì œì— ëŒ€í•´ ë‹¤ê°ë„ë¡œ ì •ë³´ ìˆ˜ì§‘
- **ì¸ìš© ì‹œìŠ¤í…œ**: ëª¨ë“  ë‚´ìš©ì— ëŒ€í•œ ì¶œì²˜ ì œê³µ
- **ì²´ê³„ì  êµ¬ì¡°**: Wikipedia ìŠ¤íƒ€ì¼ì˜ ê³„ì¸µì  ì•„ì›ƒë¼ì¸
- **ë‹¤ì¤‘ ê´€ì **: ì—¬ëŸ¬ ì „ë¬¸ê°€ ê´€ì ì—ì„œ ì ‘ê·¼
- **í˜‘ì—… ëª¨ë“œ**: Co-STORMìœ¼ë¡œ ì¸ê°„-AI í˜‘ì—… ì§€ì›
- **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ë‹¤ì–‘í•œ ë„ë©”ì¸ì— ì ìš© ê°€ëŠ¥

### STORM vs Co-STORM

```mermaid
graph TD
    A["STORM ìƒíƒœê³„"] --> B["STORM<br/>ìë™í™” ì‹œìŠ¤í…œ"]
    A --> C["Co-STORM<br/>í˜‘ì—… ì‹œìŠ¤í…œ"]
    
    B --> D["ì™„ì „ ìë™<br/>ë³´ê³ ì„œ ìƒì„±"]
    B --> E["Wikipedia ìŠ¤íƒ€ì¼<br/>ì•„ì›ƒë¼ì¸"]
    B --> F["ë‹¤ì¤‘ ê´€ì <br/>ì •ë³´ ìˆ˜ì§‘"]
    
    C --> G["ì¸ê°„-AI í˜‘ì—…"]
    C --> H["ì‹¤ì‹œê°„ ëŒ€í™”"]
    C --> I["ì‚¬ìš©ì ì°¸ì—¬<br/>ì§€ì‹ íë ˆì´ì…˜"]
```

## STORM ì•„í‚¤í…ì²˜ ì‹¬í™” ë¶„ì„

### 4ê°œ í•µì‹¬ ëª¨ë“ˆ

**STORM**ì€ ë‹¤ìŒ 4ê°œ ëª¨ë“ˆë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

#### 1. Knowledge Curation Module (ì§€ì‹ íë ˆì´ì…˜)
```python
# ì§€ì‹ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤
class KnowledgeCurationModule:
    def __init__(self, retriever, llm):
        self.retriever = retriever  # Bing, Google ë“±
        self.llm = llm
    
    def collect_information(self, topic):
        """ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ ìˆ˜ì§‘"""
        perspectives = self.generate_perspectives(topic)
        collected_info = []
        
        for perspective in perspectives:
            queries = self.generate_queries(topic, perspective)
            for query in queries:
                results = self.retriever.search(query)
                collected_info.extend(results)
        
        return self.deduplicate_and_filter(collected_info)
```

#### 2. Outline Generation Module (ì•„ì›ƒë¼ì¸ ìƒì„±)
```python
# ê³„ì¸µì  ì•„ì›ƒë¼ì¸ ìƒì„±
class OutlineGenerationModule:
    def generate_outline(self, collected_info, topic):
        """ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì§í™”"""
        key_concepts = self.extract_key_concepts(collected_info)
        hierarchy = self.build_hierarchy(key_concepts)
        
        outline = {
            "title": topic,
            "sections": []
        }
        
        for section in hierarchy:
            outline["sections"].append({
                "title": section["title"],
                "subsections": section["subsections"],
                "key_points": section["key_points"]
            })
        
        return outline
```

#### 3. Article Generation Module (ê¸°ì‚¬ ìƒì„±)
```python
# ì•„ì›ƒë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì‚¬ ì‘ì„±
class ArticleGenerationModule:
    def populate_outline(self, outline, knowledge_base):
        """ì•„ì›ƒë¼ì¸ì„ ì‹¤ì œ ë‚´ìš©ìœ¼ë¡œ ì±„ìš°ê¸°"""
        article = {
            "title": outline["title"],
            "content": []
        }
        
        for section in outline["sections"]:
            section_content = self.write_section(
                section, 
                knowledge_base,
                citation_style="wikipedia"
            )
            article["content"].append(section_content)
        
        return article
```

#### 4. Article Polishing Module (ê¸°ì‚¬ ë‹¤ë“¬ê¸°)
```python
# ìµœì¢… ê¸°ì‚¬ ê°œì„ 
class ArticlePolishingModule:
    def polish_article(self, article):
        """ê¸°ì‚¬ì˜ í’ˆì§ˆê³¼ ì¼ê´€ì„± í–¥ìƒ"""
        polished = {
            "title": article["title"],
            "content": []
        }
        
        for section in article["content"]:
            # ë¬¸ì²´ í†µì¼, ì¤‘ë³µ ì œê±°, ì¸ìš© ì •ë¦¬
            polished_section = self.improve_writing_quality(section)
            polished_section = self.verify_citations(polished_section)
            polished["content"].append(polished_section)
        
        return polished
```

## ì‹¤ì „ ì„¤ì¹˜ ë° ì‚¬ìš©ë²•

### 1. ì„¤ì¹˜ ê³¼ì •

```bash
# pipë¥¼ í†µí•œ ê°„ë‹¨ ì„¤ì¹˜
pip install knowledge-storm

# ë˜ëŠ” ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜
git clone https://github.com/stanford-oval/storm.git
cd storm
pip install -r requirements.txt
pip install -e .
```

### 2. API í‚¤ ì„¤ì •

```toml
# secrets.toml íŒŒì¼ ìƒì„±
# ============ language model configurations ============ 
OPENAI_API_KEY="your_openai_api_key"
OPENAI_API_TYPE="openai"

# Azure OpenAI ì‚¬ìš©ì‹œ
OPENAI_API_TYPE="azure"
AZURE_API_BASE="your_azure_api_base_url"
AZURE_API_VERSION="your_azure_api_version"

# ============ retriever configurations ============ 
BING_SEARCH_API_KEY="your_bing_search_api_key"

# ============ encoder configurations ============ 
ENCODER_API_TYPE="openai"
```

### 3. ê¸°ë³¸ STORM ì‚¬ìš©ë²•

```python
# ê¸°ë³¸ STORM ì‚¬ìš© ì˜ˆì‹œ
from knowledge_storm import STORMWikiRunnerArguments, STORMWikiRunner
from knowledge_storm import STORMWikiLMConfigs

# ì„¤ì • ì´ˆê¸°í™”
lm_configs = STORMWikiLMConfigs()
runner_args = STORMWikiRunnerArguments(
    output_dir="./storm_output",
    max_conv_turn=5,
    max_perspective=5
)

# STORM ì‹¤í–‰
runner = STORMWikiRunner(lm_configs)

# ì£¼ì œ ì„¤ì • ë° ì‹¤í–‰
topic = "Artificial Intelligence in Healthcare"
runner.run(
    topic=topic,
    do_research=True,
    do_generate_outline=True,  
    do_generate_article=True,
    do_polish_article=True
)

# ê²°ê³¼ í™•ì¸
print(f"Generated article saved to: {runner_args.output_dir}")
```

### 4. ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤

```bash
# ì™„ì „ ìë™ ì‹¤í–‰
python examples/storm_examples/run_storm_wiki_gpt.py \
    --output-dir ./results \
    --retriever bing \
    --do-research \
    --do-generate-outline \
    --do-generate-article \
    --do-polish-article

# íŠ¹ì • ë‹¨ê³„ë§Œ ì‹¤í–‰
python examples/storm_examples/run_storm_wiki_gpt.py \
    --output-dir ./results \
    --retriever bing \
    --do-research  # ë¦¬ì„œì¹˜ë§Œ ìˆ˜í–‰
```

## Co-STORM: í˜‘ì—… AI ì‹œìŠ¤í…œ

### Co-STORM ê°œìš”

**Co-STORM**ì€ ì¸ê°„ê³¼ AIê°€ í˜‘ì—…í•˜ì—¬ ì§€ì‹ì„ íë ˆì´ì…˜í•˜ëŠ” í˜ì‹ ì  ì‹œìŠ¤í…œì…ë‹ˆë‹¤:

- **ì‹¤ì‹œê°„ ëŒ€í™”**: ì‚¬ìš©ìì™€ AI ì—ì´ì „íŠ¸ ê°„ ëŒ€í™”
- **ë‹¤ì¤‘ ì „ë¬¸ê°€**: ì—¬ëŸ¬ AI ì „ë¬¸ê°€ê°€ í˜‘ì—…
- **ì‚¬ìš©ì ì°¸ì—¬**: ì–¸ì œë“  ëŒ€í™”ì— ê°œì… ê°€ëŠ¥
- **ë™ì  ì¡°ì •**: ì‚¬ìš©ì í”¼ë“œë°±ì— ë”°ë¼ ë°©í–¥ ì¡°ì •

### Co-STORM ì‚¬ìš©ë²•

```python
# Co-STORM ì´ˆê¸°í™” ë° ì‹¤í–‰
from knowledge_storm import CoStormRunner

# Co-STORM ëŸ¬ë„ˆ ìƒì„±
costorm_runner = CoStormRunner(
    args=costorm_args,
    lm_configs=lm_configs,
    rm=rm,
    conv_simulator_lm=conv_simulator_lm,
    topic=topic,
    callback_handler=StreamlitCallbackHandler()
)

# í˜‘ì—… ì„¸ì…˜ ì‹œì‘
costorm_runner.warm_start()

# ëŒ€í™” ë‹¨ê³„ë³„ ì§„í–‰
conv_turn = costorm_runner.step()  # AI ì—ì´ì „íŠ¸ë“¤ì˜ ëŒ€í™” ê´€ì°°
costorm_runner.step(user_utterance="ì‚¬ìš©ì ì˜ê²¬ ì¶”ê°€")  # ì‚¬ìš©ì ê°œì…

# ìµœì¢… ë³´ê³ ì„œ ìƒì„±
costorm_runner.knowledge_base.reorganize()
article = costorm_runner.generate_report()
```

### Co-STORM ì‹¤í–‰ ì˜ˆì‹œ

```bash
# Co-STORM ì‹¤í–‰ ëª…ë ¹
python examples/costorm_examples/run_costorm_gpt.py \
    --output-dir ./costorm_results \
    --retriever bing
```

## ê³ ê¸‰ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### 1. ì‚¬ìš©ì ì •ì˜ ê²€ìƒ‰ ì—”ì§„

```python
# ì»¤ìŠ¤í…€ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„
from knowledge_storm.interface import Retriever

class CustomRetriever(Retriever):
    def __init__(self, custom_api_key):
        self.api_key = custom_api_key
    
    def retrieve(self, query, k=10):
        """ì‚¬ìš©ì ì •ì˜ ê²€ìƒ‰ ë¡œì§"""
        # ì‚¬ìš©ìì˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì—°ë™
        results = self.search_custom_database(query)
        
        return [
            {
                "title": result["title"],
                "content": result["content"], 
                "url": result["url"]
            }
            for result in results[:k]
        ]
    
    def search_custom_database(self, query):
        # ì»¤ìŠ¤í…€ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ êµ¬í˜„
        pass
```

### 2. ë„ë©”ì¸ë³„ íŠ¹í™” ëª¨ë“ˆ

```python
# ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™” STORM
class MedicalSTORMRunner(STORMWikiRunner):
    def __init__(self, lm_configs):
        super().__init__(lm_configs)
        
        # ì˜ë£Œ ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        self.medical_prompts = {
            "research": "ì˜ë£Œ ì „ë¬¸ê°€ ê´€ì ì—ì„œ ì—°êµ¬í•˜ì„¸ìš”...",
            "outline": "ì˜í•™ êµê³¼ì„œ ìŠ¤íƒ€ì¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”...",
            "article": "ì˜ë£Œì§„ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”..."
        }
    
    def customize_for_medical_domain(self):
        """ì˜ë£Œ ë„ë©”ì¸ì— ë§ëŠ” ì»¤ìŠ¤í„°ë§ˆì´ì§•"""
        # ì˜ë£Œ ìš©ì–´ì§‘ ë¡œë“œ
        self.load_medical_terminology()
        
        # ì˜ë£Œ ì¸ìš© ìŠ¤íƒ€ì¼ ì„¤ì •
        self.set_medical_citation_style()
```

### 3. ë‹¤ì–‘í•œ ì¶œë ¥ í˜•ì‹

```python
# ë‹¤ì–‘í•œ ë³´ê³ ì„œ í˜•ì‹ ì§€ì›
class MultiFormatSTORM(STORMWikiRunner):
    def generate_report(self, format_type="wikipedia"):
        """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë³´ê³ ì„œ ìƒì„±"""
        base_content = super().generate_report()
        
        if format_type == "academic":
            return self.convert_to_academic_paper(base_content)
        elif format_type == "presentation":
            return self.convert_to_slides(base_content)
        elif format_type == "executive_summary":
            return self.create_executive_summary(base_content)
        else:
            return base_content
    
    def convert_to_academic_paper(self, content):
        """í•™ìˆ  ë…¼ë¬¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return {
            "abstract": self.generate_abstract(content),
            "introduction": content["sections"][0],
            "literature_review": self.create_literature_review(content),
            "conclusion": content["sections"][-1],
            "references": content["citations"]
        }
```

## ì‹¤ì „ ë°°í¬ ê°€ì´ë“œ

### 1. Docker ì»¨í…Œì´ë„ˆí™”

```dockerfile
# Dockerfile for STORM
FROM python:3.9-slim

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    git \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# STORM ì„¤ì¹˜
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
RUN pip install knowledge-storm

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ
COPY . .

# í™˜ê²½ ë³€ìˆ˜
ENV PYTHONPATH=/app
ENV STORM_OUTPUT_DIR=/app/outputs

# ì„œë¹„ìŠ¤ í¬íŠ¸
EXPOSE 8000

# ì„œë¹„ìŠ¤ ì‹œì‘
CMD ["python", "storm_server.py"]
```

### 2. ì›¹ ì„œë¹„ìŠ¤ êµ¬í˜„

```python
# FastAPI ê¸°ë°˜ STORM ì„œë¹„ìŠ¤
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
import asyncio

app = FastAPI(title="STORM API", version="1.0.0")

class ResearchRequest(BaseModel):
    topic: str
    max_conv_turn: int = 5
    max_perspective: int = 5
    output_format: str = "wikipedia"

class ResearchResponse(BaseModel):
    task_id: str
    status: str
    result_url: str = None

@app.post("/research", response_model=ResearchResponse)
async def create_research_task(
    request: ResearchRequest, 
    background_tasks: BackgroundTasks
):
    task_id = generate_task_id()
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ STORM ì‹¤í–‰
    background_tasks.add_task(
        run_storm_research, 
        task_id, 
        request.topic,
        request.max_conv_turn,
        request.max_perspective
    )
    
    return ResearchResponse(
        task_id=task_id,
        status="processing"
    )

async def run_storm_research(task_id, topic, max_conv_turn, max_perspective):
    """ë°±ê·¸ë¼ìš´ë“œ STORM ì‹¤í–‰"""
    try:
        runner = STORMWikiRunner(lm_configs)
        result = runner.run(
            topic=topic,
            do_research=True,
            do_generate_outline=True,
            do_generate_article=True,
            do_polish_article=True
        )
        
        # ê²°ê³¼ ì €ì¥
        save_result(task_id, result)
        update_task_status(task_id, "completed")
        
    except Exception as e:
        update_task_status(task_id, "failed", str(e))

@app.get("/research/{task_id}")
async def get_research_result(task_id: str):
    """ì—°êµ¬ ê²°ê³¼ ì¡°íšŒ"""
    status = get_task_status(task_id)
    
    if status["status"] == "completed":
        result = load_result(task_id)
        return {
            "task_id": task_id,
            "status": "completed",
            "result": result
        }
    else:
        return {
            "task_id": task_id,
            "status": status["status"],
            "message": status.get("message")
        }
```

### 3. Kubernetes ë°°í¬

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: storm-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: storm-api
  template:
    metadata:
      labels:
        app: storm-api
    spec:
      containers:
      - name: storm-api
        image: your-registry/storm-api:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: storm-secrets
              key: openai-api-key
        - name: BING_SEARCH_API_KEY
          valueFrom:
            secretKeyRef:
              name: storm-secrets
              key: bing-api-key
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        volumeMounts:
        - name: storm-storage
          mountPath: /app/outputs
      volumes:
      - name: storm-storage
        persistentVolumeClaim:
          claimName: storm-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: storm-service
spec:
  selector:
    app: storm-api
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

## ì„±ëŠ¥ ìµœì í™” ë° ìŠ¤ì¼€ì¼ë§

### 1. ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”

```python
# ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ ì„±ëŠ¥ í–¥ìƒ
import asyncio
from concurrent.futures import ThreadPoolExecutor

class OptimizedSTORMRunner:
    def __init__(self, max_workers=4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def parallel_research(self, topic, perspectives):
        """ë‹¤ì¤‘ ê´€ì  ë³‘ë ¬ ë¦¬ì„œì¹˜"""
        tasks = []
        
        for perspective in perspectives:
            task = asyncio.create_task(
                self.research_perspective(topic, perspective)
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return self.merge_research_results(results)
    
    async def research_perspective(self, topic, perspective):
        """íŠ¹ì • ê´€ì ì—ì„œ ë¦¬ì„œì¹˜ ìˆ˜í–‰"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            self.single_perspective_research,
            topic,
            perspective
        )
```

### 2. ìºì‹± ì‹œìŠ¤í…œ

```python
# íš¨ìœ¨ì ì¸ ìºì‹± êµ¬í˜„
from functools import lru_cache
import hashlib
import pickle

class STORMCache:
    def __init__(self, cache_dir="./storm_cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def get_cache_key(self, topic, parameters):
        """ìºì‹œ í‚¤ ìƒì„±"""
        content = f"{topic}_{str(sorted(parameters.items()))}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get_cached_result(self, cache_key):
        """ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def save_to_cache(self, cache_key, result):
        """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        with open(cache_file, 'wb') as f:
            pickle.dump(result, f)

# ìºì‹œ ì ìš© STORM ëŸ¬ë„ˆ
class CachedSTORMRunner(STORMWikiRunner):
    def __init__(self, lm_configs):
        super().__init__(lm_configs)
        self.cache = STORMCache()
    
    def run_with_cache(self, topic, **kwargs):
        cache_key = self.cache.get_cache_key(topic, kwargs)
        cached_result = self.cache.get_cached_result(cache_key)
        
        if cached_result:
            print(f"Using cached result for: {topic}")
            return cached_result
        
        result = self.run(topic=topic, **kwargs)
        self.cache.save_to_cache(cache_key, result)
        
        return result
```

## ë°ì´í„°ì…‹ ë° ë²¤ì¹˜ë§ˆí¬

### FreshWiki ë°ì´í„°ì…‹

**FreshWiki**ëŠ” STORM í‰ê°€ë¥¼ ìœ„í•œ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤:

- **ê·œëª¨**: 100ê°œ ê³ í’ˆì§ˆ Wikipedia ê¸°ì‚¬
- **ê¸°ê°„**: 2022ë…„ 2ì›” ~ 2023ë…„ 9ì›” ìµœë‹¤ í¸ì§‘ í˜ì´ì§€
- **ìš©ë„**: ìë™ ì§€ì‹ íë ˆì´ì…˜ ì—°êµ¬

```python
# FreshWiki ë°ì´í„°ì…‹ í™œìš©
from datasets import load_dataset

# Hugging Faceì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("stanford-oval/FreshWiki")

# í‰ê°€ ë°ì´í„° ì‚¬ìš©
for item in dataset["train"]:
    topic = item["title"]
    reference_article = item["content"]
    
    # STORMìœ¼ë¡œ ê¸°ì‚¬ ìƒì„±
    generated_article = storm_runner.run(topic=topic)
    
    # í’ˆì§ˆ í‰ê°€
    score = evaluate_quality(generated_article, reference_article)
    print(f"Topic: {topic}, Quality Score: {score}")
```

### WildSeek ë°ì´í„°ì…‹

**WildSeek**ëŠ” ì‹¤ì œ ì‚¬ìš©ìì˜ ë³µì¡í•œ ì •ë³´ íƒìƒ‰ íŒ¨í„´ì„ ë‹´ì€ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤:

```python
# WildSeek ë°ì´í„°ì…‹ìœ¼ë¡œ Co-STORM í‰ê°€
wildseek_data = load_dataset("stanford-oval/WildSeek")

for item in wildseek_data["train"]:
    topic = item["topic"]
    user_goal = item["user_goal"]
    
    # Co-STORMìœ¼ë¡œ í˜‘ì—… ì„¸ì…˜ ì‹œë®¬ë ˆì´ì…˜
    costorm_runner = CoStormRunner(topic=topic)
    costorm_runner.set_user_goal(user_goal)
    
    result = costorm_runner.collaborative_research()
    print(f"Topic: {topic}")
    print(f"User Goal: {user_goal}")
    print(f"Result Quality: {evaluate_result(result)}")
```

## ë¹„êµ ë¶„ì„: STORM vs ê¸°ì¡´ ì‹œìŠ¤í…œ

### ê¸°ëŠ¥ ë¹„êµ

| ê¸°ëŠ¥ | STORM | ChatGPT | Claude | Perplexity |
|------|-------|---------|---------|------------|
| ìë™ ë¦¬ì„œì¹˜ | âœ… | âŒ | âŒ | âœ… |
| ì²´ê³„ì  ì•„ì›ƒë¼ì¸ | âœ… | âŒ | âŒ | âŒ |
| ì¸ìš© ì‹œìŠ¤í…œ | âœ… | âŒ | âŒ | âœ… |
| ë‹¤ì¤‘ ê´€ì  | âœ… | âŒ | âŒ | âŒ |
| í˜‘ì—… ëª¨ë“œ | âœ… | âŒ | âŒ | âŒ |
| ì»¤ìŠ¤í„°ë§ˆì´ì§• | âœ… | âŒ | âŒ | âŒ |

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

```python
# ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜
def benchmark_systems():
    topics = [
        "Quantum Computing Applications",
        "Climate Change Mitigation Strategies", 
        "Artificial Intelligence Ethics"
    ]
    
    results = {
        "STORM": [],
        "ChatGPT": [],
        "Claude": [],
        "Perplexity": []
    }
    
    for topic in topics:
        # STORM í‰ê°€
        storm_result = storm_runner.run(topic=topic)
        storm_score = evaluate_comprehensive_quality(storm_result, topic)
        results["STORM"].append(storm_score)
        
        # ë‹¤ë¥¸ ì‹œìŠ¤í…œë“¤ê³¼ ë¹„êµ...
    
    return results

# í‰ê°€ ì§€í‘œ
def evaluate_comprehensive_quality(result, topic):
    """ì¢…í•©ì  í’ˆì§ˆ í‰ê°€"""
    metrics = {
        "factual_accuracy": evaluate_factual_accuracy(result),
        "completeness": evaluate_completeness(result, topic),
        "citation_quality": evaluate_citations(result),
        "structure_quality": evaluate_structure(result),
        "readability": evaluate_readability(result)
    }
    
    return sum(metrics.values()) / len(metrics)
```

## ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€

### 1. êµìœ¡ ë¶„ì•¼

```python
# êµìœ¡ìš© ìë£Œ ìƒì„±
class EducationalSTORM(STORMWikiRunner):
    def generate_course_material(self, topic, education_level="university"):
        """êµìœ¡ ìˆ˜ì¤€ì— ë§ëŠ” í•™ìŠµ ìë£Œ ìƒì„±"""
        
        # êµìœ¡ ìˆ˜ì¤€ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì§•
        if education_level == "high_school":
            complexity_level = "basic"
            citation_style = "simplified"
        elif education_level == "university":
            complexity_level = "intermediate"
            citation_style = "academic"
        else:
            complexity_level = "advanced"
            citation_style = "scholarly"
        
        result = self.run(
            topic=topic,
            complexity_level=complexity_level,
            citation_style=citation_style
        )
        
        # í•™ìŠµ ëª©í‘œ ë° í€´ì¦ˆ ì¶”ê°€
        result["learning_objectives"] = self.generate_learning_objectives(result)
        result["quiz_questions"] = self.generate_quiz(result)
        
        return result

# ì‚¬ìš© ì˜ˆì‹œ
edu_storm = EducationalSTORM(lm_configs)
course_material = edu_storm.generate_course_material(
    topic="Machine Learning Fundamentals",
    education_level="university"
)
```

### 2. ê¸°ì—… ë¦¬ì„œì¹˜

```python
# ê¸°ì—…ìš© ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ
class BusinessSTORM(STORMWikiRunner):
    def generate_market_analysis(self, company_or_industry):
        """ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì  ì„¤ì •
        business_perspectives = [
            "Market Size and Growth",
            "Competitive Landscape",
            "Consumer Trends",
            "Regulatory Environment",
            "Technology Disruption",
            "Investment Opportunities"
        ]
        
        result = self.run(
            topic=company_or_industry,
            perspectives=business_perspectives,
            citation_style="business"
        )
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ì¶”ê°€
        result["executive_summary"] = self.generate_executive_summary(result)
        result["swot_analysis"] = self.generate_swot_analysis(result)
        result["recommendations"] = self.generate_recommendations(result)
        
        return result

# ì‚¬ìš© ì˜ˆì‹œ
business_storm = BusinessSTORM(lm_configs)
market_report = business_storm.generate_market_analysis("Electric Vehicle Industry")
```

### 3. ì €ë„ë¦¬ì¦˜

```python
# ì¡°ì‚¬ ë³´ë„ìš© ë¦¬ì„œì¹˜
class JournalismSTORM(STORMWikiRunner):
    def investigative_research(self, topic):
        """ì¡°ì‚¬ ë³´ë„ë¥¼ ìœ„í•œ ì‹¬ì¸µ ë¦¬ì„œì¹˜"""
        
        journalism_perspectives = [
            "Who (Key Players)",
            "What (Core Facts)",
            "When (Timeline)",
            "Where (Geographic Context)",
            "Why (Motivations and Causes)",
            "How (Mechanisms and Processes)"
        ]
        
        result = self.run(
            topic=topic,
            perspectives=journalism_perspectives,
            fact_checking=True,
            source_verification=True
        )
        
        # ì €ë„ë¦¬ì¦˜ ìš”ì†Œ ì¶”ê°€
        result["fact_check_report"] = self.generate_fact_check(result)
        result["source_credibility"] = self.assess_source_credibility(result)
        result["follow_up_questions"] = self.generate_follow_up_questions(result)
        
        return result
```

## ê²°ë¡ 

**Stanford STORM**ì€ ì§€ì‹ íë ˆì´ì…˜ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•˜ëŠ” í˜ì‹ ì ì¸ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ğŸš€

### ì£¼ìš” ì¥ì 

1. **ì²´ê³„ì  ì ‘ê·¼**: 4ë‹¨ê³„ ëª¨ë“ˆí™”ëœ íŒŒì´í”„ë¼ì¸
2. **ë‹¤ì¤‘ ê´€ì **: ì—¬ëŸ¬ ì „ë¬¸ê°€ ì‹œê°ì—ì„œ ì •ë³´ ìˆ˜ì§‘
3. **ì¸ìš© ì‹œìŠ¤í…œ**: ëª¨ë“  ë‚´ìš©ì— ëŒ€í•œ ì¶œì²˜ ì œê³µ
4. **í˜‘ì—… ê¸°ëŠ¥**: Co-STORMì„ í†µí•œ ì¸ê°„-AI í˜‘ì—…
5. **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ë‹¤ì–‘í•œ ë„ë©”ì¸ì— ì ìš© ê°€ëŠ¥
6. **ì˜¤í”ˆì†ŒìŠ¤**: MIT ë¼ì´ì„ ìŠ¤ë¡œ ììœ ë¡œìš´ í™œìš©

### í™œìš© ê¶Œì¥ì‚¬í•­

- **ì—°êµ¬ì**: ë¬¸í—Œ ì¡°ì‚¬ ë° ì²´ê³„ì  ë¦¬ë·° ì‘ì„±
- **êµìœ¡ì**: êµìœ¡ ìë£Œ ë° ê°•ì˜ ë…¸íŠ¸ ìƒì„±
- **ê¸°ì—…**: ì‹œì¥ ë¶„ì„ ë° ê²½ìŸì‚¬ ì¡°ì‚¬
- **ì €ë„ë¦¬ìŠ¤íŠ¸**: ì¡°ì‚¬ ë³´ë„ ë° íŒ©íŠ¸ ì²´í‚¹
- **í•™ìƒ**: ê³¼ì œ ë° í”„ë¡œì íŠ¸ ë¦¬ì„œì¹˜

**STORM**ê³¼ **Co-STORM**ì€ ë‹¨ìˆœí•œ ë„êµ¬ë¥¼ ë„˜ì–´ ì§€ì‹ ì‘ì—…ì˜ ë°©ì‹ì„ ê·¼ë³¸ì ìœ¼ë¡œ ë³€í™”ì‹œí‚¬ ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. 25.4k GitHub ìŠ¤íƒ€ê°€ ì¦ëª…í•˜ë“¯ì´, ì´ë¯¸ ë§ì€ ì‚¬ìš©ìë“¤ì´ ê·¸ ê°€ì¹˜ë¥¼ ì¸ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì—¬ëŸ¬ë¶„ì˜ ì§€ì‹ íë ˆì´ì…˜ ì›Œí¬í”Œë¡œìš°ì— STORMì„ ë„ì…í•´ ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤! ğŸ“šâœ¨

---

**ì°¸ê³  ë§í¬**:
- [STORM GitHub ë¦¬í¬ì§€í† ë¦¬](https://github.com/stanford-oval/storm)
- [STORM ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://storm.genie.stanford.edu)
- [NAACL 2024 ë…¼ë¬¸](https://aclanthology.org/2024.naacl-long.347/)
- [EMNLP 2024 Co-STORM ë…¼ë¬¸](https://aclanthology.org/2024.emnlp-main.554/)
- [FreshWiki ë°ì´í„°ì…‹](https://huggingface.co/datasets/stanford-oval/FreshWiki)
- [WildSeek ë°ì´í„°ì…‹](https://huggingface.co/datasets/stanford-oval/WildSeek) 