---
title: "LM Studio + DeepSeek-R1ìœ¼ë¡œ êµ¬ì¶•í•˜ëŠ” ì‹¤ì „ Agent ì‹œìŠ¤í…œ: Smolagentsì™€ AGUIApp ì™„ë²½ ê°€ì´ë“œ"
excerpt: "LM Studioì—ì„œ DeepSeek-R1 ëª¨ë¸ì„ í™œìš©í•´ ì›¹ ìŠ¤í¬ë˜í•‘ê³¼ ìš”ì•½ ê¸°ëŠ¥ì„ ê°€ì§„ Agent ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê³ , AGUIAppìœ¼ë¡œ ì›¹ UIê¹Œì§€ ì œê³µí•˜ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì†Œê°œí•©ë‹ˆë‹¤."
date: 2025-06-20
categories:
  - agentops
tags:
  - lm-studio
  - deepseek-r1
  - smolagents
  - agui-app
  - web-scraping
  - local-llm
  - agent-ui
  - python
author_profile: true
toc: true
toc_label: LM Studio Agent Tutorial
---

## ê°œìš”

ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” **LM Studio**ì—ì„œ **DeepSeek-R1 ëª¨ë¸**ì„ í™œìš©í•´ ì›¹ ìŠ¤í¬ë˜í•‘ê³¼ í…ìŠ¤íŠ¸ ìš”ì•½ ê¸°ëŠ¥ì„ ê°€ì§„ ì‹¤ì „ Agent ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤. **Smolagents**ë¡œ Agent ë¡œì§ì„ êµ¬í˜„í•˜ê³ , **AGUIApp**ìœ¼ë¡œ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì›¹ ì¸í„°í˜ì´ìŠ¤ê¹Œì§€ ì œê³µí•˜ëŠ” ì™„ì „í•œ ì†”ë£¨ì…˜ì„ ë‹¨ê³„ë³„ë¡œ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.

### ğŸ¯ í•™ìŠµ ëª©í‘œ

- LM Studio ì„¤ì¹˜ ë° DeepSeek-R1 ëª¨ë¸ ì„¤ì •
- Smolagentsë¥¼ í™œìš©í•œ ë©€í‹° ë„êµ¬ Agent êµ¬í˜„
- ì›¹ ìŠ¤í¬ë˜í•‘ê³¼ LLM ìš”ì•½ì„ ê²°í•©í•œ ì‹¤ìš©ì  ë„êµ¬ ê°œë°œ
- AGUIAppì„ í†µí•œ ì›¹ UI êµ¬ì¶•
- ë¡œì»¬ í™˜ê²½ì—ì„œì˜ ì™„ì „í•œ Agent ì‹œìŠ¤í…œ ìš´ì˜

## 1. í™˜ê²½ ì„¤ì •

### 1.1 LM Studio ì„¤ì¹˜

**macOS ì„¤ì¹˜:**

```bash
# Homebrewë¥¼ í†µí•œ ì„¤ì¹˜
brew install --cask lm-studio

# ë˜ëŠ” ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‹¤ìš´ë¡œë“œ
# https://lmstudio.ai/
```

**Windows/Linux:**

1. [LM Studio ê³µì‹ ì‚¬ì´íŠ¸](https://lmstudio.ai/)ì—ì„œ ì„¤ì¹˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
2. ì„¤ì¹˜ í›„ ì‹¤í–‰

### 1.2 DeepSeek-R1 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

LM Studioë¥¼ ì‹¤í–‰í•œ í›„:

1. **Search** íƒ­ì—ì„œ `deepseek-r1-0528-qwen3-8b-mlx` ê²€ìƒ‰
2. **Download** ë²„íŠ¼ í´ë¦­í•˜ì—¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
3. **Chat** íƒ­ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ ë¡œë“œ
4. **Local Server** íƒ­ì—ì„œ ì„œë²„ ì‹œì‘ (í¬íŠ¸: 1234)

### 1.3 Python í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv agent_env
source agent_env/bin/activate  # Windows: agent_env\Scripts\activate

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install smolagents requests beautifulsoup4 lxml
pip install agno ag-ui-protocol
pip install uvicorn fastapi
```

## 2. í•µì‹¬ Agent ì‹œìŠ¤í…œ êµ¬í˜„

### 2.1 í”„ë¡œì íŠ¸ êµ¬ì¡°

```
agent_project/
â”œâ”€â”€ main.py                 # ë©”ì¸ Agent ë¡œì§
â”œâ”€â”€ ui_app.py              # AGUIApp ì›¹ ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper.py         # ì›¹ ìŠ¤í¬ë˜í•‘ ë„êµ¬
â”‚   â””â”€â”€ summarizer.py      # í…ìŠ¤íŠ¸ ìš”ì•½ ë„êµ¬
â””â”€â”€ config.py              # ì„¤ì • íŒŒì¼
```

### 2.2 ì„¤ì • íŒŒì¼ (config.py)

```python
# config.py
import os

# LM Studio ì„œë²„ ì„¤ì •
LM_STUDIO_BASE_URL = "http://localhost:1234/v1"
LM_STUDIO_API_KEY = "not-needed"
MODEL_ID = "deepseek-r1-0528-qwen3-8b-mlx"

# ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ ì‚¬ì´íŠ¸ (ì˜ˆì‹œ)
TARGET_WEBSITES = {
    "tech_news": "https://techcrunch.com/",
    "ai_research": "https://arxiv.org/list/cs.AI/recent",
    "testing_catalog": "https://www.testingcatalog.com/"
}

# UI ì„¤ì •
UI_PORT = 8000
UI_HOST = "0.0.0.0"
```

## 3. ë„êµ¬ êµ¬í˜„

### 3.1 ì›¹ ìŠ¤í¬ë˜í•‘ ë„êµ¬ (tools/scraper.py)

```python
# tools/scraper.py
import requests
import re
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from smolagents import tool

@tool
def get_latest_articles(
    website: str = "https://www.testingcatalog.com/", 
    n_articles: int = 3
) -> List[Dict]:
    """
    ì§€ì •ëœ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ê¸°ì‚¬ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.
    
    Args:
        website: ìŠ¤í¬ë˜í•‘í•  ì›¹ì‚¬ì´íŠ¸ URL
        n_articles: ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 3)
    
    Returns:
        ê¸°ì‚¬ ì •ë³´ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        - title: ê¸°ì‚¬ ì œëª©
        - url: ê¸°ì‚¬ URL  
        - content: ê¸°ì‚¬ ë³¸ë¬¸
        - summary: ê°„ë‹¨í•œ ë¯¸ë¦¬ë³´ê¸°
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        
        response = requests.get(website, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        # ì¼ë°˜ì ì¸ ê¸°ì‚¬ êµ¬ì¡°ë¥¼ ì°¾ê¸° ìœ„í•œ ì…€ë ‰í„°ë“¤
        article_selectors = [
            "article",
            ".post", 
            ".entry",
            ".article-item",
            "h2 a, h3 a"
        ]
        
        found_articles = []
        for selector in article_selectors:
            found_articles = soup.select(selector)[:n_articles]
            if found_articles:
                break
                
        for element in found_articles:
            try:
                # ë§í¬ì™€ ì œëª© ì¶”ì¶œ
                if element.name == 'a':
                    link_elem = element
                    title = element.get_text(strip=True)
                else:
                    link_elem = element.find('a', href=True)
                    title = link_elem.get_text(strip=True) if link_elem else "ì œëª© ì—†ìŒ"
                
                if not link_elem:
                    continue
                    
                url = link_elem.get('href', '')
                if url.startswith('/'):
                    url = website.rstrip('/') + url
                elif not url.startswith('http'):
                    continue
                
                # ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                article_response = requests.get(url, headers=headers, timeout=10)
                article_soup = BeautifulSoup(article_response.text, "html.parser")
                
                # ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
                content_selectors = [
                    ".post-content", ".entry-content", ".article-content",
                    ".content", "main", ".main-content"
                ]
                
                content = ""
                for selector in content_selectors:
                    content_elem = article_soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text(" ", strip=True)
                        break
                
                if not content:
                    # ëª¨ë“  p íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    paragraphs = article_soup.find_all('p')
                    content = " ".join([p.get_text(" ", strip=True) for p in paragraphs])
                
                # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°
                content = content[:2000] + "..." if len(content) > 2000 else content
                summary = content[:200] + "..." if len(content) > 200 else content
                
                articles.append({
                    "title": title,
                    "url": url,
                    "content": content,
                    "summary": summary
                })
                
            except Exception as e:
                print(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
                
        return articles[:n_articles]
        
    except Exception as e:
        return [{"error": f"ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}"}]

@tool 
def search_web_content(query: str, max_results: int = 5) -> List[Dict]:
    """
    ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›¹ ì½˜í…ì¸ ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
        max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    # DuckDuckGo ê²€ìƒ‰ (ê°„ë‹¨í•œ êµ¬í˜„)
    try:
        search_url = f"https://duckduckgo.com/html/?q={query}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        
        response = requests.get(search_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        
        results = []
        result_elements = soup.select(".result")[:max_results]
        
        for element in result_elements:
            title_elem = element.select_one(".result__title a")
            snippet_elem = element.select_one(".result__snippet")
            
            if title_elem:
                title = title_elem.get_text(strip=True)
                url = title_elem.get('href', '')
                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                
                results.append({
                    "title": title,
                    "url": url,
                    "snippet": snippet
                })
                
        return results
        
    except Exception as e:
        return [{"error": f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"}]
```

### 3.2 í…ìŠ¤íŠ¸ ìš”ì•½ ë„êµ¬ (tools/summarizer.py)

```python
# tools/summarizer.py
from smolagents import tool, OpenAIServerModel
from config import LM_STUDIO_BASE_URL, LM_STUDIO_API_KEY, MODEL_ID

# LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
model = OpenAIServerModel(
    model_id=MODEL_ID,
    api_base=LM_STUDIO_BASE_URL,
    api_key=LM_STUDIO_API_KEY,
)

@tool
def summarize_article(text: str, max_words: int = 150, style: str = "balanced") -> str:
    """
    ê¸°ì‚¬ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
    
    Args:
        text: ìš”ì•½í•  í…ìŠ¤íŠ¸
        max_words: ìµœëŒ€ ë‹¨ì–´ ìˆ˜
        style: ìš”ì•½ ìŠ¤íƒ€ì¼ ("concise", "balanced", "detailed")
    
    Returns:
        ìš”ì•½ëœ í…ìŠ¤íŠ¸
    """
    if len(text.strip()) < 50:
        return "í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ì„œ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    style_prompts = {
        "concise": "í•µì‹¬ í¬ì¸íŠ¸ë§Œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ",
        "balanced": "ì£¼ìš” ë‚´ìš©ì„ ê· í˜•ìˆê²Œ", 
        "detailed": "ì¤‘ìš”í•œ ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•˜ì—¬ ìƒì„¸í•˜ê²Œ"
    }
    
    style_instruction = style_prompts.get(style, style_prompts["balanced"])
    
    prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {style_instruction} {max_words}ë‹¨ì–´ ì´ë‚´ë¡œ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

ìš”ì•½:"""
    
    try:
        response = model(
            [{"role": "user", "content": [{"type": "text", "text": prompt}]}],
            stop_sequences=["\n\n", "---"],
            max_tokens=300
        )
        
        summary = response.content.strip()
        return summary if summary else "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    except Exception as e:
        return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

@tool
def analyze_sentiment(text: str) -> Dict:
    """
    í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸
        
    Returns:
        ê°ì • ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

í…ìŠ¤íŠ¸: {text}

ë‹µë³€ í˜•ì‹:
{{
    "sentiment": "positive/negative/neutral",
    "confidence": 0.95,
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
    "explanation": "ë¶„ì„ ê·¼ê±°"
}}
"""
    
    try:
        response = model(
            [{"role": "user", "content": [{"type": "text", "text": prompt}]}],
            max_tokens=200
        )
        
        # JSON íŒŒì‹± ì‹œë„
        import json
        result = json.loads(response.content.strip())
        return result
        
    except Exception as e:
        return {
            "sentiment": "neutral",
            "confidence": 0.0,
            "keywords": [],
            "explanation": f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        }

@tool
def extract_keywords(text: str, max_keywords: int = 10) -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
        max_keywords: ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜
        
    Returns:
        í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ {max_keywords}ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. JSON ë°°ì—´ í˜•íƒœë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸: {text}

ë‹µë³€ í˜•ì‹: ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]
"""
    
    try:
        response = model(
            [{"role": "user", "content": [{"type": "text", "text": prompt}]}],
            max_tokens=150
        )
        
        import json
        keywords = json.loads(response.content.strip())
        return keywords if isinstance(keywords, list) else []
        
    except Exception as e:
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ í´ë°±
        import re
        words = re.findall(r'\b[ê°€-í£a-zA-Z]{2,}\b', text)
        return list(set(words))[:max_keywords]
```

## 4. ë©”ì¸ Agent ì‹œìŠ¤í…œ (main.py)

```python
# main.py
import json
from smolagents import CodeAgent, OpenAIServerModel
from tools.scraper import get_latest_articles, search_web_content  
from tools.summarizer import summarize_article, analyze_sentiment, extract_keywords
from config import LM_STUDIO_BASE_URL, LM_STUDIO_API_KEY, MODEL_ID

class NewsAnalysisAgent:
    def __init__(self):
        """ë‰´ìŠ¤ ë¶„ì„ Agent ì´ˆê¸°í™”"""
        self.model = OpenAIServerModel(
            model_id=MODEL_ID,
            api_base=LM_STUDIO_BASE_URL, 
            api_key=LM_STUDIO_API_KEY,
        )
        
        self.agent = CodeAgent(
            name="NewsAnalysisAgent",
            model=self.model,
            tools=[
                get_latest_articles,
                search_web_content, 
                summarize_article,
                analyze_sentiment,
                extract_keywords
            ],
            max_steps=15,
            verbose=True
        )
    
    def analyze_website_news(self, website_url: str, num_articles: int = 3) -> str:
        """ì›¹ì‚¬ì´íŠ¸ì˜ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        task = f"""
ë‹¤ìŒ ì‘ì—…ì„ ìˆœì„œëŒ€ë¡œ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

1. get_latest_articles ë„êµ¬ë¥¼ ì‚¬ìš©í•´ '{website_url}'ì—ì„œ ìµœì‹  ê¸°ì‚¬ {num_articles}ê°œë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”.
2. ê° ê¸°ì‚¬ì— ëŒ€í•´:
   - summarize_article ë„êµ¬ë¡œ ìš”ì•½ì„ ìƒì„±í•˜ì„¸ìš”
   - analyze_sentiment ë„êµ¬ë¡œ ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”  
   - extract_keywords ë„êµ¬ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”
3. ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì„œ final_answerë¡œ ë°˜í™˜í•˜ì„¸ìš”.

ë§ˆí¬ë‹¤ìš´ í˜•ì‹:
# ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼

## ê¸°ì‚¬ 1: [ì œëª©]
- **URL**: [ë§í¬]
- **ìš”ì•½**: [ìš”ì•½ ë‚´ìš©]
- **ê°ì •**: [ê°ì • ë¶„ì„ ê²°ê³¼]
- **í‚¤ì›Œë“œ**: [í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸]

(ë‹¤ë¥¸ ê¸°ì‚¬ë“¤ë„ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ...)

## ì „ì²´ ë¶„ì„ ìš”ì•½
- ì£¼ìš” íŠ¸ë Œë“œ
- ì „ë°˜ì ì¸ ê°ì •
- ê³µí†µ í‚¤ì›Œë“œ
        """
        
        return self.agent.run(task)
    
    def search_and_analyze(self, query: str, max_results: int = 3) -> str:
        """ê²€ìƒ‰í•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        task = f"""
ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

1. search_web_content ë„êµ¬ë¥¼ ì‚¬ìš©í•´ '{query}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ê³  {max_results}ê°œ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”.
2. ê° ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•´ ê°ì • ë¶„ì„ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìˆ˜í–‰í•˜ì„¸ìš”.
3. ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì •ë¦¬í•´ì„œ ë°˜í™˜í•˜ì„¸ìš”.
        """
        
        return self.agent.run(task)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¤– ë‰´ìŠ¤ ë¶„ì„ Agentë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ“¡ LM Studio ì„œë²„ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš” (http://localhost:1234)")
    
    agent = NewsAnalysisAgent()
    
    while True:
        print("\n" + "="*50)
        print("1. ì›¹ì‚¬ì´íŠ¸ ë‰´ìŠ¤ ë¶„ì„")
        print("2. í‚¤ì›Œë“œ ê²€ìƒ‰ ë° ë¶„ì„") 
        print("3. ì¢…ë£Œ")
        print("="*50)
        
        choice = input("ì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
        
        if choice == "1":
            url = input("ë¶„ì„í•  ì›¹ì‚¬ì´íŠ¸ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if not url:
                url = "https://www.testingcatalog.com/"
            
            num_articles = input("ë¶„ì„í•  ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 3): ").strip()
            num_articles = int(num_articles) if num_articles.isdigit() else 3
            
            print(f"\nğŸ” {url}ì—ì„œ ìµœì‹  ê¸°ì‚¬ {num_articles}ê°œë¥¼ ë¶„ì„ ì¤‘...")
            result = agent.analyze_website_news(url, num_articles)
            print("\n" + "="*70)
            print(result)
            print("="*70)
            
        elif choice == "2":
            query = input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if not query:
                print("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
                
            max_results = input("ê²€ìƒ‰í•  ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 3): ").strip()
            max_results = int(max_results) if max_results.isdigit() else 3
            
            print(f"\nğŸ” '{query}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì¤‘...")
            result = agent.search_and_analyze(query, max_results)
            print("\n" + "="*70)
            print(result)
            print("="*70)
            
        elif choice == "3":
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
            
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1-3 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
```

## 5. ì›¹ UI êµ¬í˜„ (ui_app.py)

### 5.1 AGUIApp ê¸°ë°˜ ì›¹ ì¸í„°í˜ì´ìŠ¤

```python
# ui_app.py
import asyncio
from typing import Optional
from agno.agent.agent import Agent
from agno.app.agui.app import AGUIApp
from agno.models.openai import OpenAIChat
from smolagents import OpenAIServerModel
from tools.scraper import get_latest_articles, search_web_content
from tools.summarizer import summarize_article, analyze_sentiment, extract_keywords
from config import LM_STUDIO_BASE_URL, LM_STUDIO_API_KEY, MODEL_ID, UI_PORT, UI_HOST

class NewsAnalysisWebAgent:
    def __init__(self):
        """ì›¹ ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ì„ Agent"""
        # LM Studio ëª¨ë¸ ì„¤ì •
        self.model = OpenAIChat(
            id=MODEL_ID,
            api_base=LM_STUDIO_BASE_URL,
            api_key=LM_STUDIO_API_KEY
        )
        
        # Agno Agent ìƒì„±
        self.agent = Agent(
            name="NewsAnalysisWebAgent",
            model=self.model,
            instructions="""
ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

1. **ì›¹ì‚¬ì´íŠ¸ ë‰´ìŠ¤ ë¶„ì„**: íŠ¹ì • ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ ë¶„ì„
2. **í‚¤ì›Œë“œ ê²€ìƒ‰**: ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ ê´€ë ¨ ë‰´ìŠ¤ ì°¾ê¸° ë° ë¶„ì„  
3. **í…ìŠ¤íŠ¸ ìš”ì•½**: ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½
4. **ê°ì • ë¶„ì„**: í…ìŠ¤íŠ¸ì˜ ê°ì •(ê¸ì •/ë¶€ì •/ì¤‘ë¦½) ë¶„ì„
5. **í‚¤ì›Œë“œ ì¶”ì¶œ**: í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ

ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ì ì ˆí•œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.
            """,
            add_datetime_to_instructions=True,
            markdown=True,
        )

    async def analyze_website(self, website_url: str, num_articles: int = 3) -> str:
        """ì›¹ì‚¬ì´íŠ¸ ë‰´ìŠ¤ ë¶„ì„"""
        try:
            # ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            articles = get_latest_articles(website_url, num_articles)
            
            if not articles or (len(articles) == 1 and "error" in articles[0]):
                return f"âŒ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {website_url}"
            
            analysis_results = []
            
            for i, article in enumerate(articles, 1):
                if "error" in article:
                    continue
                    
                # ê° ê¸°ì‚¬ ë¶„ì„
                summary = summarize_article(article["content"], max_words=100)
                sentiment = analyze_sentiment(article["content"])
                keywords = extract_keywords(article["content"], max_keywords=5)
                
                analysis_results.append({
                    "title": article["title"],
                    "url": article["url"],
                    "summary": summary,
                    "sentiment": sentiment,
                    "keywords": keywords
                })
            
            # ê²°ê³¼ í¬ë§·íŒ…
            result = f"# ğŸ” ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼\n\n**ë¶„ì„ ì›¹ì‚¬ì´íŠ¸**: {website_url}\n**ë¶„ì„ ê¸°ì‚¬ ìˆ˜**: {len(analysis_results)}ê°œ\n\n"
            
            for i, analysis in enumerate(analysis_results, 1):
                result += f"## ğŸ“° ê¸°ì‚¬ {i}: {analysis['title']}\n\n"
                result += f"**ğŸ”— URL**: [{analysis['url']}]({analysis['url']})\n\n"
                result += f"**ğŸ“ ìš”ì•½**: {analysis['summary']}\n\n"
                result += f"**ğŸ˜Š ê°ì •**: {analysis['sentiment'].get('sentiment', 'unknown')} "
                result += f"(ì‹ ë¢°ë„: {analysis['sentiment'].get('confidence', 0):.2f})\n\n"
                result += f"**ğŸ·ï¸ í‚¤ì›Œë“œ**: {', '.join(analysis['keywords'])}\n\n"
                result += "---\n\n"
            
            # ì „ì²´ ìš”ì•½
            all_keywords = []
            sentiments = []
            for analysis in analysis_results:
                all_keywords.extend(analysis['keywords'])
                sentiments.append(analysis['sentiment'].get('sentiment', 'neutral'))
            
            common_keywords = list(set(all_keywords))[:10]
            positive_count = sentiments.count('positive')
            negative_count = sentiments.count('negative')
            neutral_count = sentiments.count('neutral')
            
            result += "## ğŸ“Š ì „ì²´ ë¶„ì„ ìš”ì•½\n\n"
            result += f"**ì£¼ìš” í‚¤ì›Œë“œ**: {', '.join(common_keywords[:8])}\n\n"
            result += f"**ê°ì • ë¶„í¬**: ê¸ì • {positive_count}ê°œ, ë¶€ì • {negative_count}ê°œ, ì¤‘ë¦½ {neutral_count}ê°œ\n\n"
            
            return result
            
        except Exception as e:
            return f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def search_and_analyze(self, query: str, max_results: int = 3) -> str:
        """ê²€ìƒ‰ ë° ë¶„ì„"""
        try:
            # ê²€ìƒ‰ ìˆ˜í–‰
            search_results = search_web_content(query, max_results)
            
            if not search_results or (len(search_results) == 1 and "error" in search_results[0]):
                return f"âŒ '{query}' ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            result = f"# ğŸ” ê²€ìƒ‰ ë¶„ì„ ê²°ê³¼\n\n**ê²€ìƒ‰ í‚¤ì›Œë“œ**: {query}\n**ê²€ìƒ‰ ê²°ê³¼**: {len(search_results)}ê°œ\n\n"
            
            for i, item in enumerate(search_results, 1):
                if "error" in item:
                    continue
                    
                # ê°ì • ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
                sentiment = analyze_sentiment(item.get("snippet", ""))
                keywords = extract_keywords(item.get("snippet", ""), max_keywords=3)
                
                result += f"## ğŸ”— ê²°ê³¼ {i}: {item.get('title', 'ì œëª© ì—†ìŒ')}\n\n"
                result += f"**URL**: [{item.get('url', '')}]({item.get('url', '')})\n\n"
                result += f"**ìŠ¤ë‹ˆí«**: {item.get('snippet', '')}\n\n"
                result += f"**ê°ì •**: {sentiment.get('sentiment', 'unknown')}\n\n"
                result += f"**í‚¤ì›Œë“œ**: {', '.join(keywords)}\n\n"
                result += "---\n\n"
            
            return result
            
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# AGUI ì•± ìƒì„±
def create_agui_app():
    """AGUI ì•± ìƒì„±"""
    news_agent = NewsAnalysisWebAgent()
    
    # AGUI ì•± ì„¤ì •
    agui_app = AGUIApp(
        agent=news_agent.agent,
        name="ğŸ¤– ë‰´ìŠ¤ ë¶„ì„ Agent",
        app_id="news_analysis_agent",
        description="ì›¹ì‚¬ì´íŠ¸ ë‰´ìŠ¤ ë¶„ì„, ê²€ìƒ‰, ìš”ì•½, ê°ì • ë¶„ì„ì„ ì œê³µí•˜ëŠ” AI Agent",
        host=UI_HOST,
        port=UI_PORT
    )
    
    return agui_app

def main():
    """ì›¹ UI ì‹¤í–‰"""
    print("ğŸš€ ë‰´ìŠ¤ ë¶„ì„ Agent ì›¹ UIë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"ğŸ“¡ LM Studio ì„œë²„ê°€ {LM_STUDIO_BASE_URL}ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")
    print(f"ğŸŒ ì›¹ UI: http://{UI_HOST}:{UI_PORT}")
    
    app = create_agui_app()
    app.serve(reload=True)

if __name__ == "__main__":
    main()
```

## 6. ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

### 6.1 ë‹¨ê³„ë³„ ì‹¤í–‰

**1ë‹¨ê³„: LM Studio ì„œë²„ í™•ì¸**

```bash
# LM Studioì—ì„œ DeepSeek-R1 ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
curl http://localhost:1234/v1/models
```

**2ë‹¨ê³„: CLI ë²„ì „ ì‹¤í–‰**

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source agent_env/bin/activate

# CLI ë²„ì „ ì‹¤í–‰
python main.py
```

**3ë‹¨ê³„: ì›¹ UI ì‹¤í–‰**

```bash
# ì›¹ UI ì‹¤í–‰ (ë³„ë„ í„°ë¯¸ë„)
python ui_app.py
```

### 6.2 ì‚¬ìš© ì˜ˆì‹œ

**CLIì—ì„œ ë‰´ìŠ¤ ë¶„ì„:**

```
ì„ íƒí•˜ì„¸ìš” (1-3): 1
ë¶„ì„í•  ì›¹ì‚¬ì´íŠ¸ URLì„ ì…ë ¥í•˜ì„¸ìš”: https://techcrunch.com
ë¶„ì„í•  ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 3): 5
```

**ì›¹ UIì—ì„œ ì‚¬ìš©:**

```
ì›¹ì‚¬ì´íŠ¸ https://www.testingcatalog.com/ ì—ì„œ ìµœì‹  ë‰´ìŠ¤ 3ê°œë¥¼ ë¶„ì„í•´ì¤˜
```

## 7. ê³ ê¸‰ ê¸°ëŠ¥ í™•ì¥

### 7.1 ìŠ¤ì¼€ì¤„ë§ ê¸°ëŠ¥ ì¶”ê°€

```python
# scheduler.py
import schedule
import time
import json
from datetime import datetime
from main import NewsAnalysisAgent

class NewsScheduler:
    def __init__(self):
        self.agent = NewsAnalysisAgent()
        self.monitored_sites = [
            "https://techcrunch.com/",
            "https://www.testingcatalog.com/",
            "https://arxiv.org/list/cs.AI/recent"
        ]
    
    def daily_analysis(self):
        """ì¼ì¼ ë‰´ìŠ¤ ë¶„ì„"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"ğŸ“… ì¼ì¼ ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘: {timestamp}")
        
        results = {}
        for site in self.monitored_sites:
            try:
                result = self.agent.analyze_website_news(site, 2)
                results[site] = result
                print(f"âœ… {site} ë¶„ì„ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {site} ë¶„ì„ ì‹¤íŒ¨: {e}")
                results[site] = f"ë¶„ì„ ì‹¤íŒ¨: {e}"
        
        # ê²°ê³¼ ì €ì¥
        filename = f"daily_analysis_{datetime.now().strftime('%Y%m%d')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ê²°ê³¼ ì €ì¥: {filename}")

    def start_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        # ë§¤ì¼ ì˜¤ì „ 9ì‹œì— ë¶„ì„ ì‹¤í–‰
        schedule.every().day.at("09:00").do(self.daily_analysis)
        
        print("â° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ - ë§¤ì¼ ì˜¤ì „ 9ì‹œì— ë‰´ìŠ¤ ë¶„ì„")
        while True:
            schedule.run_pending()
            time.sleep(60)

if __name__ == "__main__":
    scheduler = NewsScheduler()
    scheduler.start_scheduler()
```

### 7.2 ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™

```python
# database.py
import sqlite3
import json
from datetime import datetime
from typing import List, Dict

class NewsDatabase:
    def __init__(self, db_path: str = "news_analysis.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                url TEXT UNIQUE NOT NULL,
                content TEXT,
                summary TEXT,
                sentiment TEXT,
                keywords TEXT,
                source_website TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS analysis_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                website TEXT NOT NULL,
                analysis_type TEXT NOT NULL,
                result TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        conn.commit()
        conn.close()
    
    def save_article(self, article_data: Dict):
        """ê¸°ì‚¬ ë°ì´í„° ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                INSERT OR REPLACE INTO articles 
                (title, url, content, summary, sentiment, keywords, source_website)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                article_data.get('title'),
                article_data.get('url'),
                article_data.get('content'),
                article_data.get('summary'),
                json.dumps(article_data.get('sentiment', {})),
                json.dumps(article_data.get('keywords', [])),
                article_data.get('source_website')
            ))
            conn.commit()
        finally:
            conn.close()
    
    def get_recent_articles(self, limit: int = 10) -> List[Dict]:
        """ìµœê·¼ ê¸°ì‚¬ ì¡°íšŒ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT title, url, summary, sentiment, keywords, created_at
            FROM articles 
            ORDER BY created_at DESC 
            LIMIT ?
        """, (limit,))
        
        results = []
        for row in cursor.fetchall():
            results.append({
                'title': row[0],
                'url': row[1], 
                'summary': row[2],
                'sentiment': json.loads(row[3]) if row[3] else {},
                'keywords': json.loads(row[4]) if row[4] else [],
                'created_at': row[5]
            })
        
        conn.close()
        return results
```

## 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 8.1 ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°ì±…

**ë¬¸ì œ 1: LM Studio ì—°ê²° ì‹¤íŒ¨**

```bash
# í•´ê²° ë°©ë²•
1. LM Studioê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
2. ë¡œì»¬ ì„œë²„ê°€ í¬íŠ¸ 1234ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
3. ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸

# ì—°ê²° í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "deepseek-r1-0528-qwen3-8b-mlx", "messages": [{"role": "user", "content": "Hello"}]}'
```

**ë¬¸ì œ 2: ì›¹ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨**

```python
# í•´ê²° ë°©ë²•: ë” ê²¬ê³ í•œ ìŠ¤í¬ë˜í•‘ ë¡œì§
def robust_scraping(url):
    try:
        # User-Agent ì„¤ì •
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; NewsBot/1.0)'
        }
        response = requests.get(url, headers=headers, timeout=15)
        return response
    except requests.exceptions.RequestException as e:
        print(f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        return None
```

**ë¬¸ì œ 3: ë©”ëª¨ë¦¬ ë¶€ì¡±**

```python
# í•´ê²° ë°©ë²•: ë°°ì¹˜ ì²˜ë¦¬ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
def process_articles_in_batches(articles, batch_size=2):
    for i in range(0, len(articles), batch_size):
        batch = articles[i:i+batch_size]
        yield batch
```

### 8.2 ì„±ëŠ¥ ìµœì í™”

**ë¹„ë™ê¸° ì²˜ë¦¬:**

```python
import asyncio
import aiohttp

async def async_scrape_articles(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [scrape_single_article(session, url) for url in urls]
        return await asyncio.gather(*tasks)
```

**ìºì‹± êµ¬í˜„:**

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=100)
def cached_summarize(text_hash, text):
    return summarize_article(text)

def get_text_hash(text):
    return hashlib.md5(text.encode()).hexdigest()
```

## 9. ê²°ë¡  ë° í™•ì¥ ë°©í–¥

### 9.1 êµ¬ì¶•í•œ ì‹œìŠ¤í…œì˜ ê°€ì¹˜

ì´ íŠœí† ë¦¬ì–¼ì„ í†µí•´ êµ¬ì¶•í•œ ì‹œìŠ¤í…œì˜ ì£¼ìš” íŠ¹ì§•:

- **ì™„ì „í•œ ë¡œì»¬ í™˜ê²½**: ì™¸ë¶€ API ì˜ì¡´ì„± ì—†ì´ LM Studioì—ì„œ ì‹¤í–‰
- **ì‹¤ìš©ì ì¸ ê¸°ëŠ¥**: ì›¹ ìŠ¤í¬ë˜í•‘, ìš”ì•½, ê°ì • ë¶„ì„, í‚¤ì›Œë“œ ì¶”ì¶œ
- **ì‚¬ìš©ì ì¹œí™”ì  ì¸í„°í˜ì´ìŠ¤**: CLIì™€ ì›¹ UI ëª¨ë‘ ì œê³µ
- **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: ìƒˆë¡œìš´ ë„êµ¬ì™€ ê¸°ëŠ¥ì„ ì‰½ê²Œ ì¶”ê°€

### 9.2 í–¥í›„ í™•ì¥ ê³„íš

**ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ ì¶”ê°€:**

- ì´ë¯¸ì§€ ë¶„ì„ ë° OCR
- ìŒì„± ì¸ì‹ ë° TTS
- PDF/ë¬¸ì„œ ì²˜ë¦¬

**ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥:**

- íŠ¸ë Œë“œ ë¶„ì„ ë° ì˜ˆì¸¡
- ë‹¤êµ­ì–´ ì§€ì›
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

**ì—”í„°í”„ë¼ì´ì¦ˆ ê¸°ëŠ¥:**

- ì‚¬ìš©ì ê¶Œí•œ ê´€ë¦¬
- API í‚¤ ê´€ë¦¬
- ìƒì„¸í•œ ë¡œê¹… ë° ê°ì‚¬

### 9.3 ìµœì¢… ê¶Œì¥ì‚¬í•­

1. **ì ì§„ì  í™•ì¥**: ê¸°ë³¸ ê¸°ëŠ¥ë¶€í„° ì‹œì‘í•´ì„œ í•„ìš”ì— ë”°ë¼ í™•ì¥
2. **ì•ˆì •ì„± ìš°ì„ **: ì—ëŸ¬ í•¸ë“¤ë§ê³¼ í´ë°± ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„
3. **ì‚¬ìš©ì í”¼ë“œë°±**: ì‹¤ì œ ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ê°œì„ 
4. **ë³´ì•ˆ ê³ ë ¤**: ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œ Rate Limiting ë° Robot.txt ì¤€ìˆ˜

ì´ ì‹œìŠ¤í…œì„ ê¸°ë°˜ìœ¼ë¡œ ë”ìš± ì •êµí•˜ê³  ì‹¤ìš©ì ì¸ AI Agent ì†”ë£¨ì…˜ì„ êµ¬ì¶•í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.

---

*ì´ íŠœí† ë¦¬ì–¼ì˜ ì „ì²´ ì†ŒìŠ¤ì½”ë“œëŠ” [GitHub](https://github.com/your-repo/lmstudio-agent-tutorial)ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.*

```

ì´ ê¸€ì€ ë„ˆë¬´ ê¸¸ì–´ì„œ ê³„ì†í•´ì„œ UI ë¶€ë¶„ê³¼ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì¶”ê°€ë¡œ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.
