---
title: "SpiceAI Private í´ë¼ìš°ë“œ AI í”Œë«í¼ ì™„ì „ í™œìš© ê°€ì´ë“œ - ì—”í„°í”„ë¼ì´ì¦ˆ ë°ì´í„° í†µí•©ë¶€í„° AI ì¶”ë¡ ê¹Œì§€"
excerpt: "Apache-2.0 ë¼ì´ì„ ìŠ¤ì˜ SpiceAIë¥¼ í™œìš©í•˜ì—¬ private í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ê³ ì„±ëŠ¥ AI í”Œë«í¼ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì‹¤ì œ í™œìš© ì‚¬ë¡€ì™€ í•¨ê»˜ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤."
seo_title: "SpiceAI Private í´ë¼ìš°ë“œ AI í”Œë«í¼ êµ¬ì¶• ê°€ì´ë“œ - ì—”í„°í”„ë¼ì´ì¦ˆ AI ì†”ë£¨ì…˜ - Thaki Cloud"
seo_description: "SpiceAIë¡œ private í´ë¼ìš°ë“œ AI í”Œë«í¼ êµ¬ì¶•í•˜ê¸°. ë°ì´í„° í˜ë”ë ˆì´ì…˜, ì‹¤ì‹œê°„ ML ì¶”ë¡ , RAG ì‹œìŠ¤í…œ ë“± ì—”í„°í”„ë¼ì´ì¦ˆ AI í™œìš© ì‚¬ë¡€ ì™„ì „ ê°€ì´ë“œ."
date: 2025-07-17
last_modified_at: 2025-07-17
categories:
  - tutorials
  - iaas
tags:
  - SpiceAI
  - Private-Cloud
  - AI-Platform
  - Data-Federation
  - RAG
  - ML-Inference
  - Apache-2.0
  - Kubernetes
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/tutorials/spiceai-private-cloud-ai-platform-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 22ë¶„

## ì„œë¡ 

**SpiceAI**ëŠ” Apache-2.0 ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ AI ì¸í”„ë¼ í”Œë«í¼ìœ¼ë¡œ, Rustë¡œ ì‘ì„±ëœ ê³ ì„±ëŠ¥ SQL ì¿¼ë¦¬, ê²€ìƒ‰, LLM ì¶”ë¡  ì—”ì§„ì…ë‹ˆë‹¤. Private í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ AI í”Œë«í¼ì„ êµ¬ì¶•í•˜ëŠ” íŒ€ë“¤ì—ê²Œ **ë°ì´í„° í†µí•©ë¶€í„° AI ì¶”ë¡ ê¹Œì§€ í†µí•©ëœ ì†”ë£¨ì…˜**ì„ ì œê³µí•©ë‹ˆë‹¤.

íŠ¹íˆ **ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½**ì—ì„œ ë¶„ì‚°ëœ ë°ì´í„° ì†ŒìŠ¤ë“¤ì„ í†µí•©í•˜ê³ , ì‹¤ì‹œê°„ ML ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ë©°, RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ”ë° ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” private í´ë¼ìš°ë“œ íšŒì‚¬ì˜ AI í”Œë«í¼ íŒ€ì´ SpiceAIë¥¼ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆëŠ”ì§€ ì‹¤ì œ ì‚¬ë¡€ì™€ í•¨ê»˜ ìƒì„¸íˆ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

## ğŸ“„ ë¼ì´ì„ ìŠ¤ ë¶„ì„ ë° ì—”í„°í”„ë¼ì´ì¦ˆ ì í•©ì„±

### Apache-2.0 ë¼ì´ì„ ìŠ¤ ê°œìš”

SpiceAIëŠ” **Apache License 2.0**ì„ ì‚¬ìš©í•˜ë©°, ì´ëŠ” ìƒì—…ì  í™œìš©ì— ë§¤ìš° ê´€ëŒ€í•œ ë¼ì´ì„ ìŠ¤ì…ë‹ˆë‹¤.

### ì—”í„°í”„ë¼ì´ì¦ˆ ì‚¬ìš© ê°€ëŠ¥ì„± ë§¤íŠ¸ë¦­ìŠ¤

| ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ | í—ˆìš© ì—¬ë¶€ | ì†ŒìŠ¤ì½”ë“œ ê³µê°œ ì˜ë¬´ | ì¶”ê°€ ì¡°ê±´ |
|---------------|-----------|-------------------|-----------|
| **Private í´ë¼ìš°ë“œ ë°°í¬** | âœ… **ì™„ì „ í—ˆìš©** | âŒ **ë¶ˆí•„ìš”** | ì—†ìŒ |
| **ê³ ê°ì‚¬ ì„œë¹„ìŠ¤ ì œê³µ** | âœ… **ì™„ì „ í—ˆìš©** | âŒ **ë¶ˆí•„ìš”** | ë¼ì´ì„ ìŠ¤ ê³ ì§€ë§Œ í•„ìš” |
| **ìˆ˜ì • í›„ ë‚´ë¶€ ë°°í¬** | âœ… **ì™„ì „ í—ˆìš©** | âŒ **ë¶ˆí•„ìš”** | ë¼ì´ì„ ìŠ¤ ê³ ì§€ë§Œ í•„ìš” |
| **í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ìƒí’ˆí™”** | âœ… **ì™„ì „ í—ˆìš©** | âŒ **ë¶ˆí•„ìš”** | ë¼ì´ì„ ìŠ¤ ê³ ì§€ë§Œ í•„ìš” |
| **ì„ë² ë””ë“œ ì œí’ˆ í¬í•¨** | âœ… **ì™„ì „ í—ˆìš©** | âŒ **ë¶ˆí•„ìš”** | ë¼ì´ì„ ìŠ¤ ê³ ì§€ë§Œ í•„ìš” |

### ğŸ¢ ì—”í„°í”„ë¼ì´ì¦ˆ ê¶Œì¥ ì‚¬ìš© ë°©ì‹

```yaml
ê¶Œì¥ì‚¬í•­:
  ìƒì—…ì _ì‚¬ìš©: "ì™„ì „ ììœ  - ì œì•½ ì—†ìŒ"
  ìˆ˜ì •_ë°°í¬: "ììœ ë¡­ê²Œ ê°€ëŠ¥"
  íŠ¹í—ˆ_ë³´í˜¸: "Apache-2.0 íŠ¹í—ˆ ë³´í˜¸ ì¡°í•­ ì ìš©"
  ì—”í„°í”„ë¼ì´ì¦ˆ_ì§€ì›: "ì»¤ë®¤ë‹ˆí‹° ë° ìƒìš© ì§€ì› ëª¨ë‘ ê°€ëŠ¥"
  
ì¤€ìˆ˜ì‚¬í•­:
  ë¼ì´ì„ ìŠ¤_ê³ ì§€: "ë°°í¬ ì‹œ LICENSE íŒŒì¼ í¬í•¨"
  ì €ì‘ê¶Œ_í‘œì‹œ: "ì›ë³¸ ì €ì‘ê¶Œ ê³ ì§€ ìœ ì§€"
  ë³€ê²½ì‚¬í•­_í‘œì‹œ: "ìˆ˜ì • ì‚¬í•­ì´ ìˆë‹¤ë©´ NOTICE íŒŒì¼ì— ê¸°ë¡"
```

**ê²°ë¡ **: Private í´ë¼ìš°ë“œ íšŒì‚¬ì—ì„œ **ì™„ì „íˆ ììœ ë¡­ê²Œ ìƒì—…ì  í™œìš© ê°€ëŠ¥**

## ğŸ¯ Private í´ë¼ìš°ë“œ AI í”Œë«í¼ í•µì‹¬ í™œìš© ì‚¬ë¡€

### 1. ë°ì´í„° í˜ë”ë ˆì´ì…˜ ë° í†µí•© í”Œë«í¼

#### ë¬¸ì œ ìƒí™©
```
- ì—¬ëŸ¬ ë°ì´í„°ë² ì´ìŠ¤ì— ë¶„ì‚°ëœ ê³ ê° ë°ì´í„°
- ì‹¤ì‹œê°„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° í†µí•© í•„ìš”
- ê° ì‹œìŠ¤í…œë³„ë¡œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ê³¼ API
- ë°ì´í„° ì´ë™ ì—†ì´ ì¿¼ë¦¬ ìˆ˜í–‰ ìš”êµ¬ì‚¬í•­
```

#### SpiceAI ì†”ë£¨ì…˜
```rust
// spicepod.yml - ë°ì´í„° í˜ë”ë ˆì´ì…˜ ì„¤ì •
version: v1beta1
kind: Spicepod
metadata:
  name: enterprise-data-federation

datasets:
  # PostgreSQL ê³ ê° ë°ì´í„°
  - from: postgresql://user:pass@customer-db:5432/customers
    name: customers
    description: "Customer master data"
    params:
      sql: |
        SELECT customer_id, name, email, created_at, tier
        FROM customers 
        WHERE active = true
  
  # Snowflake ë¶„ì„ ë°ì´í„°
  - from: snowflake://account.region/warehouse/database/schema
    name: analytics_data
    description: "Customer analytics and behavior data"
    params:
      sql: |
        SELECT customer_id, event_type, event_time, value
        FROM customer_events
        WHERE event_time >= NOW() - INTERVAL '30 days'
  
  # S3 ë¡œê·¸ ë°ì´í„°
  - from: s3://company-logs/user-behavior/
    name: user_logs
    description: "User behavior logs from applications"
    params:
      file_format: parquet
      
  # DynamoDB ì‹¤ì‹œê°„ ì„¸ì…˜ ë°ì´í„°
  - from: dynamodb://us-west-2/user-sessions
    name: active_sessions
    description: "Real-time user session data"

# ë°ì´í„° ê°€ì†í™” ì„¤ì •
models:
  - from: duckdb
    name: customer_360_view
    description: "Unified customer 360 view"
    datasets:
      - customers
      - analytics_data
      - user_logs
      - active_sessions
```

#### ì‹¤ì œ êµ¬í˜„ ì˜ˆì œ
```python
# enterprise_data_platform.py
import spicepy
from datetime import datetime, timedelta
import asyncio

class EnterpriseDataPlatform:
    def __init__(self):
        self.client = spicepy.Client()
        
    async def get_customer_360_view(self, customer_id: str):
        """í†µí•© ê³ ê° 360ë„ ë·° ìƒì„±"""
        
        query = f"""
        WITH customer_base AS (
            SELECT customer_id, name, email, tier, created_at
            FROM customers
            WHERE customer_id = '{customer_id}'
        ),
        recent_activity AS (
            SELECT 
                customer_id,
                COUNT(*) as event_count,
                MAX(event_time) as last_activity,
                SUM(CASE WHEN event_type = 'purchase' THEN value ELSE 0 END) as total_purchases
            FROM analytics_data
            WHERE customer_id = '{customer_id}'
            GROUP BY customer_id
        ),
        session_info AS (
            SELECT 
                customer_id,
                COUNT(*) as active_sessions,
                MAX(last_seen) as last_session
            FROM active_sessions
            WHERE customer_id = '{customer_id}'
            GROUP BY customer_id
        )
        SELECT 
            cb.customer_id,
            cb.name,
            cb.email,
            cb.tier,
            cb.created_at,
            COALESCE(ra.event_count, 0) as total_events,
            ra.last_activity,
            COALESCE(ra.total_purchases, 0) as lifetime_value,
            COALESCE(si.active_sessions, 0) as current_sessions,
            si.last_session
        FROM customer_base cb
        LEFT JOIN recent_activity ra ON cb.customer_id = ra.customer_id
        LEFT JOIN session_info si ON cb.customer_id = si.customer_id
        """
        
        result = await self.client.query(query)
        return result.to_dict()
    
    async def get_real_time_analytics(self, time_window_hours: int = 1):
        """ì‹¤ì‹œê°„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ë°ì´í„°"""
        
        query = f"""
        SELECT 
            DATE_TRUNC('minute', event_time) as time_bucket,
            event_type,
            COUNT(*) as event_count,
            COUNT(DISTINCT customer_id) as unique_users,
            AVG(value) as avg_value,
            SUM(value) as total_value
        FROM analytics_data
        WHERE event_time >= NOW() - INTERVAL '{time_window_hours} hours'
        GROUP BY time_bucket, event_type
        ORDER BY time_bucket DESC, event_count DESC
        """
        
        return await self.client.query(query)
    
    async def detect_anomalies(self):
        """ì‹¤ì‹œê°„ ì´ìƒ íƒì§€"""
        
        query = """
        WITH hourly_metrics AS (
            SELECT 
                DATE_TRUNC('hour', event_time) as hour,
                COUNT(*) as total_events,
                COUNT(DISTINCT customer_id) as unique_users,
                AVG(value) as avg_transaction_value
            FROM analytics_data
            WHERE event_time >= NOW() - INTERVAL '24 hours'
            GROUP BY hour
        ),
        stats AS (
            SELECT 
                AVG(total_events) as avg_events,
                STDDEV(total_events) as stddev_events,
                AVG(unique_users) as avg_users,
                STDDEV(unique_users) as stddev_users
            FROM hourly_metrics
            WHERE hour < DATE_TRUNC('hour', NOW())
        )
        SELECT 
            hm.hour,
            hm.total_events,
            hm.unique_users,
            hm.avg_transaction_value,
            CASE 
                WHEN ABS(hm.total_events - s.avg_events) > 2 * s.stddev_events 
                THEN 'ANOMALY_EVENTS'
                WHEN ABS(hm.unique_users - s.avg_users) > 2 * s.stddev_users 
                THEN 'ANOMALY_USERS'
                ELSE 'NORMAL'
            END as anomaly_status
        FROM hourly_metrics hm
        CROSS JOIN stats s
        WHERE hm.hour = DATE_TRUNC('hour', NOW()) - INTERVAL '1 hour'
        """
        
        return await self.client.query(query)

# ì‚¬ìš© ì˜ˆì œ
async def main():
    platform = EnterpriseDataPlatform()
    
    # ê³ ê° 360ë„ ë·°
    customer_view = await platform.get_customer_360_view("customer_12345")
    print(f"Customer 360 View: {customer_view}")
    
    # ì‹¤ì‹œê°„ ë¶„ì„
    analytics = await platform.get_real_time_analytics(6)  # ìµœê·¼ 6ì‹œê°„
    print(f"Real-time Analytics: {analytics}")
    
    # ì´ìƒ íƒì§€
    anomalies = await platform.detect_anomalies()
    print(f"Anomaly Detection: {anomalies}")

if __name__ == "__main__":
    asyncio.run(main())
```

### 2. ì‹¤ì‹œê°„ ML ì¶”ë¡  í”Œë«í¼

#### ë¬¸ì œ ìƒí™©
```
- ë‹¤ì–‘í•œ ML ëª¨ë¸ë“¤ì˜ ë¶„ì‚° ë°°í¬
- ì‹¤ì‹œê°„ íŠ¹ì„± ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬
- ëª¨ë¸ ì„œë¹™ ì¸í”„ë¼ì˜ ë³µì¡ì„±
- A/B í…ŒìŠ¤íŠ¸ ë° ëª¨ë¸ ë²„ì „ ê´€ë¦¬
```

#### SpiceAI ì†”ë£¨ì…˜
```yaml
# spicepod.yml - ML ì¶”ë¡  í”Œë«í¼ ì„¤ì •
version: v1beta1
kind: Spicepod
metadata:
  name: ml-inference-platform

datasets:
  # ì‹¤ì‹œê°„ íŠ¹ì„± ë°ì´í„°
  - from: kafka://kafka-cluster:9092/feature-stream
    name: real_time_features
    description: "Real-time feature data from Kafka"
    params:
      topic: feature-stream
      consumer_group: spice-ml-platform
      
  # ë°°ì¹˜ íŠ¹ì„± ì €ì¥ì†Œ
  - from: postgresql://features-db:5432/feature_store
    name: batch_features
    description: "Batch processed features"
    
  # ëª¨ë¸ ë©”íƒ€ë°ì´í„°
  - from: postgresql://model-registry:5432/models
    name: model_registry
    description: "ML model registry and metadata"

models:
  # ì¶”ì²œ ëª¨ë¸
  - from: huggingface://company/recommendation-model-v2
    name: recommendation_model
    description: "Product recommendation model"
    
  # ì´ìƒ íƒì§€ ëª¨ë¸
  - from: local://models/anomaly-detection.onnx
    name: anomaly_detector
    description: "Transaction anomaly detection"
    
  # ê°ì • ë¶„ì„ ëª¨ë¸
  - from: openai://gpt-4
    name: sentiment_analyzer
    description: "Customer feedback sentiment analysis"
```

#### ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬í˜„
```python
# ml_inference_platform.py
import spicepy
import asyncio
from typing import Dict, List, Any
import json
from datetime import datetime

class MLInferencePlatform:
    def __init__(self):
        self.client = spicepy.Client()
        self.model_cache = {}
        
    async def get_real_time_features(self, user_id: str) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ íŠ¹ì„± ë°ì´í„° ìˆ˜ì§‘"""
        
        query = f"""
        WITH recent_activity AS (
            SELECT 
                user_id,
                AVG(session_duration) as avg_session_duration,
                COUNT(*) as activity_count,
                MAX(last_action_time) as last_activity
            FROM real_time_features
            WHERE user_id = '{user_id}' 
                AND timestamp >= NOW() - INTERVAL '1 hour'
            GROUP BY user_id
        ),
        batch_profile AS (
            SELECT 
                user_id,
                age_group,
                preferred_category,
                purchase_frequency,
                average_order_value
            FROM batch_features
            WHERE user_id = '{user_id}'
        )
        SELECT 
            ra.user_id,
            ra.avg_session_duration,
            ra.activity_count,
            ra.last_activity,
            bp.age_group,
            bp.preferred_category,
            bp.purchase_frequency,
            bp.average_order_value
        FROM recent_activity ra
        LEFT JOIN batch_profile bp ON ra.user_id = bp.user_id
        """
        
        result = await self.client.query(query)
        return result.to_dict() if result else {}
    
    async def predict_recommendations(self, user_id: str, num_recommendations: int = 5) -> List[Dict]:
        """ìƒí’ˆ ì¶”ì²œ ì˜ˆì¸¡"""
        
        # íŠ¹ì„± ë°ì´í„° ìˆ˜ì§‘
        features = await self.get_real_time_features(user_id)
        
        if not features:
            return []
        
        # SpiceAI ML ëª¨ë¸ì„ í†µí•œ ì¶”ë¡ 
        ml_query = f"""
        SELECT 
            product_id,
            product_name,
            category,
            predicted_score,
            confidence
        FROM ML.PREDICT(
            MODEL recommendation_model,
            SELECT 
                '{user_id}' as user_id,
                {features.get('avg_session_duration', 0)} as session_duration,
                {features.get('activity_count', 0)} as recent_activity,
                '{features.get('age_group', 'unknown')}' as age_group,
                '{features.get('preferred_category', 'general')}' as preferred_category,
                {features.get('purchase_frequency', 0)} as purchase_frequency,
                {features.get('average_order_value', 0)} as avg_order_value
        )
        ORDER BY predicted_score DESC
        LIMIT {num_recommendations}
        """
        
        result = await self.client.query(ml_query)
        return result.to_list()
    
    async def detect_anomalies(self, transaction_data: Dict[str, Any]) -> Dict[str, Any]:
        """ê±°ë˜ ì´ìƒ íƒì§€"""
        
        anomaly_query = f"""
        SELECT 
            anomaly_score,
            is_anomaly,
            anomaly_reasons,
            confidence
        FROM ML.PREDICT(
            MODEL anomaly_detector,
            SELECT 
                {transaction_data['amount']} as amount,
                '{transaction_data['merchant_category']}' as merchant_category,
                {transaction_data['time_since_last_transaction']} as time_since_last,
                '{transaction_data['location']}' as location,
                {transaction_data['user_risk_score']} as user_risk_score
        )
        """
        
        result = await self.client.query(anomaly_query)
        return result.to_dict()
    
    async def analyze_sentiment(self, feedback_text: str) -> Dict[str, Any]:
        """ê³ ê° í”¼ë“œë°± ê°ì • ë¶„ì„"""
        
        sentiment_query = f"""
        SELECT 
            sentiment_label,
            sentiment_score,
            key_phrases,
            emotion_categories
        FROM ML.CHAT(
            MODEL sentiment_analyzer,
            'Analyze the sentiment and extract key insights from this customer feedback: "{feedback_text}"'
        )
        """
        
        result = await self.client.query(sentiment_query)
        return result.to_dict()
    
    async def run_ab_test(self, user_id: str, experiment_name: str) -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ ë° ì¶”ë¡ """
        
        # ì‹¤í—˜ ì„¤ì • ì¡°íšŒ
        experiment_query = f"""
        SELECT 
            model_variant,
            traffic_allocation,
            experiment_status
        FROM model_registry
        WHERE experiment_name = '{experiment_name}'
            AND experiment_status = 'active'
        """
        
        experiments = await self.client.query(experiment_query)
        
        # ì‚¬ìš©ìë¥¼ ì‹¤í—˜ ê·¸ë£¹ì— í• ë‹¹
        user_hash = hash(user_id) % 100
        cumulative_allocation = 0
        
        for exp in experiments.to_list():
            cumulative_allocation += exp['traffic_allocation']
            if user_hash < cumulative_allocation:
                selected_model = exp['model_variant']
                break
        else:
            selected_model = 'control'
        
        # ì„ íƒëœ ëª¨ë¸ë¡œ ì¶”ë¡  ìˆ˜í–‰
        if selected_model == 'recommendation_model_v2':
            return await self.predict_recommendations(user_id)
        else:
            return await self.predict_recommendations(user_id)  # ê¸°ë³¸ ëª¨ë¸

# ì‚¬ìš© ì˜ˆì œ
async def main():
    ml_platform = MLInferencePlatform()
    
    # ì‹¤ì‹œê°„ ì¶”ì²œ
    recommendations = await ml_platform.predict_recommendations("user_67890", 10)
    print(f"Recommendations: {recommendations}")
    
    # ì´ìƒ íƒì§€
    transaction = {
        'amount': 5000,
        'merchant_category': 'electronics',
        'time_since_last_transaction': 120,  # ë¶„
        'location': 'unusual_location',
        'user_risk_score': 0.7
    }
    anomaly_result = await ml_platform.detect_anomalies(transaction)
    print(f"Anomaly Detection: {anomaly_result}")
    
    # ê°ì • ë¶„ì„
    sentiment = await ml_platform.analyze_sentiment(
        "The product quality is excellent but delivery was delayed."
    )
    print(f"Sentiment Analysis: {sentiment}")

if __name__ == "__main__":
    asyncio.run(main())
```

### 3. ì—”í„°í”„ë¼ì´ì¦ˆ RAG ì‹œìŠ¤í…œ

#### ë¬¸ì œ ìƒí™©
```
- ë¶„ì‚°ëœ ë¬¸ì„œì™€ ì§€ì‹ ë² ì´ìŠ¤
- ë‹¤ì–‘í•œ í˜•íƒœì˜ ë¹„êµ¬ì¡°í™” ë°ì´í„°
- ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ ì •í™•í•œ ì •ë³´ ê²€ìƒ‰
- ì‹¤ì‹œê°„ ì§€ì‹ ì—…ë°ì´íŠ¸ ë° ë²¡í„° ì¸ë±ì‹±
```

#### SpiceAI RAG ì‹œìŠ¤í…œ êµ¬í˜„
```yaml
# spicepod.yml - Enterprise RAG ì„¤ì •
version: v1beta1
kind: Spicepod
metadata:
  name: enterprise-rag-system

datasets:
  # ë¬¸ì„œ ì €ì¥ì†Œ
  - from: s3://company-docs/knowledge-base/
    name: knowledge_documents
    description: "Enterprise knowledge base documents"
    params:
      file_format: pdf
      
  # ìœ„í‚¤/ì»¨í”Œë£¨ì–¸ìŠ¤
  - from: https://company.atlassian.net/wiki/api/v2/
    name: confluence_docs
    description: "Confluence documentation"
    
  # ì´ë©”ì¼ ì•„ì¹´ì´ë¸Œ
  - from: postgresql://email-archive:5432/emails
    name: email_archive
    description: "Corporate email archive"
    
  # ì‹¤ì‹œê°„ ì±„íŒ…/ìŠ¬ë™
  - from: slack://workspace/channels
    name: slack_messages
    description: "Slack conversation history"

# ë²¡í„° ê²€ìƒ‰ ë° ì„ë² ë”© ì„¤ì •
models:
  - from: huggingface://sentence-transformers/all-MiniLM-L6-v2
    name: embedding_model
    description: "Document embedding model"
    
  - from: openai://gpt-4
    name: generation_model
    description: "Answer generation model"
    
  - from: chromadb://localhost:8000/enterprise-vectors
    name: vector_store
    description: "Vector database for semantic search"
```

#### RAG ì‹œìŠ¤í…œ êµ¬í˜„
```python
# enterprise_rag_system.py
import spicepy
import asyncio
from typing import List, Dict, Any
import json
from datetime import datetime

class EnterpriseRAGSystem:
    def __init__(self):
        self.client = spicepy.Client()
        
    async def index_documents(self, batch_size: int = 100):
        """ë¬¸ì„œ ë²¡í„°í™” ë° ì¸ë±ì‹±"""
        
        # ìƒˆë¡œìš´ ë¬¸ì„œ ë˜ëŠ” ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œ ì¡°íšŒ
        new_docs_query = """
        SELECT 
            doc_id,
            title,
            content,
            document_type,
            last_modified,
            department,
            access_level
        FROM knowledge_documents
        WHERE last_indexed IS NULL 
           OR last_modified > last_indexed
        ORDER BY last_modified DESC
        LIMIT {batch_size}
        """.format(batch_size=batch_size)
        
        docs = await self.client.query(new_docs_query)
        
        for doc in docs.to_list():
            # ë¬¸ì„œ ì„ë² ë”© ìƒì„±
            embedding_query = f"""
            SELECT 
                doc_id,
                ML.EMBED(
                    MODEL embedding_model,
                    CONCAT(title, ' ', content)
                ) as embedding
            FROM (
                SELECT 
                    '{doc['doc_id']}' as doc_id,
                    '{doc['title']}' as title,
                    '{doc['content'][:2000]}' as content
            ) AS doc_data
            """
            
            embedding_result = await self.client.query(embedding_query)
            
            # ë²¡í„° ì €ì¥ì†Œì— ì €ì¥
            if embedding_result:
                await self.store_vector(doc, embedding_result.to_dict()['embedding'])
                
        print(f"Indexed {len(docs.to_list())} documents")
    
    async def store_vector(self, document: Dict, embedding: List[float]):
        """ë²¡í„° ì €ì¥ì†Œì— ë¬¸ì„œ ì„ë² ë”© ì €ì¥"""
        
        store_query = f"""
        INSERT INTO vector_store (
            doc_id,
            title,
            content_preview,
            embedding,
            metadata
        ) VALUES (
            '{document['doc_id']}',
            '{document['title']}',
            '{document['content'][:200]}...',
            {embedding},
            '{json.dumps({
                'document_type': document['document_type'],
                'department': document['department'],
                'access_level': document['access_level'],
                'last_modified': document['last_modified'].isoformat()
            })}'
        )
        ON CONFLICT (doc_id) 
        DO UPDATE SET 
            embedding = EXCLUDED.embedding,
            metadata = EXCLUDED.metadata,
            updated_at = NOW()
        """
        
        await self.client.query(store_query)
    
    async def semantic_search(self, query: str, user_access_level: str, top_k: int = 5) -> List[Dict]:
        """ì˜ë¯¸ì  ê²€ìƒ‰ ìˆ˜í–‰"""
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding_sql = f"""
        SELECT ML.EMBED(
            MODEL embedding_model,
            '{query}'
        ) as query_embedding
        """
        
        query_embedding_result = await self.client.query(query_embedding_sql)
        query_embedding = query_embedding_result.to_dict()['query_embedding']
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        search_query = f"""
        WITH similarity_scores AS (
            SELECT 
                doc_id,
                title,
                content_preview,
                metadata,
                COSINE_SIMILARITY(embedding, {query_embedding}) as similarity_score
            FROM vector_store
            WHERE JSON_EXTRACT(metadata, '$.access_level') <= '{user_access_level}'
        )
        SELECT 
            doc_id,
            title,
            content_preview,
            similarity_score,
            metadata
        FROM similarity_scores
        WHERE similarity_score > 0.7
        ORDER BY similarity_score DESC
        LIMIT {top_k}
        """
        
        results = await self.client.query(search_query)
        return results.to_list()
    
    async def generate_answer(self, query: str, context_docs: List[Dict], user_id: str) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([
            f"Document: {doc['title']}\nContent: {doc['content_preview']}"
            for doc in context_docs
        ])
        
        # RAG ë‹µë³€ ìƒì„±
        generation_query = f"""
        SELECT 
            response,
            confidence,
            sources_used
        FROM ML.CHAT(
            MODEL generation_model,
            'Context information:
            {context}
            
            Based on the above context, please answer the following question accurately and cite your sources:
            
            Question: {query}
            
            Please provide:
            1. A comprehensive answer based on the context
            2. Source citations
            3. If the information is not available in the context, please state so clearly'
        )
        """
        
        answer_result = await self.client.query(generation_query)
        answer_data = answer_result.to_dict()
        
        # ì‚¬ìš© ë¡œê·¸ ê¸°ë¡
        await self.log_interaction(user_id, query, answer_data, context_docs)
        
        return {
            'answer': answer_data.get('response', ''),
            'confidence': answer_data.get('confidence', 0),
            'sources': [doc['doc_id'] for doc in context_docs],
            'timestamp': datetime.now().isoformat()
        }
    
    async def log_interaction(self, user_id: str, query: str, answer: Dict, sources: List[Dict]):
        """RAG ìƒí˜¸ì‘ìš© ë¡œê¹… ë° ë¶„ì„"""
        
        log_query = f"""
        INSERT INTO rag_interactions (
            user_id,
            query,
            answer,
            sources_count,
            confidence_score,
            timestamp
        ) VALUES (
            '{user_id}',
            '{query}',
            '{answer.get("response", "")}',
            {len(sources)},
            {answer.get("confidence", 0)},
            NOW()
        )
        """
        
        await self.client.query(log_query)
    
    async def get_rag_analytics(self, days: int = 7) -> Dict[str, Any]:
        """RAG ì‹œìŠ¤í…œ ë¶„ì„ ë° ì„±ëŠ¥ ì§€í‘œ"""
        
        analytics_query = f"""
        WITH daily_stats AS (
            SELECT 
                DATE(timestamp) as date,
                COUNT(*) as total_queries,
                AVG(confidence_score) as avg_confidence,
                COUNT(CASE WHEN confidence_score > 0.8 THEN 1 END) as high_confidence_queries,
                COUNT(DISTINCT user_id) as unique_users
            FROM rag_interactions
            WHERE timestamp >= NOW() - INTERVAL '{days} days'
            GROUP BY DATE(timestamp)
        ),
        popular_topics AS (
            SELECT 
                query,
                COUNT(*) as frequency
            FROM rag_interactions
            WHERE timestamp >= NOW() - INTERVAL '{days} days'
            GROUP BY query
            ORDER BY frequency DESC
            LIMIT 10
        )
        SELECT 
            ds.date,
            ds.total_queries,
            ds.avg_confidence,
            ds.high_confidence_queries,
            ds.unique_users,
            pt.query as popular_query,
            pt.frequency
        FROM daily_stats ds
        CROSS JOIN popular_topics pt
        ORDER BY ds.date DESC, pt.frequency DESC
        """
        
        result = await self.client.query(analytics_query)
        return result.to_dict()
    
    async def ask_question(self, query: str, user_id: str, user_access_level: str = 'public') -> Dict[str, Any]:
        """ì „ì²´ RAG í”Œë¡œìš° ì‹¤í–‰"""
        
        # 1. ì˜ë¯¸ì  ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        relevant_docs = await self.semantic_search(query, user_access_level, top_k=5)
        
        if not relevant_docs:
            return {
                'answer': 'I could not find relevant information to answer your question.',
                'confidence': 0,
                'sources': [],
                'timestamp': datetime.now().isoformat()
            }
        
        # 2. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±
        answer = await self.generate_answer(query, relevant_docs, user_id)
        
        return answer

# ì‚¬ìš© ì˜ˆì œ
async def main():
    rag_system = EnterpriseRAGSystem()
    
    # ë¬¸ì„œ ì¸ë±ì‹± (ì •ê¸°ì ìœ¼ë¡œ ì‹¤í–‰)
    await rag_system.index_documents(50)
    
    # ì§ˆë¬¸ ë‹µë³€
    answer = await rag_system.ask_question(
        query="What is our company's remote work policy?",
        user_id="employee_123",
        user_access_level="internal"
    )
    print(f"RAG Answer: {answer}")
    
    # ë¶„ì„ ë°ì´í„°
    analytics = await rag_system.get_rag_analytics(30)  # ìµœê·¼ 30ì¼
    print(f"RAG Analytics: {analytics}")

if __name__ == "__main__":
    asyncio.run(main())
```

### 4. AI ì—ì´ì „íŠ¸ ë°±ì—”ë“œ í”Œë«í¼

#### ë¬¸ì œ ìƒí™©
```
- ë³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œì„¸ìŠ¤ ìë™í™”
- ë‹¤ì¤‘ ë‹¨ê³„ ì˜ì‚¬ê²°ì • ì‹œìŠ¤í…œ
- ì‹¤ì‹œê°„ ë°ì´í„° ê¸°ë°˜ ì•¡ì…˜ ìˆ˜í–‰
- ì—ì´ì „íŠ¸ ê°„ í˜‘ì—… ë° ì›Œí¬í”Œë¡œìš° ê´€ë¦¬
```

#### ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ êµ¬í˜„
```python
# ai_agent_platform.py
import spicepy
import asyncio
from typing import Dict, List, Any, Optional
from enum import Enum
import json
from datetime import datetime, timedelta

class AgentType(Enum):
    CUSTOMER_SERVICE = "customer_service"
    SALES_ASSISTANT = "sales_assistant"
    OPERATIONS_MANAGER = "operations_manager"
    SECURITY_MONITOR = "security_monitor"

class AIAgentPlatform:
    def __init__(self):
        self.client = spicepy.Client()
        self.active_agents = {}
        
    async def initialize_agent_workspace(self):
        """ì—ì´ì „íŠ¸ ì‘ì—… ê³µê°„ ì´ˆê¸°í™”"""
        
        workspace_setup = """
        -- ì—ì´ì „íŠ¸ ìƒíƒœ í…Œì´ë¸”
        CREATE TABLE IF NOT EXISTS agent_states (
            agent_id VARCHAR PRIMARY KEY,
            agent_type VARCHAR,
            status VARCHAR,
            current_task TEXT,
            context JSON,
            last_activity TIMESTAMP,
            performance_metrics JSON
        );
        
        -- ì—ì´ì „íŠ¸ ê°„ ë©”ì‹œì§€ í
        CREATE TABLE IF NOT EXISTS agent_messages (
            message_id UUID PRIMARY KEY,
            from_agent VARCHAR,
            to_agent VARCHAR,
            message_type VARCHAR,
            payload JSON,
            status VARCHAR DEFAULT 'pending',
            created_at TIMESTAMP DEFAULT NOW(),
            processed_at TIMESTAMP
        );
        
        -- íƒœìŠ¤í¬ ê´€ë¦¬
        CREATE TABLE IF NOT EXISTS agent_tasks (
            task_id UUID PRIMARY KEY,
            assigned_agent VARCHAR,
            task_type VARCHAR,
            priority INTEGER,
            status VARCHAR DEFAULT 'pending',
            input_data JSON,
            output_data JSON,
            created_at TIMESTAMP DEFAULT NOW(),
            completed_at TIMESTAMP
        );
        """
        
        await self.client.query(workspace_setup)
    
    async def deploy_customer_service_agent(self, agent_id: str):
        """ê³ ê° ì„œë¹„ìŠ¤ ì—ì´ì „íŠ¸ ë°°í¬"""
        
        agent_config = {
            'agent_id': agent_id,
            'agent_type': AgentType.CUSTOMER_SERVICE.value,
            'capabilities': [
                'handle_customer_inquiries',
                'escalate_complex_issues',
                'update_customer_records',
                'generate_support_tickets'
            ],
            'data_sources': [
                'customer_database',
                'support_knowledge_base',
                'previous_interactions'
            ]
        }
        
        # ì—ì´ì „íŠ¸ ë“±ë¡
        register_query = f"""
        INSERT INTO agent_states (
            agent_id, agent_type, status, context, last_activity
        ) VALUES (
            '{agent_id}',
            '{AgentType.CUSTOMER_SERVICE.value}',
            'active',
            '{json.dumps(agent_config)}',
            NOW()
        )
        ON CONFLICT (agent_id) DO UPDATE SET
            status = 'active',
            context = EXCLUDED.context,
            last_activity = NOW()
        """
        
        await self.client.query(register_query)
        
        # ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì„¤ì •
        workflow_query = f"""
        WITH agent_workflow AS (
            SELECT 
                '{agent_id}' as agent_id,
                'customer_inquiry' as trigger_event,
                JSON_BUILD_OBJECT(
                    'steps', JSON_BUILD_ARRAY(
                        JSON_BUILD_OBJECT('action', 'analyze_inquiry', 'timeout', 30),
                        JSON_BUILD_OBJECT('action', 'search_knowledge_base', 'timeout', 60),
                        JSON_BUILD_OBJECT('action', 'generate_response', 'timeout', 45),
                        JSON_BUILD_OBJECT('action', 'update_customer_record', 'timeout', 15)
                    )
                ) as workflow_config
        )
        SELECT agent_id, workflow_config FROM agent_workflow
        """
        
        return await self.client.query(workflow_query)
    
    async def handle_customer_inquiry(self, agent_id: str, customer_id: str, inquiry: str) -> Dict[str, Any]:
        """ê³ ê° ë¬¸ì˜ ì²˜ë¦¬"""
        
        # 1. ê³ ê° ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        context_query = f"""
        WITH customer_profile AS (
            SELECT 
                customer_id,
                name,
                tier,
                total_purchases,
                last_interaction
            FROM customers
            WHERE customer_id = '{customer_id}'
        ),
        recent_interactions AS (
            SELECT 
                interaction_type,
                description,
                resolution_status,
                timestamp
            FROM customer_interactions
            WHERE customer_id = '{customer_id}'
                AND timestamp >= NOW() - INTERVAL '30 days'
            ORDER BY timestamp DESC
            LIMIT 5
        )
        SELECT 
            cp.*,
            JSON_AGG(ri.*) as recent_interactions
        FROM customer_profile cp
        LEFT JOIN recent_interactions ri ON TRUE
        GROUP BY cp.customer_id, cp.name, cp.tier, cp.total_purchases, cp.last_interaction
        """
        
        customer_context = await self.client.query(context_query)
        
        # 2. ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰
        knowledge_search = f"""
        SELECT 
            solution_id,
            problem_description,
            solution_steps,
            success_rate,
            category
        FROM support_knowledge_base
        WHERE SIMILARITY(problem_description, '{inquiry}') > 0.7
        ORDER BY success_rate DESC, SIMILARITY(problem_description, '{inquiry}') DESC
        LIMIT 3
        """
        
        knowledge_results = await self.client.query(knowledge_search)
        
        # 3. AI ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        response_generation = f"""
        SELECT 
            response,
            confidence,
            escalation_needed,
            follow_up_actions
        FROM ML.CHAT(
            MODEL customer_service_model,
            'Customer Context: {customer_context.to_dict() if customer_context else "{}"}
             
             Knowledge Base Results: {knowledge_results.to_list() if knowledge_results else []}
             
             Customer Inquiry: {inquiry}
             
             Please provide:
             1. A helpful and empathetic response
             2. Confidence level (0-1)
             3. Whether escalation is needed
             4. Any follow-up actions required
             
             Format as JSON with keys: response, confidence, escalation_needed, follow_up_actions'
        )
        """
        
        ai_response = await self.client.query(response_generation)
        response_data = json.loads(ai_response.to_dict().get('response', '{}'))
        
        # 4. ìƒí˜¸ì‘ìš© ê¸°ë¡
        interaction_log = f"""
        INSERT INTO customer_interactions (
            customer_id,
            agent_id,
            interaction_type,
            inquiry,
            response,
            confidence_score,
            escalation_needed,
            timestamp
        ) VALUES (
            '{customer_id}',
            '{agent_id}',
            'chat_inquiry',
            '{inquiry}',
            '{response_data.get("response", "")}',
            {response_data.get("confidence", 0)},
            {response_data.get("escalation_needed", False)},
            NOW()
        )
        """
        
        await self.client.query(interaction_log)
        
        # 5. ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš” ì‹œ ì²˜ë¦¬
        if response_data.get('escalation_needed', False):
            await self.escalate_to_human_agent(customer_id, inquiry, response_data)
        
        return {
            'response': response_data.get('response', ''),
            'confidence': response_data.get('confidence', 0),
            'escalation_needed': response_data.get('escalation_needed', False),
            'agent_id': agent_id,
            'timestamp': datetime.now().isoformat()
        }
    
    async def deploy_sales_assistant_agent(self, agent_id: str):
        """ì„¸ì¼ì¦ˆ ì–´ì‹œìŠ¤í„´íŠ¸ ì—ì´ì „íŠ¸ ë°°í¬"""
        
        sales_agent_config = {
            'agent_id': agent_id,
            'agent_type': AgentType.SALES_ASSISTANT.value,
            'capabilities': [
                'lead_qualification',
                'product_recommendations',
                'pricing_analysis',
                'proposal_generation',
                'follow_up_scheduling'
            ]
        }
        
        # ì„¸ì¼ì¦ˆ ì—ì´ì „íŠ¸ ë“±ë¡ ë° ì›Œí¬í”Œë¡œìš° ì„¤ì •
        register_query = f"""
        INSERT INTO agent_states (
            agent_id, agent_type, status, context, last_activity
        ) VALUES (
            '{agent_id}',
            '{AgentType.SALES_ASSISTANT.value}',
            'active',
            '{json.dumps(sales_agent_config)}',
            NOW()
        )
        """
        
        await self.client.query(register_query)
    
    async def qualify_lead(self, agent_id: str, lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë¦¬ë“œ ìê²© í‰ê°€"""
        
        # ë¦¬ë“œ ìŠ¤ì½”ì–´ë§
        scoring_query = f"""
        WITH lead_analysis AS (
            SELECT 
                '{lead_data.get("company_size", "unknown")}' as company_size,
                '{lead_data.get("industry", "unknown")}' as industry,
                '{lead_data.get("budget_range", "unknown")}' as budget_range,
                '{lead_data.get("urgency", "unknown")}' as urgency,
                '{lead_data.get("decision_maker", "unknown")}' as decision_maker
        ),
        scoring AS (
            SELECT 
                CASE 
                    WHEN company_size IN ('enterprise', 'large') THEN 30
                    WHEN company_size = 'medium' THEN 20
                    WHEN company_size = 'small' THEN 10
                    ELSE 5
                END as size_score,
                CASE 
                    WHEN industry IN ('technology', 'finance', 'healthcare') THEN 25
                    WHEN industry IN ('retail', 'manufacturing') THEN 15
                    ELSE 10
                END as industry_score,
                CASE 
                    WHEN budget_range = 'high' THEN 25
                    WHEN budget_range = 'medium' THEN 15
                    WHEN budget_range = 'low' THEN 5
                    ELSE 0
                END as budget_score,
                CASE 
                    WHEN urgency = 'immediate' THEN 20
                    WHEN urgency = 'soon' THEN 15
                    WHEN urgency = 'future' THEN 5
                    ELSE 0
                END as urgency_score
            FROM lead_analysis
        )
        SELECT 
            size_score + industry_score + budget_score + urgency_score as total_score,
            CASE 
                WHEN size_score + industry_score + budget_score + urgency_score >= 80 THEN 'hot'
                WHEN size_score + industry_score + budget_score + urgency_score >= 60 THEN 'warm'
                WHEN size_score + industry_score + budget_score + urgency_score >= 40 THEN 'cold'
                ELSE 'unqualified'
            END as lead_status
        FROM scoring
        """
        
        score_result = await self.client.query(scoring_query)
        
        # AI ê¸°ë°˜ ë¦¬ë“œ ë¶„ì„
        analysis_query = f"""
        SELECT 
            analysis,
            recommendations,
            next_actions
        FROM ML.CHAT(
            MODEL sales_assistant_model,
            'Lead Information: {json.dumps(lead_data)}
             
             Lead Score: {score_result.to_dict() if score_result else {}}
             
             Please provide:
             1. Detailed lead analysis
             2. Specific recommendations for engagement
             3. Next action steps
             4. Estimated conversion probability
             
             Format as JSON with keys: analysis, recommendations, next_actions, conversion_probability'
        )
        """
        
        ai_analysis = await self.client.query(analysis_query)
        analysis_data = json.loads(ai_analysis.to_dict().get('analysis', '{}'))
        
        return {
            'lead_score': score_result.to_dict() if score_result else {},
            'ai_analysis': analysis_data,
            'agent_id': agent_id,
            'timestamp': datetime.now().isoformat()
        }
    
    async def deploy_operations_manager_agent(self, agent_id: str):
        """ìš´ì˜ ê´€ë¦¬ ì—ì´ì „íŠ¸ ë°°í¬"""
        
        ops_config = {
            'agent_id': agent_id,
            'agent_type': AgentType.OPERATIONS_MANAGER.value,
            'capabilities': [
                'resource_optimization',
                'anomaly_detection',
                'capacity_planning',
                'incident_response',
                'performance_monitoring'
            ]
        }
        
        # ìš´ì˜ ì—ì´ì „íŠ¸ ë“±ë¡
        await self.client.query(f"""
        INSERT INTO agent_states (
            agent_id, agent_type, status, context, last_activity
        ) VALUES (
            '{agent_id}',
            '{AgentType.OPERATIONS_MANAGER.value}',
            'active',
            '{json.dumps(ops_config)}',
            NOW()
        )
        """)
    
    async def monitor_system_health(self, agent_id: str) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”"""
        
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        metrics_query = """
        WITH system_metrics AS (
            SELECT 
                service_name,
                AVG(cpu_usage) as avg_cpu,
                AVG(memory_usage) as avg_memory,
                COUNT(CASE WHEN status = 'error' THEN 1 END) as error_count,
                COUNT(*) as total_requests,
                AVG(response_time) as avg_response_time
            FROM system_metrics
            WHERE timestamp >= NOW() - INTERVAL '1 hour'
            GROUP BY service_name
        ),
        anomalies AS (
            SELECT 
                service_name,
                CASE 
                    WHEN avg_cpu > 80 THEN 'HIGH_CPU'
                    WHEN avg_memory > 85 THEN 'HIGH_MEMORY'
                    WHEN avg_response_time > 5000 THEN 'SLOW_RESPONSE'
                    WHEN error_count > total_requests * 0.05 THEN 'HIGH_ERROR_RATE'
                    ELSE 'NORMAL'
                END as anomaly_type
            FROM system_metrics
        )
        SELECT 
            sm.*,
            a.anomaly_type
        FROM system_metrics sm
        JOIN anomalies a ON sm.service_name = a.service_name
        WHERE a.anomaly_type != 'NORMAL'
        """
        
        metrics_result = await self.client.query(metrics_query)
        
        # AI ê¸°ë°˜ ìš´ì˜ ìµœì í™” ì œì•ˆ
        optimization_query = f"""
        SELECT 
            optimization_actions,
            risk_assessment,
            resource_recommendations
        FROM ML.CHAT(
            MODEL operations_model,
            'System Metrics: {metrics_result.to_list() if metrics_result else []}
             
             Based on the system metrics, please provide:
             1. Immediate optimization actions needed
             2. Risk assessment for current anomalies
             3. Resource scaling recommendations
             4. Preventive measures
             
             Format as JSON with keys: optimization_actions, risk_assessment, resource_recommendations'
        )
        """
        
        optimization_result = await self.client.query(optimization_query)
        optimization_data = json.loads(optimization_result.to_dict().get('optimization_actions', '{}'))
        
        # ìë™ ìµœì í™” ì•¡ì…˜ ì‹¤í–‰
        if optimization_data.get('immediate_actions'):
            await self.execute_optimization_actions(optimization_data['immediate_actions'])
        
        return {
            'system_status': metrics_result.to_list() if metrics_result else [],
            'optimizations': optimization_data,
            'agent_id': agent_id,
            'timestamp': datetime.now().isoformat()
        }
    
    async def execute_optimization_actions(self, actions: List[str]):
        """ìµœì í™” ì•¡ì…˜ ìë™ ì‹¤í–‰"""
        
        for action in actions:
            action_query = f"""
            INSERT INTO optimization_actions (
                action_type,
                description,
                status,
                executed_at
            ) VALUES (
                'auto_optimization',
                '{action}',
                'executed',
                NOW()
            )
            """
            await self.client.query(action_query)
    
    async def agent_collaboration_workflow(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ê°„ í˜‘ì—… ì›Œí¬í”Œë¡œìš°"""
        
        # íƒœìŠ¤í¬ ìœ í˜•ì— ë”°ë¥¸ ì—ì´ì „íŠ¸ í• ë‹¹
        task_assignment_query = f"""
        WITH task_requirements AS (
            SELECT 
                '{task_data.get("type", "")}' as task_type,
                '{task_data.get("priority", "medium")}' as priority,
                '{json.dumps(task_data)}' as task_data
        ),
        agent_capabilities AS (
            SELECT 
                agent_id,
                agent_type,
                JSON_EXTRACT(context, '$.capabilities') as capabilities
            FROM agent_states
            WHERE status = 'active'
        )
        SELECT 
            ac.agent_id,
            ac.agent_type,
            tr.task_type,
            tr.priority
        FROM task_requirements tr
        CROSS JOIN agent_capabilities ac
        WHERE JSON_SEARCH(ac.capabilities, 'one', tr.task_type) IS NOT NULL
        ORDER BY 
            CASE tr.priority 
                WHEN 'high' THEN 1
                WHEN 'medium' THEN 2
                WHEN 'low' THEN 3
            END
        LIMIT 1
        """
        
        assignment_result = await self.client.query(task_assignment_query)
        
        if assignment_result:
            assigned_agent = assignment_result.to_dict()
            
            # íƒœìŠ¤í¬ í• ë‹¹ ë° ì‹¤í–‰
            task_execution_query = f"""
            INSERT INTO agent_tasks (
                task_id,
                assigned_agent,
                task_type,
                priority,
                input_data,
                status,
                created_at
            ) VALUES (
                '{task_data.get("task_id", "")}',
                '{assigned_agent["agent_id"]}',
                '{task_data.get("type", "")}',
                CASE '{task_data.get("priority", "medium")}'
                    WHEN 'high' THEN 1
                    WHEN 'medium' THEN 2
                    WHEN 'low' THEN 3
                END,
                '{json.dumps(task_data)}',
                'assigned',
                NOW()
            )
            """
            
            await self.client.query(task_execution_query)
            
            return {
                'assigned_agent': assigned_agent,
                'task_status': 'assigned',
                'estimated_completion': datetime.now() + timedelta(minutes=30)
            }
        
        return {'error': 'No suitable agent available'}

# ì‚¬ìš© ì˜ˆì œ
async def main():
    agent_platform = AIAgentPlatform()
    
    # ì—ì´ì „íŠ¸ í”Œë«í¼ ì´ˆê¸°í™”
    await agent_platform.initialize_agent_workspace()
    
    # ê³ ê° ì„œë¹„ìŠ¤ ì—ì´ì „íŠ¸ ë°°í¬
    await agent_platform.deploy_customer_service_agent("cs_agent_001")
    
    # ê³ ê° ë¬¸ì˜ ì²˜ë¦¬
    customer_response = await agent_platform.handle_customer_inquiry(
        agent_id="cs_agent_001",
        customer_id="customer_12345",
        inquiry="I'm having trouble with my recent order delivery."
    )
    print(f"Customer Service Response: {customer_response}")
    
    # ì„¸ì¼ì¦ˆ ì—ì´ì „íŠ¸ ë°°í¬ ë° ë¦¬ë“œ í‰ê°€
    await agent_platform.deploy_sales_assistant_agent("sales_agent_001")
    
    lead_qualification = await agent_platform.qualify_lead(
        agent_id="sales_agent_001",
        lead_data={
            "company_size": "enterprise",
            "industry": "technology",
            "budget_range": "high",
            "urgency": "immediate",
            "decision_maker": "yes"
        }
    )
    print(f"Lead Qualification: {lead_qualification}")
    
    # ìš´ì˜ ê´€ë¦¬ ì—ì´ì „íŠ¸ ë°°í¬ ë° ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
    await agent_platform.deploy_operations_manager_agent("ops_agent_001")
    
    system_health = await agent_platform.monitor_system_health("ops_agent_001")
    print(f"System Health: {system_health}")

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸš€ ì„¤ì¹˜ ë° ë°°í¬ ê°€ì´ë“œ

### ë¡œì»¬ ê°œë°œ í™˜ê²½

#### 1. Dockerë¥¼ í†µí•œ ë¹ ë¥¸ ì‹œì‘
```bash
# SpiceAI Docker ì´ë¯¸ì§€ ì‹¤í–‰
docker run -p 3000:3000 -p 50051:50051 \
  -v $(pwd)/spicepod.yml:/app/spicepod.yml \
  spiceai/spiceai:latest

# ìƒíƒœ í™•ì¸
curl http://localhost:3000/health

# SQL ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:3000/v1/sql \
  -H "Content-Type: application/json" \
  -d '{"sql": "SELECT 1 as test"}'
```

#### 2. ë¡œì»¬ ë°”ì´ë„ˆë¦¬ ì„¤ì¹˜ (macOS)
```bash
# SpiceAI CLI ì„¤ì¹˜
curl https://install.spiceai.org | /bin/bash

# í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
mkdir my-spice-project
cd my-spice-project
spice init

# ê°œë°œ ì„œë²„ ì‹œì‘
spice run
```

### Private í´ë¼ìš°ë“œ Kubernetes ë°°í¬

#### 1. Helmì„ í†µí•œ ë°°í¬
```bash
# SpiceAI Helm ë¦¬í¬ì§€í† ë¦¬ ì¶”ê°€
helm repo add spiceai https://helm.spiceai.org
helm repo update

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace spiceai-platform

# ì»¤ìŠ¤í…€ values.yaml ìƒì„±
cat > values.yaml << 'EOF'
# Production ì„¤ì •
replicaCount: 3

image:
  repository: spiceai/spiceai
  tag: "1.4.0"
  pullPolicy: IfNotPresent

# ë¦¬ì†ŒìŠ¤ ì œí•œ
resources:
  limits:
    cpu: "2"
    memory: "4Gi"
  requests:
    cpu: "1"
    memory: "2Gi"

# ì˜êµ¬ ì €ì¥ì†Œ ì„¤ì •
stateful:
  enabled: true
  storageClass: "fast-ssd"
  size: "100Gi"
  mountPath: "/data"

# ì„œë¹„ìŠ¤ ì„¤ì •
service:
  type: LoadBalancer
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-internal: "true"
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"

# ëª¨ë‹ˆí„°ë§ í™œì„±í™”
monitoring:
  podMonitor:
    enabled: true
    additionalLabels:
      release: prometheus

# í™˜ê²½ ë³€ìˆ˜
additionalEnv:
  - name: SPICED_LOG
    value: "INFO"
  - name: SPICE_SECRET_DATABASE_URL
    valueFrom:
      secretKeyRef:
        name: spice-secrets
        key: database-url
  - name: SPICE_SECRET_OPENAI_API_KEY
    valueFrom:
      secretKeyRef:
        name: spice-secrets
        key: openai-api-key

# SpicePod ì„¤ì •
spicepod:
  name: enterprise-ai-platform
  version: v1
  kind: Spicepod
  
  datasets:
    - from: postgresql://postgres-primary:5432/enterprise_data
      name: enterprise_data
      description: "Main enterprise database"
      acceleration:
        enabled: true
        engine: duckdb
        mode: file
        params:
          duckdb_file: "/data/enterprise_data.db"
    
    - from: s3://company-datalake/analytics/
      name: analytics_data
      description: "Analytics data lake"
      params:
        file_format: parquet
      acceleration:
        enabled: true
        engine: arrow
        
  models:
    - from: openai://gpt-4
      name: enterprise_llm
      description: "Enterprise LLM for various AI tasks"
    
    - from: huggingface://sentence-transformers/all-MiniLM-L6-v2
      name: embedding_model
      description: "Text embedding model"
EOF

# Secrets ìƒì„±
kubectl create secret generic spice-secrets \
  --from-literal=database-url="postgresql://user:password@postgres:5432/db" \
  --from-literal=openai-api-key="your-openai-api-key" \
  --namespace spiceai-platform

# ë°°í¬
helm upgrade --install spiceai-platform spiceai/spiceai \
  --namespace spiceai-platform \
  --values values.yaml

# ë°°í¬ ìƒíƒœ í™•ì¸
kubectl get pods -n spiceai-platform
kubectl get svc -n spiceai-platform
```

#### 2. ê³ ê°€ìš©ì„± ì„¤ì •
```yaml
# ha-spiceai-values.yaml
replicaCount: 5

# Pod Disruption Budget
podDisruptionBudget:
  enabled: true
  minAvailable: 2

# Anti-affinity for high availability
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - spiceai
        topologyKey: kubernetes.io/hostname

# Horizontal Pod Autoscaler
autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# Load Balancer ì„¤ì •
service:
  type: LoadBalancer
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "tcp"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-internal: "true"

# ë©€í‹° ê°€ìš©ì˜ì—­ ë°°í¬
nodeSelector:
  node.kubernetes.io/instance-type: "c5.2xlarge"

tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "spiceai"
    effect: "NoSchedule"
```

#### 3. ëª¨ë‹ˆí„°ë§ ë° ê´€ì°° ê°€ëŠ¥ì„±
```yaml
# monitoring-setup.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spiceai-grafana-dashboard
  namespace: spiceai-platform
data:
  dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "SpiceAI Enterprise Platform",
        "panels": [
          {
            "title": "Query Performance",
            "type": "graph",
            "targets": [
              {
                "expr": "spiceai_query_duration_seconds",
                "legendFormat": "Query Duration"
              }
            ]
          },
          {
            "title": "Memory Usage",
            "type": "graph",
            "targets": [
              {
                "expr": "spiceai_memory_usage_bytes",
                "legendFormat": "Memory Usage"
              }
            ]
          },
          {
            "title": "Active Connections",
            "type": "singlestat",
            "targets": [
              {
                "expr": "spiceai_active_connections",
                "legendFormat": "Connections"
              }
            ]
          }
        ]
      }
    }

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: spiceai-metrics
  namespace: spiceai-platform
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: spiceai
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
```

### ë³´ì•ˆ ì„¤ì •

#### 1. ë„¤íŠ¸ì›Œí¬ ì •ì±…
```yaml
# network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: spiceai-network-policy
  namespace: spiceai-platform
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: spiceai
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ai-applications
    - podSelector:
        matchLabels:
          app: allowed-client
    ports:
    - protocol: TCP
      port: 3000
    - protocol: TCP
      port: 50051
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: databases
    ports:
    - protocol: TCP
      port: 5432
  - to: []
    ports:
    - protocol: TCP
      port: 443  # HTTPS
    - protocol: TCP
      port: 53   # DNS
    - protocol: UDP
      port: 53   # DNS
```

#### 2. RBAC ì„¤ì •
```yaml
# rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spiceai-service-account
  namespace: spiceai-platform

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: spiceai-cluster-role
rules:
- apiGroups: [""]
  resources: ["pods", "services", "endpoints"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: spiceai-cluster-role-binding
subjects:
- kind: ServiceAccount
  name: spiceai-service-account
  namespace: spiceai-platform
roleRef:
  kind: ClusterRole
  name: spiceai-cluster-role
  apiGroup: rbac.authorization.k8s.io
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™” ë° ìš´ì˜

### 1. ë°ì´í„° ê°€ì†í™” ì „ëµ

#### Arrow ì¸ë©”ëª¨ë¦¬ ê°€ì†í™”
```yaml
# ê³ ì„±ëŠ¥ ì‹¤ì‹œê°„ ì¿¼ë¦¬ìš©
datasets:
  - from: postgresql://analytics-db:5432/real_time_data
    name: real_time_metrics
    acceleration:
      enabled: true
      engine: arrow
      mode: memory
      refresh_check_interval: 1m
      refresh_mode: full
```

#### DuckDB íŒŒì¼ ê¸°ë°˜ ê°€ì†í™”
```yaml
# ëŒ€ìš©ëŸ‰ ë¶„ì„ ì¿¼ë¦¬ìš©
datasets:
  - from: s3://data-lake/historical-data/
    name: historical_analytics
    acceleration:
      enabled: true
      engine: duckdb
      mode: file
      params:
        duckdb_file: "/data/historical.db"
      refresh_check_interval: 1h
      refresh_mode: incremental
```

### 2. í´ëŸ¬ìŠ¤í„° í™•ì¥ ì „ëµ

#### ìˆ˜í‰ í™•ì¥ ì„¤ì •
```bash
# CPU ê¸°ë°˜ ìë™ í™•ì¥
kubectl autoscale deployment spiceai-platform \
  --cpu-percent=70 \
  --min=3 \
  --max=20 \
  --namespace spiceai-platform

# ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ê¸°ë°˜ í™•ì¥
cat > hpa-custom.yaml << 'EOF'
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: spiceai-hpa-custom
  namespace: spiceai-platform
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: spiceai-platform
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Pods
    pods:
      metric:
        name: spiceai_query_queue_length
      target:
        type: AverageValue
        averageValue: "10"
  - type: Pods
    pods:
      metric:
        name: spiceai_memory_utilization
      target:
        type: AverageValue
        averageValue: "80"
EOF

kubectl apply -f hpa-custom.yaml
```

### 3. ë°±ì—… ë° ì¬í•´ ë³µêµ¬

#### ë°ì´í„° ë°±ì—… ì „ëµ
```bash
#!/bin/bash
# spiceai-backup.sh

BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_PATH="/backups/spiceai-${BACKUP_DATE}"

# 1. ì„¤ì • ë°±ì—…
kubectl get configmap -n spiceai-platform -o yaml > "${BACKUP_PATH}/configmaps.yaml"
kubectl get secret -n spiceai-platform -o yaml > "${BACKUP_PATH}/secrets.yaml"

# 2. ë°ì´í„° ë°±ì—… (PVC ìŠ¤ëƒ…ìƒ·)
kubectl get pvc -n spiceai-platform -o json | \
  jq -r '.items[] | select(.metadata.name | contains("spiceai")) | .metadata.name' | \
  while read pvc_name; do
    # ë³¼ë¥¨ ìŠ¤ëƒ…ìƒ· ìƒì„±
    cat > "${BACKUP_PATH}/${pvc_name}-snapshot.yaml" << EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: ${pvc_name}-snapshot-${BACKUP_DATE}
  namespace: spiceai-platform
spec:
  source:
    persistentVolumeClaimName: ${pvc_name}
  volumeSnapshotClassName: csi-aws-vsc
EOF
    kubectl apply -f "${BACKUP_PATH}/${pvc_name}-snapshot.yaml"
  done

# 3. ë©”íƒ€ë°ì´í„° ë°±ì—…
kubectl exec -n spiceai-platform deployment/spiceai-platform -- \
  pg_dump "$DATABASE_URL" > "${BACKUP_PATH}/metadata-backup.sql"

echo "Backup completed: ${BACKUP_PATH}"
```

#### ì¬í•´ ë³µêµ¬ ê³„íš
```bash
#!/bin/bash
# spiceai-restore.sh

RESTORE_PATH="$1"

if [ -z "$RESTORE_PATH" ]; then
  echo "Usage: $0 <backup-path>"
  exit 1
fi

# 1. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì¬ìƒì„±
kubectl create namespace spiceai-platform

# 2. ì„¤ì • ë³µì›
kubectl apply -f "${RESTORE_PATH}/configmaps.yaml"
kubectl apply -f "${RESTORE_PATH}/secrets.yaml"

# 3. ë³¼ë¥¨ ë³µì›
for snapshot_file in "${RESTORE_PATH}"/*-snapshot.yaml; do
  kubectl apply -f "$snapshot_file"
done

# 4. ì• í”Œë¦¬ì¼€ì´ì…˜ ì¬ë°°í¬
helm upgrade --install spiceai-platform spiceai/spiceai \
  --namespace spiceai-platform \
  --values values.yaml

echo "Restore completed from: ${RESTORE_PATH}"
```

## zshrc ë³„ì¹­ ì„¤ì •

```bash
# ~/.zshrcì— ì¶”ê°€í•  SpiceAI ê´€ë ¨ ë³„ì¹­ë“¤

# SpiceAI ê´€ë ¨ ë””ë ‰í† ë¦¬
export SPICEAI_HOME="$HOME/.spiceai"
export SPICEAI_PROJECTS_DIR="$HOME/spiceai-projects"

# ê¸°ë³¸ ë³„ì¹­
alias spice="spice"
alias spice-init="spice init"
alias spice-run="spice run"
alias spice-build="spice build"
alias spice-status="spice status"

# Docker ê´€ë ¨
alias spice-docker="docker run -p 3000:3000 -p 50051:50051 -v \$(pwd):/app spiceai/spiceai:latest"
alias spice-docker-dev="docker run -it -p 3000:3000 -p 50051:50051 -v \$(pwd):/app spiceai/spiceai:latest /bin/bash"

# Kubernetes ê´€ë ¨
alias k-spice="kubectl -n spiceai-platform"
alias spice-logs="kubectl logs -n spiceai-platform -l app.kubernetes.io/name=spiceai"
alias spice-pods="kubectl get pods -n spiceai-platform"
alias spice-svc="kubectl get svc -n spiceai-platform"

# SQL ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
function spice-query() {
    if [ -z "$1" ]; then
        echo "Usage: spice-query 'SQL_QUERY'"
        return 1
    fi
    
    curl -X POST http://localhost:3000/v1/sql \
        -H "Content-Type: application/json" \
        -d "{\"sql\": \"$1\"}" | jq .
}

# SpicePod ê²€ì¦
alias spice-validate="spice validate"
alias spice-fmt="spice fmt"

# ëª¨ë‹ˆí„°ë§
alias spice-metrics="curl -s http://localhost:3000/metrics"
alias spice-health="curl -s http://localhost:3000/health | jq ."

# í”„ë¡œì íŠ¸ ê´€ë¦¬
function spice-new() {
    local project_name="$1"
    if [ -z "$project_name" ]; then
        echo "Usage: spice-new <project-name>"
        return 1
    fi
    
    mkdir -p "$SPICEAI_PROJECTS_DIR/$project_name"
    cd "$SPICEAI_PROJECTS_DIR/$project_name"
    spice init
    
    echo "âœ… SpiceAI í”„ë¡œì íŠ¸ '$project_name' ìƒì„± ì™„ë£Œ"
    echo "ğŸ“ ê²½ë¡œ: $SPICEAI_PROJECTS_DIR/$project_name"
}

# ê°œë°œ í™˜ê²½ ì„¤ì •
function spice-dev-setup() {
    echo "ğŸš€ SpiceAI ê°œë°œ í™˜ê²½ ì„¤ì • ì¤‘..."
    
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    mkdir -p "$SPICEAI_PROJECTS_DIR"
    mkdir -p "$SPICEAI_HOME"
    
    # Docker ì´ë¯¸ì§€ pull
    docker pull spiceai/spiceai:latest
    
    # ê¸°ë³¸ í”„ë¡œì íŠ¸ ìƒì„±
    spice-new "sample-project"
    
    echo "âœ… SpiceAI ê°œë°œ í™˜ê²½ ì„¤ì • ì™„ë£Œ"
}

# ë°±ì—… ê´€ë ¨
function spice-backup() {
    local backup_name="spiceai-backup-$(date +%Y%m%d-%H%M%S)"
    mkdir -p "$HOME/spiceai-backups/$backup_name"
    
    # í”„ë¡œì íŠ¸ ë°±ì—…
    cp -r "$SPICEAI_PROJECTS_DIR" "$HOME/spiceai-backups/$backup_name/projects"
    
    # ì„¤ì • ë°±ì—…
    if [ -d "$SPICEAI_HOME" ]; then
        cp -r "$SPICEAI_HOME" "$HOME/spiceai-backups/$backup_name/config"
    fi
    
    echo "âœ… ë°±ì—… ì™„ë£Œ: $HOME/spiceai-backups/$backup_name"
}

# Helm ê´€ë ¨
alias helm-spice="helm -n spiceai-platform"
alias spice-install="helm upgrade --install spiceai-platform spiceai/spiceai --namespace spiceai-platform"
alias spice-uninstall="helm uninstall spiceai-platform --namespace spiceai-platform"

# ë¡œê·¸ ë° ë””ë²„ê¹…
function spice-debug() {
    echo "=== SpiceAI ë””ë²„ê·¸ ì •ë³´ ==="
    echo "SpiceAI ë²„ì „:"
    spice --version 2>/dev/null || echo "SpiceAI CLI not installed"
    
    echo -e "\nDocker ì´ë¯¸ì§€:"
    docker images spiceai/spiceai 2>/dev/null || echo "No SpiceAI Docker images found"
    
    echo -e "\nKubernetes ìƒíƒœ:"
    k-spice get pods 2>/dev/null || echo "Kubernetes not configured or no pods"
    
    echo -e "\në¡œì»¬ ì„œë²„ ìƒíƒœ:"
    spice-health 2>/dev/null || echo "Local SpiceAI server not running"
}

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
function spice-benchmark() {
    local query="${1:-SELECT 1 as test}"
    local iterations="${2:-100}"
    
    echo "ğŸ”¥ SpiceAI ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘..."
    echo "ì¿¼ë¦¬: $query"
    echo "ë°˜ë³µ íšŸìˆ˜: $iterations"
    
    start_time=$(date +%s%3N)
    
    for i in $(seq 1 $iterations); do
        spice-query "$query" > /dev/null 2>&1
        if [ $((i % 10)) -eq 0 ]; then
            echo "Progress: $i/$iterations"
        fi
    done
    
    end_time=$(date +%s%3N)
    total_time=$((end_time - start_time))
    avg_time=$((total_time / iterations))
    
    echo "âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ"
    echo "ì´ ì‹œê°„: ${total_time}ms"
    echo "í‰ê·  ì‘ë‹µ ì‹œê°„: ${avg_time}ms"
    echo "ì´ˆë‹¹ ì¿¼ë¦¬ ìˆ˜: $((1000 / avg_time))"
}

# ë„ì›€ë§
function spice-help() {
    echo "ğŸŒ¶ï¸  SpiceAI ë³„ì¹­ ë„ì›€ë§"
    echo ""
    echo "ê¸°ë³¸ ëª…ë ¹ì–´:"
    echo "  spice-init         - ìƒˆ SpiceAI í”„ë¡œì íŠ¸ ì´ˆê¸°í™”"
    echo "  spice-run          - SpiceAI ì„œë²„ ì‹¤í–‰"
    echo "  spice-query        - SQL ì¿¼ë¦¬ ì‹¤í–‰"
    echo ""
    echo "í”„ë¡œì íŠ¸ ê´€ë¦¬:"
    echo "  spice-new <name>   - ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±"
    echo "  spice-dev-setup    - ê°œë°œ í™˜ê²½ ì´ˆê¸° ì„¤ì •"
    echo "  spice-backup       - í”„ë¡œì íŠ¸ ë°±ì—…"
    echo ""
    echo "Docker:"
    echo "  spice-docker       - Dockerë¡œ SpiceAI ì‹¤í–‰"
    echo "  spice-docker-dev   - Docker ê°œë°œ ëª¨ë“œ"
    echo ""
    echo "Kubernetes:"
    echo "  k-spice           - kubectl with spiceai namespace"
    echo "  spice-install     - Helmìœ¼ë¡œ ì„¤ì¹˜"
    echo "  spice-logs        - Pod ë¡œê·¸ í™•ì¸"
    echo ""
    echo "ëª¨ë‹ˆí„°ë§:"
    echo "  spice-health      - ì„œë²„ ìƒíƒœ í™•ì¸"
    echo "  spice-metrics     - ë©”íŠ¸ë¦­ í™•ì¸"
    echo "  spice-debug       - ë””ë²„ê·¸ ì •ë³´"
    echo "  spice-benchmark   - ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"
}
```

## ê°œë°œí™˜ê²½ ì •ë³´

```bash
# í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë³´
echo "=== SpiceAI ê°œë°œí™˜ê²½ ì •ë³´ ==="
echo "ë‚ ì§œ: $(date)"
echo "OS: $(uname -a)"
echo "Docker: $(docker --version 2>/dev/null || echo 'Docker not installed')"
echo "Kubernetes: $(kubectl version --client --short 2>/dev/null || echo 'kubectl not installed')"
echo "Helm: $(helm version --short 2>/dev/null || echo 'Helm not installed')"
echo "SpiceAI CLI: $(spice --version 2>/dev/null || echo 'SpiceAI CLI not installed')"
echo "ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: $(free -h 2>/dev/null | grep Mem || vm_stat | head -5)"
echo "ë””ìŠ¤í¬ ê³µê°„: $(df -h . | tail -1)"
```

### ê²€ì¦ëœ í™˜ê²½

ì´ ê°€ì´ë“œëŠ” ë‹¤ìŒ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤:

```
- macOS Sonoma (Apple M4 Pro, 48GB RAM)
- Ubuntu 22.04 LTS (Intel/AMD x86_64)
- Kubernetes 1.28+
- Docker 24.0+
- Helm 3.12+
- SpiceAI 1.4.0+
```

## ê²°ë¡ 

SpiceAIëŠ” Apache-2.0 ë¼ì´ì„ ìŠ¤ í•˜ì—ì„œ **ì™„ì „íˆ ììœ ë¡œìš´ ìƒì—…ì  ì‚¬ìš©ì´ ê°€ëŠ¥**í•œ í˜ì‹ ì ì¸ AI ì¸í”„ë¼ í”Œë«í¼ì…ë‹ˆë‹¤. Private í´ë¼ìš°ë“œ í™˜ê²½ì˜ AI í”Œë«í¼ íŒ€ì—ê²Œ ë‹¤ìŒê³¼ ê°™ì€ ê°•ë ¥í•œ ê°€ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

### ğŸ¯ í•µì‹¬ ê°€ì¹˜ ì œì•ˆ

1. **í†µí•© ë°ì´í„° í”Œë«í¼**: ë¶„ì‚°ëœ ë°ì´í„° ì†ŒìŠ¤ë¥¼ SQLë¡œ í†µí•© ì¿¼ë¦¬
2. **ì‹¤ì‹œê°„ AI ì¶”ë¡ **: ê³ ì„±ëŠ¥ ML ëª¨ë¸ ì„œë¹™ ë° ì¶”ë¡ 
3. **ì—”í„°í”„ë¼ì´ì¦ˆ RAG**: ì§€ëŠ¥í˜• ê²€ìƒ‰ ë° ìƒì„± ì‹œìŠ¤í…œ
4. **AI ì—ì´ì „íŠ¸ ë°±ì—”ë“œ**: ë³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œì„¸ìŠ¤ ìë™í™”
5. **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: Kubernetes ë„¤ì´í‹°ë¸Œ ì„¤ê³„

### ğŸš€ Private í´ë¼ìš°ë“œ íŒ€ ê¶Œì¥ ë„ì… ì „ëµ

1. **POCë¶€í„° ì‹œì‘**: ë‹¨ì¼ ë°ì´í„° ì†ŒìŠ¤ ì—°ë™ìœ¼ë¡œ ê°œë… ì¦ëª…
2. **ì ì§„ì  í™•ì¥**: ì¶”ê°€ ë°ì´í„° ì†ŒìŠ¤ì™€ AI ëª¨ë¸ í†µí•©
3. **ìš´ì˜ ìµœì í™”**: ëª¨ë‹ˆí„°ë§, ë°±ì—…, ê³ ê°€ìš©ì„± êµ¬ì„±
4. **ì „ì‚¬ í™•ì‚°**: ê²€ì¦ëœ íŒ¨í„´ì„ ë‹¤ë¥¸ íŒ€ìœ¼ë¡œ í™•ì‚°

### ğŸ’¡ ì‹¤ë¬´ í™œìš© ì‹œë‚˜ë¦¬ì˜¤

- **ë°ì´í„° íŒ€**: ì‹¤ì‹œê°„ ë°ì´í„° í˜ë”ë ˆì´ì…˜ ë° ë¶„ì„
- **AI/ML íŒ€**: ëª¨ë¸ ì„œë¹™ ë° ì¶”ë¡  ì¸í”„ë¼
- **ì œí’ˆ íŒ€**: RAG ê¸°ë°˜ ì§€ëŠ¥í˜• ì„œë¹„ìŠ¤ êµ¬ì¶•
- **ìš´ì˜ íŒ€**: AI ê¸°ë°˜ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”

SpiceAIë¥¼ í†µí•´ Private í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ **ì°¨ì„¸ëŒ€ AI í”Œë«í¼**ì„ êµ¬ì¶•í•˜ê³ , ë°ì´í„° ì¤‘ì‹¬ì˜ ì§€ëŠ¥í˜• ì„œë¹„ìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸŒ¶ï¸

### ê´€ë ¨ ë§í¬

- [SpiceAI GitHub](https://github.com/spiceai/spiceai)
- [SpiceAI Cloud ë¬¸ì„œ](https://docs.spice.ai/)
- [SpiceAI OSS ë¬¸ì„œ](https://spiceai.org/docs/)
- [Apache-2.0 ë¼ì´ì„ ìŠ¤](https://www.apache.org/licenses/LICENSE-2.0)
- [Kubernetes ë°°í¬ ê°€ì´ë“œ](https://spiceai.org/docs/deployment/kubernetes) 