---
title: "ChatGPT Next Web + vLLM ë¡œì»¬ í™˜ê²½ êµ¬ì¶• ê°€ì´ë“œ - HyperCLOVA X SEED 0.5Bë¡œ ê²½ëŸ‰ í•œêµ­ì–´ AI"
date: 2025-05-28
categories: 
  - tutorials
  - ai
  - local-ai
tags: 
  - ChatGPT
  - vLLM
  - ë¡œì»¬AI
  - ë§¥ë¶
  - ìì²´í˜¸ìŠ¤íŒ…
  - LLM
  - Next.js
  - HyperCLOVA
  - í•œêµ­ì–´AI
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
excerpt: "ChatGPT Next Webê³¼ vLLMì„ ë§¥ë¶ì—ì„œ ì—°ê²°í•˜ì—¬ ë„¤ì´ë²„ í´ë¡œë°”ì˜ HyperCLOVA X SEED 0.5B ëª¨ë¸ë¡œ ê²½ëŸ‰ í•œêµ­ì–´ AI í™˜ê²½ì„ êµ¬ì¶•í•˜ëŠ” ì‹¤ì „ ê°€ì´ë“œ. Python 3.12ì™€ Yarnì„ í™œìš©í•œ ìµœì‹  ê°œë°œ í™˜ê²½ ì„¤ì •ë²•ì„ í¬í•¨í•©ë‹ˆë‹¤."
---

## ê°œìš”

ì´ ê°€ì´ë“œëŠ” ChatGPT Next Webê³¼ vLLMì„ ë§¥ë¶ì—ì„œ ì—°ê²°í•˜ì—¬ ë„¤ì´ë²„ í´ë¡œë°”ì˜ HyperCLOVA X SEED 0.5B ëª¨ë¸ë¡œ ê²½ëŸ‰ í•œêµ­ì–´ AI í™˜ê²½ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. OpenAI API ì—†ì´ë„ ë¡œì»¬ì—ì„œ ê°•ë ¥í•œ LLM ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ìì²´ í˜¸ìŠ¤íŒ… ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.

### ì™œ ë¡œì»¬ í™˜ê²½ì¸ê°€?

- **í”„ë¼ì´ë²„ì‹œ**: ëª¨ë“  ë°ì´í„°ê°€ ë¡œì»¬ì—ì„œ ì²˜ë¦¬
- **ë¹„ìš© ì ˆì•½**: API ì‚¬ìš©ë£Œ ì—†ìŒ
- **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ëª¨ë¸ê³¼ ì„¤ì •ì„ ììœ ë¡­ê²Œ ì¡°ì •
- **ì˜¤í”„ë¼ì¸ ì‚¬ìš©**: ì¸í„°ë„· ì—°ê²° ì—†ì´ë„ AI í™œìš© ê°€ëŠ¥
- **ì‹¤í—˜ í™˜ê²½**: ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ì„¤ì •ì„ ì•ˆì „í•˜ê²Œ í…ŒìŠ¤íŠ¸

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

#### ìµœì†Œ ìš”êµ¬ì‚¬í•­
- **macOS**: 12.0 (Monterey) ì´ìƒ
- **ë©”ëª¨ë¦¬**: 8GB RAM (0.5B ëª¨ë¸ ê¸°ì¤€)
- **ì €ì¥ê³µê°„**: 20GB ì´ìƒ ì—¬ìœ  ê³µê°„
- **í”„ë¡œì„¸ì„œ**: Apple Silicon (M1/M2/M3) ë˜ëŠ” Intel x86_64

#### ê¶Œì¥ ìš”êµ¬ì‚¬í•­
- **ë©”ëª¨ë¦¬**: 16GB RAM ì´ìƒ
- **ì €ì¥ê³µê°„**: 50GB ì´ìƒ SSD
- **í”„ë¡œì„¸ì„œ**: Apple Silicon M1 ì´ìƒ

## 1ë‹¨ê³„: ê°œë°œ í™˜ê²½ ì¤€ë¹„

### Python í™˜ê²½ ì„¤ì •

```bash
# Homebrew ì„¤ì¹˜ (ì—†ëŠ” ê²½ìš°)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Python 3.12 ì„¤ì¹˜
brew install python@3.12

# pyenv ì„¤ì¹˜ (Python ë²„ì „ ê´€ë¦¬)
brew install pyenv
echo 'export PATH="$HOME/.pyenv/bin:$PATH"' >> ~/.zshrc
echo 'eval "$(pyenv init -)"' >> ~/.zshrc
source ~/.zshrc

# Python 3.12 ì„¤ì¹˜ ë° ì„¤ì •
pyenv install 3.12.8
pyenv global 3.12.8
```

### Node.js í™˜ê²½ ì„¤ì •

```bash
# Node.js 22+ ì„¤ì¹˜
brew install node@22

# Yarn ì„¤ì¹˜
npm install -g yarn

# ë²„ì „ í™•ì¸
node --version  # v22.0.0+
yarn --version  # 1.22.0+
```

### Git ì„¤ì • í™•ì¸

```bash
# Git ì„¤ì¹˜ í™•ì¸
git --version

# Git ì„¤ì • (í•„ìš”í•œ ê²½ìš°)
git config --global user.name "Your Name"
git config --global user.email "your.email@example.com"
```

## 2ë‹¨ê³„: vLLM ì„œë²„ ì„¤ì¹˜ ë° ì„¤ì •

### vLLM ì„¤ì¹˜

#### ë°©ë²• 1: pip ì„¤ì¹˜ (ê°„ë‹¨í•œ ë°©ë²•)

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv vllm-env
source vllm-env/bin/activate

# vLLM ì„¤ì¹˜ (Apple Silicon ìµœì í™”)
pip install vllm

# ì¶”ê°€ ì˜ì¡´ì„± ì„¤ì¹˜
pip install torch torchvision torchaudio
pip install transformers accelerate
pip install fastapi uvicorn
```

#### ë°©ë²• 2: ì†ŒìŠ¤ ë¹Œë“œ (macOS Apple Silicon ê¶Œì¥)

macOS Apple Siliconì—ì„œ ìµœì ì˜ ì„±ëŠ¥ì„ ìœ„í•´ì„œëŠ” ì†ŒìŠ¤ì—ì„œ ë¹Œë“œí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

```bash
# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python -m venv vllm-env
source vllm-env/bin/activate

# ì¢…ì† íŒ¨í‚¤ì§€ ì„¤ì¹˜
brew install llvm cmake coreutils wget

# Clang 18+ ì„¤ì • (Apple Silicon ìµœì í™”)
export CC=/opt/homebrew/opt/llvm/bin/clang
export CXX=/opt/homebrew/opt/llvm/bin/clang++

# CPU ë¹Œë“œ ê°•ì œ ì„¤ì • (Apple Siliconì—ì„œ ì•ˆì •ì„± í–¥ìƒ)
export FORCE_CPU=1

# vLLM ì†ŒìŠ¤ í´ë¡  ë° ë¹Œë“œ
git clone https://github.com/vllm-project/vllm.git
cd vllm

# CPU ì „ìš© ìš”êµ¬ì‚¬í•­ ì„¤ì¹˜
pip install -r requirements/cpu.txt
# ì†ŒìŠ¤ì—ì„œ ë¹Œë“œ
pip install -e . 

echo "âœ… vLLM ì†ŒìŠ¤ ë¹Œë“œ ì™„ë£Œ"
```

#### Build image from sourceÂ¶

```bash
# vLLM ì„¤ì¹˜ í™•ì¸
$ docker build -f docker/Dockerfile.cpu --tag vllm-cpu-env --target vllm-openai .

# Launching OpenAI server 
$ docker run --rm \
             --privileged=true \
             --shm-size=4g \
             -p 8000:8000 \
             -e VLLM_CPU_KVCACHE_SPACE=<KV cache space> \
             -e VLLM_CPU_OMP_THREADS_BIND=<CPU cores for inference> \
             vllm-cpu-env \
             --model=meta-llama/Llama-3.2-1B-Instruct \
             --dtype=bfloat16 \
             other vLLM OpenAI server arguments
```

#### ë¹Œë“œ í™•ì¸

```bash
# vLLM ì„¤ì¹˜ í™•ì¸
python -c "import vllm; print(f'vLLM ë²„ì „: {vllm.__version__}')"

# CPU ëª¨ë“œ í™•ì¸
python -c "import vllm; print('vLLM CPU ëª¨ë“œ ì„¤ì¹˜ ì™„ë£Œ')"
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •

#### ì¶”ì²œ ëª¨ë¸ ëª©ë¡

```bash
# 1. HyperCLOVA X SEED 0.5B (ê²½ëŸ‰ í•œêµ­ì–´ ëª¨ë¸)
# ëª¨ë¸ í¬ê¸°: ~1GB
MODEL_NAME="naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B"

MODEL_NAME="Mungert/HyperCLOVAX-SEED-Text-Instruct-0.5B-GGUF"
# 2. CodeLlama 7B (ì½”ë”© íŠ¹í™”)
# ëª¨ë¸ í¬ê¸°: ~13GB  
MODEL_NAME="codellama/CodeLlama-7b-Instruct-hf"

# 3. Mistral 7B (ì„±ëŠ¥ ìš°ìˆ˜)
# ëª¨ë¸ í¬ê¸°: ~13GB
MODEL_NAME="mistralai/Mistral-7B-Instruct-v0.1"

# 4. Qwen 7B (ë‹¤êµ­ì–´ ì§€ì›)
# ëª¨ë¸ í¬ê¸°: ~13GB
MODEL_NAME="Qwen/Qwen-7B-Chat"

# 5. í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸
MODEL_NAME="beomi/KoAlpaca-Polyglot-12.8B"
```

#### ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ

```bash
# Hugging Face CLI ì„¤ì¹˜
pip install huggingface_hub

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì˜ˆ: HyperCLOVA X SEED 0.5B)
huggingface-cli download naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B \
  --local-dir ./models/HyperCLOVAX-SEED-Text-Instruct-0.5Bb \
  --local-dir-use-symlinks False

huggingface-cli download Mungert/HyperCLOVAX-SEED-Text-Instruct-0.5B-GGUF \
  --local-dir ./models/HyperCLOVAX-SEED-Text-Instruct-0.5B-GGUF \
  --local-dir-use-symlinks False

python - <<EOF
from huggingface_hub import hf_hub_download

print(
    hf_hub_download(
        repo_id="Mungert/HyperCLOVAX-SEED-Text-Instruct-0.5B-GGUF",
        filename="HyperCLOVAX-SEED-Text-Instruct-0.5B-f16.gguf"
    )
)
EOF
```

### vLLM ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

```bash
# vllm-server.py íŒŒì¼ ìƒì„±
cat > vllm-server.py << 'EOF'
#!/usr/bin/env python3
"""
vLLM ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ChatGPT Next Webê³¼ í˜¸í™˜ë˜ëŠ” OpenAI API í˜•ì‹ìœ¼ë¡œ ì„œë¹„ìŠ¤ ì œê³µ
"""

import argparse
import asyncio
from vllm import AsyncLLMEngine, AsyncEngineArgs
from vllm.sampling_params import SamplingParams
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
import time
import uuid

app = FastAPI(title="vLLM OpenAI Compatible Server")

# CORS ì„¤ì • (ChatGPT Next Web ì—°ê²°ìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
engine = None
model_name = None

# OpenAI API í˜¸í™˜ ëª¨ë¸ ì •ì˜
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 2048
    stream: Optional[bool] = False
    top_p: Optional[float] = 0.9
    frequency_penalty: Optional[float] = 0.0
    presence_penalty: Optional[float] = 0.0

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, int]

@app.get("/v1/models")
async def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return {
        "object": "list",
        "data": [
            {
                "id": model_name,
                "object": "model",
                "created": int(time.time()),
                "owned_by": "vllm",
                "permission": [],
                "root": model_name,
                "parent": None,
            }
        ]
    }

@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest):
    """ì±„íŒ… ì™„ì„± API (OpenAI í˜¸í™˜)"""
    if engine is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    # ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
    prompt = format_messages_to_prompt(request.messages)
    
    # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
    sampling_params = SamplingParams(
        temperature=request.temperature,
        max_tokens=request.max_tokens,
        top_p=request.top_p,
        frequency_penalty=request.frequency_penalty,
        presence_penalty=request.presence_penalty,
    )
    
    # ì¶”ë¡  ì‹¤í–‰
    request_id = str(uuid.uuid4())
    results = engine.generate(prompt, sampling_params, request_id)
    
    # ê²°ê³¼ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìˆ˜ì§‘
    final_output = None
    async for request_output in results:
        final_output = request_output
    
    if final_output is None:
        raise HTTPException(status_code=500, detail="Generation failed")
    
    # OpenAI API í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ êµ¬ì„±
    response = ChatCompletionResponse(
        id=f"chatcmpl-{request_id}",
        created=int(time.time()),
        model=request.model,
        choices=[
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": final_output.outputs[0].text.strip()
                },
                "finish_reason": "stop"
            }
        ],
        usage={
            "prompt_tokens": len(final_output.prompt_token_ids),
            "completion_tokens": len(final_output.outputs[0].token_ids),
            "total_tokens": len(final_output.prompt_token_ids) + len(final_output.outputs[0].token_ids)
        }
    )
    
    return response

def format_messages_to_prompt(messages: List[ChatMessage]) -> str:
    """ë©”ì‹œì§€ ëª©ë¡ì„ ëª¨ë¸ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
    prompt_parts = []
    
    for message in messages:
        if message.role == "system":
            prompt_parts.append(f"System: {message.content}")
        elif message.role == "user":
            prompt_parts.append(f"Human: {message.content}")
        elif message.role == "assistant":
            prompt_parts.append(f"Assistant: {message.content}")
    
    prompt_parts.append("Assistant:")
    return "\n\n".join(prompt_parts)

async def init_engine(model_path: str, tensor_parallel_size: int = 1):
    """vLLM ì—”ì§„ ì´ˆê¸°í™”"""
    global engine, model_name
    
    engine_args = AsyncEngineArgs(
        model=model_path,
        tensor_parallel_size=tensor_parallel_size,
        dtype="auto",
        trust_remote_code=True,
        max_model_len=4096,  # ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
        gpu_memory_utilization=0.8,  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
    )
    
    engine = AsyncLLMEngine.from_engine_args(engine_args)
    model_name = model_path.split("/")[-1]
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=True, help="ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--host", type=str, default="127.0.0.1", help="ì„œë²„ í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--port", type=int, default=8000, help="ì„œë²„ í¬íŠ¸")
    parser.add_argument("--tensor-parallel-size", type=int, default=1, help="í…ì„œ ë³‘ë ¬ í¬ê¸°")
    
    args = parser.parse_args()
    
    # ì—”ì§„ ì´ˆê¸°í™”
    asyncio.run(init_engine(args.model, args.tensor_parallel_size))
    
    # ì„œë²„ ì‹¤í–‰
    print(f"ğŸš€ vLLM ì„œë²„ ì‹œì‘: http://{args.host}:{args.port}")
    uvicorn.run(app, host=args.host, port=args.port)
EOF

chmod +x vllm-server.py
```

### vLLM ì„œë²„ ì‹¤í–‰

#### ë°©ë²• 1: ì»¤ìŠ¤í…€ Python ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source vllm-env/bin/activate

# ì„œë²„ ì‹¤í–‰ (HyperCLOVA X SEED 0.5B ì˜ˆì‹œ)
python vllm-server.py \
  --model naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B \
  --host 127.0.0.1 \
  --port 8000

# ë˜ëŠ” ë¡œì»¬ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©
python vllm-server.py \
  --model ./models/HyperCLOVAX-SEED-Text-Instruct-0.5B \
  --host 127.0.0.1 \
  --port 8000

python vllm-server.py \
  --model ./models/hyperclovax-f16 \
  --host 127.0.0.1 \
  --port 8000
```

#### ë°©ë²• 2: vLLM CLI ì§ì ‘ ì‚¬ìš© (Apple Silicon CPU ëª¨ë“œ)

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source vllm-env/bin/activate

# CPU ëª¨ë“œë¡œ ì„œë²„ ì‹¤í–‰ (Apple Silicon ìµœì í™”)
vllm serve naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B \
  --device cpu \
  --max-num-seqs 4 \
  --host 127.0.0.1 \
  --port 8000

# ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©
vllm serve ./models/HyperCLOVAX-SEED-Text-Instruct-0.5B \
  --device cpu \
  --max-num-seqs 4 \
  --host 127.0.0.1 \
  --port 8000

vllm serve ./models/hyperclovax-f16 \
  --device cpu \
  --max-num-seqs 4 \
  --host 127.0.0.1 \
  --port 8000

```

#### CPU ëª¨ë“œ ìµœì í™” ì˜µì…˜

```bash
# ë©”ëª¨ë¦¬ ì œí•œ í™˜ê²½ì—ì„œì˜ ìµœì í™” ì‹¤í–‰
vllm serve naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B\
  --device cpu \
  --max-num-seqs 2 \
  --max-model-len 1024 \
  --host 127.0.0.1 \
  --port 8000 \
  --disable-log-requests
```

### ì„œë²„ ìƒíƒœ í™•ì¸

```bash
# ìƒˆ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰
# ëª¨ë¸ ëª©ë¡ í™•ì¸
curl http://localhost:8000/v1/models

# ê°„ë‹¨í•œ ì±„íŒ… í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B",
    "messages": [
      {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”."}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

```

## 3ë‹¨ê³„: ChatGPT Next Web ì„¤ì¹˜ ë° ì„¤ì •

### í”„ë¡œì íŠ¸ í´ë¡  ë° ì„¤ì •

```bash
# ìƒˆ í„°ë¯¸ë„ ì—´ê¸°
# ChatGPT Next Web í´ë¡ 
git clone https://github.com/ChatGPTNextWeb/NextChat.git
cd NextChat

# ì˜ì¡´ì„± ì„¤ì¹˜ (yarn ì‚¬ìš©)
yarn install
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env.local íŒŒì¼ ìƒì„±
cat > .env.local << 'EOF'
# vLLM ì„œë²„ ì„¤ì •
OPENAI_API_KEY=sk-dummy-key-for-local-vllm
BASE_URL=http://localhost:8000/v1

# ì ‘ê·¼ ì œì–´ (ì„ íƒì‚¬í•­)
CODE=1234

# ê¸°ëŠ¥ ì„¤ì •
HIDE_USER_API_KEY=1
DISABLE_GPT4=0
CUSTOM_MODELS=hyperclova-x-seed-0.5b

# ê°œë°œ í™˜ê²½ ì„¤ì •
NEXT_PUBLIC_BUILD_MODE=development
NEXT_PUBLIC_COMMIT_ID=local-vllm
EOF
```

### ì»¤ìŠ¤í…€ ëª¨ë¸ ì„¤ì •

```typescript
// app/constant.ts íŒŒì¼ ìˆ˜ì •
// ë˜ëŠ” ìƒˆë¡œìš´ ì„¤ì • íŒŒì¼ ìƒì„±: app/config/local-models.ts

export const LOCAL_MODELS = [
  {
    name: "hyperclova-x-seed-0.5b",
    displayName: "HyperCLOVA X SEED 0.5B",
    provider: "vLLM",
    maxTokens: 2048,
    description: "ë„¤ì´ë²„ í´ë¡œë°”ì˜ ê²½ëŸ‰ í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸"
  },
  {
    name: "codellama-7b-instruct", 
    displayName: "CodeLlama 7B",
    provider: "vLLM",
    maxTokens: 4096,
    description: "ì½”ë”© íŠ¹í™” ë¡œì»¬ ëª¨ë¸"
  }
];
```

### ChatGPT Next Web ì‹¤í–‰

```bash
# ê°œë°œ ì„œë²„ ì‹œì‘
yarn dev

# ë˜ëŠ” í”„ë¡œë•ì…˜ ë¹Œë“œ
yarn build && yarn start
```

## 4ë‹¨ê³„: ì—°ê²° í…ŒìŠ¤íŠ¸ ë° ìµœì í™”

### ì—°ê²° í™•ì¸

1. **ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†**: `http://localhost:3000`
2. **ì„¤ì • í˜ì´ì§€ ì´ë™**: ìš°ì¸¡ ìƒë‹¨ ì„¤ì • ì•„ì´ì½˜ í´ë¦­
3. **API ì„¤ì • í™•ì¸**:
   - API Host: `http://localhost:8000/v1`
   - API Key: `sk-dummy-key-for-local-vllm`
   - ëª¨ë¸: `hyperclova-x-seed-0.5b`

### ì„±ëŠ¥ ìµœì í™”

#### vLLM ì„œë²„ ìµœì í™”

```bash
# ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰ (0.5B ëª¨ë¸ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ìŒ)
python vllm-server.py \
  --model naver-clova-ix/HyperCLOVA-X-SEED-0.5B \
  --host 127.0.0.1 \
  --port 8000 \
  --max-model-len 2048 \
  --gpu-memory-utilization 0.3 \
  --swap-space 2
```

#### ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§

```bash
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
top -pid $(pgrep -f vllm-server)

# GPU ì‚¬ìš©ëŸ‰ í™•ì¸ (Apple Silicon)
sudo powermetrics --samplers gpu_power -n 1

# ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
lsof -i :8000
lsof -i :3000
```

## 5ë‹¨ê³„: ê³ ê¸‰ ì„¤ì • ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ë‹¤ì¤‘ ëª¨ë¸ ì„œë²„ ì„¤ì •

```bash
# ëª¨ë¸ë³„ í¬íŠ¸ ë¶„ë¦¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
cat > start-multi-models.sh << 'EOF'
#!/bin/bash

# HyperCLOVA X SEED 0.5B (í¬íŠ¸ 8000)
python vllm-server.py \
  --model naver-clova-ix/HyperCLOVA-X-SEED-0.5B \
  --port 8000 &

# CodeLlama 7B (í¬íŠ¸ 8001)  
python vllm-server.py \
  --model codellama/CodeLlama-7b-Instruct-hf \
  --port 8001 &

# í•œêµ­ì–´ ëª¨ë¸ (í¬íŠ¸ 8002)
python vllm-server.py \
  --model beomi/KoAlpaca-Polyglot-12.8B \
  --port 8002 &

echo "ëª¨ë“  ëª¨ë¸ ì„œë²„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
echo "HyperCLOVA X SEED: http://localhost:8000"
echo "CodeLlama: http://localhost:8001" 
echo "KoAlpaca: http://localhost:8002"
EOF

chmod +x start-multi-models.sh
```

### ë¡œë“œ ë°¸ëŸ°ì„œ ì„¤ì •

```nginx
# nginx.conf (Homebrew nginx ì‚¬ìš©)
upstream vllm_backend {
    server 127.0.0.1:8000 weight=3;
    server 127.0.0.1:8001 weight=2;
    server 127.0.0.1:8002 weight=1;
}

server {
    listen 9000;
    server_name localhost;
    
    location /v1/ {
        proxy_pass http://vllm_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_connect_timeout 300s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
    }
}
```

### ìë™ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸

```bash
# auto-start.sh ìƒì„±
cat > auto-start.sh << 'EOF'
#!/bin/bash

echo "ğŸš€ ë¡œì»¬ AI í™˜ê²½ ì‹œì‘ ì¤‘..."

# vLLM ê°€ìƒí™˜ê²½ í™œì„±í™” ë° ì„œë²„ ì‹œì‘
echo "ğŸ“¦ vLLM ì„œë²„ ì‹œì‘..."
source vllm-env/bin/activate

# ë°©ë²• 1: ì»¤ìŠ¤í…€ Python ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
python vllm-server.py \
  --model naver-clova-ix/HyperCLOVA-X-SEED-0.5B \
  --host 127.0.0.1 \
  --port 8000 &

# ë°©ë²• 2: vLLM CLI ì§ì ‘ ì‚¬ìš© (Apple Silicon CPU ëª¨ë“œ)
# vllm serve naver-clova-ix/HyperCLOVA-X-SEED-0.5B \
#   --device cpu \
#   --max-num-seqs 4 \
#   --host 127.0.0.1 \
#   --port 8000 &

VLLM_PID=$!
echo "vLLM ì„œë²„ PID: $VLLM_PID"

# ì„œë²„ ì‹œì‘ ëŒ€ê¸°
sleep 10

# ChatGPT Next Web ì‹œì‘
echo "ğŸŒ ChatGPT Next Web ì‹œì‘..."
cd NextChat
yarn dev &

NEXTJS_PID=$!
echo "Next.js ì„œë²„ PID: $NEXTJS_PID"

echo "âœ… ëª¨ë“  ì„œë¹„ìŠ¤ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!"
echo "ğŸ“± ChatGPT Next Web: http://localhost:3000"
echo "ğŸ¤– vLLM API: http://localhost:8000"

# ì¢…ë£Œ ì‹œ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
trap "echo 'ğŸ›‘ ì„œë¹„ìŠ¤ ì¢…ë£Œ ì¤‘...'; kill $VLLM_PID $NEXTJS_PID; exit" INT TERM

# ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ëŒ€ê¸°
wait
EOF

chmod +x auto-start.sh
```

## 6ë‹¨ê³„: ë¬¸ì œ í•´ê²° ë° ë””ë²„ê¹…

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜

```bash
# ì¦ìƒ: OOM (Out of Memory) ì—ëŸ¬
# í•´ê²°ë°©ë²•: (0.5B ëª¨ë¸ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ì–´ ë¬¸ì œê°€ ì ìŒ)
python vllm-server.py \
  --model naver-clova-ix/HyperCLOVA-X-SEED-0.5B \
  --max-model-len 1024 \
  --gpu-memory-utilization 0.2 \
  --swap-space 1
```

#### 2. ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨

```bash
# ëª¨ë¸ ìºì‹œ ì •ë¦¬
rm -rf ~/.cache/huggingface/transformers/

# ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ
huggingface-cli download naver-clova-ix/HyperCLOVA-X-SEED-0.5B \
  --local-dir ./models/hyperclova-x-seed-0.5b \
  --local-dir-use-symlinks False \
  --resume-download
```

#### 3. API ì—°ê²° ì˜¤ë¥˜

```bash
# í¬íŠ¸ ì¶©ëŒ í™•ì¸
lsof -i :8000
lsof -i :3000

# ë°©í™”ë²½ ì„¤ì • í™•ì¸ (macOS)
sudo pfctl -sr | grep 8000

# ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸
curl -v http://localhost:8000/v1/models
```

#### 4. ì„±ëŠ¥ ì €í•˜ ë¬¸ì œ

```bash
# CPU ì‚¬ìš©ë¥  í™•ì¸
top -o cpu

# ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸
vm_stat

# ë””ìŠ¤í¬ I/O í™•ì¸
iostat -d 1
```

### ë¡œê·¸ ë¶„ì„

```bash
# vLLM ì„œë²„ ë¡œê·¸ í™•ì¸
tail -f vllm-server.log

# Next.js ë¡œê·¸ í™•ì¸
tail -f .next/trace

# ì‹œìŠ¤í…œ ë¡œê·¸ í™•ì¸
log show --predicate 'process == "python"' --last 1h
```

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

```python
# benchmark.py
import time
import requests
import json
import statistics

def benchmark_api(url, messages, num_requests=10):
    """API ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    response_times = []
    
    for i in range(num_requests):
        start_time = time.time()
        
        response = requests.post(f"{url}/v1/chat/completions", 
            headers={"Content-Type": "application/json"},
            json={
                "model": "hyperclova-x-seed-0.5b",
                "messages": messages,
                "max_tokens": 100
            }
        )
        
        end_time = time.time()
        response_times.append(end_time - start_time)
        
        print(f"ìš”ì²­ {i+1}: {response_times[-1]:.2f}ì´ˆ")
    
    print(f"\nğŸ“Š ì„±ëŠ¥ í†µê³„:")
    print(f"í‰ê·  ì‘ë‹µì‹œê°„: {statistics.mean(response_times):.2f}ì´ˆ")
    print(f"ìµœì†Œ ì‘ë‹µì‹œê°„: {min(response_times):.2f}ì´ˆ")
    print(f"ìµœëŒ€ ì‘ë‹µì‹œê°„: {max(response_times):.2f}ì´ˆ")
    print(f"í‘œì¤€í¸ì°¨: {statistics.stdev(response_times):.2f}ì´ˆ")

if __name__ == "__main__":
    messages = [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ì¸ì‚¬ë§ì„ í•´ì£¼ì„¸ìš”."}]
    benchmark_api("http://localhost:8000", messages)
```

## 7ë‹¨ê³„: ë³´ì•ˆ ë° ìš´ì˜

### ë³´ì•ˆ ì„¤ì •

```bash
# ë°©í™”ë²½ ê·œì¹™ ì„¤ì • (ë¡œì»¬ ì ‘ê·¼ë§Œ í—ˆìš©)
sudo pfctl -f /etc/pf.conf

# SSL ì¸ì¦ì„œ ìƒì„± (HTTPS ì‚¬ìš© ì‹œ)
openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes
```

### ëª¨ë‹ˆí„°ë§ ì„¤ì •

```python
# monitoring.py
import psutil
import time
import json
from datetime import datetime

def monitor_system():
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""
    while True:
        stats = {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent,
            "network_io": psutil.net_io_counters()._asdict()
        }
        
        print(json.dumps(stats, indent=2))
        time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬

if __name__ == "__main__":
    monitor_system()
```

### ë°±ì—… ë° ë³µêµ¬

```bash
# ì„¤ì • ë°±ì—…
tar -czf chatgpt-nextjs-backup-$(date +%Y%m%d).tar.gz \
  NextChat/.env.local \
  NextChat/app/config/ \
  vllm-server.py \
  auto-start.sh

# ëª¨ë¸ ë°±ì—… (ì‹¬ë³¼ë¦­ ë§í¬ ì œì™¸)
rsync -av --exclude='*.git*' ./models/ ./models-backup/
```

## ë§ˆë¬´ë¦¬

### ì™„ì„±ëœ í™˜ê²½ í™•ì¸

1. **vLLM ì„œë²„**: `http://localhost:8000` - API ì„œë²„
2. **ChatGPT Next Web**: `http://localhost:3000` - ì›¹ ì¸í„°í˜ì´ìŠ¤
3. **ëª¨ë¸ ê´€ë¦¬**: Hugging Face Hub ì—°ë™
4. **ëª¨ë‹ˆí„°ë§**: ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¶”ì 

### ë‹¤ìŒ ë‹¨ê³„

1. **ëª¨ë¸ ì‹¤í—˜**: ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ í…ŒìŠ¤íŠ¸
2. **íŒŒì¸íŠœë‹**: íŠ¹ì • ë„ë©”ì¸ì— ë§ëŠ” ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•
3. **ìŠ¤ì¼€ì¼ë§**: ë‹¤ì¤‘ GPU ë˜ëŠ” ë¶„ì‚° ì²˜ë¦¬ í™˜ê²½ êµ¬ì¶•
4. **í”„ë¡œë•ì…˜**: Docker ì»¨í…Œì´ë„ˆí™” ë° ë°°í¬ ìë™í™”

### ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- **vLLM ê³µì‹ ë¬¸ì„œ**: [https://docs.vllm.ai](https://docs.vllm.ai)
- **ChatGPT Next Web**: [https://github.com/ChatGPTNextWeb/NextChat](https://github.com/ChatGPTNextWeb/NextChat)
- **Hugging Face ëª¨ë¸ í—ˆë¸Œ**: [https://huggingface.co/models](https://huggingface.co/models)
- **Apple Silicon ìµœì í™”**: [https://developer.apple.com/metal/](https://developer.apple.com/metal/)

ì´ ê°€ì´ë“œë¥¼ í†µí•´ ì™„ì „í•œ ë¡œì»¬ AI í™˜ê²½ì„ êµ¬ì¶•í•˜ì—¬ í”„ë¼ì´ë²„ì‹œë¥¼ ë³´ì¥í•˜ë©´ì„œë„ ê°•ë ¥í•œ AI ê¸°ëŠ¥ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸš€ 