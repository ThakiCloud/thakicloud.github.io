---
title: "BrowserOS: AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ì˜¤í”ˆì†ŒìŠ¤ ì›¹ ë¸Œë¼ìš°ì € ì™„ì „ ê°€ì´ë“œ"
excerpt: "Chromium ê¸°ë°˜ BrowserOSë¡œ ë¡œì»¬ AI ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•œ ìë™í™”ëœ ì›¹ ë¸Œë¼ìš°ì§• ê²½í—˜ì„ êµ¬ì¶•í•˜ê³ , MCP í”„ë¡œí† ì½œì„ í†µí•œ ìƒì‚°ì„± í–¥ìƒ ë„êµ¬ë¥¼ ê°œë°œí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤."
seo_title: "BrowserOS AI ì—ì´ì „íŠ¸ ì›¹ ë¸Œë¼ìš°ì € ì™„ì „ ê°€ì´ë“œ - ì„¤ì¹˜ë¶€í„° í™œìš©ê¹Œì§€ - Thaki Cloud"
seo_description: "ì˜¤í”ˆì†ŒìŠ¤ BrowserOSë¡œ AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ìë™í™” ì›¹ ë¸Œë¼ìš°ì§•, ë¡œì»¬ AI ì±„íŒ…, ìƒì‚°ì„± ë„êµ¬ ê°œë°œ ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ìƒì„¸ ì„¤ëª…í•©ë‹ˆë‹¤."
date: 2025-08-03
last_modified_at: 2025-08-03
categories:
  - tutorials
  - dev
tags:
  - BrowserOS
  - AI-Agent
  - ì›¹ë¸Œë¼ìš°ì €
  - Chromium
  - ìë™í™”
  - MCP
  - ë¡œì»¬AI
  - ìƒì‚°ì„±ë„êµ¬
  - ì˜¤í”ˆì†ŒìŠ¤
  - Python
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/tutorials/browseros-agentic-web-browser-complete-tutorial/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 18ë¶„

## ì„œë¡ 

**ì›¹ ë¸Œë¼ìš°ì €ë„ AI ì‹œëŒ€ì— ë§ì¶° ì§„í™”í•´ì•¼ í•œë‹¤ë©´?** [BrowserOS](https://github.com/browseros-ai/BrowserOS)ëŠ” ë°”ë¡œ ê·¸ ë‹µì…ë‹ˆë‹¤. 1994ë…„ ë„·ìŠ¤ì¼€ì´í”„ ì´í›„ ì²˜ìŒìœ¼ë¡œ ë¸Œë¼ìš°ì €ë¥¼ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ìƒìƒí•˜ì—¬, **AI ì—ì´ì „íŠ¸ ê¸°ë°˜ì˜ ì™„ì „íˆ ìƒˆë¡œìš´ ì›¹ ë¸Œë¼ìš°ì§• ê²½í—˜**ì„ ì œê³µí•©ë‹ˆë‹¤.

70ê°œ ì´ìƒì˜ íƒ­ì„ ì—´ì–´ë‘ê³  ë¸Œë¼ìš°ì €ì™€ ì‹¸ìš°ëŠ” ëŒ€ì‹ , **"ì•„ë§ˆì¡´ ì£¼ë¬¸ ë‚´ì—­ì—ì„œ íƒ€ì´ë“œ íŒŸ ì£¼ë¬¸í•´ì¤˜"** ê°™ì€ ê°„ë‹¨í•œ ëª…ë ¹ìœ¼ë¡œ AI ì—ì´ì „íŠ¸ê°€ ìë™ìœ¼ë¡œ ì‘ì—…ì„ ì²˜ë¦¬í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## BrowserOSì˜ í˜ì‹ ì  íŠ¹ì§•

### ğŸ¤– AI ì—ì´ì „íŠ¸ ì¤‘ì‹¬ ì„¤ê³„

BrowserOSëŠ” **AI ì—ì´ì „íŠ¸ê°€ ì›¹ì„ ìë™ìœ¼ë¡œ íƒìƒ‰í•˜ê³  ì‘ì—…ì„ ìˆ˜í–‰**í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤:

```
ì „í†µì  ë¸Œë¼ìš°ì €:
ì‚¬ìš©ì â†’ ìˆ˜ë™ í´ë¦­/íƒ€ì´í•‘ â†’ ì›¹í˜ì´ì§€

BrowserOS:
ì‚¬ìš©ì â†’ AI ì—ì´ì „íŠ¸ â†’ ìë™ ì›¹ ì‘ì—… ì™„ë£Œ
```

### ğŸ”’ ë¡œì»¬ ë° ë³´ì•ˆ ìš°ì„ 

- **ëª¨ë“  AI ì²˜ë¦¬ê°€ ë¡œì»¬ì—ì„œ ì‹¤í–‰**
- **ê°œì¸ ë°ì´í„°ê°€ ì™¸ë¶€ë¡œ ì „ì†¡ë˜ì§€ ì•ŠìŒ**
- **ê²€ìƒ‰ì´ë‚˜ ê´‘ê³  íšŒì‚¬ì˜ ì œí’ˆì´ ë˜ì§€ ì•ŠìŒ**

### ğŸš€ í˜„ëŒ€ì  ê¸°ëŠ¥ë“¤

- **MCP(Model Context Protocol) ì§€ì›**
- **ë¡œì»¬ AI ì±„íŒ… í†µí•©**
- **ìƒì‚°ì„± ë„êµ¬ ë‚´ì¥**
- **100% ì˜¤í”ˆì†ŒìŠ¤** (AGPL-3.0)

## ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

```bash
# ì§€ì› ìš´ì˜ì²´ì œ
- macOS 10.15+
- Ubuntu 20.04+
- Windows 10+

# Python ìš”êµ¬ì‚¬í•­
- Python 3.8 ì´ìƒ
- pip íŒ¨í‚¤ì§€ ê´€ë¦¬ì

# í•˜ë“œì›¨ì–´ ê¶Œì¥ì‚¬í•­
- RAM: 8GB ì´ìƒ
- ë””ìŠ¤í¬: 2GB ì—¬ìœ  ê³µê°„
- GPU: CUDA ì§€ì› GPU (ì„ íƒì‚¬í•­, AI ê°€ì†ìš©)
```

### BrowserOS ì„¤ì¹˜

#### ë°©ë²• 1: GitHub ë¦´ë¦¬ìŠ¤ì—ì„œ ì„¤ì¹˜

```bash
# ìµœì‹  ë¦´ë¦¬ìŠ¤ ë‹¤ìš´ë¡œë“œ
curl -L https://github.com/browseros-ai/BrowserOS/releases/latest/download/browseros-installer.sh | bash

# ë˜ëŠ” ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
wget https://github.com/browseros-ai/BrowserOS/releases/latest/download/browseros-linux.tar.gz
tar -xzf browseros-linux.tar.gz
cd browseros
./install.sh
```

#### ë°©ë²• 2: ì†ŒìŠ¤ ì½”ë“œì—ì„œ ë¹Œë“œ

```bash
# ë¦¬í¬ì§€í† ë¦¬ í´ë¡ 
git clone https://github.com/browseros-ai/BrowserOS.git
cd BrowserOS

# Python ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv browseros-env
source browseros-env/bin/activate  # Windows: browseros-env\Scripts\activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# BrowserOS ì‹¤í–‰
python main.py
```

#### ë°©ë²• 3: Dockerë¡œ ì‹¤í–‰

```bash
# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t browseros .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -p 8080:8080 -v $(pwd)/data:/app/data browseros

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8080 ì ‘ì†
```

### ì´ˆê¸° ì„¤ì •

#### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
cat > .env << EOF
# AI ëª¨ë¸ ì„¤ì •
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here

# ë¡œì»¬ AI ëª¨ë¸ ì„¤ì • (Ollama ì‚¬ìš© ì‹œ)
OLLAMA_HOST=http://localhost:11434
LOCAL_MODEL=llama3.2

# BrowserOS ì„¤ì •
BROWSER_PROFILE_PATH=./profiles
DEBUG_MODE=true
LOG_LEVEL=INFO

# MCP ì„¤ì •
MCP_SERVER_PORT=3000
MCP_ENABLED=true
EOF
```

#### 2. ë¡œì»¬ AI ëª¨ë¸ ì„¤ì • (Ollama)

```bash
# Ollama ì„¤ì¹˜ (macOS)
brew install ollama

# Ollama ì„¤ì¹˜ (Linux)
curl -fsSL https://ollama.ai/install.sh | sh

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull llama3.2
ollama pull qwen2.5:7b

# Ollama ì„œë²„ ì‹œì‘
ollama serve
```

#### 3. ì„¤ì • íŒŒì¼ í™•ì¸

```python
# config.py ì˜ˆì œ
import os
from pathlib import Path

class BrowserOSConfig:
    # ê¸°ë³¸ ì„¤ì •
    APP_NAME = "BrowserOS"
    VERSION = "0.18.0"
    
    # AI ì„¤ì •
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
    OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
    LOCAL_MODEL = os.getenv("LOCAL_MODEL", "llama3.2")
    
    # ë¸Œë¼ìš°ì € ì„¤ì •
    BROWSER_PROFILE_PATH = Path(os.getenv("BROWSER_PROFILE_PATH", "./profiles"))
    DEFAULT_USER_AGENT = "BrowserOS/0.18.0 (AI-Powered Browser)"
    
    # MCP ì„¤ì •
    MCP_SERVER_PORT = int(os.getenv("MCP_SERVER_PORT", 3000))
    MCP_ENABLED = os.getenv("MCP_ENABLED", "true").lower() == "true"
    
    # ë³´ì•ˆ ì„¤ì •
    ALLOW_INSECURE_CONNECTIONS = False
    SANDBOX_MODE = True
    
    # ë””ë²„ê·¸ ì„¤ì •
    DEBUG_MODE = os.getenv("DEBUG_MODE", "false").lower() == "true"
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

config = BrowserOSConfig()
```

## ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. BrowserOS ì‹œì‘í•˜ê¸°

```python
# ê¸°ë³¸ ì‹¤í–‰
from browseros import BrowserOS
import asyncio

async def main():
    # BrowserOS ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    browser = BrowserOS(
        headless=False,  # GUI ëª¨ë“œ
        profile_path="./my_profile"
    )
    
    # ë¸Œë¼ìš°ì € ì‹œì‘
    await browser.start()
    
    # ê¸°ë³¸ í˜ì´ì§€ ë¡œë“œ
    await browser.navigate("https://example.com")
    
    print("BrowserOSê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ë¸Œë¼ìš°ì € ì¢…ë£Œ
    # await browser.close()

# ì‹¤í–‰
asyncio.run(main())
```

### 2. AI ì—ì´ì „íŠ¸ì™€ ìƒí˜¸ì‘ìš©

```python
# AI ì—ì´ì „íŠ¸ ê¸°ë³¸ ì‚¬ìš©
from browseros.agents import WebAgent
from browseros.models import LocalAIModel

async def ai_browsing_example():
    # ë¡œì»¬ AI ëª¨ë¸ ì´ˆê¸°í™”
    ai_model = LocalAIModel(
        model_name="llama3.2",
        host="http://localhost:11434"
    )
    
    # ì›¹ ì—ì´ì „íŠ¸ ìƒì„±
    agent = WebAgent(
        browser=browser,
        ai_model=ai_model,
        verbose=True
    )
    
    # AI ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—… ìš”ì²­
    result = await agent.execute_task(
        "Googleì—ì„œ 'Python íŠœí† ë¦¬ì–¼'ì„ ê²€ìƒ‰í•˜ê³  ì²« ë²ˆì§¸ ê²°ê³¼ë¥¼ ì—´ì–´ì¤˜"
    )
    
    print(f"ì‘ì—… ê²°ê³¼: {result.success}")
    print(f"ìˆ˜í–‰í•œ ì‘ì—…: {result.actions}")
    print(f"ìµœì¢… URL: {result.final_url}")

asyncio.run(ai_browsing_example())
```

### 3. ì›¹ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘

```python
# ê³ ê¸‰ ì›¹ ìŠ¤í¬ë˜í•‘
from browseros.scrapers import SmartScraper

async def scraping_example():
    scraper = SmartScraper(browser=browser)
    
    # íŠ¹ì • ì›¹í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ
    await browser.navigate("https://news.ycombinator.com")
    
    # AIë¥¼ ì‚¬ìš©í•œ ìŠ¤ë§ˆíŠ¸ ìŠ¤í¬ë˜í•‘
    data = await scraper.extract_data(
        instruction="ë‰´ìŠ¤ ì œëª©, ì ìˆ˜, ëŒ“ê¸€ ìˆ˜ë¥¼ ì¶”ì¶œí•´ì¤˜",
        format="json"
    )
    
    print("ì¶”ì¶œëœ ë°ì´í„°:")
    for item in data:
        print(f"- {item['title']} (ì ìˆ˜: {item['score']})")

asyncio.run(scraping_example())
```

## ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©

### 1. ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

```python
# automation_scripts.py
from browseros.automation import AutomationScript
from browseros.selectors import SmartSelector

class AmazonOrderScript(AutomationScript):
    """ì•„ë§ˆì¡´ ìë™ ì£¼ë¬¸ ìŠ¤í¬ë¦½íŠ¸"""
    
    def __init__(self, browser, ai_model):
        super().__init__(browser, ai_model)
        self.name = "Amazon Auto Order"
        self.description = "ì•„ë§ˆì¡´ì—ì„œ ìë™ìœ¼ë¡œ ìƒí’ˆì„ ì£¼ë¬¸í•©ë‹ˆë‹¤"
    
    async def execute(self, product_name: str):
        """ìƒí’ˆ ìë™ ì£¼ë¬¸ ì‹¤í–‰"""
        try:
            # 1. ì•„ë§ˆì¡´ ì ‘ì†
            await self.browser.navigate("https://amazon.com")
            
            # 2. ë¡œê·¸ì¸ í™•ì¸
            if not await self.is_logged_in():
                await self.login()
            
            # 3. ìƒí’ˆ ê²€ìƒ‰
            search_result = await self.search_product(product_name)
            
            # 4. ìƒí’ˆ ì„ íƒ ë° ì£¼ë¬¸
            order_result = await self.place_order(search_result)
            
            return {
                "success": True,
                "order_id": order_result.order_id,
                "total_price": order_result.total_price
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    async def is_logged_in(self):
        """ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸"""
        selector = SmartSelector(self.ai_model)
        login_elements = await selector.find_elements(
            "ë¡œê·¸ì¸ ë²„íŠ¼ ë˜ëŠ” ê³„ì • ë©”ë‰´"
        )
        
        # AIê°€ í˜ì´ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë¡œê·¸ì¸ ìƒíƒœ íŒë‹¨
        analysis = await self.ai_model.analyze_page(
            "ì´ í˜ì´ì§€ì—ì„œ ì‚¬ìš©ìê°€ ë¡œê·¸ì¸ëœ ìƒíƒœì¸ì§€ í™•ì¸í•´ì¤˜"
        )
        
        return "ë¡œê·¸ì¸ë¨" in analysis.result
    
    async def login(self):
        """ìë™ ë¡œê·¸ì¸"""
        # ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™
        await self.browser.click("a[href*='signin']")
        
        # ë¡œê·¸ì¸ ì •ë³´ ì…ë ¥ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        await self.browser.type("#ap_email", os.getenv("AMAZON_EMAIL"))
        await self.browser.click("#continue")
        
        await self.browser.type("#ap_password", os.getenv("AMAZON_PASSWORD"))
        await self.browser.click("#signInSubmit")
        
        # ë¡œê·¸ì¸ ì™„ë£Œ ëŒ€ê¸°
        await self.browser.wait_for_selector("#nav-link-accountList")
    
    async def search_product(self, product_name):
        """ìƒí’ˆ ê²€ìƒ‰"""
        # ê²€ìƒ‰ ë°•ìŠ¤ì— ìƒí’ˆëª… ì…ë ¥
        await self.browser.type("#twotabsearchtextbox", product_name)
        await self.browser.click("#nav-search-submit-button")
        
        # AIê°€ ìµœì ì˜ ìƒí’ˆ ì„ íƒ
        best_product = await self.ai_model.select_best_product(
            instruction=f"{product_name}ì— ê°€ì¥ ì í•©í•œ ìƒí’ˆì„ ì„ íƒí•´ì¤˜",
            criteria=["í‰ì ", "ê°€ê²©", "ë¦¬ë·° ìˆ˜", "ë¸Œëœë“œ"]
        )
        
        return best_product
    
    async def place_order(self, product):
        """ì£¼ë¬¸ ì§„í–‰"""
        # ìƒí’ˆ í˜ì´ì§€ë¡œ ì´ë™
        await self.browser.click(product.link)
        
        # ì¥ë°”êµ¬ë‹ˆì— ì¶”ê°€
        await self.browser.click("#add-to-cart-button")
        
        # ì¥ë°”êµ¬ë‹ˆë¡œ ì´ë™
        await self.browser.click("#nav-cart")
        
        # ê²°ì œ ì§„í–‰
        await self.browser.click("name='proceedToRetailCheckout'")
        
        # ë°°ì†¡ ì£¼ì†Œ í™•ì¸
        await self.confirm_shipping_address()
        
        # ê²°ì œ ë°©ë²• í™•ì¸
        await self.confirm_payment_method()
        
        # ìµœì¢… ì£¼ë¬¸
        order_info = await self.place_final_order()
        
        return order_info

# ì‚¬ìš© ì˜ˆì œ
async def main():
    browser = BrowserOS()
    await browser.start()
    
    ai_model = LocalAIModel("llama3.2")
    amazon_script = AmazonOrderScript(browser, ai_model)
    
    # "íƒ€ì´ë“œ íŒŸ" ìë™ ì£¼ë¬¸
    result = await amazon_script.execute("Tide Pods")
    
    if result["success"]:
        print(f"ì£¼ë¬¸ ì„±ê³µ! ì£¼ë¬¸ë²ˆí˜¸: {result['order_id']}")
    else:
        print(f"ì£¼ë¬¸ ì‹¤íŒ¨: {result['error']}")

asyncio.run(main())
```

### 2. MCP (Model Context Protocol) í†µí•©

```python
# mcp_integration.py
from browseros.mcp import MCPServer, MCPTool
from browseros.agents import WebAgent

class WebScrapingTool(MCPTool):
    """MCP ê¸°ë°˜ ì›¹ ìŠ¤í¬ë˜í•‘ ë„êµ¬"""
    
    def __init__(self, browser):
        super().__init__(
            name="web_scraping",
            description="ì›¹í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"
        )
        self.browser = browser
    
    async def execute(self, url: str, selector: str, data_type: str = "text"):
        """ì›¹ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        await self.browser.navigate(url)
        
        if data_type == "text":
            result = await self.browser.extract_text(selector)
        elif data_type == "html":
            result = await self.browser.extract_html(selector)
        elif data_type == "attributes":
            result = await self.browser.extract_attributes(selector)
        else:
            result = await self.browser.extract_text(selector)
        
        return {
            "url": url,
            "selector": selector,
            "data": result,
            "timestamp": datetime.now().isoformat()
        }

class AutomationTool(MCPTool):
    """MCP ê¸°ë°˜ ë¸Œë¼ìš°ì € ìë™í™” ë„êµ¬"""
    
    def __init__(self, browser, ai_model):
        super().__init__(
            name="browser_automation",
            description="AI ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•œ ë¸Œë¼ìš°ì € ìë™í™”"
        )
        self.browser = browser
        self.agent = WebAgent(browser, ai_model)
    
    async def execute(self, task: str, max_steps: int = 10):
        """ìë™í™” ì‘ì—… ì‹¤í–‰"""
        result = await self.agent.execute_task(
            task=task,
            max_steps=max_steps,
            return_detailed_log=True
        )
        
        return {
            "task": task,
            "success": result.success,
            "steps": result.steps,
            "final_state": result.final_state,
            "execution_time": result.execution_time
        }

# MCP ì„œë²„ ì„¤ì •
async def setup_mcp_server():
    """MCP ì„œë²„ ì„¤ì • ë° ì‹œì‘"""
    browser = BrowserOS()
    await browser.start()
    
    ai_model = LocalAIModel("llama3.2")
    
    # MCP ì„œë²„ ìƒì„±
    mcp_server = MCPServer(port=3000)
    
    # ë„êµ¬ ë“±ë¡
    web_scraping_tool = WebScrapingTool(browser)
    automation_tool = AutomationTool(browser, ai_model)
    
    mcp_server.add_tool(web_scraping_tool)
    mcp_server.add_tool(automation_tool)
    
    # ì„œë²„ ì‹œì‘
    await mcp_server.start()
    print("MCP ì„œë²„ê°€ í¬íŠ¸ 3000ì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    # í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ì˜ˆì œ
    client = MCPClient("http://localhost:3000")
    
    # ì›¹ ìŠ¤í¬ë˜í•‘ ìš”ì²­
    scraping_result = await client.call_tool(
        "web_scraping",
        {
            "url": "https://example.com",
            "selector": "h1",
            "data_type": "text"
        }
    )
    
    print(f"ìŠ¤í¬ë˜í•‘ ê²°ê³¼: {scraping_result}")
    
    # ìë™í™” ìš”ì²­
    automation_result = await client.call_tool(
        "browser_automation",
        {
            "task": "GitHubì— ë¡œê·¸ì¸í•˜ê³  ìƒˆ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ë§Œë“¤ì–´ì¤˜",
            "max_steps": 15
        }
    )
    
    print(f"ìë™í™” ê²°ê³¼: {automation_result}")

asyncio.run(setup_mcp_server())
```

### 3. ìƒì‚°ì„± ë„êµ¬ ê°œë°œ

```python
# productivity_tools.py
from browseros.productivity import ProductivitySuite
from browseros.ai import TaskPlanner

class EmailAutomation:
    """ì´ë©”ì¼ ìë™í™” ë„êµ¬"""
    
    def __init__(self, browser, ai_model):
        self.browser = browser
        self.ai_model = ai_model
    
    async def process_emails(self, email_provider="gmail"):
        """ì´ë©”ì¼ ìë™ ì²˜ë¦¬"""
        if email_provider == "gmail":
            await self.browser.navigate("https://mail.google.com")
        
        # ì½ì§€ ì•Šì€ ì´ë©”ì¼ ì°¾ê¸°
        unread_emails = await self.browser.find_elements(
            ".zA.zE"  # Gmail ì½ì§€ ì•Šì€ ì´ë©”ì¼ í´ë˜ìŠ¤
        )
        
        processed_emails = []
        
        for email in unread_emails[:10]:  # ìµœê·¼ 10ê°œë§Œ ì²˜ë¦¬
            # ì´ë©”ì¼ ì—´ê¸°
            await self.browser.click(email)
            
            # AIê°€ ì´ë©”ì¼ ë‚´ìš© ë¶„ì„
            email_content = await self.browser.extract_text(".ii.gt")
            analysis = await self.ai_model.analyze_email(email_content)
            
            # ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ìë™ ì²˜ë¦¬
            if analysis.priority == "high":
                await self.handle_high_priority_email(analysis)
            elif analysis.category == "newsletter":
                await self.handle_newsletter(analysis)
            elif analysis.requires_response:
                await self.draft_response(analysis)
            
            processed_emails.append(analysis)
            
            # ì´ë©”ì¼ ëª©ë¡ìœ¼ë¡œ ëŒì•„ê°€ê¸°
            await self.browser.press("u")  # Gmail ë‹¨ì¶•í‚¤
        
        return processed_emails
    
    async def handle_high_priority_email(self, analysis):
        """ê¸´ê¸‰ ì´ë©”ì¼ ì²˜ë¦¬"""
        # ë³„í‘œ í‘œì‹œ
        await self.browser.press("s")
        
        # ì¤‘ìš” ë¼ë²¨ ì¶”ê°€
        await self.browser.press("l")
        await self.browser.type("ì¤‘ìš”")
        await self.browser.press("Enter")
        
        # ìŠ¬ë™ìœ¼ë¡œ ì•Œë¦¼ (ì„ íƒì‚¬í•­)
        if hasattr(self, 'slack_webhook'):
            await self.send_slack_notification(
                f"ê¸´ê¸‰ ì´ë©”ì¼ ë„ì°©: {analysis.subject}"
            )
    
    async def draft_response(self, analysis):
        """ì‘ë‹µ ì´ˆì•ˆ ì‘ì„±"""
        # ë‹µì¥ ë²„íŠ¼ í´ë¦­
        await self.browser.press("r")
        
        # AIê°€ ì‘ë‹µ ì´ˆì•ˆ ì‘ì„±
        draft = await self.ai_model.generate_email_response(
            original_email=analysis.content,
            context=analysis.context,
            tone="professional"
        )
        
        # ì´ˆì•ˆ ì…ë ¥
        compose_field = await self.browser.find_element("[contenteditable='true']")
        await self.browser.type(compose_field, draft.content)
        
        # ì´ˆì•ˆìœ¼ë¡œ ì €ì¥ (ìë™ ë°œì†¡í•˜ì§€ ì•ŠìŒ)
        await self.browser.press("Ctrl+s")

class TaskManager:
    """ì‘ì—… ê´€ë¦¬ ë„êµ¬"""
    
    def __init__(self, browser, ai_model):
        self.browser = browser
        self.ai_model = ai_model
        self.task_planner = TaskPlanner(ai_model)
    
    async def organize_daily_tasks(self):
        """ì¼ì¼ ì‘ì—… ì •ë¦¬"""
        tasks = []
        
        # ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì‘ì—… ìˆ˜ì§‘
        # 1. ì´ë©”ì¼ì—ì„œ ì‘ì—… ì¶”ì¶œ
        email_tasks = await self.extract_tasks_from_emails()
        tasks.extend(email_tasks)
        
        # 2. ìº˜ë¦°ë”ì—ì„œ ì¼ì • ê°€ì ¸ì˜¤ê¸°
        calendar_tasks = await self.extract_tasks_from_calendar()
        tasks.extend(calendar_tasks)
        
        # 3. ê¸°ì¡´ TODO ì•±ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        todo_tasks = await self.extract_tasks_from_todo_apps()
        tasks.extend(todo_tasks)
        
        # AIê°€ ì‘ì—… ìš°ì„ ìˆœìœ„ ì •ë¦¬
        organized_tasks = await self.task_planner.organize_tasks(
            tasks=tasks,
            time_available="8 hours",
            energy_level="high",
            priorities=["urgent", "important", "easy_wins"]
        )
        
        # ì •ë¦¬ëœ ì‘ì—…ì„ ë…¸ì…˜ì´ë‚˜ íŠ¸ë ë¡œì— ìë™ ì—…ë°ì´íŠ¸
        await self.update_task_management_system(organized_tasks)
        
        return organized_tasks
    
    async def extract_tasks_from_emails(self):
        """ì´ë©”ì¼ì—ì„œ ì‘ì—… ì¶”ì¶œ"""
        await self.browser.navigate("https://mail.google.com")
        
        # ìµœê·¼ ì´ë©”ì¼ë“¤ ìŠ¤ìº”
        emails = await self.browser.find_elements(".zA")
        
        tasks = []
        for email in emails[:20]:  # ìµœê·¼ 20ê°œ ì´ë©”ì¼
            await self.browser.click(email)
            
            content = await self.browser.extract_text(".ii.gt")
            
            # AIê°€ ì´ë©”ì¼ì—ì„œ ì‘ì—… ì¶”ì¶œ
            extracted_tasks = await self.ai_model.extract_tasks_from_text(
                text=content,
                context="email"
            )
            
            tasks.extend(extracted_tasks)
            await self.browser.press("u")  # ë’¤ë¡œê°€ê¸°
        
        return tasks

class ResearchAssistant:
    """ë¦¬ì„œì¹˜ ì–´ì‹œìŠ¤í„´íŠ¸"""
    
    def __init__(self, browser, ai_model):
        self.browser = browser
        self.ai_model = ai_model
        self.research_data = []
    
    async def conduct_research(self, topic: str, depth: str = "comprehensive"):
        """ìë™ ë¦¬ì„œì¹˜ ìˆ˜í–‰"""
        research_plan = await self.ai_model.create_research_plan(
            topic=topic,
            depth=depth,
            sources=["academic", "news", "industry", "social"]
        )
        
        results = {}
        
        # 1. í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰
        academic_results = await self.search_academic_papers(topic)
        results["academic"] = academic_results
        
        # 2. ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰
        news_results = await self.search_news_articles(topic)
        results["news"] = news_results
        
        # 3. ì—…ê³„ ë¦¬í¬íŠ¸ ê²€ìƒ‰
        industry_results = await self.search_industry_reports(topic)
        results["industry"] = industry_results
        
        # 4. ì†Œì…œ ë¯¸ë””ì–´ ë™í–¥ ë¶„ì„
        social_results = await self.analyze_social_trends(topic)
        results["social"] = social_results
        
        # AIê°€ ëª¨ë“  ê²°ê³¼ë¥¼ ì¢…í•© ë¶„ì„
        comprehensive_analysis = await self.ai_model.synthesize_research(
            topic=topic,
            sources=results,
            research_plan=research_plan
        )
        
        # ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ë¡œ ìƒì„±
        report = await self.generate_research_report(
            topic=topic,
            analysis=comprehensive_analysis,
            sources=results
        )
        
        return report
    
    async def search_academic_papers(self, topic):
        """í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰"""
        await self.browser.navigate("https://scholar.google.com")
        
        search_box = await self.browser.find_element("#gs_hdr_tsi")
        await self.browser.type(search_box, topic)
        await self.browser.press("Enter")
        
        # ìƒìœ„ ë…¼ë¬¸ë“¤ ë¶„ì„
        papers = await self.browser.find_elements(".gs_rt a")
        
        paper_data = []
        for paper in papers[:10]:  # ìƒìœ„ 10ê°œ ë…¼ë¬¸
            paper_title = await self.browser.extract_text(paper)
            paper_url = await self.browser.get_attribute(paper, "href")
            
            # ë…¼ë¬¸ ì´ˆë¡ ì¶”ì¶œ
            abstract = await self.extract_paper_abstract(paper_url)
            
            paper_data.append({
                "title": paper_title,
                "url": paper_url,
                "abstract": abstract,
                "source": "academic"
            })
        
        return paper_data

# ìƒì‚°ì„± ë„êµ¬ í†µí•© ì‚¬ìš©
async def main_productivity_demo():
    """ìƒì‚°ì„± ë„êµ¬ ë°ëª¨"""
    browser = BrowserOS()
    await browser.start()
    
    ai_model = LocalAIModel("llama3.2")
    
    # ì´ë©”ì¼ ìë™í™”
    email_automation = EmailAutomation(browser, ai_model)
    processed_emails = await email_automation.process_emails()
    print(f"ì²˜ë¦¬ëœ ì´ë©”ì¼: {len(processed_emails)}ê°œ")
    
    # ì‘ì—… ê´€ë¦¬
    task_manager = TaskManager(browser, ai_model)
    organized_tasks = await task_manager.organize_daily_tasks()
    print(f"ì •ë¦¬ëœ ì‘ì—…: {len(organized_tasks)}ê°œ")
    
    # ë¦¬ì„œì¹˜ ì–´ì‹œìŠ¤í„´íŠ¸
    research_assistant = ResearchAssistant(browser, ai_model)
    research_report = await research_assistant.conduct_research(
        topic="AI in Healthcare 2025",
        depth="comprehensive"
    )
    print(f"ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {len(research_report)} í˜ì´ì§€")

asyncio.run(main_productivity_demo())
```

## ì‹¤ì „ í”„ë¡œì íŠ¸: ìë™í™” ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°

### ì „ì²´ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
automated_news_collector/
â”œâ”€â”€ main.py                 # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
â”œâ”€â”€ config.py              # ì„¤ì • íŒŒì¼
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ news_item.py       # ë‰´ìŠ¤ ë°ì´í„° ëª¨ë¸
â”‚   â””â”€â”€ summary.py         # ìš”ì•½ ë°ì´í„° ëª¨ë¸
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_scraper.py    # ê¸°ë³¸ ìŠ¤í¬ë˜í¼
â”‚   â”œâ”€â”€ tech_scraper.py    # ê¸°ìˆ  ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼
â”‚   â””â”€â”€ finance_scraper.py # ê¸ˆìœµ ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ summarizer.py      # AI ìš”ì•½ê¸°
â”‚   â””â”€â”€ categorizer.py     # AI ë¶„ë¥˜ê¸°
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ markdown_generator.py
â”‚   â””â”€â”€ email_sender.py
â””â”€â”€ requirements.txt
```

### í•µì‹¬ êµ¬í˜„

```python
# main.py
import asyncio
from datetime import datetime
from browseros import BrowserOS
from scrapers.tech_scraper import TechNewsScraper
from scrapers.finance_scraper import FinanceNewsScraper
from ai.summarizer import NewsSummarizer
from ai.categorizer import NewsCategorizer
from output.markdown_generator import MarkdownGenerator
from output.email_sender import EmailSender

class AutomatedNewsCollector:
    def __init__(self):
        self.browser = None
        self.scrapers = []
        self.summarizer = None
        self.categorizer = None
        self.output_generators = []
    
    async def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        # BrowserOS ì‹œì‘
        self.browser = BrowserOS(
            headless=True,  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
            profile_path="./news_collector_profile"
        )
        await self.browser.start()
        
        # AI ëª¨ë¸ ì´ˆê¸°í™”
        ai_model = LocalAIModel("llama3.2")
        self.summarizer = NewsSummarizer(ai_model)
        self.categorizer = NewsCategorizer(ai_model)
        
        # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
        self.scrapers = [
            TechNewsScraper(self.browser, ai_model),
            FinanceNewsScraper(self.browser, ai_model)
        ]
        
        # ì¶œë ¥ ìƒì„±ê¸° ì´ˆê¸°í™”
        self.output_generators = [
            MarkdownGenerator(),
            EmailSender()
        ]
    
    async def collect_news(self):
        """ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰"""
        all_news = []
        
        # ê° ìŠ¤í¬ë˜í¼ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
        for scraper in self.scrapers:
            try:
                news_items = await scraper.scrape()
                all_news.extend(news_items)
                print(f"{scraper.__class__.__name__}ì—ì„œ {len(news_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
            except Exception as e:
                print(f"{scraper.__class__.__name__} ì˜¤ë¥˜: {e}")
        
        # AIë¡œ ë‰´ìŠ¤ ë¶„ë¥˜ ë° ìš”ì•½
        processed_news = []
        for news in all_news:
            # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            category = await self.categorizer.categorize(news)
            news.category = category
            
            # ë‰´ìŠ¤ ìš”ì•½
            summary = await self.summarizer.summarize(news)
            news.summary = summary
            
            processed_news.append(news)
        
        # ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§
        filtered_news = self.filter_and_deduplicate(processed_news)
        
        return filtered_news
    
    def filter_and_deduplicate(self, news_list):
        """ë‰´ìŠ¤ í•„í„°ë§ ë° ì¤‘ë³µ ì œê±°"""
        seen_titles = set()
        filtered = []
        
        for news in news_list:
            # ì œëª© ìœ ì‚¬ì„± ì²´í¬ (ê°„ë‹¨í•œ ë°©ë²•)
            title_words = set(news.title.lower().split())
            
            is_duplicate = False
            for seen_title in seen_titles:
                seen_words = set(seen_title.lower().split())
                # 70% ì´ìƒ ë‹¨ì–´ê°€ ê²¹ì¹˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
                overlap = len(title_words & seen_words) / len(title_words | seen_words)
                if overlap > 0.7:
                    is_duplicate = True
                    break
            
            if not is_duplicate and len(news.title) > 10:
                seen_titles.add(news.title)
                filtered.append(news)
        
        return filtered
    
    async def generate_outputs(self, news_list):
        """ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì¶œë ¥ ìƒì„±"""
        results = {}
        
        for generator in self.output_generators:
            try:
                output = await generator.generate(news_list)
                results[generator.__class__.__name__] = output
            except Exception as e:
                print(f"{generator.__class__.__name__} ìƒì„± ì˜¤ë¥˜: {e}")
        
        return results
    
    async def run_daily_collection(self):
        """ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰"""
        try:
            await self.initialize()
            
            print(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {datetime.now()}")
            
            # ë‰´ìŠ¤ ìˆ˜ì§‘
            news_list = await self.collect_news()
            print(f"ì´ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            
            # ì¶œë ¥ ìƒì„±
            outputs = await self.generate_outputs(news_list)
            print("ì¶œë ¥ ìƒì„± ì™„ë£Œ")
            
            return {
                "collected_news": len(news_list),
                "outputs": outputs,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"error": str(e)}
        
        finally:
            if self.browser:
                await self.browser.close()

# ìŠ¤í¬ë˜í¼ êµ¬í˜„ ì˜ˆì œ
# scrapers/tech_scraper.py
from .base_scraper import BaseScraper
from models.news_item import NewsItem

class TechNewsScraper(BaseScraper):
    def __init__(self, browser, ai_model):
        super().__init__(browser, ai_model)
        self.sources = [
            "https://news.ycombinator.com",
            "https://techcrunch.com",
            "https://arstechnica.com"
        ]
    
    async def scrape(self):
        """ê¸°ìˆ  ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘"""
        all_news = []
        
        for source in self.sources:
            try:
                if "ycombinator" in source:
                    news = await self.scrape_hackernews()
                elif "techcrunch" in source:
                    news = await self.scrape_techcrunch()
                elif "arstechnica" in source:
                    news = await self.scrape_arstechnica()
                
                all_news.extend(news)
                
            except Exception as e:
                print(f"{source} ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        
        return all_news
    
    async def scrape_hackernews(self):
        """Hacker News ìŠ¤í¬ë˜í•‘"""
        await self.browser.navigate("https://news.ycombinator.com")
        
        # ë‰´ìŠ¤ í•­ëª© ì„ íƒ
        news_items = await self.browser.find_elements(".titleline > a")
        
        news_list = []
        for i, item in enumerate(news_items[:20]):  # ìƒìœ„ 20ê°œë§Œ
            try:
                title = await self.browser.extract_text(item)
                url = await self.browser.get_attribute(item, "href")
                
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                if url.startswith("item?"):
                    url = f"https://news.ycombinator.com/{url}"
                
                # ì ìˆ˜ì™€ ëŒ“ê¸€ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
                score_element = await self.browser.find_element(
                    f".athing:nth-child({(i+1)*3}) + tr .score"
                )
                score = await self.browser.extract_text(score_element) if score_element else "0 points"
                
                news_item = NewsItem(
                    title=title,
                    url=url,
                    source="Hacker News",
                    score=score,
                    timestamp=datetime.now()
                )
                
                news_list.append(news_item)
                
            except Exception as e:
                print(f"Hacker News í•­ëª© ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        return news_list

# AI ìš”ì•½ê¸° êµ¬í˜„
# ai/summarizer.py
class NewsSummarizer:
    def __init__(self, ai_model):
        self.ai_model = ai_model
    
    async def summarize(self, news_item):
        """ë‰´ìŠ¤ ìš”ì•½ ìƒì„±"""
        try:
            # ë‰´ìŠ¤ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (í•„ìš”ì‹œ)
            if not news_item.content:
                content = await self.fetch_article_content(news_item.url)
                news_item.content = content
            
            # AIë¡œ ìš”ì•½ ìƒì„±
            prompt = f"""
            ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
            
            ì œëª©: {news_item.title}
            ë‚´ìš©: {news_item.content[:1000]}...
            
            ìš”ì•½:
            """
            
            summary = await self.ai_model.generate_text(
                prompt=prompt,
                max_tokens=200,
                temperature=0.3
            )
            
            return summary.strip()
            
        except Exception as e:
            return f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"
    
    async def fetch_article_content(self, url):
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        # ì´ ë¶€ë¶„ì€ ë³„ë„ì˜ ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤ë‚˜ 
        # ê¸°ì¡´ ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„
        pass

# ì¶œë ¥ ìƒì„±ê¸°
# output/markdown_generator.py
class MarkdownGenerator:
    async def generate(self, news_list):
        """ë§ˆí¬ë‹¤ìš´ ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë‰´ìŠ¤ ê·¸ë£¹í™”
        categorized_news = {}
        for news in news_list:
            category = news.category or "ê¸°íƒ€"
            if category not in categorized_news:
                categorized_news[category] = []
            categorized_news[category].append(news)
        
        # ë§ˆí¬ë‹¤ìš´ ìƒì„±
        markdown = f"# ì¼ì¼ ë‰´ìŠ¤ ìš”ì•½ - {datetime.now().strftime('%Yë…„ %mì›” %dì¼')}\n\n"
        
        for category, news_items in categorized_news.items():
            markdown += f"## ğŸ“° {category}\n\n"
            
            for news in news_items:
                markdown += f"### {news.title}\n"
                markdown += f"**ì¶œì²˜**: {news.source}\n"
                markdown += f"**ë§í¬**: [{news.url}]({news.url})\n"
                if news.summary:
                    markdown += f"**ìš”ì•½**: {news.summary}\n"
                markdown += "\n---\n\n"
        
        # íŒŒì¼ë¡œ ì €ì¥
        filename = f"daily_news_{datetime.now().strftime('%Y%m%d')}.md"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(markdown)
        
        return {
            "format": "markdown",
            "filename": filename,
            "content_length": len(markdown)
        }

# ì‹¤í–‰
async def main():
    collector = AutomatedNewsCollector()
    result = await collector.run_daily_collection()
    print(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ê²°ê³¼: {result}")

if __name__ == "__main__":
    asyncio.run(main())
```

## ì„±ëŠ¥ ìµœì í™” ë° ìš´ì˜

### 1. ë©”ëª¨ë¦¬ ë° CPU ìµœì í™”

```python
# performance_optimizer.py
import psutil
import asyncio
from browseros.optimization import PerformanceMonitor

class BrowserOSOptimizer:
    def __init__(self, browser):
        self.browser = browser
        self.monitor = PerformanceMonitor()
    
    async def optimize_performance(self):
        """ì„±ëŠ¥ ìµœì í™” ì‹¤í–‰"""
        
        # 1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        memory_usage = psutil.virtual_memory().percent
        if memory_usage > 80:
            await self.cleanup_memory()
        
        # 2. CPU ì‚¬ìš©ëŸ‰ ì²´í¬
        cpu_usage = psutil.cpu_percent(interval=1)
        if cpu_usage > 90:
            await self.reduce_cpu_load()
        
        # 3. ë¸Œë¼ìš°ì € íƒ­ ê´€ë¦¬
        await self.manage_browser_tabs()
        
        # 4. ìºì‹œ ì •ë¦¬
        await self.cleanup_cache()
    
    async def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” íƒ­ ë‹«ê¸°
        tabs = await self.browser.get_all_tabs()
        for tab in tabs:
            if not tab.is_active and tab.idle_time > 300:  # 5ë¶„ ì´ìƒ ë¹„í™œì„±
                await tab.close()
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        import gc
        gc.collect()
    
    async def manage_browser_tabs(self):
        """ë¸Œë¼ìš°ì € íƒ­ ê´€ë¦¬"""
        tabs = await self.browser.get_all_tabs()
        
        # íƒ­ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ì˜¤ë˜ëœ ê²ƒë¶€í„° ì •ë¦¬
        if len(tabs) > 10:
            sorted_tabs = sorted(tabs, key=lambda t: t.last_activity)
            for tab in sorted_tabs[:-5]:  # ìµœê·¼ 5ê°œë§Œ ìœ ì§€
                await tab.close()
```

### 2. ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬

```python
# error_handling.py
import asyncio
from contextlib import asynccontextmanager

class RobustBrowserSession:
    def __init__(self, max_retries=3):
        self.max_retries = max_retries
        self.browser = None
    
    @asynccontextmanager
    async def get_browser(self):
        """ì•ˆì •ì ì¸ ë¸Œë¼ìš°ì € ì„¸ì…˜ ê´€ë¦¬"""
        try:
            if not self.browser:
                self.browser = await self.create_browser()
            
            yield self.browser
            
        except Exception as e:
            print(f"ë¸Œë¼ìš°ì € ì˜¤ë¥˜ ë°œìƒ: {e}")
            await self.restart_browser()
            yield self.browser
        
        finally:
            # ì •ë¦¬ ì‘ì—…ì€ ì„¸ì…˜ ì¢…ë£Œ ì‹œì—ë§Œ
            pass
    
    async def create_browser(self):
        """ë¸Œë¼ìš°ì € ìƒì„±"""
        browser = BrowserOS(
            headless=True,
            error_recovery=True,
            max_memory_mb=2048
        )
        await browser.start()
        return browser
    
    async def restart_browser(self):
        """ë¸Œë¼ìš°ì € ì¬ì‹œì‘"""
        if self.browser:
            try:
                await self.browser.close()
            except:
                pass
        
        self.browser = await self.create_browser()
    
    async def safe_navigate(self, url, retries=None):
        """ì•ˆì „í•œ í˜ì´ì§€ ì´ë™"""
        if retries is None:
            retries = self.max_retries
        
        for attempt in range(retries):
            try:
                async with self.get_browser() as browser:
                    await browser.navigate(url)
                    return True
            
            except Exception as e:
                print(f"ì´ë™ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{retries}): {e}")
                if attempt == retries - 1:
                    raise
                
                await asyncio.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        return False

# ì‚¬ìš© ì˜ˆì œ
async def robust_automation_example():
    session = RobustBrowserSession()
    
    async with session.get_browser() as browser:
        # ì•ˆì „í•œ ì‘ì—… ìˆ˜í–‰
        success = await session.safe_navigate("https://example.com")
        if success:
            # ì¶”ê°€ ì‘ì—… ìˆ˜í–‰
            pass
```

### 3. ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§

```python
# monitoring.py
import logging
import json
from datetime import datetime
from pathlib import Path

class BrowserOSLogger:
    def __init__(self, log_level=logging.INFO):
        self.setup_logging(log_level)
        self.metrics = {}
    
    def setup_logging(self, level):
        """ë¡œê¹… ì„¤ì •"""
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        # ë¡œê±° ì„¤ì •
        logging.basicConfig(
            level=level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(
                    log_dir / f"browseros_{datetime.now().strftime('%Y%m%d')}.log"
                ),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger("BrowserOS")
    
    def log_automation_start(self, task_name):
        """ìë™í™” ì‹œì‘ ë¡œê·¸"""
        self.logger.info(f"ìë™í™” ì‘ì—… ì‹œì‘: {task_name}")
        self.metrics[task_name] = {
            "start_time": datetime.now(),
            "steps": [],
            "errors": []
        }
    
    def log_automation_step(self, task_name, step_description, success=True):
        """ìë™í™” ë‹¨ê³„ ë¡œê·¸"""
        step_info = {
            "timestamp": datetime.now(),
            "description": step_description,
            "success": success
        }
        
        if task_name in self.metrics:
            self.metrics[task_name]["steps"].append(step_info)
        
        if success:
            self.logger.debug(f"[{task_name}] ë‹¨ê³„ ì™„ë£Œ: {step_description}")
        else:
            self.logger.warning(f"[{task_name}] ë‹¨ê³„ ì‹¤íŒ¨: {step_description}")
    
    def log_automation_end(self, task_name, success=True, error=None):
        """ìë™í™” ì¢…ë£Œ ë¡œê·¸"""
        if task_name in self.metrics:
            end_time = datetime.now()
            start_time = self.metrics[task_name]["start_time"]
            duration = (end_time - start_time).total_seconds()
            
            self.metrics[task_name]["end_time"] = end_time
            self.metrics[task_name]["duration"] = duration
            self.metrics[task_name]["success"] = success
            
            if error:
                self.metrics[task_name]["error"] = str(error)
                self.logger.error(f"[{task_name}] ì‘ì—… ì‹¤íŒ¨: {error}")
            else:
                self.logger.info(f"[{task_name}] ì‘ì—… ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ)")
    
    def export_metrics(self, filename=None):
        """ë©”íŠ¸ë¦­ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        if not filename:
            filename = f"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.metrics, f, default=str, indent=2, ensure_ascii=False)
        
        return filename
```

## í™•ì¥ ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•

### 1. í”ŒëŸ¬ê·¸ì¸ ì‹œìŠ¤í…œ

```python
# plugins/base_plugin.py
from abc import ABC, abstractmethod

class BrowserOSPlugin(ABC):
    """BrowserOS í”ŒëŸ¬ê·¸ì¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, name, version="1.0.0"):
        self.name = name
        self.version = version
        self.enabled = True
    
    @abstractmethod
    async def initialize(self, browser, ai_model):
        """í”ŒëŸ¬ê·¸ì¸ ì´ˆê¸°í™”"""
        pass
    
    @abstractmethod
    async def execute(self, *args, **kwargs):
        """í”ŒëŸ¬ê·¸ì¸ ì‹¤í–‰"""
        pass
    
    async def cleanup(self):
        """í”ŒëŸ¬ê·¸ì¸ ì •ë¦¬"""
        pass

# plugins/social_media_plugin.py
class SocialMediaAutomationPlugin(BrowserOSPlugin):
    """ì†Œì…œ ë¯¸ë””ì–´ ìë™í™” í”ŒëŸ¬ê·¸ì¸"""
    
    def __init__(self):
        super().__init__("SocialMediaAutomation", "1.0.0")
        self.browser = None
        self.ai_model = None
    
    async def initialize(self, browser, ai_model):
        self.browser = browser
        self.ai_model = ai_model
    
    async def execute(self, action, platform, content=None):
        """ì†Œì…œ ë¯¸ë””ì–´ ì‘ì—… ì‹¤í–‰"""
        if platform == "twitter":
            return await self.handle_twitter_action(action, content)
        elif platform == "linkedin":
            return await self.handle_linkedin_action(action, content)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í”Œë«í¼: {platform}")
    
    async def handle_twitter_action(self, action, content):
        """íŠ¸ìœ„í„° ì‘ì—… ì²˜ë¦¬"""
        await self.browser.navigate("https://twitter.com")
        
        if action == "post_tweet":
            # íŠ¸ìœ— ì‘ì„±
            tweet_box = await self.browser.find_element("[data-testid='tweetTextarea_0']")
            await self.browser.type(tweet_box, content)
            
            # íŠ¸ìœ— ë²„íŠ¼ í´ë¦­
            tweet_button = await self.browser.find_element("[data-testid='tweetButtonInline']")
            await self.browser.click(tweet_button)
            
            return {"success": True, "action": "tweet_posted"}
        
        elif action == "check_mentions":
            # ë©˜ì…˜ í™•ì¸
            await self.browser.navigate("https://twitter.com/notifications/mentions")
            mentions = await self.browser.find_elements("[data-testid='tweet']")
            
            return {
                "success": True,
                "mentions_count": len(mentions),
                "action": "mentions_checked"
            }

# í”ŒëŸ¬ê·¸ì¸ ë§¤ë‹ˆì €
class PluginManager:
    def __init__(self):
        self.plugins = {}
        self.browser = None
        self.ai_model = None
    
    async def initialize(self, browser, ai_model):
        self.browser = browser
        self.ai_model = ai_model
    
    def register_plugin(self, plugin: BrowserOSPlugin):
        """í”ŒëŸ¬ê·¸ì¸ ë“±ë¡"""
        self.plugins[plugin.name] = plugin
    
    async def load_plugin(self, plugin_name):
        """í”ŒëŸ¬ê·¸ì¸ ë¡œë“œ"""
        if plugin_name in self.plugins:
            plugin = self.plugins[plugin_name]
            await plugin.initialize(self.browser, self.ai_model)
            return plugin
        else:
            raise ValueError(f"í”ŒëŸ¬ê·¸ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {plugin_name}")
    
    async def execute_plugin(self, plugin_name, *args, **kwargs):
        """í”ŒëŸ¬ê·¸ì¸ ì‹¤í–‰"""
        plugin = await self.load_plugin(plugin_name)
        return await plugin.execute(*args, **kwargs)

# ì‚¬ìš© ì˜ˆì œ
async def plugin_example():
    browser = BrowserOS()
    await browser.start()
    
    ai_model = LocalAIModel("llama3.2")
    
    # í”ŒëŸ¬ê·¸ì¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    plugin_manager = PluginManager()
    await plugin_manager.initialize(browser, ai_model)
    
    # ì†Œì…œ ë¯¸ë””ì–´ í”ŒëŸ¬ê·¸ì¸ ë“±ë¡
    social_plugin = SocialMediaAutomationPlugin()
    plugin_manager.register_plugin(social_plugin)
    
    # í”ŒëŸ¬ê·¸ì¸ ì‚¬ìš©
    result = await plugin_manager.execute_plugin(
        "SocialMediaAutomation",
        action="post_tweet",
        platform="twitter",
        content="BrowserOSë¡œ ìë™ íŠ¸ìœ— í…ŒìŠ¤íŠ¸!"
    )
    
    print(f"í”ŒëŸ¬ê·¸ì¸ ì‹¤í–‰ ê²°ê³¼: {result}")
```

### 2. API ì„œë²„ ëª¨ë“œ

```python
# api_server.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, Dict, Any
import asyncio

app = FastAPI(title="BrowserOS API", version="1.0.0")

# ìš”ì²­ ëª¨ë¸
class NavigationRequest(BaseModel):
    url: str
    wait_for_load: bool = True

class AutomationRequest(BaseModel):
    task: str
    max_steps: int = 10
    return_screenshots: bool = False

class ScrapingRequest(BaseModel):
    url: str
    selector: str
    data_type: str = "text"

# ê¸€ë¡œë²Œ ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤
browser_instance = None

@app.on_event("startup")
async def startup():
    """API ì„œë²„ ì‹œì‘ ì‹œ ë¸Œë¼ìš°ì € ì´ˆê¸°í™”"""
    global browser_instance
    browser_instance = BrowserOS(headless=True)
    await browser_instance.start()

@app.on_event("shutdown")
async def shutdown():
    """API ì„œë²„ ì¢…ë£Œ ì‹œ ë¸Œë¼ìš°ì € ì •ë¦¬"""
    global browser_instance
    if browser_instance:
        await browser_instance.close()

@app.post("/navigate")
async def navigate(request: NavigationRequest):
    """í˜ì´ì§€ ì´ë™ API"""
    try:
        await browser_instance.navigate(request.url)
        if request.wait_for_load:
            await browser_instance.wait_for_load()
        
        return {
            "success": True,
            "current_url": await browser_instance.get_current_url(),
            "title": await browser_instance.get_title()
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/automate")
async def automate_task(request: AutomationRequest):
    """AI ìë™í™” ì‘ì—… API"""
    try:
        ai_model = LocalAIModel("llama3.2")
        agent = WebAgent(browser_instance, ai_model)
        
        result = await agent.execute_task(
            task=request.task,
            max_steps=request.max_steps
        )
        
        response_data = {
            "success": result.success,
            "steps": result.steps,
            "final_url": result.final_url,
            "execution_time": result.execution_time
        }
        
        if request.return_screenshots:
            response_data["screenshots"] = result.screenshots
        
        return response_data
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/scrape")
async def scrape_data(request: ScrapingRequest):
    """ë°ì´í„° ìŠ¤í¬ë˜í•‘ API"""
    try:
        await browser_instance.navigate(request.url)
        
        if request.data_type == "text":
            data = await browser_instance.extract_text(request.selector)
        elif request.data_type == "html":
            data = await browser_instance.extract_html(request.selector)
        elif request.data_type == "attributes":
            data = await browser_instance.extract_attributes(request.selector)
        else:
            data = await browser_instance.extract_text(request.selector)
        
        return {
            "success": True,
            "data": data,
            "url": request.url,
            "selector": request.selector
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/status")
async def get_status():
    """ë¸Œë¼ìš°ì € ìƒíƒœ í™•ì¸ API"""
    try:
        current_url = await browser_instance.get_current_url()
        title = await browser_instance.get_title()
        
        return {
            "status": "running",
            "current_url": current_url,
            "title": title,
            "browser_version": browser_instance.version
        }
    
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

# API ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## ê²°ë¡ 

**BrowserOS**ëŠ” ì›¹ ë¸Œë¼ìš°ì§•ì˜ ë¯¸ë˜ë¥¼ ì œì‹œí•˜ëŠ” í˜ì‹ ì ì¸ í”Œë«í¼ì…ë‹ˆë‹¤. AI ì—ì´ì „íŠ¸ì™€ í•¨ê»˜í•˜ëŠ” ìƒˆë¡œìš´ ì›¹ ê²½í—˜ì„ í†µí•´:

### ğŸ¯ í•µì‹¬ ê°€ì¹˜

1. **ìë™í™”ëœ ìƒì‚°ì„±**: AIê°€ ë°˜ë³µì ì¸ ì›¹ ì‘ì—…ì„ ëŒ€ì‹  ì²˜ë¦¬
2. **ê°œì¸ì •ë³´ ë³´í˜¸**: ëª¨ë“  AI ì²˜ë¦¬ê°€ ë¡œì»¬ì—ì„œ ì‹¤í–‰
3. **ì™„ì „í•œ ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ì˜¤í”ˆì†ŒìŠ¤ë¡œ ë¬´í•œí•œ í™•ì¥ ê°€ëŠ¥
4. **í†µí•©ëœ ì›Œí¬í”Œë¡œìš°**: ì›¹ ë¸Œë¼ìš°ì§•ê³¼ AI ë„êµ¬ì˜ ì™„ë²½í•œ ê²°í•©

### ğŸš€ í™œìš© ë¶„ì•¼

- **ê°œë°œì ë„êµ¬**: ìë™í™”ëœ í…ŒìŠ¤íŒ… ë° ìŠ¤í¬ë˜í•‘
- **ë§ˆì¼€íŒ… ìë™í™”**: ì†Œì…œ ë¯¸ë””ì–´ ë° ì½˜í…ì¸  ê´€ë¦¬
- **ì—°êµ¬ ì§€ì›**: ìë™í™”ëœ ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„
- **ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤**: ê²½ìŸ ë¶„ì„ ë° ì‹œì¥ ì¡°ì‚¬

### ğŸ”® ë¯¸ë˜ ì „ë§

BrowserOSëŠ” ë‹¨ìˆœí•œ ë¸Œë¼ìš°ì €ë¥¼ ë„˜ì–´ì„œ **AI ì¤‘ì‹¬ì˜ ì›¹ ìš´ì˜ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ë°œì „í•  ê²ƒì…ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ë” ì´ìƒ ì›¹ê³¼ ì‹¸ìš°ì§€ ì•Šê³ , AI ì—ì´ì „íŠ¸ì™€ í˜‘ë ¥í•˜ì—¬ ë” íš¨ìœ¨ì ì´ê³  ìƒì‚°ì ì¸ ì˜¨ë¼ì¸ ê²½í—˜ì„ ëˆ„ë¦´ ìˆ˜ ìˆê²Œ ë  ê²ƒì…ë‹ˆë‹¤.

Chromeì´ ì§€ë‚œ 10ë…„ê°„ ê·¸ëŒ€ë¡œì˜€ë‹¤ë©´, BrowserOSëŠ” **ë‹¤ìŒ 10ë…„ì˜ ì›¹ ë¸Œë¼ìš°ì§•ì„ ì •ì˜**í•  í˜ì‹ ì ì¸ í”Œë«í¼ì…ë‹ˆë‹¤.

---

**ì°¸ê³  ìë£Œ:**
- [BrowserOS ê³µì‹ GitHub](https://github.com/browseros-ai/BrowserOS)
- [BrowserOS ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://browserOS.com)
- **Star**: 3.3k | **Forks**: 209 | **License**: AGPL-3.0
- **ìµœì‹  ë¦´ë¦¬ìŠ¤**: v0.18.0 (2025ë…„ 8ì›” 1ì¼)

**ê´€ë ¨ í‚¤ì›Œë“œ:** `#BrowserOS` `#AIì—ì´ì „íŠ¸` `#ì›¹ìë™í™”` `#Chromium` `#ë¡œì»¬AI` `#MCP` `#ìƒì‚°ì„±ë„êµ¬`