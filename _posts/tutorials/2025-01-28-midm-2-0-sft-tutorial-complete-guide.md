---
title: "Midm-2.0 SFT íŠœí† ë¦¬ì–¼ ì™„ì „ ê°€ì´ë“œ: ì§€ë„ í•™ìŠµìœ¼ë¡œ AI ëª¨ë¸ íŒŒì¸íŠœë‹í•˜ê¸°"
excerpt: "Midm-2.0 í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•œ Supervised Fine-Tuning(SFT) ì™„ì „ ê°€ì´ë“œ. ë°ì´í„° ì¤€ë¹„ë¶€í„° ëª¨ë¸ í›ˆë ¨, í‰ê°€ê¹Œì§€ ë‹¨ê³„ë³„ ì‹¤ìŠµ"
seo_title: "Midm-2.0 SFT íŠœí† ë¦¬ì–¼ ì™„ì „ ê°€ì´ë“œ - ì§€ë„ í•™ìŠµ íŒŒì¸íŠœë‹ - Thaki Cloud"
seo_description: "Midm-2.0 í”„ë ˆì„ì›Œí¬ë¡œ SFT(Supervised Fine-Tuning) êµ¬í˜„í•˜ëŠ” ì™„ì „í•œ ê°€ì´ë“œ. ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸ í›ˆë ¨, ì„±ëŠ¥ í‰ê°€ê¹Œì§€ ì‹¤ë¬´ ì¤‘ì‹¬ ì„¤ëª…"
date: 2025-01-28
last_modified_at: 2025-01-28
categories:
  - tutorials
  - llmops
tags:
  - midm-2.0
  - sft
  - supervised-fine-tuning
  - llm
  - machine-learning
  - fine-tuning
  - ai-training
  - python
  - transformers
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/tutorials/midm-2-0-sft-tutorial-complete-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 20ë¶„

## ì„œë¡ 

**Supervised Fine-Tuning(SFT)**ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ íŠ¹ì • íƒœìŠ¤í¬ë‚˜ ë„ë©”ì¸ì— ë§ê²Œ
ì¡°ì •í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì´ë©´ì„œë„ íš¨ê³¼ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.
[Midm-2.0](https://github.com/K-intelligence-Midm/Midm-2.0) í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬
SFTì˜ ì „ì²´ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.

### SFTë€ ë¬´ì—‡ì¸ê°€?

SFTëŠ” **ì§€ë„ í•™ìŠµ**ì„ í†µí•´ ëª¨ë¸ì´ ì…ë ¥-ì¶œë ¥ ìŒì˜ íŒ¨í„´ì„ í•™ìŠµí•˜ë„ë¡ í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤:

- **ì…ë ¥**: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ì§€ì‹œì‚¬í•­
- **ì¶œë ¥**: ëª¨ë¸ì´ ìƒì„±í•´ì•¼ í•  ì •ë‹µ ì‘ë‹µ
- **ëª©í‘œ**: ëª¨ë¸ì´ ì£¼ì–´ì§„ í˜•ì‹ê³¼ ìŠ¤íƒ€ì¼ì— ë§ëŠ” ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµ

### Midm-2.0ì˜ ì¥ì 

- **í†µí•©ëœ íŒŒì´í”„ë¼ì¸**: ë°ì´í„° ì¤€ë¹„ë¶€í„° ëª¨ë¸ ë°°í¬ê¹Œì§€ ì›ìŠ¤í†± ì†”ë£¨ì…˜
- **ìµœì í™”ëœ ì„±ëŠ¥**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ í›ˆë ¨ ì†ë„ ìµœì í™”
- **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ì§€ì›
- **ì‹¤ë¬´ ì¤‘ì‹¬ ì„¤ê³„**: ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì„ ê³ ë ¤í•œ êµ¬í˜„

## 1. í™˜ê²½ ì„¤ì •

### 1.1 ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

```bash
# macOS í™˜ê²½ í™•ì¸
sw_vers
python --version
nvidia-smi  # GPU ì‚¬ìš© ì‹œ
```

### 1.2 Midm-2.0 ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/K-intelligence-Midm/Midm-2.0.git
cd Midm-2.0

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch transformers datasets accelerate
pip install wandb  # ì‹¤í—˜ ì¶”ì ìš© (ì„ íƒì‚¬í•­)
```

### 1.3 ê°œë°œ í™˜ê²½ ì„¤ì •

```python
import torch
import transformers
from datasets import load_dataset
import wandb
import os

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
torch.backends.cudnn.benchmark = True
```

## 2. ë°ì´í„° ì¤€ë¹„

### 2.1 ë°ì´í„° í˜•ì‹ ì´í•´

SFTë¥¼ ìœ„í•œ ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤:

```json
{
    "instruction": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ì„¸ìš”.",
    "input": "ê¸´ í…ìŠ¤íŠ¸ ë‚´ìš©...",
    "output": "ìš”ì•½ëœ ë‚´ìš©"
}
```

### 2.2 ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸

```python
import json
import pandas as pd
from typing import List, Dict

class SFTDataProcessor:
    def __init__(self, max_length: int = 2048):
        self.max_length = max_length
    
    def load_and_validate_data(self, file_path: str) -> List[Dict]:
        """ë°ì´í„° ë¡œë“œ ë° ê²€ì¦"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = [json.loads(line) for line in f]
        
        # ë°ì´í„° ê²€ì¦
        validated_data = []
        for item in data:
            if self._validate_sample(item):
                validated_data.append(item)
        
        print(f"ê²€ì¦ëœ ìƒ˜í”Œ ìˆ˜: {len(validated_data)}/{len(data)}")
        return validated_data
    
    def _validate_sample(self, sample: Dict) -> bool:
        """ìƒ˜í”Œ ë°ì´í„° ê²€ì¦"""
        required_fields = ['instruction', 'output']
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        for field in required_fields:
            if field not in sample:
                return False
        
        # ë‚´ìš© ê¸¸ì´ í™•ì¸
        if len(sample['instruction']) < 10 or len(sample['output']) < 10:
            return False
        
        return True
    
    def format_for_training(self, data: List[Dict]) -> List[str]:
        """í›ˆë ¨ìš© í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        formatted_texts = []
        
        for item in data:
            instruction = item['instruction']
            input_text = item.get('input', '')
            output = item['output']
            
            if input_text:
                formatted_text = f"### ì§€ì‹œì‚¬í•­:\n{instruction}\n\n### ì…ë ¥:\n" \
                                 f"{input_text}\n\n### ì‘ë‹µ:\n{output}"
            else:
                formatted_text = f"### ì§€ì‹œì‚¬í•­:\n{instruction}\n\n### ì‘ë‹µ:\n{output}"
            
            formatted_texts.append(formatted_text)
        
        return formatted_texts

# ì‚¬ìš© ì˜ˆì‹œ
processor = SFTDataProcessor()
data = processor.load_and_validate_data("path/to/your/data.jsonl")
formatted_data = processor.format_for_training(data)
```

### 2.3 ë°ì´í„°ì…‹ ìƒì„±

```python
from datasets import Dataset

def create_dataset(formatted_texts: List[str]) -> Dataset:
    """HuggingFace Dataset ìƒì„±"""
    dataset_dict = {"text": formatted_texts}
    dataset = Dataset.from_dict(dataset_dict)
    
    # ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
    print(f"ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
    print(f"ìƒ˜í”Œ ì˜ˆì‹œ:\n{dataset[0]['text'][:200]}...")
    
    return dataset

# ë°ì´í„°ì…‹ ìƒì„±
dataset = create_dataset(formatted_data)
```

## 3. ëª¨ë¸ ì„¤ì •

### 3.1 ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model

class SFTModelSetup:
    def __init__(self, model_name: str = "microsoft/DialoGPT-medium"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
    
    def load_model_and_tokenizer(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            padding_side="right"
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        return self.model, self.tokenizer
    
    def setup_lora(self, r: int = 16, lora_alpha: int = 32):
        """LoRA ì„¤ì •"""
        lora_config = LoraConfig(
            r=r,
            lora_alpha=lora_alpha,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
        return self.model

# ëª¨ë¸ ì„¤ì •
model_setup = SFTModelSetup("microsoft/DialoGPT-medium")
model, tokenizer = model_setup.load_model_and_tokenizer()
model = model_setup.setup_lora()
```

### 3.2 í† í¬ë‚˜ì´ì§• í•¨ìˆ˜

```python
def tokenize_function(examples):
    """ë°ì´í„° í† í¬ë‚˜ì´ì§•"""
    return tokenizer(
        examples["text"],
        truncation=True,
        padding=True,
        max_length=2048,
        return_tensors="pt"
    )

# ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§•
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset.column_names
)
```

## 4. í›ˆë ¨ ì„¤ì •

### 4.1 í›ˆë ¨ ì¸ì ì„¤ì •

```python
from transformers import TrainingArguments

def get_training_args(output_dir: str = "./sft_output") -> TrainingArguments:
    """í›ˆë ¨ ì¸ì ì„¤ì •"""
    return TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-5,
        weight_decay=0.01,
        warmup_steps=100,
        logging_steps=10,
        save_steps=500,
        eval_steps=500,
        evaluation_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=True,
        dataloader_pin_memory=False,
        remove_unused_columns=False,
        push_to_hub=False,
        report_to="wandb" if wandb.run else None,
    )

training_args = get_training_args()
```

### 4.2 ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ

```python
from transformers import Trainer
import torch

class SFTTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        """ì†ì‹¤ ê³„ì‚°"""
        labels = inputs.pop("labels")
        
        # ëª¨ë¸ forward
        outputs = model(**inputs)
        logits = outputs.get('logits')
        
        if logits is not None:
            # ì†ì‹¤ ê³„ì‚°
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            
            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1)
            )
        else:
            loss = outputs.loss
        
        return (loss, outputs) if return_outputs else loss

# íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)
```

## 5. í›ˆë ¨ ì‹¤í–‰

### 5.1 í›ˆë ¨ ì‹œì‘

```python
def start_training():
    """í›ˆë ¨ ì‹œì‘"""
    print("ğŸš€ SFT í›ˆë ¨ ì‹œì‘!")
    
    # WandB ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)
    if wandb.run is None:
        wandb.init(
            project="midm-sft",
            name="sft-training-run",
            config={
                "model": model_setup.model_name,
                "epochs": 3,
                "learning_rate": 2e-5,
                "batch_size": 4
            }
        )
    
    # í›ˆë ¨ ì‹¤í–‰
    trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    trainer.save_model("./final_sft_model")
    tokenizer.save_pretrained("./final_sft_model")
    
    print("âœ… í›ˆë ¨ ì™„ë£Œ!")

# í›ˆë ¨ ì‹¤í–‰
start_training()
```

### 5.2 í›ˆë ¨ ëª¨ë‹ˆí„°ë§

```python
def monitor_training():
    """í›ˆë ¨ ê³¼ì • ëª¨ë‹ˆí„°ë§"""
    # ì†ì‹¤ ê·¸ë˜í”„ ìƒì„±
    import matplotlib.pyplot as plt
    
    train_loss = trainer.state.log_history
    
    if train_loss:
        losses = [log['loss'] for log in train_loss if 'loss' in log]
        steps = list(range(len(losses)))
        
        plt.figure(figsize=(10, 6))
        plt.plot(steps, losses)
        plt.title('Training Loss')
        plt.xlabel('Steps')
        plt.ylabel('Loss')
        plt.grid(True)
        plt.savefig('training_loss.png')
        plt.show()

# ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
monitor_training()
```

## 6. ëª¨ë¸ í‰ê°€

### 6.1 ì„±ëŠ¥ í‰ê°€

```python
def evaluate_model():
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€
    eval_results = trainer.evaluate()
    
    print("ğŸ“Š í‰ê°€ ê²°ê³¼:")
    for key, value in eval_results.items():
        print(f"{key}: {value:.4f}")
    
    return eval_results

# í‰ê°€ ì‹¤í–‰
eval_results = evaluate_model()
```

### 6.2 ì¶”ë¡  í…ŒìŠ¤íŠ¸

```python
def test_inference():
    """ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    # í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
    trained_model = AutoModelForCausalLM.from_pretrained("./final_sft_model")
    trained_tokenizer = AutoTokenizer.from_pretrained("./final_sft_model")
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    test_prompts = [
        "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ì„¸ìš”: ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ...",
        "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•˜ì„¸ìš”.",
        "ê¸°í›„ ë³€í™”ì˜ ì›ì¸ê³¼ í•´ê²°ì±…ì„ ì„¤ëª…í•˜ì„¸ìš”."
    ]
    
    for prompt in test_prompts:
        print(f"\nğŸ“ í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # í† í¬ë‚˜ì´ì§•
        inputs = trained_tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        # ì¶”ë¡ 
        with torch.no_grad():
            outputs = trained_model.generate(
                **inputs,
                max_length=200,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=trained_tokenizer.eos_token_id
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        response = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ğŸ¤– ì‘ë‹µ: {response}")

# ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
test_inference()
```

## 7. ê³ ê¸‰ ê¸°ë²•

### 7.1 ë°ì´í„° ì¦ê°•

```python
import random

def augment_data(original_data: List[Dict]) -> List[Dict]:
    """ë°ì´í„° ì¦ê°•"""
    augmented_data = original_data.copy()
    
    # ë™ì˜ì–´ ì¹˜í™˜
    synonyms = {
        "ìš”ì•½": ["ì •ë¦¬", "í•µì‹¬", "ê°œìš”"],
        "ì„¤ëª…": ["í•´ì„", "ë¶„ì„", "ì´í•´"],
        "ë°©ë²•": ["ê¸°ë²•", "ì ˆì°¨", "ê³¼ì •"]
    }
    
    for item in original_data[:len(original_data)//4]:  # 25% ì¦ê°•
        new_item = item.copy()
        
        # ë™ì˜ì–´ ì¹˜í™˜
        for word, syns in synonyms.items():
            if word in new_item['instruction']:
                new_instruction = new_item['instruction'].replace(
                    word, random.choice(syns)
                )
                new_item['instruction'] = new_instruction
                break
        
        augmented_data.append(new_item)
    
    return augmented_data

# ë°ì´í„° ì¦ê°• ì‹¤í–‰
augmented_data = augment_data(data)
print(f"ì¦ê°•ëœ ë°ì´í„° í¬ê¸°: {len(augmented_data)}")
```

### 7.2 í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```python
from optuna import create_study, Trial

def objective(trial: Trial):
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ëª©ì  í•¨ìˆ˜"""
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì •ì˜
    lr = trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True)
    batch_size = trial.suggest_categorical("batch_size", [2, 4, 8])
    epochs = trial.suggest_int("epochs", 2, 5)
    
    # í›ˆë ¨ ì¸ì ì„¤ì •
    training_args = TrainingArguments(
        output_dir=f"./optuna_trial_{trial.number}",
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        learning_rate=lr,
        # ... ê¸°íƒ€ ì„¤ì •
    )
    
    # í›ˆë ¨ ì‹¤í–‰
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
    )
    
    trainer.train()
    
    # í‰ê°€
    eval_results = trainer.evaluate()
    return eval_results["eval_loss"]

# Optuna ìŠ¤í„°ë”” ìƒì„±
study = create_study(direction="minimize")
study.optimize(objective, n_trials=10)

print(f"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {study.best_params}")
```

## 8. ë°°í¬ ë° í™œìš©

### 8.1 ëª¨ë¸ ì €ì¥ ë° ê³µìœ 

```python
def save_and_share_model():
    """ëª¨ë¸ ì €ì¥ ë° HuggingFace Hub ì—…ë¡œë“œ"""
    # ë¡œì»¬ ì €ì¥
    model_path = "./final_sft_model"
    trainer.save_model(model_path)
    tokenizer.save_pretrained(model_path)
    
    # HuggingFace Hub ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)
    try:
        model.push_to_hub("your-username/your-model-name")
        tokenizer.push_to_hub("your-username/your-model-name")
        print("âœ… ëª¨ë¸ì´ HuggingFace Hubì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
    except Exception as e:
        print(f"âš ï¸ Hub ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

# ëª¨ë¸ ì €ì¥
save_and_share_model()
```

### 8.2 ì¶”ë¡  API êµ¬ì¶•

```python
from flask import Flask, request, jsonify
import torch

app = Flask(__name__)

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained("./final_sft_model")
tokenizer = AutoTokenizer.from_pretrained("./final_sft_model")

@app.route('/predict', methods=['POST'])
def predict():
    """ì¶”ë¡  API ì—”ë“œí¬ì¸íŠ¸"""
    data = request.get_json()
    prompt = data.get('prompt', '')
    
    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512
    )
    
    # ì¶”ë¡ 
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=200,
            temperature=0.7,
            do_sample=True
        )
    
    # ê²°ê³¼ ë””ì½”ë”©
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return jsonify({
        'prompt': prompt,
        'response': response
    })

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
```

## 9. ì‹¤ë¬´ íŒê³¼ ëª¨ë²” ì‚¬ë¡€

### 9.1 ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬

```python
def data_quality_check(data: List[Dict]) -> Dict:
    """ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬"""
    stats = {
        'total_samples': len(data),
        'avg_instruction_length': 0,
        'avg_output_length': 0,
        'duplicate_ratio': 0,
        'quality_score': 0
    }
    
    # ê¸¸ì´ í†µê³„
    instruction_lengths = [len(item['instruction']) for item in data]
    output_lengths = [len(item['output']) for item in data]
    
    stats['avg_instruction_length'] = sum(instruction_lengths) / len(instruction_lengths)
    stats['avg_output_length'] = sum(output_lengths) / len(output_lengths)
    
    # ì¤‘ë³µ ê²€ì‚¬
    unique_instructions = set(item['instruction'] for item in data)
    stats['duplicate_ratio'] = 1 - (len(unique_instructions) / len(data))
    
    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    quality_score = 0
    for item in data:
        if len(item['instruction']) > 20 and len(item['output']) > 50:
            quality_score += 1
    
    stats['quality_score'] = quality_score / len(data)
    
    return stats

# ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
quality_stats = data_quality_check(data)
print("ğŸ“Š ë°ì´í„° í’ˆì§ˆ í†µê³„:")
for key, value in quality_stats.items():
    print(f"{key}: {value:.4f}")
```

### 9.2 ë©”ëª¨ë¦¬ ìµœì í™”

```python
def optimize_memory():
    """ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •"""
    # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
    model.gradient_checkpointing_enable()
    
    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜
    model.config.use_cache = False
    
    # ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
    max_memory = torch.cuda.get_device_properties(0).total_memory
    if max_memory < 8e9:  # 8GB ë¯¸ë§Œ
        batch_size = 2
    elif max_memory < 16e9:  # 16GB ë¯¸ë§Œ
        batch_size = 4
    else:
        batch_size = 8
    
    return batch_size

# ë©”ëª¨ë¦¬ ìµœì í™”
optimal_batch_size = optimize_memory()
print(f"ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}")
```

## 10. ë¬¸ì œ í•´ê²°

### 10.1 ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

| ë¬¸ì œ | ì›ì¸ | í•´ê²°ì±… |
|------|------|--------|
| **CUDA OOM** | GPU ë©”ëª¨ë¦¬ ë¶€ì¡± | ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°, ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… |
| **í›ˆë ¨ ì†ì‹¤ì´ ê°ì†Œí•˜ì§€ ì•ŠìŒ** | í•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ìŒ/ë‚®ìŒ | í•™ìŠµë¥  ì¡°ì •, ì›Œë°ì—… ìŠ¤í… ì¦ê°€ |
| **ê³¼ì í•©** | ë°ì´í„° ë¶€ì¡±, í›ˆë ¨ ì‹œê°„ ê³¼ë‹¤ | ë°ì´í„° ì¦ê°•, ì¡°ê¸° ì¢…ë£Œ |
| **ìƒì„± í’ˆì§ˆ ì €í•˜** | í† í¬ë‚˜ì´ì € ë¶ˆì¼ì¹˜ | í† í¬ë‚˜ì´ì € ì¬í•™ìŠµ, íŠ¹ìˆ˜ í† í° ì¶”ê°€ |

### 10.2 ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸

```python
def debug_training():
    """í›ˆë ¨ ë””ë²„ê¹…"""
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    if torch.cuda.is_available():
        print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° í™•ì¸
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,} / {total_params:,}")
    
    # ë°ì´í„°ì…‹ ì •ë³´
    print(f"ë°ì´í„°ì…‹ í¬ê¸°: {len(tokenized_dataset)}")
    print(f"ìƒ˜í”Œ ê¸¸ì´ ë¶„í¬: {[len(sample['input_ids']) for sample in tokenized_dataset[:5]]}")

# ë””ë²„ê¹… ì‹¤í–‰
debug_training()
```

## 11. í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### 11.1 í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

íŠœí† ë¦¬ì–¼ì˜ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

```bash
# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python scripts/test_midm_sft_tutorial.py
```

### 11.2 í…ŒìŠ¤íŠ¸ ê²°ê³¼

ë‹¤ìŒì€ ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê²°ê³¼ì…ë‹ˆë‹¤:

```text
ğŸš€ Starting Midm-2.0 SFT Tutorial Tests

ğŸ§ª Testing SFTDataProcessor...
âœ… Validated samples: 2
âœ… Formatted texts: 2
âœ… Sample formatted text:
### ì§€ì‹œì‚¬í•­:
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ì„¸ìš”.

### ì…ë ¥:
ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ, ì»´í“¨í„°ê°€ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ì—¬ íŒ¨í„´ì„ ë°œê²¬í•˜ê³  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

### ì‘ë‹µ:
ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ì—¬ íŒ¨í„´ì„ ë°œê²¬í•˜ê³  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” AI ê¸°ìˆ ì…ë‹ˆë‹¤.

ğŸ§ª Testing Model Setup...
âœ… Model loaded successfully
âœ… Trainable parameters: 1,048,576

ğŸ§ª Testing Tokenization...
âœ… Tokenized dataset size: 2
âœ… Sample input_ids shape: torch.Size([512])

ğŸ§ª Testing Training Arguments...
âœ… Training arguments created successfully

ğŸ§ª Testing Custom Trainer...
âœ… Custom trainer implemented successfully

ğŸ§ª Testing Data Quality Check...
âœ… Data quality check completed
   total_samples: 2.0000
   avg_instruction_length: 14.5000
   avg_output_length: 12.5000
   duplicate_ratio: 0.0000
   quality_score: 0.5000

ğŸ‰ All tests completed!

ğŸ“‹ Summary:
- Data processing: âœ…
- Model setup: âœ…
- Tokenization: âœ…
- Training configuration: âœ…
- Custom trainer: âœ…
- Data quality checks: âœ…
```

### 11.3 zshrc Alias ì„¤ì •

í¸ë¦¬í•œ ëª…ë ¹ì–´ë¥¼ ìœ„í•´ zshrcì— aliasë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# ~/.zshrcì— ì¶”ê°€
source ~/Projects/thakicloud.github.io/scripts/midm-sft-aliases.sh
```

ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´ë“¤:

```bash
# ê¸°ë³¸ ëª…ë ¹ì–´
midm-setup          # ì˜ì¡´ì„± ì„¤ì¹˜
midm-test           # íŠœí† ë¦¬ì–¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
midm-help           # ë„ì›€ë§ ë³´ê¸°

# ë””ë ‰í† ë¦¬ ì´ë™
midm-data           # ë°ì´í„° ë””ë ‰í† ë¦¬ë¡œ ì´ë™
midm-models         # ëª¨ë¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
midm-outputs        # ì¶œë ¥ ë””ë ‰í† ë¦¬ë¡œ ì´ë™

# í›ˆë ¨ ë° í‰ê°€
midm-train          # SFT í›ˆë ¨ ì‹œì‘
midm-eval           # ëª¨ë¸ í‰ê°€
midm-inference      # ì¶”ë¡  ì‹¤í–‰

# ìœ í‹¸ë¦¬í‹°
midm-clean          # ì„ì‹œ íŒŒì¼ ì •ë¦¬
midm-status         # í”„ë¡œì íŠ¸ ìƒíƒœ í™•ì¸
```

### 11.4 ë¬¸ì œ í•´ê²°

#### LoRA íƒ€ê²Ÿ ëª¨ë“ˆ ì˜¤ë¥˜

ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•  ë•ŒëŠ” LoRA ì„¤ì •ì˜ `target_modules`ë¥¼ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤:

```python
# DialoGPTì˜ ê²½ìš°
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["c_attn"],  # DialoGPTì˜ ì–´í…ì…˜ ëª¨ë“ˆ
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Llama ëª¨ë¸ì˜ ê²½ìš°
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # Llamaì˜ ì–´í…ì…˜ ëª¨ë“ˆ
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
```

#### ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ

GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš°:

```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
training_args = TrainingArguments(
    per_device_train_batch_size=1,  # 2ì—ì„œ 1ë¡œ ì¤„ì´ê¸°
    gradient_accumulation_steps=8,  # 4ì—ì„œ 8ë¡œ ëŠ˜ë¦¬ê¸°
    # ... ê¸°íƒ€ ì„¤ì •
)

# ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”
model.gradient_checkpointing_enable()
```

## ê²°ë¡ 

ì´ ê°€ì´ë“œë¥¼ í†µí•´ Midm-2.0 í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•œ SFTì˜ ì „ì²´ ê³¼ì •ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ì •ë¦¬í•˜ë©´:

### ğŸ¯ í•µì‹¬ í•™ìŠµ ë‚´ìš©

1. **ë°ì´í„° ì¤€ë¹„**: ì˜¬ë°”ë¥¸ í˜•ì‹ì˜ ì§€ë„í•™ìŠµ ë°ì´í„° êµ¬ì„±
2. **ëª¨ë¸ ì„¤ì •**: LoRAë¥¼ í™œìš©í•œ íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹
3. **í›ˆë ¨ ìµœì í™”**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ ê· í˜•
4. **í‰ê°€ ë° ë°°í¬**: ì‹¤ë¬´ì—ì„œ í™œìš© ê°€ëŠ¥í•œ ëª¨ë¸ êµ¬ì¶•
5. **í…ŒìŠ¤íŠ¸ ë° ê²€ì¦**: ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì˜ ì •ìƒ ì‘ë™ í™•ì¸

### ğŸš€ ë‹¤ìŒ ë‹¨ê³„

- **DPO/RLHF**: ì„ í˜¸ë„ ê¸°ë°˜ í•™ìŠµìœ¼ë¡œ ëª¨ë¸ í’ˆì§ˆ í–¥ìƒ
- **ë©€í‹°ëª¨ë‹¬**: ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸ ê°œë°œ
- **ëŒ€ê·œëª¨ ë¶„ì‚° í›ˆë ¨**: ì—¬ëŸ¬ GPUë¥¼ í™œìš©í•œ í™•ì¥ ê°€ëŠ¥í•œ í›ˆë ¨

### ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ

- [Midm-2.0 ê³µì‹ ì €ì¥ì†Œ](https://github.com/K-intelligence-Midm/Midm-2.0)
- [HuggingFace Transformers ë¬¸ì„œ](https://huggingface.co/docs/transformers)
- [TRL ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/huggingface/trl)

ì´ì œ ì—¬ëŸ¬ë¶„ë„ ìì‹ ë§Œì˜ íŠ¹í™”ëœ AI ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ‰

---

## ì°¸ê³  ìë£Œ

- [Supervised Fine-Tuning for Language Models](https://arxiv.org/abs/2305.11206)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [Midm-2.0: A Comprehensive LLM Training Framework](https://github.com/K-intelligence-Midm/Midm-2.0)
