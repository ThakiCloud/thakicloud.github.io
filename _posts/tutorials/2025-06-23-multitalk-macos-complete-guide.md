---
title: "MultiTalk ë§¥ë¶ ì™„ì „ ê°€ì´ë“œ: ì˜¤ë””ì˜¤ ê¸°ë°˜ ë‹¤ì¤‘ ì¸ë¬¼ ëŒ€í™” ë¹„ë””ì˜¤ ìƒì„±"
excerpt: "MultiTalkë¥¼ ë§¥ë¶ì—ì„œ ì„¤ì¹˜í•˜ê³  ì‚¬ìš©í•˜ëŠ” ë‹¨ê³„ë³„ íŠœí† ë¦¬ì–¼ê³¼ ì‹¤ì „ ì‘ìš© í”„ë¡œê·¸ë¨ ê°œë°œ ê°€ì´ë“œ"
date: 2025-06-23
categories: 
  - tutorials
  - dev
tags: 
  - MultiTalk
  - AI Video Generation
  - Conversational AI
  - macOS
  - ì˜¤ë””ì˜¤ ë¹„ë””ì˜¤
  - ë”¥ëŸ¬ë‹
author_profile: true
toc: true
toc_label: "MultiTalk ë§¥ë¶ ê°€ì´ë“œ"
---

## MultiTalk ì†Œê°œ

MultiTalkëŠ” ì˜¤ë””ì˜¤ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì¤‘ ì¸ë¬¼ ëŒ€í™” ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” í˜ì‹ ì ì¸ AI í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ë‹¨ì¼ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì´ë‚˜ ë‹¤ì¤‘ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ì…ë ¥ë°›ì•„ ì°¸ì¡° ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì…ìˆ  ë™ì‘ê³¼ ìƒí˜¸ì‘ìš©ì´ í¬í•¨ëœ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

- **ğŸ¬ ì‹¤ì œê°™ì€ ëŒ€í™”**: ë‹¨ì¼ ë° ë‹¤ì¤‘ ì¸ë¬¼ ëŒ€í™” ë¹„ë””ì˜¤ ìƒì„±
- **ğŸ‘¥ ëŒ€í™”í˜• ìºë¦­í„° ì œì–´**: í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•œ ê°€ìƒ ì¸ê°„ ì œì–´
- **ğŸ¤ ë†’ì€ ì¼ë°˜í™” ì„±ëŠ¥**: ë§Œí™” ìºë¦­í„°, ë…¸ë˜, ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›
- **âš¡ íš¨ìœ¨ì ì¸ ì¶”ë¡ **: TeaCache ê°€ì†ì„ í†µí•œ 2-3ë°° ì†ë„ í–¥ìƒ
- **ğŸ“± ë‹¤ì–‘í•œ í•´ìƒë„**: 480P, 720P ì§€ì›

## ë§¥ë¶ í™˜ê²½ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­

```bash
# ìµœì†Œ ì‚¬ì–‘
- Apple Silicon M1/M2/M3 (ê¶Œì¥: M2 Pro ì´ìƒ)
- RAM: 16GB ì´ìƒ (ê¶Œì¥: 32GB)
- ì €ì¥ê³µê°„: 50GB ì´ìƒ ì—¬ìœ ê³µê°„
- GPU ë©”ëª¨ë¦¬: 8GB ì´ìƒ (í†µí•© ë©”ëª¨ë¦¬ ê¸°ì¤€)

# ê¶Œì¥ ì‚¬ì–‘
- Apple Silicon M3 Pro/Max
- RAM: 32GB ì´ìƒ
- ì €ì¥ê³µê°„: 100GB ì´ìƒ
- í†µí•© ë©”ëª¨ë¦¬: 18GB ì´ìƒ
```

### ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­

```bash
# í•„ìˆ˜ ì†Œí”„íŠ¸ì›¨ì–´
- macOS 13.0 (Ventura) ì´ìƒ
- Python 3.8-3.11
- Git
- Homebrew (ê¶Œì¥)
```

## ë‹¨ê³„ë³„ ì„¤ì¹˜ ê°€ì´ë“œ

### 1ë‹¨ê³„: ê°œë°œ í™˜ê²½ ì¤€ë¹„

```bash
# Homebrew ì„¤ì¹˜ (ë¯¸ì„¤ì¹˜ì‹œ)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Git ì„¤ì¹˜ í™•ì¸
git --version

# Python ì„¤ì¹˜ (pyenv ì‚¬ìš© ê¶Œì¥)
brew install pyenv
pyenv install 3.10.12
pyenv global 3.10.12
```

### 2ë‹¨ê³„: í”„ë¡œì íŠ¸ í´ë¡  ë° ê°€ìƒí™˜ê²½ ì„¤ì •

```bash
# í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/MeiGen-AI/MultiTalk.git
cd MultiTalk

# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python -m venv multitalk_env
source multitalk_env/bin/activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 3ë‹¨ê³„: ì¶”ê°€ ì˜ì¡´ì„± ì„¤ì¹˜ (macOS íŠ¹í™”)

```bash
# PyTorch Metal Performance Shader (MPS) ì§€ì› ë²„ì „ ì„¤ì¹˜
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# FFmpeg ì„¤ì¹˜ (ë¹„ë””ì˜¤ ì²˜ë¦¬ìš©)
brew install ffmpeg

# ì¶”ê°€ macOS ì˜ì¡´ì„±
brew install pkg-config
brew install libsndfile
```

### 4ë‹¨ê³„: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# Hugging Face CLI ì„¤ì¹˜
pip install huggingface_hub

# í•„ìš”í•œ ëª¨ë¸ë“¤ ë‹¤ìš´ë¡œë“œ
mkdir -p weights

# ê¸°ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./weights/Wan2.1-I2V-14B-480P

# ì˜¤ë””ì˜¤ ì¸ì½”ë” ë‹¤ìš´ë¡œë“œ
huggingface-cli download TencentGameMate/chinese-wav2vec2-base --local-dir ./weights/chinese-wav2vec2-base
huggingface-cli download TencentGameMate/chinese-wav2vec2-base model.safetensors --revision refs/pr/1 --local-dir ./weights/chinese-wav2vec2-base

# MultiTalk ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
huggingface-cli download MeiGen-AI/MeiGen-MultiTalk --local-dir ./weights/MeiGen-MultiTalk
```

### 5ë‹¨ê³„: ëª¨ë¸ ì„¤ì •

```bash
# ê¸°ì¡´ ì„¤ì • ë°±ì—…
mv weights/Wan2.1-I2V-14B-480P/diffusion_pytorch_model.safetensors.index.json weights/Wan2.1-I2V-14B-480P/diffusion_pytorch_model.safetensors.index.json_old

# MultiTalk ëª¨ë¸ ë§í¬ (ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©)
CURRENT_DIR=$(pwd)
ln -s "$CURRENT_DIR/weights/MeiGen-MultiTalk/diffusion_pytorch_model.safetensors.index.json" weights/Wan2.1-I2V-14B-480P/
ln -s "$CURRENT_DIR/weights/MeiGen-MultiTalk/multitalk.safetensors" weights/Wan2.1-I2V-14B-480P/
```

## ê¸°ë³¸ ì‚¬ìš©ë²•

### ë‹¨ì¼ ì¸ë¬¼ ë¹„ë””ì˜¤ ìƒì„±

```bash
# ê¸°ë³¸ ë‹¨ì¼ ì¸ë¬¼ ìƒì„±
python generate_multitalk.py \
    --ckpt_dir weights/Wan2.1-I2V-14B-480P \
    --wav2vec_dir 'weights/chinese-wav2vec2-base' \
    --input_json examples/single_example_1.json \
    --sample_steps 40 \
    --mode streaming \
    --use_teacache \
    --save_file single_person_output
```

### ë‹¤ì¤‘ ì¸ë¬¼ ë¹„ë””ì˜¤ ìƒì„±

```bash
# ë‹¤ì¤‘ ì¸ë¬¼ ëŒ€í™” ìƒì„±
python generate_multitalk.py \
    --ckpt_dir weights/Wan2.1-I2V-14B-480P \
    --wav2vec_dir 'weights/chinese-wav2vec2-base' \
    --input_json examples/multitalk_example_2.json \
    --sample_steps 40 \
    --mode streaming \
    --use_teacache \
    --save_file multi_person_output
```

### ì €ë©”ëª¨ë¦¬ ëª¨ë“œ (8GB ì´í•˜ RAM)

```bash
# ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰
python generate_multitalk.py \
    --ckpt_dir weights/Wan2.1-I2V-14B-480P \
    --wav2vec_dir 'weights/chinese-wav2vec2-base' \
    --input_json examples/single_example_1.json \
    --sample_steps 40 \
    --mode streaming \
    --num_persistent_param_in_dit 0 \
    --use_teacache \
    --save_file low_vram_output
```

## ë§¥ë¶ ìµœì í™” íŒ

### ì„±ëŠ¥ ìµœì í™”

```bash
# MPS ë°±ì—”ë“œ ì‚¬ìš© (Apple Silicon ê°€ì†)
export PYTORCH_ENABLE_MPS_FALLBACK=1

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
pip install psutil
```

### ë°°ì¹˜ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸

```python
# batch_process.py
import os
import subprocess
import json
from pathlib import Path

def process_audio_batch(audio_dir, output_dir):
    """ì˜¤ë””ì˜¤ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬"""
    audio_files = list(Path(audio_dir).glob("*.wav"))
    
    for audio_file in audio_files:
        # JSON ì„¤ì • íŒŒì¼ ìƒì„±
        config = {
            "reference_image": "examples/reference.jpg",
            "audio_path": str(audio_file),
            "prompt": "A person speaking naturally"
        }
        
        config_path = f"temp_{audio_file.stem}.json"
        with open(config_path, 'w') as f:
            json.dump(config, f)
        
        # MultiTalk ì‹¤í–‰
        cmd = [
            "python", "generate_multitalk.py",
            "--ckpt_dir", "weights/Wan2.1-I2V-14B-480P",
            "--wav2vec_dir", "weights/chinese-wav2vec2-base",
            "--input_json", config_path,
            "--sample_steps", "40",
            "--mode", "streaming",
            "--use_teacache",
            "--save_file", f"{output_dir}/{audio_file.stem}"
        ]
        
        subprocess.run(cmd)
        os.remove(config_path)

if __name__ == "__main__":
    process_audio_batch("input_audio", "output_videos")
```

## ì‹¤ì „ ì‘ìš© í”„ë¡œê·¸ë¨: ëŒ€í™”í˜• êµìœ¡ ì½˜í…ì¸  ìƒì„±ê¸°

ì´ì œ MultiTalkë¥¼ í™œìš©í•œ ì‹¤ì „ ì‘ìš© í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤. êµìœ¡ìš© ëŒ€í™”í˜• ì½˜í…ì¸ ë¥¼ ìë™ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤.

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```bash
mkdir educational_content_generator
cd educational_content_generator

# í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
mkdir -p {src,templates,static,output,config}
touch src/{app.py,content_generator.py,audio_processor.py}
touch templates/index.html
touch config/settings.py
```

### ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜

```python
# src/app.py
import streamlit as st
import json
import os
import subprocess
from pathlib import Path
import tempfile
from audio_processor import AudioProcessor
from content_generator import ContentGenerator

class EducationalContentApp:
    def __init__(self):
        self.audio_processor = AudioProcessor()
        self.content_generator = ContentGenerator()
        
    def run(self):
        st.set_page_config(
            page_title="êµìœ¡ ì½˜í…ì¸  ìƒì„±ê¸°",
            page_icon="ğŸ“",
            layout="wide"
        )
        
        st.title("ğŸ“ AI êµìœ¡ ì½˜í…ì¸  ìƒì„±ê¸°")
        st.markdown("MultiTalkë¥¼ í™œìš©í•œ ëŒ€í™”í˜• êµìœ¡ ë¹„ë””ì˜¤ ìƒì„±")
        
        # ì‚¬ì´ë“œë°” ì„¤ì •
        with st.sidebar:
            st.header("ì„¤ì •")
            
            # ì½˜í…ì¸  íƒ€ì… ì„ íƒ
            content_type = st.selectbox(
                "ì½˜í…ì¸  íƒ€ì…",
                ["ë‹¨ì¼ ê°•ì‚¬", "ëŒ€í™”í˜• ìˆ˜ì—…", "í† ë¡  í˜•ì‹", "Q&A ì„¸ì…˜"]
            )
            
            # ì£¼ì œ ì…ë ¥
            topic = st.text_input("ì£¼ì œ", "íŒŒì´ì¬ ê¸°ì´ˆ í”„ë¡œê·¸ë˜ë°")
            
            # ë‚œì´ë„ ì„ íƒ
            difficulty = st.selectbox(
                "ë‚œì´ë„", 
                ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"]
            )
            
            # ë¹„ë””ì˜¤ ê¸¸ì´
            duration = st.slider("ë¹„ë””ì˜¤ ê¸¸ì´ (ë¶„)", 1, 10, 3)
        
        # ë©”ì¸ ì½˜í…ì¸ 
        col1, col2 = st.columns(2)
        
        with col1:
            st.header("ğŸ“ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±")
            
            if st.button("ìŠ¤í¬ë¦½íŠ¸ ìë™ ìƒì„±"):
                with st.spinner("ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘..."):
                    script = self.content_generator.generate_script(
                        topic, difficulty, duration, content_type
                    )
                    st.session_state.script = script
            
            if hasattr(st.session_state, 'script'):
                edited_script = st.text_area(
                    "ìŠ¤í¬ë¦½íŠ¸ í¸ì§‘", 
                    st.session_state.script, 
                    height=300
                )
                st.session_state.edited_script = edited_script
        
        with col2:
            st.header("ğŸ¤ ì˜¤ë””ì˜¤ ìƒì„±")
            
            # ìŒì„± ì„¤ì •
            voice_type = st.selectbox("ìŒì„± íƒ€ì…", ["ë‚¨ì„±", "ì—¬ì„±", "ì•„ë™"])
            language = st.selectbox("ì–¸ì–´", ["í•œêµ­ì–´", "ì˜ì–´", "ì¤‘êµ­ì–´"])
            
            if hasattr(st.session_state, 'edited_script'):
                if st.button("ìŒì„± ìƒì„±"):
                    with st.spinner("ìŒì„± ìƒì„± ì¤‘..."):
                        audio_path = self.audio_processor.text_to_speech(
                            st.session_state.edited_script,
                            voice_type,
                            language
                        )
                        st.session_state.audio_path = audio_path
                        st.audio(audio_path)
        
        # ë¹„ë””ì˜¤ ìƒì„± ì„¹ì…˜
        st.header("ğŸ¬ ë¹„ë””ì˜¤ ìƒì„±")
        
        col3, col4 = st.columns(2)
        
        with col3:
            # ìºë¦­í„° ì´ë¯¸ì§€ ì—…ë¡œë“œ
            uploaded_image = st.file_uploader(
                "ìºë¦­í„° ì´ë¯¸ì§€ ì—…ë¡œë“œ", 
                type=['jpg', 'jpeg', 'png']
            )
            
            if uploaded_image:
                st.image(uploaded_image, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€")
        
        with col4:
            # ë¹„ë””ì˜¤ ì„¤ì •
            resolution = st.selectbox("í•´ìƒë„", ["480P", "720P"])
            quality = st.selectbox("í’ˆì§ˆ", ["ë¹ ë¦„", "ë³´í†µ", "ê³ í’ˆì§ˆ"])
            
            if hasattr(st.session_state, 'audio_path') and uploaded_image:
                if st.button("ë¹„ë””ì˜¤ ìƒì„±"):
                    with st.spinner("ë¹„ë””ì˜¤ ìƒì„± ì¤‘... (ìˆ˜ ë¶„ ì†Œìš”)"):
                        video_path = self.generate_educational_video(
                            st.session_state.audio_path,
                            uploaded_image,
                            content_type,
                            resolution,
                            quality
                        )
                        
                        if video_path:
                            st.success("ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ!")
                            st.video(video_path)
                            
                            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                            with open(video_path, 'rb') as f:
                                st.download_button(
                                    "ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ",
                                    f.read(),
                                    file_name=f"{topic}_êµìœ¡ë¹„ë””ì˜¤.mp4",
                                    mime="video/mp4"
                                )
    
    def generate_educational_video(self, audio_path, image_file, content_type, resolution, quality):
        """êµìœ¡ ë¹„ë””ì˜¤ ìƒì„±"""
        try:
            # ì„ì‹œ íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_img:
                tmp_img.write(image_file.read())
                image_path = tmp_img.name
            
            # MultiTalk ì„¤ì • íŒŒì¼ ìƒì„±
            config = {
                "reference_image": image_path,
                "audio_path": audio_path,
                "prompt": self.get_prompt_for_content_type(content_type),
                "resolution": resolution.lower(),
                "quality_preset": quality
            }
            
            config_path = "temp_config.json"
            with open(config_path, 'w') as f:
                json.dump(config, f)
            
            # MultiTalk ì‹¤í–‰
            output_name = f"educational_video_{int(time.time())}"
            cmd = [
                "python", "../MultiTalk/generate_multitalk.py",
                "--ckpt_dir", "../MultiTalk/weights/Wan2.1-I2V-14B-480P",
                "--wav2vec_dir", "../MultiTalk/weights/chinese-wav2vec2-base",
                "--input_json", config_path,
                "--sample_steps", "40" if quality != "ê³ í’ˆì§ˆ" else "60",
                "--mode", "streaming",
                "--use_teacache",
                "--save_file", output_name
            ]
            
            if quality == "ë¹ ë¦„":
                cmd.extend(["--num_persistent_param_in_dit", "0"])
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.unlink(image_path)
            os.unlink(config_path)
            
            if result.returncode == 0:
                return f"{output_name}.mp4"
            else:
                st.error(f"ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: {result.stderr}")
                return None
                
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
    
    def get_prompt_for_content_type(self, content_type):
        """ì½˜í…ì¸  íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
        prompts = {
            "ë‹¨ì¼ ê°•ì‚¬": "A professional teacher explaining concepts clearly and enthusiastically",
            "ëŒ€í™”í˜• ìˆ˜ì—…": "Two people having an engaging educational conversation",
            "í† ë¡  í˜•ì‹": "People having a structured academic discussion",
            "Q&A ì„¸ì…˜": "A teacher answering student questions with patience and clarity"
        }
        return prompts.get(content_type, "A person speaking naturally")

if __name__ == "__main__":
    app = EducationalContentApp()
    app.run()
```

### ì½˜í…ì¸  ìƒì„±ê¸°

```python
# src/content_generator.py
import openai
from typing import Dict, List
import json

class ContentGenerator:
    def __init__(self):
        # OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°)
        self.openai_client = openai.OpenAI()
    
    def generate_script(self, topic: str, difficulty: str, duration: int, content_type: str) -> str:
        """ì£¼ì œì™€ ì„¤ì •ì— ë”°ë¥¸ êµìœ¡ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        
        prompt = f"""
        ë‹¤ìŒ ì„¤ì •ì— ë”°ë¼ êµìœ¡ìš© ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
        
        ì£¼ì œ: {topic}
        ë‚œì´ë„: {difficulty}
        ì½˜í…ì¸  íƒ€ì…: {content_type}
        ì˜ˆìƒ ì‹œê°„: {duration}ë¶„
        
        ìš”êµ¬ì‚¬í•­:
        1. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…
        2. ì‹¤ì œ ì˜ˆì‹œì™€ ë¹„ìœ  í¬í•¨
        3. í•™ìŠµìì˜ ì°¸ì—¬ë¥¼ ìœ ë„í•˜ëŠ” ì§ˆë¬¸
        4. ë…¼ë¦¬ì ì¸ êµ¬ì„± (ë„ì…-ì „ê°œ-ì •ë¦¬)
        5. {difficulty} ìˆ˜ì¤€ì— ë§ëŠ” ìš©ì–´ì™€ ê°œë… ì‚¬ìš©
        
        ìŠ¤í¬ë¦½íŠ¸ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ êµìœ¡ ì½˜í…ì¸  ì‘ì„±ìì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            # API ì˜¤ë¥˜ì‹œ ê¸°ë³¸ ìŠ¤í¬ë¦½íŠ¸ ë°˜í™˜
            return self.get_default_script(topic, difficulty)
    
    def get_default_script(self, topic: str, difficulty: str) -> str:
        """ê¸°ë³¸ ìŠ¤í¬ë¦½íŠ¸ í…œí”Œë¦¿"""
        return f"""
        ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€ {topic}ì— ëŒ€í•´ í•™ìŠµí•´ë³´ê² ìŠµë‹ˆë‹¤.
        
        ë¨¼ì € {topic}ì´ ë¬´ì—‡ì¸ì§€ ê¸°ë³¸ ê°œë…ë¶€í„° ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.
        
        [ì£¼ìš” ë‚´ìš© ì„¤ëª… ë¶€ë¶„]
        
        ì´ì œ ì‹¤ì œ ì˜ˆì‹œë¥¼ í†µí•´ ë” ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.
        
        [ì˜ˆì‹œ ë° ì‹¤ìŠµ ë¶€ë¶„]
        
        ë§ˆì§€ë§‰ìœ¼ë¡œ ì˜¤ëŠ˜ í•™ìŠµí•œ ë‚´ìš©ì„ ì •ë¦¬í•´ë³´ê² ìŠµë‹ˆë‹¤.
        
        {topic}ì— ëŒ€í•œ í•™ìŠµì´ ë„ì›€ì´ ë˜ì—ˆê¸°ë¥¼ ë°”ëë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!
        """
    
    def generate_qa_pairs(self, script: str) -> List[Dict]:
        """ìŠ¤í¬ë¦½íŠ¸ì—ì„œ Q&A ìŒ ìƒì„±"""
        prompt = f"""
        ë‹¤ìŒ êµìœ¡ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ 5ê°œì˜ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”:
        
        {script}
        
        JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
        [{"question": "ì§ˆë¬¸", "answer": "ë‹µë³€"}, ...]
        """
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "êµìœ¡ìš© Q&Aë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.5
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            return [{"question": "ì£¼ìš” ê°œë…ì€ ë¬´ì—‡ì¸ê°€ìš”?", "answer": "ìŠ¤í¬ë¦½íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì…ë‹ˆë‹¤."}]
```

### ì˜¤ë””ì˜¤ ì²˜ë¦¬ê¸°

```python
# src/audio_processor.py
import os
import tempfile
from pathlib import Path
import subprocess
from typing import Optional
import librosa
import soundfile as sf

class AudioProcessor:
    def __init__(self):
        self.sample_rate = 22050
        self.temp_dir = Path("temp_audio")
        self.temp_dir.mkdir(exist_ok=True)
    
    def text_to_speech(self, text: str, voice_type: str, language: str) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜"""
        
        # macOSì˜ ë‚´ì¥ TTS ì‚¬ìš©
        voice_map = {
            ("í•œêµ­ì–´", "ë‚¨ì„±"): "Yuna",  # í•œêµ­ì–´ëŠ” Yunaë§Œ ì§€ì›
            ("í•œêµ­ì–´", "ì—¬ì„±"): "Yuna",
            ("ì˜ì–´", "ë‚¨ì„±"): "Alex",
            ("ì˜ì–´", "ì—¬ì„±"): "Samantha",
            ("ì¤‘êµ­ì–´", "ë‚¨ì„±"): "Ting-Ting",
            ("ì¤‘êµ­ì–´", "ì—¬ì„±"): "Ting-Ting"
        }
        
        voice = voice_map.get((language, voice_type), "Samantha")
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(delete=False, suffix='.aiff', dir=self.temp_dir) as tmp_file:
            temp_path = tmp_file.name
        
        # macOS say ëª…ë ¹ì–´ ì‚¬ìš©
        cmd = ["say", "-v", voice, "-o", temp_path, text]
        
        try:
            subprocess.run(cmd, check=True)
            
            # AIFFë¥¼ WAVë¡œ ë³€í™˜
            wav_path = temp_path.replace('.aiff', '.wav')
            audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)
            sf.write(wav_path, audio_data, self.sample_rate)
            
            # ì„ì‹œ AIFF íŒŒì¼ ì‚­ì œ
            os.unlink(temp_path)
            
            return wav_path
            
        except subprocess.CalledProcessError as e:
            print(f"TTS ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def process_audio_for_multitalk(self, audio_path: str) -> str:
        """MultiTalkì— ì í•©í•˜ë„ë¡ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬"""
        
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio_data, sr = librosa.load(audio_path, sr=self.sample_rate)
        
        # ì •ê·œí™”
        audio_data = librosa.util.normalize(audio_data)
        
        # ë¬´ìŒ êµ¬ê°„ ì œê±°
        audio_data, _ = librosa.effects.trim(audio_data, top_db=20)
        
        # ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ì €ì¥
        processed_path = audio_path.replace('.wav', '_processed.wav')
        sf.write(processed_path, audio_data, self.sample_rate)
        
        return processed_path
    
    def split_audio_by_sentences(self, audio_path: str, script: str) -> List[str]:
        """ìŠ¤í¬ë¦½íŠ¸ ë¬¸ì¥ë³„ë¡œ ì˜¤ë””ì˜¤ ë¶„í• """
        sentences = script.split('.')
        audio_data, sr = librosa.load(audio_path, sr=self.sample_rate)
        
        # ê°„ë‹¨í•œ ì‹œê°„ ê¸°ë°˜ ë¶„í•  (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• í•„ìš”)
        duration_per_sentence = len(audio_data) // len(sentences)
        
        audio_segments = []
        for i, sentence in enumerate(sentences):
            if sentence.strip():
                start_idx = i * duration_per_sentence
                end_idx = (i + 1) * duration_per_sentence
                
                segment = audio_data[start_idx:end_idx]
                segment_path = f"{self.temp_dir}/segment_{i}.wav"
                sf.write(segment_path, segment, sr)
                audio_segments.append(segment_path)
        
        return audio_segments
```

### ì‹¤í–‰ ë°©ë²•

```bash
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
cd educational_content_generator
pip install streamlit openai librosa soundfile
streamlit run src/app.py
```

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜ì™€ í•´ê²°ë°©ë²•

```bash
# CUDA ê´€ë ¨ ì˜¤ë¥˜ (macOSì—ì„œëŠ” í•´ë‹¹ì—†ìŒ)
export CUDA_VISIBLE_DEVICES=""

# ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0

# ffmpeg ì˜¤ë¥˜
brew reinstall ffmpeg

# ê¶Œí•œ ì˜¤ë¥˜
chmod +x generate_multitalk.py
```

### ì„±ëŠ¥ ìµœì í™”

```python
# config/optimization.py
import torch
import os

def optimize_for_macos():
    """macOS ìµœì í™” ì„¤ì •"""
    
    # MPS ë°±ì—”ë“œ ì‚¬ìš©
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    else:
        device = torch.device("cpu")
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    torch.backends.cudnn.benchmark = False
    
    return device
```

## ë§ˆë¬´ë¦¬

MultiTalkë¥¼ ë§¥ë¶ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì„¤ì¹˜í•˜ê³  í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤. ë‹¨ìˆœí•œ ë¹„ë””ì˜¤ ìƒì„±ì„ ë„˜ì–´ì„œ êµìœ¡ ì½˜í…ì¸  ìƒì„±ê¸°ì™€ ê°™ì€ ì‹¤ìš©ì ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ

1. **ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**: ë” ì •êµí•œ ìºë¦­í„° ì œì–´
2. **ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ**: ëŒ€ëŸ‰ì˜ ì½˜í…ì¸  ìë™ ìƒì„±
3. **ì›¹ ì„œë¹„ìŠ¤ ë°°í¬**: Flask/FastAPIë¥¼ í†µí•œ ì„œë¹„ìŠ¤í™”
4. **í’ˆì§ˆ ê°œì„ **: í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

MultiTalkì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ì°½ì˜ì ì´ê³  ì‹¤ìš©ì ì¸ AI ë¹„ë””ì˜¤ ìƒì„± ì†”ë£¨ì…˜ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”! 