---
title: "ScrapeGraphAI ì™„ì „ ê°€ì´ë“œ: AI ê¸°ë°˜ ì§€ëŠ¥í˜• ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ í˜ì‹ í•˜ê¸°"
excerpt: "LLMì„ í™œìš©í•œ í˜ì‹ ì ì¸ ì›¹ ìŠ¤í¬ë˜í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ ScrapeGraphAIë¡œ ë³µì¡í•œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì •í™•í•œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ í•™ìŠµí•˜ëŠ” ì™„ì „í•œ ì‹¤ì „ ê°€ì´ë“œ"
seo_title: "ScrapeGraphAI AI ì›¹ ìŠ¤í¬ë˜í•‘ íŠœí† ë¦¬ì–¼ - LLM ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ ì™„ì „ ê°€ì´ë“œ - Thaki Cloud"
seo_description: "ScrapeGraphAIë¥¼ ì‚¬ìš©í•œ AI ê¸°ë°˜ ì›¹ ìŠ¤í¬ë˜í•‘ ë°©ë²•ì„ ì‹¤ì „ ì˜ˆì œì™€ í•¨ê»˜ ìƒì„¸íˆ ì•Œì•„ë³´ì„¸ìš”. OpenAI, Ollama ì—°ë™ë¶€í„° ê³ ê¸‰ íŒŒì´í”„ë¼ì¸ê¹Œì§€ í¬í•¨ëœ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤."
date: 2025-07-23
last_modified_at: 2025-07-23
categories:
  - tutorials
  - dev
tags:
  - ScrapeGraphAI
  - AIìŠ¤í¬ë˜í•‘
  - ì›¹í¬ë¡¤ë§
  - LLMë°ì´í„°ì¶”ì¶œ
  - PythonìŠ¤í¬ë˜í•‘
  - OpenAI
  - Ollama
  - ìë™í™”
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/tutorials/scrapegraph-ai-intelligent-web-scraping-complete-tutorial/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 18ë¶„

## ì„œë¡ 

ì „í†µì ì¸ ì›¹ ìŠ¤í¬ë˜í•‘ì€ CSS ì„ íƒìë‚˜ XPathë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‘ì„±í•˜ê³  ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ì— ëŒ€ì‘í•˜ê¸° ì–´ë ¤ìš´ ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ì œ AIì˜ ì‹œëŒ€ê°€ ì™”ìŠµë‹ˆë‹¤. [ScrapeGraphAI](https://github.com/ScrapeGraphAI/Scrapegraph-ai)ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•˜ì—¬ ìì—°ì–´ í”„ë¡¬í”„íŠ¸ë§Œìœ¼ë¡œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” í˜ì‹ ì ì¸ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ScrapeGraphAIì˜ í•µì‹¬ ê¸°ëŠ¥ë¶€í„° ê³ ê¸‰ í™œìš©ë²•ê¹Œì§€, ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ì™„ì „í•œ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. 20.5k ìŠ¤íƒ€ë¥¼ ë°›ìœ¼ë©° ê¸‰ì„±ì¥í•˜ê³  ìˆëŠ” ì´ ë„êµ¬ë¡œ ë°ì´í„° ìˆ˜ì§‘ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ê²½í—˜í•´ë³´ì„¸ìš”.

## ScrapeGraphAI ê°œìš”

### í•µì‹¬ íŠ¹ì§•

**ğŸ¤– AI ê¸°ë°˜ ìŠ¤í¬ë˜í•‘**: CSS ì„ íƒì ëŒ€ì‹  ìì—°ì–´ë¡œ ìŠ¤í¬ë˜í•‘ ë¡œì§ ì •ì˜  
**ğŸ”„ ì ì‘í˜• íŒŒì‹±**: ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ì— ìë™ ì ì‘  
**ğŸ¯ ì •í™•í•œ ì¶”ì¶œ**: LLMì˜ ì»¨í…ìŠ¤íŠ¸ ì´í•´ ëŠ¥ë ¥ìœ¼ë¡œ ë†’ì€ ì •í™•ë„  
**ğŸŒ ë‹¤ì–‘í•œ ì†ŒìŠ¤**: ì›¹í˜ì´ì§€, ë¡œì»¬ íŒŒì¼, API ì‘ë‹µ ì²˜ë¦¬  
**ğŸ”§ ìœ ì—°í•œ ì„¤ì •**: ë‹¤ì–‘í•œ LLM ëª¨ë¸ê³¼ ì„¤ì • ì§€ì›

### ì§€ì› ê¸°ëŠ¥

```yaml
features:
  scraping_types:
    - single_page: "SmartScraperGraph"
    - multi_page: "SmartScraperMultiGraph"
    - search_results: "SearchGraph"
    - script_generation: "ScriptCreatorGraph"
    - audio_generation: "SpeechGraph"
  
  llm_support:
    - cloud: ["OpenAI", "Google Gemini", "Anthropic Claude", "Groq"]
    - local: ["Ollama", "Hugging Face"]
    - azure: ["Azure OpenAI"]
  
  integrations:
    - apis: "Official REST API"
    - sdks: ["Python SDK", "Node.js SDK"]
    - frameworks: ["LangChain", "LlamaIndex", "CrewAI"]
    - no_code: ["Zapier", "n8n", "Pipedream"]
```

## í™˜ê²½ ì„¤ì •

### 1. ê¸°ë³¸ ì„¤ì¹˜

```bash
# ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)
python -m venv scrapegraph_env
source scrapegraph_env/bin/activate  # Windows: scrapegraph_env\Scripts\activate

# ScrapeGraphAI ì„¤ì¹˜
pip install scrapegraphai

# ë¸Œë¼ìš°ì € ì—”ì§„ ì„¤ì¹˜ (ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°ìš©)
playwright install

# ì¶”ê°€ ì˜ì¡´ì„± (í•„ìš”ì‹œ)
pip install torch torchvision torchaudio  # ë¡œì»¬ ëª¨ë¸ìš©
pip install ollama  # Ollama ì‚¬ìš©ì‹œ
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
cat > .env << EOF
# OpenAI ì„¤ì •
OPENAI_API_KEY="your_openai_api_key"

# Google Gemini ì„¤ì •
GOOGLE_API_KEY="your_google_api_key"

# Groq ì„¤ì •
GROQ_API_KEY="your_groq_api_key"

# Azure OpenAI ì„¤ì •
AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/"
AZURE_OPENAI_API_KEY="your_azure_api_key"
AZURE_OPENAI_API_VERSION="2023-05-15"

# í”„ë¡ì‹œ ì„¤ì • (í•„ìš”ì‹œ)
HTTP_PROXY="http://proxy.company.com:8080"
HTTPS_PROXY="http://proxy.company.com:8080"
EOF
```

### 3. Ollama ë¡œì»¬ ëª¨ë¸ ì„¤ì •

```bash
# Ollama ì„¤ì¹˜ (macOS)
brew install ollama

# Ollama ì„œë¹„ìŠ¤ ì‹œì‘
ollama serve

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull llama3.2        # 8B ëª¨ë¸
ollama pull llama3.2:70b    # 70B ëª¨ë¸ (ë” ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
ollama pull codellama       # ì½”ë“œ ìƒì„± íŠ¹í™”
ollama pull mistral         # ê²½ëŸ‰ ëª¨ë¸

# ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
ollama list
```

## ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. SmartScraperGraph ê¸°ë³¸ ì˜ˆì œ

```python
# basic_scraping.py
import json
from scrapegraphai.graphs import SmartScraperGraph

def basic_scraping_example():
    """ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ ì˜ˆì œ"""
    
    # Ollamaë¥¼ ì‚¬ìš©í•œ ì„¤ì •
    graph_config = {
        "llm": {
            "model": "ollama/llama3.2",
            "model_tokens": 8192,
            "base_url": "http://localhost:11434"
        },
        "verbose": True,
        "headless": True,  # ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¹€
    }
    
    # ìŠ¤í¬ë˜í¼ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    smart_scraper = SmartScraperGraph(
        prompt="íšŒì‚¬ ì†Œê°œ, ì°½ë¦½ì ì •ë³´, ì†Œì…œ ë¯¸ë””ì–´ ë§í¬ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”",
        source="https://scrapegraphai.com/",
        config=graph_config
    )
    
    # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
    result = smart_scraper.run()
    
    print(json.dumps(result, indent=2, ensure_ascii=False))
    return result

if __name__ == "__main__":
    basic_scraping_example()
```

### 2. ë‹¤ì–‘í•œ LLM ì„¤ì • ì˜ˆì œ

```python
# llm_configurations.py
from scrapegraphai.graphs import SmartScraperGraph

class LLMConfigurations:
    @staticmethod
    def openai_config():
        """OpenAI GPT ì„¤ì •"""
        return {
            "llm": {
                "api_key": "your_openai_api_key",
                "model": "openai/gpt-4o-mini",
                "temperature": 0.1,
                "max_tokens": 2048
            },
            "verbose": True,
            "headless": True
        }
    
    @staticmethod
    def groq_config():
        """Groq ì„¤ì • (ë¹ ë¥¸ ì¶”ë¡ )"""
        return {
            "llm": {
                "api_key": "your_groq_api_key",
                "model": "groq/llama-3.1-70b-versatile",
                "temperature": 0
            },
            "verbose": True,
            "headless": True
        }
    
    @staticmethod
    def gemini_config():
        """Google Gemini ì„¤ì •"""
        return {
            "llm": {
                "api_key": "your_google_api_key",
                "model": "google/gemini-1.5-flash",
                "temperature": 0.1
            },
            "verbose": True,
            "headless": True
        }
    
    @staticmethod
    def ollama_config(model_name="llama3.2"):
        """Ollama ë¡œì»¬ ëª¨ë¸ ì„¤ì •"""
        return {
            "llm": {
                "model": f"ollama/{model_name}",
                "model_tokens": 8192,
                "base_url": "http://localhost:11434"
            },
            "verbose": True,
            "headless": True,
            "loader_kwargs": {
                "requests_per_second": 1,  # ìš”ì²­ ì œí•œ
                "load_timeout": 30
            }
        }

def test_different_llms():
    """ë‹¤ì–‘í•œ LLM í…ŒìŠ¤íŠ¸"""
    configs = LLMConfigurations()
    
    test_cases = [
        ("Ollama", configs.ollama_config()),
        ("OpenAI", configs.openai_config()),
        ("Groq", configs.groq_config()),
        ("Gemini", configs.gemini_config())
    ]
    
    prompt = "ì´ ì›¹ì‚¬ì´íŠ¸ì˜ ì£¼ìš” ì œí’ˆì´ë‚˜ ì„œë¹„ìŠ¤ë¥¼ 3ê°œ ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”"
    url = "https://openai.com"
    
    for name, config in test_cases:
        try:
            print(f"\n=== {name} í…ŒìŠ¤íŠ¸ ===")
            scraper = SmartScraperGraph(
                prompt=prompt,
                source=url,
                config=config
            )
            result = scraper.run()
            print(f"ê²°ê³¼: {result}")
        except Exception as e:
            print(f"{name} ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    test_different_llms()
```

## ê³ ê¸‰ ìŠ¤í¬ë˜í•‘ íŒŒì´í”„ë¼ì¸

### 1. MultiGraphë¡œ ì—¬ëŸ¬ í˜ì´ì§€ ì²˜ë¦¬

```python
# multi_page_scraping.py
import json
from scrapegraphai.graphs import SmartScraperMultiGraph

def multi_page_scraping():
    """ì—¬ëŸ¬ í˜ì´ì§€ì—ì„œ ì¼ê´€ëœ ì •ë³´ ì¶”ì¶œ"""
    
    graph_config = {
        "llm": {
            "model": "ollama/llama3.2",
            "model_tokens": 8192
        },
        "verbose": True,
        "headless": True,
        "max_concurrent": 3  # ë™ì‹œ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜
    }
    
    # AI ê´€ë ¨ ê¸°ì—…ë“¤ì˜ ì •ë³´ ìˆ˜ì§‘
    sources = [
        "https://openai.com",
        "https://anthropic.com", 
        "https://huggingface.co",
        "https://cohere.ai",
        "https://stability.ai"
    ]
    
    # í†µì¼ëœ í”„ë¡¬í”„íŠ¸ë¡œ ì •ë³´ ì¶”ì¶œ
    prompt = """
    ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•íƒœë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
    - company_name: íšŒì‚¬ëª…
    - main_product: ì£¼ìš” ì œí’ˆ/ì„œë¹„ìŠ¤
    - founded_year: ì„¤ë¦½ì—°ë„ (ê°€ëŠ¥í•œ ê²½ìš°)
    - headquarters: ë³¸ì‚¬ ìœ„ì¹˜
    - key_technology: í•µì‹¬ ê¸°ìˆ 
    """
    
    multi_scraper = SmartScraperMultiGraph(
        prompt=prompt,
        source=sources,
        config=graph_config
    )
    
    results = multi_scraper.run()
    
    # ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥
    formatted_results = []
    for i, result in enumerate(results):
        formatted_results.append({
            "url": sources[i],
            "data": result
        })
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open("ai_companies_data.json", "w", encoding="utf-8") as f:
        json.dump(formatted_results, f, indent=2, ensure_ascii=False)
    
    print("ë‹¤ì¤‘ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ. ê²°ê³¼: ai_companies_data.json")
    return formatted_results

if __name__ == "__main__":
    multi_page_scraping()
```

### 2. SearchGraphë¡œ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬

```python
# search_scraping.py
from scrapegraphai.graphs import SearchGraph

def search_based_scraping():
    """ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ì •ë³´ ìˆ˜ì§‘"""
    
    graph_config = {
        "llm": {
            "model": "ollama/llama3.2",
            "model_tokens": 8192
        },
        "verbose": True,
        "headless": True,
        "max_results": 5  # ìƒìœ„ 5ê°œ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì²˜ë¦¬
    }
    
    # ê²€ìƒ‰ ê¸°ë°˜ ìŠ¤í¬ë˜í•‘
    search_scraper = SearchGraph(
        prompt="""
        ìµœì‹  AI íŠ¸ë Œë“œì™€ ê´€ë ¨ëœ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
        - trend_name: íŠ¸ë Œë“œ ì´ë¦„
        - description: ê°„ë‹¨í•œ ì„¤ëª…
        - key_companies: ê´€ë ¨ ì£¼ìš” ê¸°ì—…ë“¤
        - impact_level: ì˜í–¥ë„ (High/Medium/Low)
        """,
        config=graph_config
    )
    
    # ê²€ìƒ‰ì–´ë¡œ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
    search_query = "AI trends 2024 artificial intelligence latest"
    results = search_scraper.run(search_query)
    
    print(json.dumps(results, indent=2, ensure_ascii=False))
    return results

def news_monitoring():
    """ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì˜ˆì œ"""
    
    config = {
        "llm": {
            "model": "ollama/llama3.2",
            "model_tokens": 8192
        },
        "verbose": True,
        "headless": True,
        "max_results": 3
    }
    
    news_scraper = SearchGraph(
        prompt="""
        ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
        - headline: ì œëª©
        - summary: 3ì¤„ ìš”ì•½
        - date: ë°œí–‰ì¼
        - sentiment: ê¸ì •/ë¶€ì •/ì¤‘ë¦½
        - key_points: í•µì‹¬ í¬ì¸íŠ¸ 3ê°œ
        """,
        config=config
    )
    
    search_queries = [
        "OpenAI GPT-5 news latest",
        "Google Gemini updates 2024",
        "Claude AI Anthropic developments"
    ]
    
    all_results = {}
    for query in search_queries:
        print(f"\nê²€ìƒ‰ ì¤‘: {query}")
        results = news_scraper.run(query)
        all_results[query] = results
    
    return all_results

if __name__ == "__main__":
    print("=== ê²€ìƒ‰ ê¸°ë°˜ ìŠ¤í¬ë˜í•‘ ===")
    search_based_scraping()
    
    print("\n=== ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ===")
    news_monitoring()
```

### 3. ScriptCreatorGraphë¡œ ì½”ë“œ ìƒì„±

```python
# script_generator.py
from scrapegraphai.graphs import ScriptCreatorGraph

def generate_scraping_script():
    """ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸ ìë™ ìƒì„±"""
    
    graph_config = {
        "llm": {
            "model": "ollama/codellama",  # ì½”ë“œ ìƒì„± íŠ¹í™” ëª¨ë¸
            "model_tokens": 8192
        },
        "verbose": True,
        "headless": True
    }
    
    script_creator = ScriptCreatorGraph(
        prompt="""
        ì´ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì œí’ˆ ì •ë³´ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ëŠ” Python ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
        ì¶”ì¶œí•  ì •ë³´:
        - ì œí’ˆëª…
        - ê°€ê²©
        - ì„¤ëª…
        - í‰ì 
        - ë¦¬ë·° ìˆ˜
        
        BeautifulSoupê³¼ requestsë¥¼ ì‚¬ìš©í•´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """,
        source="https://example-ecommerce.com/products",
        config=graph_config
    )
    
    generated_script = script_creator.run()
    
    # ìƒì„±ëœ ìŠ¤í¬ë¦½íŠ¸ ì €ì¥
    with open("generated_scraper.py", "w", encoding="utf-8") as f:
        f.write(generated_script)
    
    print("ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸ê°€ generated_scraper.pyë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ìƒì„±ëœ ìŠ¤í¬ë¦½íŠ¸:\n{generated_script}")
    
    return generated_script

def generate_multiple_scripts():
    """ì—¬ëŸ¬ ìš©ë„ì˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    
    config = {
        "llm": {
            "model": "ollama/codellama",
            "model_tokens": 8192
        },
        "verbose": True,
        "headless": True
    }
    
    script_requests = [
        {
            "name": "job_scraper",
            "prompt": "ì±„ìš© ì •ë³´ ì‚¬ì´íŠ¸ì—ì„œ ì§ë¬´, íšŒì‚¬ëª…, ì—°ë´‰, ì§€ì—­ì„ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸",
            "url": "https://jobs.example.com"
        },
        {
            "name": "news_scraper", 
            "prompt": "ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ í—¤ë“œë¼ì¸, ë³¸ë¬¸, ì‘ì„±ì, ë‚ ì§œë¥¼ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸",
            "url": "https://news.example.com"
        },
        {
            "name": "social_scraper",
            "prompt": "ì†Œì…œ ë¯¸ë””ì–´ì—ì„œ ê²Œì‹œë¬¼, ì¢‹ì•„ìš” ìˆ˜, ëŒ“ê¸€ ìˆ˜ë¥¼ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸",
            "url": "https://social.example.com"
        }
    ]
    
    for request in script_requests:
        try:
            creator = ScriptCreatorGraph(
                prompt=request["prompt"],
                source=request["url"],
                config=config
            )
            
            script = creator.run()
            
            filename = f"{request['name']}.py"
            with open(filename, "w", encoding="utf-8") as f:
                f.write(script)
            
            print(f"âœ… {filename} ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ {request['name']} ìƒì„± ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    generate_scraping_script()
    generate_multiple_scripts()
```

## ì‹¤ì „ í™œìš© ì‚¬ë¡€

### 1. ê²½ìŸì‚¬ ë¶„ì„ ìë™í™”

```python
# competitor_analysis.py
import json
import pandas as pd
from datetime import datetime
from scrapegraphai.graphs import SmartScraperMultiGraph

class CompetitorAnalyzer:
    def __init__(self, config):
        self.config = config
        self.results = []
    
    def analyze_competitors(self, competitor_urls):
        """ê²½ìŸì‚¬ ì›¹ì‚¬ì´íŠ¸ ë¶„ì„"""
        
        analysis_prompt = """
        ì´ íšŒì‚¬ì˜ ë‹¤ìŒ ì •ë³´ë¥¼ ë¶„ì„í•´ì„œ JSON í˜•íƒœë¡œ ì œê³µí•´ì£¼ì„¸ìš”:
        
        {
            "company_info": {
                "name": "íšŒì‚¬ëª…",
                "tagline": "ìŠ¬ë¡œê±´/íƒœê·¸ë¼ì¸",
                "founding_year": "ì„¤ë¦½ì—°ë„",
                "employee_count": "ì§ì› ìˆ˜ (ì¶”ì •)"
            },
            "products_services": [
                {
                    "name": "ì œí’ˆ/ì„œë¹„ìŠ¤ëª…",
                    "category": "ì¹´í…Œê³ ë¦¬",
                    "description": "ê°„ë‹¨í•œ ì„¤ëª…",
                    "target_market": "íƒ€ê²Ÿ ì‹œì¥"
                }
            ],
            "pricing": {
                "model": "ê°€ê²© ëª¨ë¸ (êµ¬ë…/ì¼íšŒì„±/í”„ë¦¬ë¯¸ì—„ ë“±)",
                "starting_price": "ì‹œì‘ ê°€ê²©",
                "enterprise_available": "ê¸°ì—…ìš© í”Œëœ ì—¬ë¶€"
            },
            "technology": {
                "stack": ["ì‚¬ìš© ê¸°ìˆ  ìŠ¤íƒ"],
                "ai_integration": "AI ê¸°ìˆ  í™œìš© ì—¬ë¶€",
                "api_available": "API ì œê³µ ì—¬ë¶€"
            },
            "marketing": {
                "value_propositions": ["í•µì‹¬ ê°€ì¹˜ ì œì•ˆë“¤"],
                "customer_testimonials": "ê³ ê° í›„ê¸° ìœ ë¬´",
                "case_studies": "ì‚¬ë¡€ ì—°êµ¬ ìœ ë¬´"
            },
            "social_presence": {
                "linkedin": "LinkedIn URL",
                "twitter": "Twitter URL", 
                "github": "GitHub URL",
                "blog": "ë¸”ë¡œê·¸ URL"
            }
        }
        """
        
        multi_scraper = SmartScraperMultiGraph(
            prompt=analysis_prompt,
            source=competitor_urls,
            config=self.config
        )
        
        results = multi_scraper.run()
        
        # ê²°ê³¼ ì •ë¦¬
        for i, result in enumerate(results):
            self.results.append({
                "url": competitor_urls[i],
                "timestamp": datetime.now().isoformat(),
                "analysis": result
            })
        
        return self.results
    
    def save_analysis(self, filename="competitor_analysis.json"):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"ë¶„ì„ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def generate_comparison_report(self):
        """ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.results:
            print("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë¹„êµ ë°ì´í„° ì¶”ì¶œ
        comparison_data = []
        for result in self.results:
            analysis = result.get("analysis", {})
            company_info = analysis.get("company_info", {})
            pricing = analysis.get("pricing", {})
            
            comparison_data.append({
                "URL": result["url"],
                "íšŒì‚¬ëª…": company_info.get("name", "N/A"),
                "ìŠ¬ë¡œê±´": company_info.get("tagline", "N/A"),
                "ì„¤ë¦½ì—°ë„": company_info.get("founding_year", "N/A"),
                "ê°€ê²©ëª¨ë¸": pricing.get("model", "N/A"),
                "ì‹œì‘ê°€ê²©": pricing.get("starting_price", "N/A"),
                "AIí™œìš©": analysis.get("technology", {}).get("ai_integration", "N/A")
            })
        
        # DataFrame ìƒì„± ë° ì €ì¥
        df = pd.DataFrame(comparison_data)
        df.to_excel("competitor_comparison.xlsx", index=False, engine='openpyxl')
        
        print("ë¹„êµ ë¦¬í¬íŠ¸ê°€ competitor_comparison.xlsxì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(df.to_string(index=False))

def run_competitor_analysis():
    """ê²½ìŸì‚¬ ë¶„ì„ ì‹¤í–‰"""
    
    config = {
        "llm": {
            "model": "ollama/llama3.2",
            "model_tokens": 8192
        },
        "verbose": True,
        "headless": True,
        "max_concurrent": 2
    }
    
    # AI ë„êµ¬ ê²½ìŸì‚¬ë“¤
    competitors = [
        "https://openai.com",
        "https://anthropic.com",
        "https://cohere.ai",
        "https://huggingface.co",
        "https://replicate.com"
    ]
    
    analyzer = CompetitorAnalyzer(config)
    analyzer.analyze_competitors(competitors)
    analyzer.save_analysis()
    analyzer.generate_comparison_report()

if __name__ == "__main__":
    run_competitor_analysis()
```

### 2. ì‹¤ì‹œê°„ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§

```python
# news_monitor.py
import json
import time
from datetime import datetime, timedelta
from scrapegraphai.graphs import SearchGraph

class NewsMonitor:
    def __init__(self, config):
        self.config = config
        self.monitored_keywords = []
        self.alerts = []
    
    def add_keywords(self, keywords):
        """ëª¨ë‹ˆí„°ë§í•  í‚¤ì›Œë“œ ì¶”ê°€"""
        self.monitored_keywords.extend(keywords)
    
    def monitor_news(self, interval_minutes=30):
        """ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        
        news_prompt = """
        ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
        
        {
            "articles": [
                {
                    "title": "ê¸°ì‚¬ ì œëª©",
                    "summary": "3ì¤„ ìš”ì•½",
                    "published_date": "ë°œí–‰ì¼",
                    "source": "ì¶œì²˜",
                    "sentiment": "ê¸ì •/ë¶€ì •/ì¤‘ë¦½",
                    "importance": "ë†’ìŒ/ë³´í†µ/ë‚®ìŒ",
                    "key_points": ["í•µì‹¬ í¬ì¸íŠ¸ 1", "í•µì‹¬ í¬ì¸íŠ¸ 2"],
                    "companies_mentioned": ["ì–¸ê¸‰ëœ íšŒì‚¬ë“¤"],
                    "technologies_mentioned": ["ì–¸ê¸‰ëœ ê¸°ìˆ ë“¤"]
                }
            ],
            "trend_analysis": "ì „ë°˜ì ì¸ íŠ¸ë Œë“œ ë¶„ì„"
        }
        """
        
        search_graph = SearchGraph(
            prompt=news_prompt,
            config=self.config
        )
        
        print(f"ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ê°„ê²©: {interval_minutes}ë¶„)")
        
        while True:
            try:
                current_time = datetime.now()
                print(f"\n[{current_time}] ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
                
                for keyword in self.monitored_keywords:
                    # ìµœê·¼ 24ì‹œê°„ ë‰´ìŠ¤ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
                    search_query = f"{keyword} news latest 24 hours"
                    results = search_graph.run(search_query)
                    
                    # ì¤‘ìš”í•œ ë‰´ìŠ¤ í•„í„°ë§
                    important_news = self._filter_important_news(results, keyword)
                    
                    if important_news:
                        self._send_alert(keyword, important_news)
                
                print(f"ë‹¤ìŒ ìˆ˜ì§‘ê¹Œì§€ {interval_minutes}ë¶„ ëŒ€ê¸°...")
                time.sleep(interval_minutes * 60)
                
            except KeyboardInterrupt:
                print("\nëª¨ë‹ˆí„°ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(60)  # ì˜¤ë¥˜ ì‹œ 1ë¶„ ëŒ€ê¸°
    
    def _filter_important_news(self, results, keyword):
        """ì¤‘ìš”í•œ ë‰´ìŠ¤ í•„í„°ë§"""
        if not isinstance(results, dict) or 'articles' not in results:
            return []
        
        important_articles = []
        for article in results['articles']:
            # ì¤‘ìš”ë„ê°€ 'ë†’ìŒ'ì´ê±°ë‚˜ ë¶€ì •ì  ë‰´ìŠ¤ì¸ ê²½ìš°
            importance = article.get('importance', '').lower()
            sentiment = article.get('sentiment', '').lower()
            
            if importance == 'ë†’ìŒ' or sentiment == 'ë¶€ì •':
                article['keyword'] = keyword
                article['detected_at'] = datetime.now().isoformat()
                important_articles.append(article)
        
        return important_articles
    
    def _send_alert(self, keyword, articles):
        """ì•Œë¦¼ ë°œì†¡"""
        alert = {
            "timestamp": datetime.now().isoformat(),
            "keyword": keyword,
            "article_count": len(articles),
            "articles": articles
        }
        
        self.alerts.append(alert)
        
        # ì½˜ì†” ì•Œë¦¼
        print(f"\nğŸš¨ ì•Œë¦¼: '{keyword}' ê´€ë ¨ ì¤‘ìš” ë‰´ìŠ¤ {len(articles)}ê±´ ë°œê²¬!")
        for article in articles:
            print(f"  ğŸ“° {article['title']}")
            print(f"     ì¤‘ìš”ë„: {article['importance']}, ê°ì •: {article['sentiment']}")
        
        # íŒŒì¼ë¡œ ì €ì¥
        self._save_alerts()
    
    def _save_alerts(self):
        """ì•Œë¦¼ ë‚´ì—­ ì €ì¥"""
        with open("news_alerts.json", "w", encoding="utf-8") as f:
            json.dump(self.alerts, f, indent=2, ensure_ascii=False)
    
    def get_daily_summary(self):
        """ì¼ì¼ ìš”ì•½ ìƒì„±"""
        today = datetime.now().date()
        today_alerts = [
            alert for alert in self.alerts 
            if datetime.fromisoformat(alert['timestamp']).date() == today
        ]
        
        if not today_alerts:
            print("ì˜¤ëŠ˜ ì•Œë¦¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“Š {today} ë‰´ìŠ¤ ìš”ì•½")
        print("=" * 50)
        
        for alert in today_alerts:
            print(f"í‚¤ì›Œë“œ: {alert['keyword']}")
            print(f"ê¸°ì‚¬ ìˆ˜: {alert['article_count']}")
            print("-" * 30)

def setup_news_monitoring():
    """ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
    
    config = {
        "llm": {
            "model": "ollama/llama3.2",
            "model_tokens": 8192
        },
        "verbose": False,  # ë¡œê·¸ ìµœì†Œí™”
        "headless": True,
        "max_results": 5
    }
    
    monitor = NewsMonitor(config)
    
    # ëª¨ë‹ˆí„°ë§í•  í‚¤ì›Œë“œ ì„¤ì •
    ai_keywords = [
        "OpenAI GPT-5",
        "Google Gemini Advanced", 
        "Anthropic Claude",
        "AI regulation",
        "artificial general intelligence AGI"
    ]
    
    monitor.add_keywords(ai_keywords)
    
    try:
        # 30ë¶„ ê°„ê²©ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§
        monitor.monitor_news(interval_minutes=30)
    except KeyboardInterrupt:
        print("\nëª¨ë‹ˆí„°ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        monitor.get_daily_summary()

if __name__ == "__main__":
    setup_news_monitoring()
```

### 3. ê°€ê²© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

```python
# price_monitor.py
import json
import pandas as pd
from datetime import datetime
from scrapegraphai.graphs import SmartScraperMultiGraph
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

class PriceMonitor:
    def __init__(self, config):
        self.config = config
        self.products = []
        self.price_history = []
    
    def add_product(self, name, url, target_price=None):
        """ëª¨ë‹ˆí„°ë§í•  ì œí’ˆ ì¶”ê°€"""
        product = {
            "name": name,
            "url": url,
            "target_price": target_price,
            "added_date": datetime.now().isoformat()
        }
        self.products.append(product)
    
    def check_prices(self):
        """ê°€ê²© í™•ì¸"""
        
        price_prompt = """
        ì´ ì‡¼í•‘ëª° ìƒí’ˆ í˜ì´ì§€ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
        
        {
            "product_name": "ìƒí’ˆëª…",
            "current_price": "í˜„ì¬ ê°€ê²© (ìˆ«ìë§Œ)",
            "original_price": "ì •ê°€ (ìˆ«ìë§Œ, í• ì¸ ì „ ê°€ê²©)",
            "discount_rate": "í• ì¸ìœ¨ (%)",
            "availability": "ì¬ê³  ìƒíƒœ (ì¬ê³ ìˆìŒ/í’ˆì ˆ/ì˜ˆì•½ì£¼ë¬¸)",
            "seller": "íŒë§¤ì",
            "rating": "í‰ì ",
            "review_count": "ë¦¬ë·° ìˆ˜",
            "shipping_cost": "ë°°ì†¡ë¹„",
            "estimated_delivery": "ë°°ì†¡ ì˜ˆìƒì¼"
        }
        
        ê°€ê²©ì€ ìˆ«ìë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”. í†µí™” ê¸°í˜¸ë‚˜ ì½¤ë§ˆëŠ” ì œì™¸í•˜ê³ ìš”.
        """
        
        urls = [product["url"] for product in self.products]
        
        multi_scraper = SmartScraperMultiGraph(
            prompt=price_prompt,
            source=urls,
            config=self.config
        )
        
        results = multi_scraper.run()
        
        # ê²°ê³¼ ì²˜ë¦¬
        timestamp = datetime.now().isoformat()
        
        for i, result in enumerate(results):
            product = self.products[i]
            
            price_data = {
                "timestamp": timestamp,
                "product_name": product["name"],
                "url": product["url"],
                "target_price": product["target_price"],
                "scraped_data": result
            }
            
            self.price_history.append(price_data)
            
            # ê°€ê²© ì•Œë¦¼ í™•ì¸
            self._check_price_alert(product, result)
        
        self._save_price_history()
        return results
    
    def _check_price_alert(self, product, scraped_data):
        """ê°€ê²© ì•Œë¦¼ í™•ì¸"""
        if not product["target_price"]:
            return
        
        current_price_str = scraped_data.get("current_price", "")
        
        try:
            # ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
            current_price = float(''.join(filter(str.isdigit, str(current_price_str))))
            target_price = float(product["target_price"])
            
            if current_price <= target_price:
                self._send_price_alert(product, current_price, scraped_data)
                
        except (ValueError, TypeError):
            print(f"ê°€ê²© íŒŒì‹± ì˜¤ë¥˜: {current_price_str}")
    
    def _send_price_alert(self, product, current_price, scraped_data):
        """ê°€ê²© ì•Œë¦¼ ë°œì†¡"""
        print(f"\nğŸ¯ ê°€ê²© ì•Œë¦¼!")
        print(f"ìƒí’ˆ: {product['name']}")
        print(f"ëª©í‘œ ê°€ê²©: {product['target_price']:,}ì›")
        print(f"í˜„ì¬ ê°€ê²©: {current_price:,}ì›")
        print(f"URL: {product['url']}")
        
        # ì´ë©”ì¼ ì•Œë¦¼ (ì„ íƒì‚¬í•­)
        # self._send_email_alert(product, current_price, scraped_data)
    
    def _send_email_alert(self, product, current_price, scraped_data):
        """ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡"""
        # ì´ë©”ì¼ ì„¤ì • (ì‹¤ì œ ì‚¬ìš©ì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬)
        smtp_server = "smtp.gmail.com"
        smtp_port = 587
        sender_email = "your_email@gmail.com"
        sender_password = "your_app_password"
        recipient_email = "recipient@gmail.com"
        
        try:
            msg = MIMEMultipart()
            msg['From'] = sender_email
            msg['To'] = recipient_email
            msg['Subject'] = f"ê°€ê²© ì•Œë¦¼: {product['name']}"
            
            body = f"""
            ëª©í‘œ ê°€ê²©ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤!
            
            ìƒí’ˆëª…: {product['name']}
            ëª©í‘œ ê°€ê²©: {product['target_price']:,}ì›
            í˜„ì¬ ê°€ê²©: {current_price:,}ì›
            
            ìƒí’ˆ í˜ì´ì§€: {product['url']}
            
            ì¬ê³  ìƒíƒœ: {scraped_data.get('availability', 'N/A')}
            í‰ì : {scraped_data.get('rating', 'N/A')}
            """
            
            msg.attach(MIMEText(body, 'plain'))
            
            server = smtplib.SMTP(smtp_server, smtp_port)
            server.starttls()
            server.login(sender_email, sender_password)
            server.send_message(msg)
            server.quit()
            
            print("ğŸ“§ ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ")
            
        except Exception as e:
            print(f"ì´ë©”ì¼ ë°œì†¡ ì˜¤ë¥˜: {e}")
    
    def _save_price_history(self):
        """ê°€ê²© íˆìŠ¤í† ë¦¬ ì €ì¥"""
        with open("price_history.json", "w", encoding="utf-8") as f:
            json.dump(self.price_history, f, indent=2, ensure_ascii=False)
    
    def generate_price_report(self):
        """ê°€ê²© ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.price_history:
            print("ê°€ê²© íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìµœì‹  ê°€ê²© ì •ë³´ë§Œ ì¶”ì¶œ
        latest_prices = {}
        for entry in self.price_history:
            product_name = entry["product_name"]
            if product_name not in latest_prices or entry["timestamp"] > latest_prices[product_name]["timestamp"]:
                latest_prices[product_name] = entry
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        report_data = []
        for product_name, entry in latest_prices.items():
            scraped = entry["scraped_data"]
            
            report_data.append({
                "ìƒí’ˆëª…": product_name,
                "í˜„ì¬ê°€ê²©": scraped.get("current_price", "N/A"),
                "ì •ê°€": scraped.get("original_price", "N/A"),
                "í• ì¸ìœ¨": scraped.get("discount_rate", "N/A"),
                "ì¬ê³ ìƒíƒœ": scraped.get("availability", "N/A"),
                "í‰ì ": scraped.get("rating", "N/A"),
                "ë¦¬ë·°ìˆ˜": scraped.get("review_count", "N/A"),
                "ëª©í‘œê°€ê²©": entry["target_price"],
                "í™•ì¸ì‹œê°": entry["timestamp"]
            })
        
        df = pd.DataFrame(report_data)
        df.to_excel("price_report.xlsx", index=False, engine='openpyxl')
        
        print("ê°€ê²© ë¦¬í¬íŠ¸ê°€ price_report.xlsxì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(df.to_string(index=False))

def setup_price_monitoring():
    """ê°€ê²© ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
    
    config = {
        "llm": {
            "model": "ollama/llama3.2",
            "model_tokens": 8192
        },
        "verbose": True,
        "headless": True
    }
    
    monitor = PriceMonitor(config)
    
    # ëª¨ë‹ˆí„°ë§í•  ì œí’ˆë“¤ ì¶”ê°€
    products = [
        {
            "name": "MacBook Pro 14ì¸ì¹˜",
            "url": "https://www.apple.com/kr/macbook-pro-14-and-16/",
            "target_price": 2500000
        },
        {
            "name": "iPhone 15 Pro",
            "url": "https://www.apple.com/kr/iphone-15-pro/",
            "target_price": 1200000
        }
    ]
    
    for product in products:
        monitor.add_product(
            name=product["name"],
            url=product["url"],
            target_price=product["target_price"]
        )
    
    # ê°€ê²© í™•ì¸ ì‹¤í–‰
    monitor.check_prices()
    monitor.generate_price_report()

if __name__ == "__main__":
    setup_price_monitoring()
```

## API ë° SDK í™œìš©

### 1. ScrapeGraph API ì‚¬ìš©

```python
# api_usage.py
import requests
import json

class ScrapeGraphAPI:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://api.scrapegraphai.com/v1"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    def scrape_single_page(self, url, prompt, config=None):
        """ë‹¨ì¼ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        
        payload = {
            "url": url,
            "prompt": prompt,
            "config": config or {
                "llm_model": "gpt-4o-mini",
                "temperature": 0.1
            }
        }
        
        response = requests.post(
            f"{self.base_url}/scrape",
            headers=self.headers,
            json=payload
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(f"API ì˜¤ë¥˜: {response.status_code} - {response.text}")
    
    def scrape_multiple_pages(self, urls, prompt, config=None):
        """ë‹¤ì¤‘ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        
        payload = {
            "urls": urls,
            "prompt": prompt,
            "config": config or {
                "llm_model": "gpt-4o-mini",
                "temperature": 0.1,
                "max_concurrent": 3
            }
        }
        
        response = requests.post(
            f"{self.base_url}/scrape/multi",
            headers=self.headers,
            json=payload
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(f"API ì˜¤ë¥˜: {response.status_code} - {response.text}")
    
    def get_usage_stats(self):
        """ì‚¬ìš©ëŸ‰ í†µê³„ ì¡°íšŒ"""
        
        response = requests.get(
            f"{self.base_url}/usage",
            headers=self.headers
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(f"API ì˜¤ë¥˜: {response.status_code} - {response.text}")

def api_example():
    """API ì‚¬ìš© ì˜ˆì œ"""
    
    # API í‚¤ ì„¤ì • (ì‹¤ì œ í‚¤ë¡œ êµì²´ í•„ìš”)
    api_key = "your_scrapegraph_api_key"
    api = ScrapeGraphAPI(api_key)
    
    try:
        # ë‹¨ì¼ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
        result = api.scrape_single_page(
            url="https://openai.com",
            prompt="íšŒì‚¬ ì •ë³´ì™€ ì£¼ìš” ì œí’ˆì„ JSON í˜•íƒœë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”",
            config={
                "llm_model": "gpt-4o-mini",
                "temperature": 0.1,
                "max_tokens": 2048
            }
        )
        
        print("ë‹¨ì¼ í˜ì´ì§€ ê²°ê³¼:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        
        # ë‹¤ì¤‘ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
        urls = [
            "https://openai.com",
            "https://anthropic.com",
            "https://cohere.ai"
        ]
        
        multi_result = api.scrape_multiple_pages(
            urls=urls,
            prompt="íšŒì‚¬ëª…, ì£¼ìš” ì œí’ˆ, ì„¤ë¦½ì—°ë„ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”",
            config={
                "llm_model": "gpt-4o-mini",
                "temperature": 0.1,
                "max_concurrent": 2
            }
        )
        
        print("\në‹¤ì¤‘ í˜ì´ì§€ ê²°ê³¼:")
        print(json.dumps(multi_result, indent=2, ensure_ascii=False))
        
        # ì‚¬ìš©ëŸ‰ í™•ì¸
        usage = api.get_usage_stats()
        print(f"\nì‚¬ìš©ëŸ‰ í†µê³„: {usage}")
        
    except Exception as e:
        print(f"API ì‚¬ìš© ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    api_example()
```

### 2. Python SDK í™œìš©

```python
# sdk_usage.py
from scrapegraph_py import Client

def sdk_example():
    """Python SDK ì‚¬ìš© ì˜ˆì œ"""
    
    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = Client(api_key="your_api_key")
    
    # ê°„ë‹¨í•œ ìŠ¤í¬ë˜í•‘
    result = client.smartscraper(
        website_url="https://scrapegraphai.com/",
        user_prompt="Extract company information and founder details"
    )
    
    print(f"SDK ê²°ê³¼: {result}")
    
    # ì„¤ì •ì´ ìˆëŠ” ìŠ¤í¬ë˜í•‘
    advanced_result = client.smartscraper(
        website_url="https://openai.com",
        user_prompt="Extract product information and pricing details",
        llm_config={
            "model": "gpt-4o-mini",
            "temperature": 0.1,
            "max_tokens": 1500
        }
    )
    
    print(f"ê³ ê¸‰ ì„¤ì • ê²°ê³¼: {advanced_result}")

if __name__ == "__main__":
    sdk_example()
```

## ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë²” ì‚¬ë¡€

### 1. ì„±ëŠ¥ ìµœì í™” ê¸°ë²•

```python
# optimization_tips.py
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor
from scrapegraphai.graphs import SmartScraperGraph

class OptimizedScraper:
    def __init__(self, config):
        self.config = config
        self.scrapers = {}
    
    def get_optimized_config(self, use_case="general"):
        """ìš©ë„ë³„ ìµœì í™”ëœ ì„¤ì •"""
        
        base_config = self.config.copy()
        
        optimizations = {
            "speed": {
                "llm": {
                    "model": "groq/llama-3.1-70b-versatile",  # ë¹ ë¥¸ ëª¨ë¸
                    "temperature": 0,
                    "max_tokens": 1024
                },
                "loader_kwargs": {
                    "requests_per_second": 5,
                    "load_timeout": 10
                },
                "headless": True,
                "verbose": False
            },
            "accuracy": {
                "llm": {
                    "model": "openai/gpt-4o",  # ì •í™•í•œ ëª¨ë¸
                    "temperature": 0.1,
                    "max_tokens": 4096
                },
                "loader_kwargs": {
                    "requests_per_second": 1,
                    "load_timeout": 30
                },
                "headless": True,
                "verbose": True
            },
            "cost": {
                "llm": {
                    "model": "ollama/llama3.2",  # ë¡œì»¬ ëª¨ë¸
                    "model_tokens": 8192
                },
                "loader_kwargs": {
                    "requests_per_second": 2,
                    "load_timeout": 20
                },
                "headless": True,
                "verbose": False
            }
        }
        
        if use_case in optimizations:
            base_config.update(optimizations[use_case])
        
        return base_config
    
    def batch_scrape_with_pooling(self, url_prompt_pairs, max_workers=3):
        """ìŠ¤ë ˆë“œ í’€ì„ ì‚¬ìš©í•œ ë°°ì¹˜ ìŠ¤í¬ë˜í•‘"""
        
        def scrape_single(url_prompt):
            url, prompt = url_prompt
            scraper = SmartScraperGraph(
                prompt=prompt,
                source=url,
                config=self.get_optimized_config("speed")
            )
            return scraper.run()
        
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(scrape_single, url_prompt_pairs))
        
        end_time = time.time()
        print(f"ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(url_prompt_pairs)}ê°œ URL, {end_time - start_time:.2f}ì´ˆ")
        
        return results
    
    def cached_scraper(self, url, prompt, cache_duration_minutes=60):
        """ìºì‹œë¥¼ í™œìš©í•œ ìŠ¤í¬ë˜í•‘"""
        import hashlib
        import pickle
        import os
        from datetime import datetime, timedelta
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = hashlib.md5(f"{url}_{prompt}".encode()).hexdigest()
        cache_file = f"cache/{cache_key}.pkl"
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("cache", exist_ok=True)
        
        # ìºì‹œ í™•ì¸
        if os.path.exists(cache_file):
            cache_time = datetime.fromtimestamp(os.path.getmtime(cache_file))
            if datetime.now() - cache_time < timedelta(minutes=cache_duration_minutes):
                print(f"ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜: {url}")
                with open(cache_file, "rb") as f:
                    return pickle.load(f)
        
        # ìƒˆë¡œ ìŠ¤í¬ë˜í•‘
        print(f"ìƒˆë¡œ ìŠ¤í¬ë˜í•‘: {url}")
        scraper = SmartScraperGraph(
            prompt=prompt,
            source=url,
            config=self.get_optimized_config("speed")
        )
        result = scraper.run()
        
        # ìºì‹œì— ì €ì¥
        with open(cache_file, "wb") as f:
            pickle.dump(result, f)
        
        return result
    
    def adaptive_retry_scraper(self, url, prompt, max_retries=3):
        """ì ì‘í˜• ì¬ì‹œë„ ìŠ¤í¬ë˜í•‘"""
        
        configs = [
            self.get_optimized_config("speed"),
            self.get_optimized_config("general"),
            self.get_optimized_config("accuracy")
        ]
        
        for attempt in range(max_retries):
            try:
                config = configs[min(attempt, len(configs) - 1)]
                
                scraper = SmartScraperGraph(
                    prompt=prompt,
                    source=url,
                    config=config
                )
                
                result = scraper.run()
                
                if result and len(str(result)) > 50:  # ê²°ê³¼ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
                    return result
                
            except Exception as e:
                print(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                if attempt == max_retries - 1:
                    raise
                
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        return None

def performance_comparison():
    """ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸"""
    
    base_config = {
        "verbose": False,
        "headless": True
    }
    
    optimizer = OptimizedScraper(base_config)
    
    test_urls = [
        "https://openai.com",
        "https://anthropic.com", 
        "https://huggingface.co"
    ]
    
    prompt = "Extract company name and main product information"
    
    # ìˆœì°¨ ì²˜ë¦¬
    start_time = time.time()
    sequential_results = []
    for url in test_urls:
        scraper = SmartScraperGraph(
            prompt=prompt,
            source=url,
            config=base_config
        )
        result = scraper.run()
        sequential_results.append(result)
    sequential_time = time.time() - start_time
    
    # ë³‘ë ¬ ì²˜ë¦¬
    url_prompt_pairs = [(url, prompt) for url in test_urls]
    start_time = time.time()
    parallel_results = optimizer.batch_scrape_with_pooling(url_prompt_pairs)
    parallel_time = time.time() - start_time
    
    print(f"ìˆœì°¨ ì²˜ë¦¬: {sequential_time:.2f}ì´ˆ")
    print(f"ë³‘ë ¬ ì²˜ë¦¬: {parallel_time:.2f}ì´ˆ")
    print(f"ì„±ëŠ¥ í–¥ìƒ: {sequential_time/parallel_time:.2f}ë°°")

if __name__ == "__main__":
    performance_comparison()
```

### 2. ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë¡œê¹…

```python
# error_handling.py
import logging
import traceback
from datetime import datetime
from scrapegraphai.graphs import SmartScraperGraph

class RobustScraper:
    def __init__(self, config):
        self.config = config
        self.setup_logging()
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('scraping.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def safe_scrape(self, url, prompt, timeout=60):
        """ì•ˆì „í•œ ìŠ¤í¬ë˜í•‘ (ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨)"""
        
        try:
            self.logger.info(f"ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            config = self.config.copy()
            config.setdefault('loader_kwargs', {})['load_timeout'] = timeout
            
            scraper = SmartScraperGraph(
                prompt=prompt,
                source=url,
                config=config
            )
            
            result = scraper.run()
            
            # ê²°ê³¼ ê²€ì¦
            if not result or len(str(result)) < 10:
                raise ValueError("ìŠ¤í¬ë˜í•‘ ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
            
            self.logger.info(f"ìŠ¤í¬ë˜í•‘ ì„±ê³µ: {url}")
            return {
                "success": True,
                "url": url,
                "result": result,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            error_msg = f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜ ({url}): {str(e)}"
            self.logger.error(error_msg)
            self.logger.error(traceback.format_exc())
            
            return {
                "success": False,
                "url": url,
                "error": str(e),
                "error_type": type(e).__name__,
                "timestamp": datetime.now().isoformat()
            }
    
    def scrape_with_fallback(self, url, prompt):
        """ëŒ€ì²´ ì „ëµì„ ì‚¬ìš©í•œ ìŠ¤í¬ë˜í•‘"""
        
        strategies = [
            {
                "name": "Primary (Ollama)",
                "config": {
                    "llm": {"model": "ollama/llama3.2"},
                    "headless": True
                }
            },
            {
                "name": "Fallback (OpenAI)",
                "config": {
                    "llm": {
                        "model": "openai/gpt-4o-mini",
                        "api_key": "your_api_key"
                    },
                    "headless": True
                }
            },
            {
                "name": "Emergency (Groq)",
                "config": {
                    "llm": {
                        "model": "groq/llama-3.1-70b-versatile",
                        "api_key": "your_groq_key"
                    },
                    "headless": True
                }
            }
        ]
        
        for strategy in strategies:
            try:
                self.logger.info(f"ì‹œë„ ì¤‘: {strategy['name']}")
                
                updated_config = self.config.copy()
                updated_config.update(strategy['config'])
                
                scraper = SmartScraperGraph(
                    prompt=prompt,
                    source=url,
                    config=updated_config
                )
                
                result = scraper.run()
                
                if result and len(str(result)) > 10:
                    self.logger.info(f"ì„±ê³µ: {strategy['name']}")
                    return result
                
            except Exception as e:
                self.logger.warning(f"{strategy['name']} ì‹¤íŒ¨: {e}")
                continue
        
        raise Exception("ëª¨ë“  ëŒ€ì²´ ì „ëµì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

def error_handling_example():
    """ì˜¤ë¥˜ ì²˜ë¦¬ ì˜ˆì œ"""
    
    config = {
        "verbose": True,
        "headless": True
    }
    
    robust_scraper = RobustScraper(config)
    
    # ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆëŠ” URLë“¤ í…ŒìŠ¤íŠ¸
    test_cases = [
        {
            "url": "https://httpbin.org/delay/5",  # ëŠë¦° ì‘ë‹µ
            "prompt": "Extract any data from this page"
        },
        {
            "url": "https://httpbin.org/status/404",  # 404 ì˜¤ë¥˜
            "prompt": "Extract content"
        },
        {
            "url": "https://httpbin.org/status/500",  # ì„œë²„ ì˜¤ë¥˜
            "prompt": "Extract information"
        },
        {
            "url": "https://scrapegraphai.com/",  # ì •ìƒ ì‚¬ì´íŠ¸
            "prompt": "Extract company information"
        }
    ]
    
    results = []
    for test_case in test_cases:
        result = robust_scraper.safe_scrape(
            url=test_case["url"],
            prompt=test_case["prompt"],
            timeout=10
        )
        results.append(result)
    
    # ê²°ê³¼ ìš”ì•½
    successful = [r for r in results if r["success"]]
    failed = [r for r in results if not r["success"]]
    
    print(f"\nê²°ê³¼ ìš”ì•½:")
    print(f"ì„±ê³µ: {len(successful)}ê°œ")
    print(f"ì‹¤íŒ¨: {len(failed)}ê°œ")
    
    if failed:
        print("\nì‹¤íŒ¨í•œ ì¼€ì´ìŠ¤:")
        for fail in failed:
            print(f"  - {fail['url']}: {fail['error']}")

if __name__ == "__main__":
    error_handling_example()
```

## ê³ ê¸‰ ê¸°ëŠ¥ ë° í™•ì¥

### 1. ì»¤ìŠ¤í…€ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

```python
# custom_pipeline.py
from scrapegraphai.graphs.base_graph import BaseGraph
from scrapegraphai.nodes import FetchNode, ParseNode, GenerateAnswerNode

class CustomAnalysisGraph(BaseGraph):
    """ì»¤ìŠ¤í…€ ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, prompt, source, config):
        super().__init__(prompt, source, config)
        
        # ë…¸ë“œ ì •ì˜
        self.input_key = "url"
        self.output_key = "answer"
        
        # ê·¸ë˜í”„ êµ¬ì„±
        self.graph = self._create_graph()
    
    def _create_graph(self):
        """ì»¤ìŠ¤í…€ ê·¸ë˜í”„ ìƒì„±"""
        
        # 1. ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        fetch_node = FetchNode(
            input="url",
            output=["doc"],
            node_config={
                "loader_kwargs": self.config.get("loader_kwargs", {}),
                "headless": self.config.get("headless", True)
            }
        )
        
        # 2. êµ¬ì¡°í™”ëœ íŒŒì‹±
        parse_node = ParseNode(
            input="doc",
            output=["parsed_doc"],
            node_config={
                "llm_model": self.config["llm"],
                "chunk_size": 1000,
                "parse_instructions": """
                ì›¹í˜ì´ì§€ë¥¼ ë‹¤ìŒ ì„¹ì…˜ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ íŒŒì‹±í•˜ì„¸ìš”:
                - header: í—¤ë” ì •ë³´
                - navigation: ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´
                - main_content: ì£¼ìš” ì½˜í…ì¸ 
                - sidebar: ì‚¬ì´ë“œë°” ë‚´ìš©
                - footer: í‘¸í„° ì •ë³´
                """
            }
        )
        
        # 3. ê³ ê¸‰ ë¶„ì„ ë° ë‹µë³€ ìƒì„±
        analysis_node = GenerateAnswerNode(
            input="parsed_doc",
            output=["answer"],
            node_config={
                "llm_model": self.config["llm"],
                "analysis_prompt": """
                íŒŒì‹±ëœ ì›¹í˜ì´ì§€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:
                
                1. ì½˜í…ì¸  ë¶„ì„:
                   - ì£¼ìš” ì£¼ì œì™€ í‚¤ì›Œë“œ
                   - ì½˜í…ì¸ ì˜ í’ˆì§ˆê³¼ ê¹Šì´
                   - íƒ€ê²Ÿ ì˜¤ë””ì–¸ìŠ¤
                
                2. êµ¬ì¡° ë¶„ì„:
                   - ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°ì˜ ë³µì¡ì„±
                   - ì‚¬ìš©ì ê²½í—˜ ìš”ì†Œ
                   - ì ‘ê·¼ì„± ê³ ë ¤ì‚¬í•­
                
                3. ê¸°ìˆ  ë¶„ì„:
                   - ì‚¬ìš©ëœ ê¸°ìˆ  ìŠ¤íƒ (ì¶”ì •)
                   - ì„±ëŠ¥ ê´€ë ¨ ìš”ì†Œ
                   - SEO ìµœì í™” ìˆ˜ì¤€
                
                4. ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„:
                   - ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ì¶”ì •
                   - ìˆ˜ìµí™” ë°©ì‹
                   - ê²½ìŸë ¥ ìš”ì†Œ
                
                JSON í˜•íƒœë¡œ êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”.
                """
            }
        )
        
        # ê·¸ë˜í”„ ì—°ê²°
        return fetch_node >> parse_node >> analysis_node

def custom_pipeline_example():
    """ì»¤ìŠ¤í…€ íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ì˜ˆì œ"""
    
    config = {
        "llm": {
            "model": "ollama/llama3.2",
            "model_tokens": 8192
        },
        "verbose": True,
        "headless": True
    }
    
    # ì»¤ìŠ¤í…€ ë¶„ì„ ì‹¤í–‰
    analyzer = CustomAnalysisGraph(
        prompt="ì›¹ì‚¬ì´íŠ¸ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”",
        source="https://scrapegraphai.com/",
        config=config
    )
    
    result = analyzer.run()
    
    print("ì»¤ìŠ¤í…€ ë¶„ì„ ê²°ê³¼:")
    print(json.dumps(result, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    custom_pipeline_example()
```

### 2. í”ŒëŸ¬ê·¸ì¸ ì‹œìŠ¤í…œ

```python
# plugin_system.py
import json
from abc import ABC, abstractmethod
from scrapegraphai.graphs import SmartScraperGraph

class ScrapePlugin(ABC):
    """ìŠ¤í¬ë˜í•‘ í”ŒëŸ¬ê·¸ì¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def preprocess(self, url, prompt):
        """ì „ì²˜ë¦¬"""
        pass
    
    @abstractmethod
    def postprocess(self, result):
        """í›„ì²˜ë¦¬"""
        pass
    
    @abstractmethod
    def get_name(self):
        """í”ŒëŸ¬ê·¸ì¸ ì´ë¦„"""
        pass

class DataValidationPlugin(ScrapePlugin):
    """ë°ì´í„° ê²€ì¦ í”ŒëŸ¬ê·¸ì¸"""
    
    def get_name(self):
        return "DataValidation"
    
    def preprocess(self, url, prompt):
        """URL ìœ íš¨ì„± ê²€ì‚¬"""
        if not url.startswith(('http://', 'https://')):
            raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ URL: {url}")
        return url, prompt
    
    def postprocess(self, result):
        """ê²°ê³¼ ê²€ì¦"""
        if not result:
            raise ValueError("ìŠ¤í¬ë˜í•‘ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        if isinstance(result, str) and len(result) < 10:
            raise ValueError("ìŠ¤í¬ë˜í•‘ ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
        
        return result

class DataEnrichmentPlugin(ScrapePlugin):
    """ë°ì´í„° ë³´ê°• í”ŒëŸ¬ê·¸ì¸"""
    
    def get_name(self):
        return "DataEnrichment"
    
    def preprocess(self, url, prompt):
        """í”„ë¡¬í”„íŠ¸ ë³´ê°•"""
        enhanced_prompt = f"""
        {prompt}
        
        ì¶”ê°€ë¡œ ë‹¤ìŒ ë©”íƒ€ë°ì´í„°ë„ í¬í•¨í•´ì£¼ì„¸ìš”:
        - ìŠ¤í¬ë˜í•‘ ì‹œê°„: í˜„ì¬ ì‹œê°„
        - í˜ì´ì§€ ì–¸ì–´: ì¶”ì • ì–¸ì–´
        - ì½˜í…ì¸  ê¸¸ì´: ëŒ€ëµì ì¸ ë‹¨ì–´ ìˆ˜
        - êµ¬ì¡°í™” ìˆ˜ì¤€: ì–¼ë§ˆë‚˜ ì˜ êµ¬ì¡°í™”ë˜ì–´ ìˆëŠ”ì§€
        """
        return url, enhanced_prompt
    
    def postprocess(self, result):
        """ê²°ê³¼ ë³´ê°•"""
        if isinstance(result, dict):
            result['_metadata'] = {
                'processed_by': 'ScrapeGraphAI',
                'enriched': True,
                'plugin_version': '1.0'
            }
        return result

class TranslationPlugin(ScrapePlugin):
    """ë²ˆì—­ í”ŒëŸ¬ê·¸ì¸"""
    
    def __init__(self, target_language="ko"):
        self.target_language = target_language
    
    def get_name(self):
        return f"Translation_{self.target_language}"
    
    def preprocess(self, url, prompt):
        """ë²ˆì—­ ìš”ì²­ ì¶”ê°€"""
        if self.target_language != "en":
            prompt += f"\n\nê²°ê³¼ë¥¼ {self.target_language}ë¡œ ë²ˆì—­í•´ì„œ ì œê³µí•´ì£¼ì„¸ìš”."
        return url, prompt
    
    def postprocess(self, result):
        """ë²ˆì—­ í›„ì²˜ë¦¬"""
        # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ ì¶”ê°€ ë²ˆì—­ ê²€ì¦ì„ í•  ìˆ˜ ìˆìŒ
        return result

class PluginManager:
    """í”ŒëŸ¬ê·¸ì¸ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.plugins = []
    
    def register_plugin(self, plugin):
        """í”ŒëŸ¬ê·¸ì¸ ë“±ë¡"""
        if not isinstance(plugin, ScrapePlugin):
            raise TypeError("ScrapePluginì„ ìƒì†ë°›ì€ í´ë˜ìŠ¤ì—¬ì•¼ í•©ë‹ˆë‹¤")
        
        self.plugins.append(plugin)
        print(f"í”ŒëŸ¬ê·¸ì¸ ë“±ë¡ë¨: {plugin.get_name()}")
    
    def execute_scraping(self, url, prompt, config):
        """í”ŒëŸ¬ê·¸ì¸ì„ ì ìš©í•œ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        
        # ì „ì²˜ë¦¬ ë‹¨ê³„
        processed_url, processed_prompt = url, prompt
        for plugin in self.plugins:
            try:
                processed_url, processed_prompt = plugin.preprocess(processed_url, processed_prompt)
                print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {plugin.get_name()}")
            except Exception as e:
                print(f"ì „ì²˜ë¦¬ ì˜¤ë¥˜ ({plugin.get_name()}): {e}")
                raise
        
        # ì‹¤ì œ ìŠ¤í¬ë˜í•‘
        scraper = SmartScraperGraph(
            prompt=processed_prompt,
            source=processed_url,
            config=config
        )
        
        result = scraper.run()
        
        # í›„ì²˜ë¦¬ ë‹¨ê³„
        for plugin in reversed(self.plugins):  # ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬
            try:
                result = plugin.postprocess(result)
                print(f"í›„ì²˜ë¦¬ ì™„ë£Œ: {plugin.get_name()}")
            except Exception as e:
                print(f"í›„ì²˜ë¦¬ ì˜¤ë¥˜ ({plugin.get_name()}): {e}")
                raise
        
        return result

def plugin_system_example():
    """í”ŒëŸ¬ê·¸ì¸ ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì œ"""
    
    config = {
        "llm": {
            "model": "ollama/llama3.2",
            "model_tokens": 8192
        },
        "verbose": True,
        "headless": True
    }
    
    # í”ŒëŸ¬ê·¸ì¸ ë§¤ë‹ˆì € ìƒì„±
    manager = PluginManager()
    
    # í”ŒëŸ¬ê·¸ì¸ë“¤ ë“±ë¡
    manager.register_plugin(DataValidationPlugin())
    manager.register_plugin(DataEnrichmentPlugin())
    manager.register_plugin(TranslationPlugin("ko"))
    
    # í”ŒëŸ¬ê·¸ì¸ì´ ì ìš©ëœ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
    result = manager.execute_scraping(
        url="https://openai.com",
        prompt="Extract company information and main products",
        config=config
    )
    
    print("\ní”ŒëŸ¬ê·¸ì¸ ì ìš© ê²°ê³¼:")
    print(json.dumps(result, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    plugin_system_example()
```

## ê²°ë¡ 

ScrapeGraphAIëŠ” ì „í†µì ì¸ ì›¹ ìŠ¤í¬ë˜í•‘ì˜ í•œê³„ë¥¼ ë›°ì–´ë„˜ëŠ” í˜ì‹ ì ì¸ ë„êµ¬ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œë¥¼ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì—­ëŸ‰ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### í•µì‹¬ ì„±ê³¼

1. **AI ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ**: CSS ì„ íƒì ì—†ì´ë„ ì •í™•í•œ ë°ì´í„° ìˆ˜ì§‘
2. **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: ë‹¤ì–‘í•œ LLMê³¼ íŒŒì´í”„ë¼ì¸ ì§€ì›
3. **ì‹¤ë¬´ ì ìš©**: ê²½ìŸì‚¬ ë¶„ì„, ê°€ê²© ëª¨ë‹ˆí„°ë§, ë‰´ìŠ¤ ìˆ˜ì§‘ ë“±
4. **ì„±ëŠ¥ ìµœì í™”**: ìºì‹±, ë³‘ë ¬ ì²˜ë¦¬, ì˜¤ë¥˜ ë³µêµ¬ ì „ëµ

### ê¸°ìˆ ì  ì¥ì 

- **ìì—°ì–´ í”„ë¡¬í”„íŠ¸**: ë³µì¡í•œ ì½”ë“œ ì—†ì´ ì§ê´€ì ì¸ ë°ì´í„° ì¶”ì¶œ
- **ì ì‘ì„±**: ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ì— ìë™ ëŒ€ì‘
- **ë‹¤ì–‘ì„±**: ë‹¨ì¼/ë‹¤ì¤‘ í˜ì´ì§€, ê²€ìƒ‰, ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ë“± ë‹¤ì–‘í•œ íŒŒì´í”„ë¼ì¸
- **í†µí•©ì„±**: API, SDK, ë…¸ì½”ë“œ í”Œë«í¼ê³¼ì˜ ì™„ë²½í•œ ì—°ë™

### ì‹¤ë¬´ ê°€ì¹˜

ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ë‹¤ë£¬ ì˜ˆì œë“¤ì€ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ í™˜ê²½ì—ì„œ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **ë§ˆì¼€íŒ… íŒ€**: ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ë° ì‹œì¥ ë™í–¥ ë¶„ì„
- **ê°œë°œ íŒ€**: ìë™í™”ëœ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- **ë¶„ì„ê°€**: ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
- **ì—°êµ¬ì**: ëŒ€ê·œëª¨ ì›¹ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„

### ë¯¸ë˜ ë°œì „ ë°©í–¥

```python
future_enhancements = {
    "multimodal_support": "ì´ë¯¸ì§€, ë¹„ë””ì˜¤ ì½˜í…ì¸  ë¶„ì„",
    "real_time_streaming": "ì‹¤ì‹œê°„ ì›¹ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°",
    "advanced_ai": "ë” ì •êµí•œ AI ëª¨ë¸ í†µí•©",
    "enterprise_features": "ê¸°ì—…ê¸‰ ë³´ì•ˆ ë° ê´€ë¦¬ ê¸°ëŠ¥",
    "collaborative_tools": "íŒ€ í˜‘ì—… ë° ì›Œí¬í”Œë¡œìš° ê´€ë¦¬"
}
```

ScrapeGraphAIì˜ 20.5k GitHub ìŠ¤íƒ€ëŠ” ë‹¨ìˆœí•œ ì¸ê¸°ê°€ ì•„ë‹Œ, ì‹¤ì œ ê°œë°œìë“¤ì´ ì²´ê°í•˜ëŠ” í˜ì‹ ì˜ ì¦ê±°ì…ë‹ˆë‹¤. [ê³µì‹ ë ˆí¬ì§€í† ë¦¬](https://github.com/ScrapeGraphAI/Scrapegraph-ai)ì—ì„œ ìµœì‹  ì—…ë°ì´íŠ¸ë¥¼ í™•ì¸í•˜ê³ , ì»¤ë®¤ë‹ˆí‹°ì™€ í•¨ê»˜ AI ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ì˜ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ íƒí—˜í•´ë³´ì„¸ìš”!

---

**ê´€ë ¨ ë¦¬ì†ŒìŠ¤**:
- [ScrapeGraphAI GitHub](https://github.com/ScrapeGraphAI/Scrapegraph-ai)
- [ê³µì‹ API ë¬¸ì„œ](https://docs.scrapegraphai.com/)
- [Python SDK](https://github.com/ScrapeGraphAI/scrapegraph-sdk/tree/main/scrapegraph-py)
- [Node.js SDK](https://github.com/ScrapeGraphAI/scrapegraph-sdk/tree/main/scrapegraph-js) 