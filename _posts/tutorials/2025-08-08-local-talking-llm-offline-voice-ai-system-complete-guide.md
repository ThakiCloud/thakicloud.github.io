---
title: "Local Talking LLM: ì™„ì „ ì˜¤í”„ë¼ì¸ ìŒì„± AI ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ì „ ê°€ì´ë“œ"
excerpt: "ì¸í„°ë„· ì—°ê²° ì—†ì´ ì™„ì „íˆ ë¡œì»¬ì—ì„œ ë™ì‘í•˜ëŠ” ìŒì„± AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì„¸ìš”. Whisper, LLaMA, TTSë¥¼ ê²°í•©í•œ í”„ë¼ì´ë¹— AI ì–´ì‹œìŠ¤í„´íŠ¸ êµ¬í˜„ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤."
seo_title: "Local Talking LLM ì˜¤í”„ë¼ì¸ ìŒì„± AI ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ì „ ê°€ì´ë“œ - Thaki Cloud"
seo_description: "Whisper, LLaMA, TTSë¥¼ í™œìš©í•œ ì™„ì „ ì˜¤í”„ë¼ì¸ ìŒì„± AI ì‹œìŠ¤í…œ êµ¬ì¶• ë°©ë²•. ë°ì´í„° í”„ë¼ì´ë²„ì‹œë¥¼ ë³´ì¥í•˜ë©´ì„œ ë¡œì»¬ì—ì„œ ë™ì‘í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ ê°œë°œ íŠœí† ë¦¬ì–¼ì„ ì œê³µí•©ë‹ˆë‹¤."
date: 2025-08-08
last_modified_at: 2025-08-08
categories:
  - tutorials
  - llmops
tags:
  - local-llm
  - whisper
  - text-to-speech
  - offline-ai
  - privacy
  - voice-assistant
  - speech-recognition
  - llama
  - ai-system
  - python
  - docker
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/tutorials/local-talking-llm-offline-voice-ai-system-complete-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 18ë¶„

## ì„œë¡ 

ë°ì´í„° í”„ë¼ì´ë²„ì‹œì™€ ë³´ì•ˆì´ ì¤‘ìš”í•œ ì‹œëŒ€ì—, í´ë¼ìš°ë“œ ê¸°ë°˜ AI ì„œë¹„ìŠ¤ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ì™„ì „íˆ ë¡œì»¬ì—ì„œ ë™ì‘í•˜ëŠ” ìŒì„± AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. Local Talking LLMì€ ì¸í„°ë„· ì—°ê²° ì—†ì´ë„ ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ëŒ€í™”ê°€ ê°€ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ êµ¬í˜„í•˜ëŠ” í˜ì‹ ì ì¸ ì ‘ê·¼ë²•ì…ë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œëŠ” OpenAI Whisper (ìŒì„± ì¸ì‹), LLaMA (ì–¸ì–´ ëª¨ë¸), ê·¸ë¦¬ê³  TTS (í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜) ì‹œìŠ¤í…œì„ ê²°í•©í•˜ì—¬ ì™„ì „í•œ ì˜¤í”„ë¼ì¸ ìŒì„± AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”

### ğŸ—ï¸ í•µì‹¬ êµ¬ì„± ìš”ì†Œ

Local Talking LLM ì‹œìŠ¤í…œì€ 3ê°œì˜ ì£¼ìš” ì‹ ê²½ë§ì´ í˜‘ë ¥í•˜ì—¬ ë™ì‘í•©ë‹ˆë‹¤:

```python
# ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”
system_architecture = {
    "input_pipeline": {
        "component": "OpenAI Whisper",
        "function": "ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜",
        "model_size": "small (39MB) ~ large (1.5GB)",
        "languages": "99ê°œ ì–¸ì–´ ì§€ì›"
    },
    "processing_core": {
        "component": "LLaMA-based LLM",
        "function": "ìì—°ì–´ ì´í•´ ë° ì‘ë‹µ ìƒì„±",
        "variants": ["Wizard-Vicuna", "Alpaca", "Guanaco"],
        "memory_requirement": "4GB ~ 32GB"
    },
    "output_pipeline": {
        "component": "TTS Engine",
        "function": "í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜",
        "options": ["Mimic 3", "Piper", "Coqui TTS"],
        "voice_quality": "ìì—°ìŠ¤ëŸ¬ìš´ ì¸ê°„ ìŒì„±"
    }
}
```

### ğŸ”„ ë°ì´í„° í”Œë¡œìš°

```mermaid
graph LR
    A[ì‚¬ìš©ì ìŒì„±] --> B[Whisper ASR]
    B --> C[í…ìŠ¤íŠ¸ ì…ë ¥]
    C --> D[LLaMA LLM]
    D --> E[ì‘ë‹µ í…ìŠ¤íŠ¸]
    E --> F[TTS ì—”ì§„]
    F --> G[AI ìŒì„± ì¶œë ¥]
    
    H[ì¸í„°ëŸ½íŠ¸ ê°ì§€] --> D
    I[ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°] --> F
```

## ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### ğŸ’» í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­

#### ìµœì†Œ ìš”êµ¬ì‚¬í•­
```yaml
minimum_specs:
  ram: "16GB"
  cpu: "4ì½”ì–´ ì´ìƒ"
  storage: "50GB ì—¬ìœ  ê³µê°„"
  microphone: "USB í—¤ë“œì…‹ ë˜ëŠ” ë‚´ì¥ ë§ˆì´í¬"
  speakers: "ì˜¤ë””ì˜¤ ì¶œë ¥ ì¥ì¹˜"
  
  performance: "ê¸°ë³¸ì ì¸ ëŒ€í™” ê°€ëŠ¥"
  response_time: "5-10ì´ˆ"
```

#### ê¶Œì¥ ìš”êµ¬ì‚¬í•­
```yaml
recommended_specs:
  ram: "32GB ì´ìƒ"
  cpu: "8ì½”ì–´ ì´ìƒ (Intel i7/AMD Ryzen 7)"
  gpu: "NVIDIA RTX 3060 ì´ìƒ (ì„ íƒì‚¬í•­)"
  storage: "100GB SSD"
  
  performance: "ë¹ ë¥´ê³  ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”"
  response_time: "1-3ì´ˆ"
```

#### ê³ ì„±ëŠ¥ ì„¤ì •
```yaml
high_end_specs:
  ram: "64GB"
  cpu: "16ì½”ì–´ ì´ìƒ"
  gpu: "NVIDIA RTX 4090"
  storage: "200GB NVMe SSD"
  
  features:
    - "ëŒ€ìš©ëŸ‰ ëª¨ë¸ ì§€ì› (65B íŒŒë¼ë¯¸í„°)"
    - "ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°"
    - "ë‹¤ì¤‘ ì–¸ì–´ ë™ì‹œ ì§€ì›"
```

### ğŸ§ ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­

```bash
# ì§€ì› ìš´ì˜ì²´ì œ
supported_os = [
    "Ubuntu 22.04 LTS",
    "Debian 11/12",
    "CentOS 8+",
    "macOS 12+",
    "Windows 10/11 (WSL2)"
]

# Python í™˜ê²½
python_requirements = {
    "version": "3.8+",
    "recommended": "3.11",
    "virtual_env": "ê°•ë ¥ ê¶Œì¥"
}
```

## ì„¤ì¹˜ ê°€ì´ë“œ

### ğŸš€ ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# install_local_talking_llm.sh

echo "ğŸ™ï¸ Local Talking LLM ìë™ ì„¤ì¹˜ ì‹œì‘..."

# ì‹œìŠ¤í…œ ì²´í¬
check_system() {
    echo "ğŸ“‹ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸..."
    
    # RAM ì²´í¬
    RAM_GB=$(free -g | awk '/^Mem:/{print $2}')
    if [ $RAM_GB -lt 16 ]; then
        echo "âŒ ìµœì†Œ 16GB RAMì´ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: ${RAM_GB}GB"
        exit 1
    fi
    
    # CPU ì½”ì–´ ì²´í¬
    CPU_CORES=$(nproc)
    if [ $CPU_CORES -lt 4 ]; then
        echo "âŒ ìµœì†Œ 4ì½”ì–´ CPUê°€ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: ${CPU_CORES}ì½”ì–´"
        exit 1
    fi
    
    # ë””ìŠ¤í¬ ê³µê°„ ì²´í¬
    DISK_GB=$(df -BG . | awk 'NR==2{print $4}' | sed 's/G//')
    if [ $DISK_GB -lt 50 ]; then
        echo "âŒ ìµœì†Œ 50GB ë””ìŠ¤í¬ ê³µê°„ì´ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: ${DISK_GB}GB"
        exit 1
    fi
    
    echo "âœ… ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±"
}

# ì˜ì¡´ì„± ì„¤ì¹˜
install_dependencies() {
    echo "ğŸ“¦ ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜..."
    
    # Ubuntu/Debian
    if command -v apt &> /dev/null; then
        sudo apt update
        sudo apt install -y \
            python3 python3-pip python3-venv \
            git curl wget \
            build-essential cmake \
            portaudio19-dev \
            alsa-utils speech-dispatcher \
            libsdl2-dev \
            ffmpeg
    
    # macOS
    elif command -v brew &> /dev/null; then
        brew install python3 git cmake portaudio ffmpeg
    
    # CentOS/RHEL
    elif command -v dnf &> /dev/null; then
        sudo dnf install -y \
            python3 python3-pip \
            git curl wget \
            gcc gcc-c++ cmake \
            portaudio-devel \
            alsa-lib-devel \
            SDL2-devel \
            ffmpeg-devel
    fi
}

# Python ê°€ìƒí™˜ê²½ ì„¤ì •
setup_python_env() {
    echo "ğŸ Python ê°€ìƒí™˜ê²½ ì„¤ì •..."
    
    python3 -m venv local_talking_llm_env
    source local_talking_llm_env/bin/activate
    
    pip install --upgrade pip wheel setuptools
    
    # í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
    pip install \
        torch torchvision torchaudio \
        transformers \
        openai-whisper \
        TTS \
        streamlit \
        streamlit-mic-recorder \
        sounddevice \
        pygame \
        numpy \
        scipy \
        librosa \
        soundfile
}

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
download_models() {
    echo "ğŸ“¥ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ..."
    
    mkdir -p models/{whisper,llm,tts}
    
    # Whisper ëª¨ë¸
    echo "ğŸ¤ Whisper ëª¨ë¸ ë‹¤ìš´ë¡œë“œ..."
    python3 -c "import whisper; whisper.load_model('small')"
    
    # LLM ëª¨ë¸ (Ollama ì‚¬ìš©)
    if ! command -v ollama &> /dev/null; then
        echo "ğŸ“¥ Ollama ì„¤ì¹˜..."
        curl -fsSL https://ollama.ai/install.sh | sh
    fi
    
    echo "ğŸ§  LLM ëª¨ë¸ ë‹¤ìš´ë¡œë“œ..."
    ollama pull llama3.2:3b
    
    # TTS ëª¨ë¸
    echo "ğŸ”Š TTS ëª¨ë¸ ì„¤ì •..."
    python3 -c "from TTS.api import TTS; TTS(model_name='tts_models/en/ljspeech/tacotron2-DDC')"
}

# ë©”ì¸ ì‹¤í–‰
main() {
    check_system
    install_dependencies
    setup_python_env
    download_models
    
    echo "âœ… Local Talking LLM ì„¤ì¹˜ ì™„ë£Œ!"
    echo "ğŸš€ ì‹¤í–‰: source local_talking_llm_env/bin/activate && python talking_llm.py"
}

main "$@"
```

### ğŸ³ Dockerë¥¼ ì´ìš©í•œ ì„¤ì¹˜

```dockerfile
# Dockerfile
FROM nvidia/cuda:11.8-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    python3 python3-pip python3-venv \
    git curl wget \
    build-essential cmake \
    portaudio19-dev \
    alsa-utils speech-dispatcher \
    libsdl2-dev \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . .

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
RUN python3 download_models.py

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8501

# ì‹¤í–‰ ëª…ë ¹
CMD ["python3", "talking_llm.py"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  local-talking-llm:
    build: .
    container_name: talking_llm
    restart: unless-stopped
    ports:
      - "8501:8501"
    volumes:
      - ./models:/app/models
      - ./data:/app/data
      - /dev/snd:/dev/snd  # ì˜¤ë””ì˜¤ ì¥ì¹˜ ì ‘ê·¼
    devices:
      - /dev/dri:/dev/dri  # GPU ì ‘ê·¼
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  ollama:
    image: ollama/ollama:latest
    container_name: ollama_server
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - NVIDIA_VISIBLE_DEVICES=all

volumes:
  ollama_data:
```

### ğŸ“¦ Conda í™˜ê²½ ì„¤ì •

```bash
# conda_setup.sh
#!/bin/bash

echo "ğŸ Conda í™˜ê²½ ì„¤ì •..."

# Miniconda ì„¤ì¹˜ (ì—†ëŠ” ê²½ìš°)
if ! command -v conda &> /dev/null; then
    echo "ğŸ“¥ Miniconda ì„¤ì¹˜..."
    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
    bash Miniconda3-latest-Linux-x86_64.sh -b
    source ~/miniconda3/bin/activate
    conda init
fi

# í™˜ê²½ ìƒì„±
conda create -n talking_llm python=3.11 -y
conda activate talking_llm

# PyTorch ì„¤ì¹˜ (CUDA ì§€ì›)
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y

# ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install \
    transformers \
    openai-whisper \
    TTS \
    streamlit \
    streamlit-mic-recorder \
    sounddevice \
    pygame \
    ollama

echo "âœ… Conda í™˜ê²½ ì„¤ì • ì™„ë£Œ"
echo "ğŸ”„ í™˜ê²½ í™œì„±í™”: conda activate talking_llm"
```

## í•µì‹¬ êµ¬í˜„

### ğŸ¤ ìŒì„± ì¸ì‹ (Whisper) êµ¬í˜„

```python
# whisper_asr.py
import whisper
import numpy as np
import sounddevice as sd
import queue
import threading
import time
from typing import Optional, Callable

class WhisperASR:
    def __init__(self, model_name: str = "small", device: str = "auto"):
        """
        Whisper ìŒì„± ì¸ì‹ í´ë˜ìŠ¤
        
        Args:
            model_name: Whisper ëª¨ë¸ í¬ê¸° (tiny, base, small, medium, large)
            device: ì‹¤í–‰ ì¥ì¹˜ (cpu, cuda, auto)
        """
        self.model = whisper.load_model(model_name)
        self.device = device
        self.sample_rate = 16000
        self.chunk_duration = 2.0  # 2ì´ˆ ì²­í¬
        self.chunk_size = int(self.sample_rate * self.chunk_duration)
        
        self.audio_queue = queue.Queue()
        self.is_recording = False
        self.callback_func: Optional[Callable] = None
        
    def audio_callback(self, indata, frames, time, status):
        """ì˜¤ë””ì˜¤ ì½œë°± í•¨ìˆ˜"""
        if status:
            print(f"Audio status: {status}")
        
        # ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ íì— ì¶”ê°€
        self.audio_queue.put(indata.copy())
    
    def start_recording(self, callback: Optional[Callable] = None):
        """ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œì‘"""
        self.callback_func = callback
        self.is_recording = True
        
        # ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì‹œì‘
        self.stream = sd.InputStream(
            samplerate=self.sample_rate,
            channels=1,
            callback=self.audio_callback,
            blocksize=self.chunk_size,
            dtype=np.float32
        )
        
        self.stream.start()
        
        # ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
        self.processing_thread = threading.Thread(target=self._process_audio)
        self.processing_thread.start()
        
        print("ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘ë¨")
    
    def stop_recording(self):
        """ìŒì„± ì¸ì‹ ì¤‘ì§€"""
        self.is_recording = False
        
        if hasattr(self, 'stream'):
            self.stream.stop()
            self.stream.close()
        
        if hasattr(self, 'processing_thread'):
            self.processing_thread.join()
        
        print("ğŸ›‘ ìŒì„± ì¸ì‹ ì¤‘ì§€ë¨")
    
    def _process_audio(self):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œ"""
        audio_buffer = np.array([], dtype=np.float32)
        
        while self.is_recording:
            try:
                # íì—ì„œ ì˜¤ë””ì˜¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                chunk = self.audio_queue.get(timeout=1.0)
                audio_buffer = np.concatenate([audio_buffer, chunk.flatten()])
                
                # ì¶©ë¶„í•œ ì˜¤ë””ì˜¤ê°€ ìŒ“ì´ë©´ ì²˜ë¦¬
                if len(audio_buffer) >= self.chunk_size:
                    # ìŒì„± í™œë™ ê°ì§€ (ê°„ë‹¨í•œ ì—ë„ˆì§€ ê¸°ë°˜)
                    energy = np.mean(audio_buffer ** 2)
                    
                    if energy > 0.001:  # ì„ê³„ê°’
                        # Whisperë¡œ ìŒì„± ì¸ì‹
                        text = self._transcribe_audio(audio_buffer)
                        
                        if text and self.callback_func:
                            self.callback_func(text)
                    
                    # ë²„í¼ ì´ˆê¸°í™” (ì˜¤ë²„ë© ìœ ì§€)
                    overlap_size = self.chunk_size // 4
                    audio_buffer = audio_buffer[-overlap_size:]
                    
            except queue.Empty:
                continue
            except Exception as e:
                print(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    def _transcribe_audio(self, audio: np.ndarray) -> str:
        """ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        try:
            # Whisper ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            audio_whisper = whisper.pad_or_trim(audio)
            
            # ìŒì„± ì¸ì‹ ì‹¤í–‰
            result = self.model.transcribe(
                audio_whisper,
                language="ko",  # í•œêµ­ì–´ ì„¤ì •
                task="transcribe"
            )
            
            text = result["text"].strip()
            
            if text:
                print(f"ğŸ‘¤ ì‚¬ìš©ì: {text}")
                return text
                
        except Exception as e:
            print(f"ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {e}")
        
        return ""
    
    def transcribe_file(self, audio_file: str) -> str:
        """íŒŒì¼ì—ì„œ ìŒì„± ì¸ì‹"""
        try:
            result = self.model.transcribe(audio_file, language="ko")
            return result["text"].strip()
        except Exception as e:
            print(f"íŒŒì¼ ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {e}")
            return ""

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    def on_speech_detected(text):
        print(f"ì¸ì‹ëœ í…ìŠ¤íŠ¸: {text}")
    
    asr = WhisperASR(model_name="small")
    asr.start_recording(callback=on_speech_detected)
    
    try:
        print("ìŒì„±ì„ ë§í•´ë³´ì„¸ìš”... (Ctrl+Cë¡œ ì¢…ë£Œ)")
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        asr.stop_recording()
```

### ğŸ§  LLM ì²˜ë¦¬ ì—”ì§„

```python
# llm_engine.py
import ollama
import threading
import queue
from typing import Iterator, Optional, Dict, Any
import json
import time

class LocalLLMEngine:
    def __init__(self, model_name: str = "llama3.2:3b"):
        """
        ë¡œì»¬ LLM ì—”ì§„
        
        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
        """
        self.model_name = model_name
        self.conversation_history = []
        self.max_history = 10  # ìµœëŒ€ ëŒ€í™” ê¸°ë¡ ìˆ˜
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        self.system_prompt = """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ê³  ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê³  ìœ ìµí•œ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”.
ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ í•´ì£¼ì„¸ìš”.
í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”."""
        
        self._initialize_model()
    
    def _initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™” ë° í™•ì¸"""
        try:
            # ëª¨ë¸ ì¡´ì¬ í™•ì¸
            models = ollama.list()
            model_names = [model['name'] for model in models['models']]
            
            if self.model_name not in model_names:
                print(f"ğŸ“¥ ëª¨ë¸ {self.model_name} ë‹¤ìš´ë¡œë“œ ì¤‘...")
                ollama.pull(self.model_name)
            
            # í…ŒìŠ¤íŠ¸ ìƒì„±
            response = ollama.generate(
                model=self.model_name,
                prompt="Hello",
                stream=False
            )
            
            print(f"âœ… LLM ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ: {self.model_name}")
            
        except Exception as e:
            print(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def generate_response(self, user_input: str) -> Iterator[str]:
        """
        ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            
        Yields:
            str: ìƒì„±ëœ ì‘ë‹µ í† í°
        """
        # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        self.conversation_history.append({
            "role": "user",
            "content": user_input
        })
        
        # ëŒ€í™” ê¸°ë¡ ì œí•œ
        if len(self.conversation_history) > self.max_history * 2:
            self.conversation_history = self.conversation_history[-self.max_history * 2:]
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        messages = [{"role": "system", "content": self.system_prompt}]
        messages.extend(self.conversation_history)
        
        try:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            response_text = ""
            stream = ollama.chat(
                model=self.model_name,
                messages=messages,
                stream=True
            )
            
            for chunk in stream:
                if chunk['done']:
                    break
                
                content = chunk['message']['content']
                response_text += content
                yield content
            
            # ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            self.conversation_history.append({
                "role": "assistant",
                "content": response_text
            })
            
        except Exception as e:
            error_msg = f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            print(f"âŒ {error_msg}")
            yield error_msg
    
    def generate_complete_response(self, user_input: str) -> str:
        """ì™„ì „í•œ ì‘ë‹µ ìƒì„± (ë¹„ìŠ¤íŠ¸ë¦¬ë°)"""
        response_parts = list(self.generate_response(user_input))
        return "".join(response_parts)
    
    def clear_history(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.conversation_history = []
        print("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def set_system_prompt(self, prompt: str):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •"""
        self.system_prompt = prompt
        print(f"ğŸ“ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸ë¨")
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        try:
            info = ollama.show(self.model_name)
            return {
                "name": info.get("name", ""),
                "size": info.get("size", ""),
                "modified": info.get("modified_at", ""),
                "parameters": info.get("details", {}).get("parameter_size", ""),
                "quantization": info.get("details", {}).get("quantization_level", "")
            }
        except Exception as e:
            return {"error": str(e)}

# ì„±ëŠ¥ ìµœì í™”ëœ LLM ì—”ì§„
class OptimizedLLMEngine(LocalLLMEngine):
    def __init__(self, model_name: str = "llama3.2:3b", **kwargs):
        super().__init__(model_name)
        
        # ì„±ëŠ¥ ì„¤ì •
        self.generation_config = {
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "repeat_penalty": 1.1,
            "num_predict": 512,  # ìµœëŒ€ í† í° ìˆ˜
        }
        
        # ì‘ë‹µ ìºì‹œ
        self.response_cache = {}
        self.cache_size_limit = 100
        
    def generate_response(self, user_input: str) -> Iterator[str]:
        """ìºì‹œë¥¼ í™œìš©í•œ ìµœì í™”ëœ ì‘ë‹µ ìƒì„±"""
        # ìºì‹œ í™•ì¸
        cache_key = hash(user_input)
        if cache_key in self.response_cache:
            cached_response = self.response_cache[cache_key]
            for char in cached_response:
                yield char
                time.sleep(0.01)  # íƒ€ì´í•‘ íš¨ê³¼
            return
        
        # ìƒˆë¡œìš´ ì‘ë‹µ ìƒì„±
        response_text = ""
        for token in super().generate_response(user_input):
            response_text += token
            yield token
        
        # ìºì‹œì— ì €ì¥
        if len(self.response_cache) >= self.cache_size_limit:
            # ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = next(iter(self.response_cache))
            del self.response_cache[oldest_key]
        
        self.response_cache[cache_key] = response_text

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    engine = LocalLLMEngine()
    
    print("ğŸ’¬ LLM ì—”ì§„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"ğŸ“Š ëª¨ë¸ ì •ë³´: {engine.get_model_info()}")
    
    while True:
        try:
            user_input = input("\nì‚¬ìš©ì: ")
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                break
            
            print("ğŸ¤– AI: ", end="", flush=True)
            for token in engine.generate_response(user_input):
                print(token, end="", flush=True)
            print()  # ì¤„ë°”ê¿ˆ
            
        except KeyboardInterrupt:
            break
    
    print("\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
```

### ğŸ”Š í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜ (TTS)

```python
# tts_engine.py
import pygame
import io
from TTS.api import TTS
import numpy as np
import threading
import queue
import time
from typing import Optional, List
import tempfile
import os

class LocalTTSEngine:
    def __init__(self, model_name: str = "tts_models/en/ljspeech/tacotron2-DDC"):
        """
        ë¡œì»¬ TTS ì—”ì§„
        
        Args:
            model_name: ì‚¬ìš©í•  TTS ëª¨ë¸
        """
        # pygame ì´ˆê¸°í™”
        pygame.mixer.init(frequency=22050, size=-16, channels=1, buffer=512)
        
        # TTS ëª¨ë¸ ë¡œë“œ
        self.tts = TTS(model_name=model_name)
        
        # í•œêµ­ì–´ ëª¨ë¸ë„ ì¶”ê°€ ë¡œë“œ
        try:
            self.tts_ko = TTS(model_name="tts_models/ko/kss/tacotron2-DDC")
            self.korean_available = True
        except:
            self.korean_available = False
            print("âš ï¸ í•œêµ­ì–´ TTS ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì˜¤ë””ì˜¤ í
        self.audio_queue = queue.Queue()
        self.is_playing = False
        self.current_playback = None
        
        # ìŒì„± ì„¤ì •
        self.speech_rate = 1.0  # ë§í•˜ê¸° ì†ë„
        self.volume = 0.8
        
        print("âœ… TTS ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def text_to_speech(self, text: str, language: str = "auto") -> bytes:
        """
        í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
        
        Args:
            text: ë³€í™˜í•  í…ìŠ¤íŠ¸
            language: ì–¸ì–´ ì„¤ì • (auto, en, ko)
            
        Returns:
            bytes: WAV ì˜¤ë””ì˜¤ ë°ì´í„°
        """
        try:
            # ì–¸ì–´ ìë™ ê°ì§€
            if language == "auto":
                # í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ í•œêµ­ì–´
                if any('\uac00' <= char <= '\ud7a3' for char in text):
                    language = "ko"
                else:
                    language = "en"
            
            # ëª¨ë¸ ì„ íƒ
            if language == "ko" and self.korean_available:
                tts_model = self.tts_ko
            else:
                tts_model = self.tts
            
            # ì„ì‹œ íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                tmp_path = tmp_file.name
            
            # TTS ìƒì„±
            tts_model.tts_to_file(text=text, file_path=tmp_path)
            
            # ì˜¤ë””ì˜¤ ë°ì´í„° ì½ê¸°
            with open(tmp_path, 'rb') as f:
                audio_data = f.read()
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_path)
            
            return audio_data
            
        except Exception as e:
            print(f"âŒ TTS ë³€í™˜ ì˜¤ë¥˜: {e}")
            return b""
    
    def speak(self, text: str, language: str = "auto", blocking: bool = False):
        """
        í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ì¬ìƒ
        
        Args:
            text: ì¬ìƒí•  í…ìŠ¤íŠ¸
            language: ì–¸ì–´ ì„¤ì •
            blocking: ì°¨ë‹¨ ëª¨ë“œ (Trueë©´ ì¬ìƒ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°)
        """
        if not text.strip():
            return
        
        print(f"ğŸ”Š TTS: {text}")
        
        # ìŒì„± ìƒì„±
        audio_data = self.text_to_speech(text, language)
        
        if audio_data:
            # íì— ì¶”ê°€
            self.audio_queue.put(audio_data)
            
            # ì¬ìƒ ìŠ¤ë ˆë“œ ì‹œì‘ (ì•„ì§ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹Œ ê²½ìš°)
            if not self.is_playing:
                playback_thread = threading.Thread(target=self._playback_worker)
                playback_thread.start()
            
            # ë¸”ë¡œí‚¹ ëª¨ë“œì¸ ê²½ìš° ì¬ìƒ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
            if blocking:
                while not self.audio_queue.empty() or self.is_playing:
                    time.sleep(0.1)
    
    def speak_streaming(self, text_stream, language: str = "auto"):
        """
        ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
        
        Args:
            text_stream: í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ (Iterator)
            language: ì–¸ì–´ ì„¤ì •
        """
        sentence_buffer = ""
        
        for token in text_stream:
            sentence_buffer += token
            
            # ë¬¸ì¥ ë ê°ì§€
            if any(punct in sentence_buffer for punct in ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ']):
                # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìŒì„± ìƒì„±
                if sentence_buffer.strip():
                    self.speak(sentence_buffer.strip(), language)
                sentence_buffer = ""
        
        # ë‚¨ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if sentence_buffer.strip():
            self.speak(sentence_buffer.strip(), language)
    
    def _playback_worker(self):
        """ì˜¤ë””ì˜¤ ì¬ìƒ ì›Œì»¤ ìŠ¤ë ˆë“œ"""
        self.is_playing = True
        
        while not self.audio_queue.empty():
            try:
                audio_data = self.audio_queue.get_nowait()
                
                # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                    tmp_file.write(audio_data)
                    tmp_path = tmp_file.name
                
                # pygameìœ¼ë¡œ ì¬ìƒ
                sound = pygame.mixer.Sound(tmp_path)
                sound.set_volume(self.volume)
                channel = sound.play()
                
                # ì¬ìƒ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
                while channel.get_busy():
                    time.sleep(0.1)
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.unlink(tmp_path)
                
            except queue.Empty:
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
        
        self.is_playing = False
    
    def stop(self):
        """ì¬ìƒ ì¤‘ì§€"""
        pygame.mixer.stop()
        
        # í ë¹„ìš°ê¸°
        while not self.audio_queue.empty():
            try:
                self.audio_queue.get_nowait()
            except queue.Empty:
                break
        
        print("ğŸ›‘ TTS ì¬ìƒ ì¤‘ì§€")
    
    def set_volume(self, volume: float):
        """ë³¼ë¥¨ ì„¤ì • (0.0 ~ 1.0)"""
        self.volume = max(0.0, min(1.0, volume))
        print(f"ğŸ”Š ë³¼ë¥¨ ì„¤ì •: {self.volume:.1f}")
    
    def set_speech_rate(self, rate: float):
        """ë§í•˜ê¸° ì†ë„ ì„¤ì •"""
        self.speech_rate = max(0.5, min(2.0, rate))
        print(f"âš¡ ë§í•˜ê¸° ì†ë„: {self.speech_rate:.1f}x")
    
    def get_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ TTS ëª¨ë¸ ëª©ë¡"""
        return TTS.list_models()

# ê³ ê¸‰ TTS ì—”ì§„ (ê°ì • í‘œí˜„ ì§€ì›)
class AdvancedTTSEngine(LocalTTSEngine):
    def __init__(self):
        super().__init__()
        
        # ê°ì •ë³„ ìŒì„± ì„¤ì •
        self.emotion_settings = {
            "neutral": {"rate": 1.0, "volume": 0.8},
            "happy": {"rate": 1.2, "volume": 0.9},
            "sad": {"rate": 0.8, "volume": 0.6},
            "excited": {"rate": 1.4, "volume": 1.0},
            "calm": {"rate": 0.9, "volume": 0.7}
        }
    
    def speak_with_emotion(self, text: str, emotion: str = "neutral"):
        """ê°ì •ì„ í¬í•¨í•œ ìŒì„± ì¶œë ¥"""
        settings = self.emotion_settings.get(emotion, self.emotion_settings["neutral"])
        
        # ì„ì‹œë¡œ ì„¤ì • ë³€ê²½
        original_rate = self.speech_rate
        original_volume = self.volume
        
        self.set_speech_rate(settings["rate"])
        self.set_volume(settings["volume"])
        
        # ìŒì„± ì¶œë ¥
        self.speak(text)
        
        # ì„¤ì • ë³µì›
        self.set_speech_rate(original_rate)
        self.set_volume(original_volume)

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    tts = LocalTTSEngine()
    
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    tts.speak("ì•ˆë…•í•˜ì„¸ìš”! ë¡œì»¬ TTS ì—”ì§„ì…ë‹ˆë‹¤.", blocking=True)
    tts.speak("Hello! This is a local TTS engine.", blocking=True)
    
    # ë³¼ë¥¨ í…ŒìŠ¤íŠ¸
    tts.set_volume(0.5)
    tts.speak("ë³¼ë¥¨ì´ ì ˆë°˜ìœ¼ë¡œ ì¤„ì—ˆìŠµë‹ˆë‹¤.", blocking=True)
    
    print("âœ… TTS í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
```

### ğŸ­ í†µí•© ì‹œìŠ¤í…œ

```python
# talking_llm.py
import streamlit as st
import threading
import time
from whisper_asr import WhisperASR
from llm_engine import LocalLLMEngine
from tts_engine import LocalTTSEngine
import queue

class TalkingLLMSystem:
    def __init__(self):
        """í†µí•© Talking LLM ì‹œìŠ¤í…œ"""
        self.initialize_components()
        self.setup_ui()
        
    def initialize_components(self):
        """êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        with st.spinner("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
            # ê° êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
            self.asr = WhisperASR(model_name="small")
            self.llm = LocalLLMEngine(model_name="llama3.2:3b")
            self.tts = LocalTTSEngine()
            
            # ìƒíƒœ ë³€ìˆ˜
            self.is_listening = False
            self.conversation_active = False
            
            # ë©”ì‹œì§€ í
            self.message_queue = queue.Queue()
            
        st.success("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def setup_ui(self):
        """Streamlit UI ì„¤ì •"""
        st.set_page_config(
            page_title="Local Talking LLM",
            page_icon="ğŸ™ï¸",
            layout="wide"
        )
        
        st.title("ğŸ™ï¸ Local Talking LLM")
        st.markdown("ì™„ì „ ì˜¤í”„ë¼ì¸ ìŒì„± AI ì–´ì‹œìŠ¤í„´íŠ¸")
        
        # ì‚¬ì´ë“œë°” ì„¤ì •
        with st.sidebar:
            st.header("âš™ï¸ ì„¤ì •")
            
            # ëª¨ë¸ ì •ë³´
            st.subheader("ğŸ“Š ëª¨ë¸ ì •ë³´")
            model_info = self.llm.get_model_info()
            st.json(model_info)
            
            # ìŒì„± ì„¤ì •
            st.subheader("ğŸ”Š ìŒì„± ì„¤ì •")
            volume = st.slider("ë³¼ë¥¨", 0.0, 1.0, 0.8, 0.1)
            self.tts.set_volume(volume)
            
            speech_rate = st.slider("ë§í•˜ê¸° ì†ë„", 0.5, 2.0, 1.0, 0.1)
            self.tts.set_speech_rate(speech_rate)
            
            # ì‹œìŠ¤í…œ ì œì–´
            st.subheader("ğŸ›ï¸ ì‹œìŠ¤í…œ ì œì–´")
            if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
                self.llm.clear_history()
                st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ë©”ì¸ ì˜ì—­
        self.main_interface()
    
    def main_interface(self):
        """ë©”ì¸ ì¸í„°í˜ì´ìŠ¤"""
        col1, col2 = st.columns([3, 1])
        
        with col1:
            # ëŒ€í™” ì˜ì—­
            st.subheader("ğŸ’¬ ëŒ€í™”")
            
            # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
            chat_container = st.container()
            with chat_container:
                if hasattr(self.llm, 'conversation_history'):
                    for msg in self.llm.conversation_history:
                        if msg['role'] == 'user':
                            st.chat_message("user").write(msg['content'])
                        elif msg['role'] == 'assistant':
                            st.chat_message("assistant").write(msg['content'])
            
            # í…ìŠ¤íŠ¸ ì…ë ¥
            user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ìŒì„±ìœ¼ë¡œ ë§í•˜ì„¸ìš”...")
            
            if user_input:
                self.process_user_input(user_input)
        
        with col2:
            # ìŒì„± ì œì–´
            st.subheader("ğŸ¤ ìŒì„± ì œì–´")
            
            if st.button("ğŸ™ï¸ ìŒì„± ëŒ€í™” ì‹œì‘", key="start_voice"):
                self.start_voice_conversation()
            
            if st.button("ğŸ›‘ ìŒì„± ëŒ€í™” ì¤‘ì§€", key="stop_voice"):
                self.stop_voice_conversation()
            
            # ìƒíƒœ í‘œì‹œ
            status_placeholder = st.empty()
            
            if self.is_listening:
                status_placeholder.success("ğŸ¤ ë“£ê³  ìˆìŠµë‹ˆë‹¤...")
            elif self.conversation_active:
                status_placeholder.info("ğŸ’­ ìƒê° ì¤‘...")
            else:
                status_placeholder.info("â¸ï¸ ëŒ€ê¸° ì¤‘")
    
    def process_user_input(self, user_input: str):
        """ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬"""
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        st.chat_message("user").write(user_input)
        
        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            for token in self.llm.generate_response(user_input):
                full_response += token
                response_placeholder.write(full_response)
            
            # TTSë¡œ ìŒì„± ì¶œë ¥
            self.tts.speak(full_response)
    
    def start_voice_conversation(self):
        """ìŒì„± ëŒ€í™” ì‹œì‘"""
        if not self.is_listening:
            self.is_listening = True
            self.asr.start_recording(callback=self.on_speech_detected)
            st.success("ğŸ¤ ìŒì„± ëŒ€í™”ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    def stop_voice_conversation(self):
        """ìŒì„± ëŒ€í™” ì¤‘ì§€"""
        if self.is_listening:
            self.is_listening = False
            self.asr.stop_recording()
            self.tts.stop()
            st.info("ğŸ›‘ ìŒì„± ëŒ€í™”ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def on_speech_detected(self, text: str):
        """ìŒì„± ê°ì§€ ì½œë°±"""
        if text and len(text.strip()) > 3:  # ìµœì†Œ ê¸¸ì´ í•„í„°
            self.message_queue.put(text)
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
            threading.Thread(
                target=self.process_speech_input,
                args=(text,),
                daemon=True
            ).start()
    
    def process_speech_input(self, text: str):
        """ìŒì„± ì…ë ¥ ì²˜ë¦¬"""
        self.conversation_active = True
        
        try:
            # LLM ì‘ë‹µ ìƒì„±
            response_text = ""
            for token in self.llm.generate_response(text):
                response_text += token
            
            # TTS ì¶œë ¥
            if response_text:
                self.tts.speak(response_text)
            
        except Exception as e:
            st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        finally:
            self.conversation_active = False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        system = TalkingLLMSystem()
        
        # ìë™ ìƒˆë¡œê³ ì¹¨ (ê°œë°œ ì¤‘)
        if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨"):
            st.rerun()
            
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.info("ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
```

## ê³ ê¸‰ ê¸°ëŠ¥ êµ¬í˜„

### ğŸ¯ ì‹¤ì‹œê°„ ìŒì„± ì¤‘ë‹¨ ë° ìŠ¤íŠ¸ë¦¬ë°

```python
# advanced_features.py
import threading
import time
from typing import Generator
import numpy as np

class InterruptibleTTS:
    def __init__(self, tts_engine, asr_engine):
        self.tts = tts_engine
        self.asr = asr_engine
        self.is_speaking = False
        self.interrupt_flag = threading.Event()
        
    def speak_with_interruption(self, text: str):
        """ì¤‘ë‹¨ ê°€ëŠ¥í•œ ìŒì„± ì¶œë ¥"""
        self.interrupt_flag.clear()
        self.is_speaking = True
        
        # ìŒì„± ê°ì§€ ìŠ¤ë ˆë“œ ì‹œì‘
        interrupt_thread = threading.Thread(
            target=self._monitor_interruption,
            daemon=True
        )
        interrupt_thread.start()
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì¬ìƒ
        sentences = self._split_sentences(text)
        
        for sentence in sentences:
            if self.interrupt_flag.is_set():
                print("ğŸ›‘ ìŒì„± ì¶œë ¥ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break
                
            self.tts.speak(sentence, blocking=True)
        
        self.is_speaking = False
    
    def _monitor_interruption(self):
        """ìŒì„± ì¤‘ë‹¨ ëª¨ë‹ˆí„°ë§"""
        silence_threshold = 0.01
        speech_threshold = 0.05
        
        while self.is_speaking and not self.interrupt_flag.is_set():
            # ë§ˆì´í¬ì—ì„œ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬
            audio_level = self._get_audio_level()
            
            if audio_level > speech_threshold:
                # ì‚¬ìš©ìê°€ ë§í•˜ê¸° ì‹œì‘í•¨
                self.interrupt_flag.set()
                self.tts.stop()
                break
            
            time.sleep(0.1)
    
    def _get_audio_level(self) -> float:
        """í˜„ì¬ ì˜¤ë””ì˜¤ ë ˆë²¨ ì¸¡ì •"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” sounddevice ë˜ëŠ” pyaudio ì‚¬ìš©
        return np.random.random() * 0.1  # ë”ë¯¸ êµ¬í˜„
    
    def _split_sentences(self, text: str) -> list:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• """
        import re
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text)
        return [s.strip() for s in sentences if s.strip()]

class StreamingLLMProcessor:
    def __init__(self, llm_engine, tts_engine):
        self.llm = llm_engine
        self.tts = tts_engine
        
    def process_streaming(self, user_input: str) -> Generator[str, None, None]:
        """ìŠ¤íŠ¸ë¦¬ë° LLM ì²˜ë¦¬"""
        sentence_buffer = ""
        
        for token in self.llm.generate_response(user_input):
            sentence_buffer += token
            yield token
            
            # ë¬¸ì¥ ì™„ì„± ì‹œ ì¦‰ì‹œ TTS ì‹œì‘
            if self._is_sentence_complete(sentence_buffer):
                # ë°±ê·¸ë¼ìš´ë“œì—ì„œ TTS ì‹¤í–‰
                threading.Thread(
                    target=self.tts.speak,
                    args=(sentence_buffer.strip(),),
                    daemon=True
                ).start()
                
                sentence_buffer = ""
        
        # ë‚¨ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if sentence_buffer.strip():
            threading.Thread(
                target=self.tts.speak,
                args=(sentence_buffer.strip(),),
                daemon=True
            ).start()
    
    def _is_sentence_complete(self, text: str) -> bool:
        """ë¬¸ì¥ ì™„ì„± ì—¬ë¶€ í™•ì¸"""
        endings = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ']
        return any(text.strip().endswith(end) for end in endings)
```

### ğŸŒ ë‹¤êµ­ì–´ ì§€ì›

```python
# multilingual_support.py
import langdetect
from typing import Dict, List

class MultilingualProcessor:
    def __init__(self):
        # ì§€ì› ì–¸ì–´ ì„¤ì •
        self.supported_languages = {
            'ko': {
                'name': 'í•œêµ­ì–´',
                'whisper_code': 'ko',
                'tts_model': 'tts_models/ko/kss/tacotron2-DDC',
                'system_prompt': 'ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.'
            },
            'en': {
                'name': 'English',
                'whisper_code': 'en',
                'tts_model': 'tts_models/en/ljspeech/tacotron2-DDC',
                'system_prompt': 'You are a helpful English AI assistant.'
            },
            'ja': {
                'name': 'æ—¥æœ¬èª',
                'whisper_code': 'ja',
                'tts_model': 'tts_models/ja/kokoro/tacotron2-DDC',
                'system_prompt': 'ã‚ãªãŸã¯å½¹ã«ç«‹ã¤æ—¥æœ¬èªã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚'
            },
            'zh': {
                'name': 'ä¸­æ–‡',
                'whisper_code': 'zh',
                'tts_model': 'tts_models/zh-CN/baker/tacotron2-DDC',
                'system_prompt': 'ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ä¸­æ–‡AIåŠ©æ‰‹ã€‚'
            }
        }
        
        self.current_language = 'ko'  # ê¸°ë³¸ ì–¸ì–´
    
    def detect_language(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì–¸ì–´ ê°ì§€"""
        try:
            detected = langdetect.detect(text)
            if detected in self.supported_languages:
                return detected
        except:
            pass
        
        # í•œê¸€ ë¬¸ì í¬í•¨ ì—¬ë¶€ë¡œ íŒë‹¨
        if any('\uac00' <= char <= '\ud7a3' for char in text):
            return 'ko'
        
        return 'en'  # ê¸°ë³¸ê°’
    
    def set_language(self, language_code: str):
        """ì–¸ì–´ ì„¤ì •"""
        if language_code in self.supported_languages:
            self.current_language = language_code
            print(f"ğŸŒ ì–¸ì–´ ì„¤ì •: {self.supported_languages[language_code]['name']}")
        else:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ì–´: {language_code}")
    
    def get_language_config(self, language_code: str = None) -> Dict:
        """ì–¸ì–´ë³„ ì„¤ì • ì¡°íšŒ"""
        lang = language_code or self.current_language
        return self.supported_languages.get(lang, self.supported_languages['ko'])
    
    def get_system_prompt(self, language_code: str = None) -> str:
        """ì–¸ì–´ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        config = self.get_language_config(language_code)
        return config['system_prompt']

# ì–¸ì–´ë³„ ëª¨ë¸ ê´€ë¦¬ì
class LanguageModelManager:
    def __init__(self):
        self.loaded_models = {}
        self.multilingual = MultilingualProcessor()
    
    def load_model_for_language(self, language_code: str):
        """ì–¸ì–´ë³„ ëª¨ë¸ ë¡œë“œ"""
        if language_code not in self.loaded_models:
            config = self.multilingual.get_language_config(language_code)
            
            # TTS ëª¨ë¸ ë¡œë“œ
            try:
                from TTS.api import TTS
                tts_model = TTS(model_name=config['tts_model'])
                self.loaded_models[language_code] = {
                    'tts': tts_model,
                    'config': config
                }
                print(f"âœ… {config['name']} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {config['name']} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def get_tts_model(self, language_code: str):
        """ì–¸ì–´ë³„ TTS ëª¨ë¸ ì¡°íšŒ"""
        if language_code not in self.loaded_models:
            self.load_model_for_language(language_code)
        
        return self.loaded_models.get(language_code, {}).get('tts')
```

### ğŸ›ï¸ ê³ ê¸‰ ì„¤ì • ë° ìµœì í™”

```python
# advanced_config.py
import json
import os
from dataclasses import dataclass, asdict
from typing import Optional, Dict, Any

@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì • í´ë˜ìŠ¤"""
    
    # ìŒì„± ì¸ì‹ ì„¤ì •
    whisper_model: str = "small"
    whisper_language: str = "auto"
    audio_threshold: float = 0.001
    chunk_duration: float = 2.0
    
    # LLM ì„¤ì •
    llm_model: str = "llama3.2:3b"
    max_history: int = 10
    temperature: float = 0.7
    max_tokens: int = 512
    
    # TTS ì„¤ì •
    tts_model: str = "tts_models/en/ljspeech/tacotron2-DDC"
    speech_rate: float = 1.0
    volume: float = 0.8
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    enable_interruption: bool = True
    enable_streaming: bool = True
    cache_responses: bool = True
    log_conversations: bool = False
    
    # ì„±ëŠ¥ ì„¤ì •
    num_threads: int = 4
    gpu_acceleration: bool = True
    memory_limit: str = "8GB"

class ConfigManager:
    def __init__(self, config_file: str = "config.json"):
        self.config_file = config_file
        self.config = self.load_config()
    
    def load_config(self) -> SystemConfig:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return SystemConfig(**data)
            except Exception as e:
                print(f"ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ ì„¤ì • ë°˜í™˜
        return SystemConfig()
    
    def save_config(self):
        """ì„¤ì • íŒŒì¼ ì €ì¥"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(self.config), f, indent=2, ensure_ascii=False)
            print(f"âœ… ì„¤ì • ì €ì¥ë¨: {self.config_file}")
        except Exception as e:
            print(f"âŒ ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def update_config(self, **kwargs):
        """ì„¤ì • ì—…ë°ì´íŠ¸"""
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
                print(f"ğŸ“ ì„¤ì • ì—…ë°ì´íŠ¸: {key} = {value}")
        
        self.save_config()
    
    def get_optimized_config(self) -> Dict[str, Any]:
        """í•˜ë“œì›¨ì–´ ê¸°ë°˜ ìµœì í™”ëœ ì„¤ì •"""
        import psutil
        
        # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
        ram_gb = psutil.virtual_memory().total // (1024**3)
        cpu_count = psutil.cpu_count()
        
        optimized = asdict(self.config)
        
        # RAM ê¸°ë°˜ ìµœì í™”
        if ram_gb >= 32:
            optimized['llm_model'] = "llama3.2:8b"
            optimized['whisper_model'] = "medium"
            optimized['max_tokens'] = 1024
        elif ram_gb >= 16:
            optimized['llm_model'] = "llama3.2:3b"
            optimized['whisper_model'] = "small"
            optimized['max_tokens'] = 512
        else:
            optimized['llm_model'] = "llama3.2:1b"
            optimized['whisper_model'] = "tiny"
            optimized['max_tokens'] = 256
        
        # CPU ê¸°ë°˜ ìµœì í™”
        optimized['num_threads'] = min(cpu_count, 8)
        
        return optimized

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            'asr_latency': [],
            'llm_latency': [],
            'tts_latency': [],
            'memory_usage': [],
            'cpu_usage': []
        }
    
    def record_metric(self, metric_name: str, value: float):
        """ë©”íŠ¸ë¦­ ê¸°ë¡"""
        if metric_name in self.metrics:
            self.metrics[metric_name].append(value)
            
            # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
            if len(self.metrics[metric_name]) > 100:
                self.metrics[metric_name] = self.metrics[metric_name][-100:]
    
    def get_average_latency(self, component: str) -> float:
        """í‰ê·  ì§€ì—°ì‹œê°„ ê³„ì‚°"""
        latencies = self.metrics.get(f'{component}_latency', [])
        return sum(latencies) / len(latencies) if latencies else 0.0
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        return {
            'asr_avg_latency': self.get_average_latency('asr'),
            'llm_avg_latency': self.get_average_latency('llm'),
            'tts_avg_latency': self.get_average_latency('tts'),
            'total_conversations': len(self.metrics['asr_latency']),
            'system_health': self._calculate_health_score()
        }
    
    def _calculate_health_score(self) -> str:
        """ì‹œìŠ¤í…œ ê±´ê°•ë„ ê³„ì‚°"""
        total_latency = (
            self.get_average_latency('asr') +
            self.get_average_latency('llm') +
            self.get_average_latency('tts')
        )
        
        if total_latency < 3.0:
            return "excellent"
        elif total_latency < 5.0:
            return "good"
        elif total_latency < 8.0:
            return "fair"
        else:
            return "poor"
```

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ğŸ”§ ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. ìŒì„± ì¸ì‹ ë¬¸ì œ

```python
# ë””ë²„ê¹… ë„êµ¬
class ASRDebugger:
    def __init__(self, asr_engine):
        self.asr = asr_engine
    
    def test_microphone(self):
        """ë§ˆì´í¬ í…ŒìŠ¤íŠ¸"""
        import sounddevice as sd
        
        print("ğŸ¤ ë§ˆì´í¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì¥ì¹˜ í™•ì¸
        devices = sd.query_devices()
        print("ğŸ“± ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì¥ì¹˜:")
        for i, device in enumerate(devices):
            print(f"  {i}: {device['name']}")
        
        # ê¸°ë³¸ ì…ë ¥ ì¥ì¹˜ í…ŒìŠ¤íŠ¸
        try:
            duration = 3  # 3ì´ˆ
            print(f"ğŸ”´ {duration}ì´ˆê°„ ë…¹ìŒ ì‹œì‘...")
            
            recording = sd.rec(
                int(duration * 16000),
                samplerate=16000,
                channels=1,
                dtype='float32'
            )
            sd.wait()
            
            # ì˜¤ë””ì˜¤ ë ˆë²¨ í™•ì¸
            max_level = np.max(np.abs(recording))
            print(f"ğŸ“Š ìµœëŒ€ ì˜¤ë””ì˜¤ ë ˆë²¨: {max_level:.4f}")
            
            if max_level < 0.001:
                print("âš ï¸ ì˜¤ë””ì˜¤ ë ˆë²¨ì´ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤. ë§ˆì´í¬ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            else:
                print("âœ… ë§ˆì´í¬ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
                
                # Whisper í…ŒìŠ¤íŠ¸
                text = self.asr._transcribe_audio(recording.flatten())
                if text:
                    print(f"ğŸ¯ ì¸ì‹ëœ í…ìŠ¤íŠ¸: '{text}'")
                else:
                    print("âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨")
            
        except Exception as e:
            print(f"âŒ ë§ˆì´í¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    def check_whisper_models(self):
        """Whisper ëª¨ë¸ í™•ì¸"""
        import whisper
        
        available_models = whisper.available_models()
        print(f"ğŸ“¦ ì‚¬ìš© ê°€ëŠ¥í•œ Whisper ëª¨ë¸: {available_models}")
        
        for model_name in ["tiny", "base", "small"]:
            try:
                model = whisper.load_model(model_name)
                print(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                print(f"âŒ {model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
```

#### 2. LLM ì„±ëŠ¥ ë¬¸ì œ

```bash
# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash
# check_system_resources.sh

echo "ğŸ–¥ï¸ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸"
echo "===================="

# RAM ì‚¬ìš©ëŸ‰
echo "ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
free -h

echo ""

# CPU ì •ë³´
echo "ğŸ–¥ï¸ CPU ì •ë³´:"
lscpu | grep -E "(Model name|CPU\(s\)|Core\(s\)|Thread\(s\))"

echo ""

# ë””ìŠ¤í¬ ê³µê°„
echo "ğŸ’¿ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰:"
df -h

echo ""

# GPU ì •ë³´ (NVIDIA)
if command -v nvidia-smi &> /dev/null; then
    echo "ğŸ® GPU ì •ë³´:"
    nvidia-smi --query-gpu=name,memory.total,memory.used --format=csv,noheader,nounits
else
    echo "ğŸ® NVIDIA GPU ì—†ìŒ"
fi

echo ""

# Ollama ìƒíƒœ í™•ì¸
if command -v ollama &> /dev/null; then
    echo "ğŸ¦™ Ollama ëª¨ë¸ ëª©ë¡:"
    ollama list
else
    echo "âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ"
fi
```

#### 3. TTS ìŒì§ˆ ë¬¸ì œ

```python
# TTS ìµœì í™” ì„¤ì •
class TTSOptimizer:
    def __init__(self):
        self.optimization_settings = {
            "high_quality": {
                "model": "tts_models/en/ljspeech/glow-tts",
                "vocoder": "vocoder_models/en/ljspeech/hifigan_v2",
                "sample_rate": 22050
            },
            "fast": {
                "model": "tts_models/en/ljspeech/tacotron2-DDC",
                "vocoder": "vocoder_models/en/ljspeech/hifigan_v1",
                "sample_rate": 16000
            },
            "balanced": {
                "model": "tts_models/en/ljspeech/tacotron2-DCA",
                "vocoder": "vocoder_models/en/ljspeech/multiband-melgan",
                "sample_rate": 22050
            }
        }
    
    def optimize_for_hardware(self) -> str:
        """í•˜ë“œì›¨ì–´ì— ë”°ë¥¸ ìµœì í™”"""
        import psutil
        
        ram_gb = psutil.virtual_memory().total // (1024**3)
        cpu_count = psutil.cpu_count()
        
        if ram_gb >= 16 and cpu_count >= 8:
            return "high_quality"
        elif ram_gb >= 8 and cpu_count >= 4:
            return "balanced"
        else:
            return "fast"
    
    def get_recommended_settings(self) -> dict:
        """ê¶Œì¥ ì„¤ì • ë°˜í™˜"""
        profile = self.optimize_for_hardware()
        return self.optimization_settings[profile]
```

## ë°°í¬ ë° ìš´ì˜

### ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬

```bash
#!/bin/bash
# deploy_production.sh

echo "ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬ ì‹œì‘..."

# í™˜ê²½ ì„¤ì •
export ENVIRONMENT="production"
export LOG_LEVEL="INFO"
export MAX_WORKERS=4

# ê°€ìƒí™˜ê²½ í™œì„±í™”
source local_talking_llm_env/bin/activate

# ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
pip install --upgrade -r requirements.txt

# ëª¨ë¸ ìµœì í™”
python optimize_models.py

# ì„¤ì • ê²€ì¦
python validate_config.py

# ì„œë¹„ìŠ¤ ì‹œì‘
if command -v systemctl &> /dev/null; then
    # systemd ì„œë¹„ìŠ¤ë¡œ ì‹¤í–‰
    sudo cp talking-llm.service /etc/systemd/system/
    sudo systemctl daemon-reload
    sudo systemctl enable talking-llm
    sudo systemctl start talking-llm
    
    echo "âœ… systemd ì„œë¹„ìŠ¤ë¡œ ì‹œì‘ë¨"
else
    # ì§ì ‘ ì‹¤í–‰
    nohup python talking_llm.py > logs/app.log 2>&1 &
    echo "âœ… ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë¨"
fi

echo "ğŸ‰ ë°°í¬ ì™„ë£Œ!"
```

```ini
# talking-llm.service
[Unit]
Description=Local Talking LLM Service
After=network.target

[Service]
Type=simple
User=talking-llm
WorkingDirectory=/opt/talking-llm
Environment=PATH=/opt/talking-llm/venv/bin
ExecStart=/opt/talking-llm/venv/bin/python talking_llm.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

### ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

```python
# monitoring.py
import logging
import time
import psutil
from datetime import datetime
import json

class SystemMonitor:
    def __init__(self, log_file: str = "system_monitor.log"):
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        self.start_time = time.time()
        self.conversation_count = 0
        
    def log_conversation(self, user_input: str, ai_response: str, 
                        processing_time: float):
        """ëŒ€í™” ë¡œê¹…"""
        self.conversation_count += 1
        
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "conversation_id": self.conversation_count,
            "user_input_length": len(user_input),
            "ai_response_length": len(ai_response),
            "processing_time": processing_time,
            "system_memory": psutil.virtual_memory().percent,
            "system_cpu": psutil.cpu_percent()
        }
        
        self.logger.info(f"CONVERSATION: {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_system_status(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê¹…"""
        uptime = time.time() - self.start_time
        
        status = {
            "uptime_hours": uptime / 3600,
            "total_conversations": self.conversation_count,
            "memory_usage": psutil.virtual_memory().percent,
            "cpu_usage": psutil.cpu_percent(interval=1),
            "disk_usage": psutil.disk_usage('/').percent
        }
        
        self.logger.info(f"SYSTEM_STATUS: {json.dumps(status)}")
    
    def check_health(self) -> bool:
        """ì‹œìŠ¤í…œ ê±´ê°•ë„ ì²´í¬"""
        memory_usage = psutil.virtual_memory().percent
        cpu_usage = psutil.cpu_percent(interval=1)
        disk_usage = psutil.disk_usage('/').percent
        
        if memory_usage > 90:
            self.logger.warning(f"High memory usage: {memory_usage}%")
            return False
        
        if cpu_usage > 95:
            self.logger.warning(f"High CPU usage: {cpu_usage}%")
            return False
        
        if disk_usage > 95:
            self.logger.warning(f"High disk usage: {disk_usage}%")
            return False
        
        return True
```

## ê²°ë¡ 

Local Talking LLM ì‹œìŠ¤í…œì€ ì™„ì „í•œ ë°ì´í„° í”„ë¼ì´ë²„ì‹œë¥¼ ë³´ì¥í•˜ë©´ì„œë„ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ëŒ€í™”ê°€ ê°€ëŠ¥í•œ í˜ì‹ ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ì´ ê°€ì´ë“œë¥¼ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### ğŸ¯ ì£¼ìš” ì„±ê³¼

- **ì™„ì „í•œ ì˜¤í”„ë¼ì¸ ë™ì‘**: ì¸í„°ë„· ì—°ê²° ì—†ì´ ë…ë¦½ì  ìš´ì˜
- **ë°ì´í„° í”„ë¼ì´ë²„ì‹œ**: ëª¨ë“  ëŒ€í™” ë‚´ìš©ì´ ë¡œì»¬ì—ì„œ ì²˜ë¦¬
- **ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©**: ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ëŒ€í™” ê²½í—˜
- **ë‹¤êµ­ì–´ ì§€ì›**: ì—¬ëŸ¬ ì–¸ì–´ì—ì„œ ë™ì‹œ ë™ì‘ ê°€ëŠ¥
- **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ê°œì¸ ë˜ëŠ” ê¸°ì—… ìš”êµ¬ì— ë§ì¶˜ ì„¤ì •

### ğŸš€ í–¥í›„ ë°œì „ ë°©í–¥

```python
future_enhancements = {
    "performance": [
        "GPU ê°€ì† ìµœì í™”",
        "ë” ì‘ì€ ëª¨ë¸ í¬ê¸°",
        "ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ê°œì„ "
    ],
    "features": [
        "ê°ì • ì¸ì‹ ë° í‘œí˜„",
        "ì»¨í…ìŠ¤íŠ¸ ê¸°ì–µ í™•ì¥",
        "ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ì§€ì›"
    ],
    "integration": [
        "ìŠ¤ë§ˆíŠ¸í™ˆ ì—°ë™",
        "IoT ë””ë°”ì´ìŠ¤ ì œì–´",
        "ì™¸ë¶€ API ì—°ê²° ì˜µì…˜"
    ]
}
```

### ğŸ’¡ ì‹¤ë¬´ ì ìš© ê¶Œì¥ì‚¬í•­

1. **í•˜ë“œì›¨ì–´ íˆ¬ì**: 16GB+ RAMê³¼ SSDëŠ” í•„ìˆ˜
2. **ì ì§„ì  ë°°í¬**: ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ë¶€í„° ì‹œì‘
3. **ëª¨ë‹ˆí„°ë§**: ì‹œìŠ¤í…œ ì„±ëŠ¥ ì§€ì†ì  ê´€ì°°
4. **ë³´ì•ˆ**: ì •ê¸°ì ì¸ ëª¨ë¸ ë° ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸

Local Talking LLMì€ AI ê¸°ìˆ ì˜ ë¯¼ì£¼í™”ì™€ ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ë™ì‹œì— ì‹¤í˜„í•˜ëŠ” ë¯¸ë˜ì§€í–¥ì ì¸ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—¬ëŸ¬ë¶„ë§Œì˜ í”„ë¼ì´ë¹— AI ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ êµ¬ì¶•í•´ë³´ì„¸ìš”.

---

**ë” ì•Œì•„ë³´ê¸°:**
- [OpenAI Whisper GitHub](https://github.com/openai/whisper)
- [Ollama ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://ollama.ai/)
- [Coqui TTS](https://github.com/coqui-ai/TTS)
- [LLaMA ëª¨ë¸ ê°€ì´ë“œ](https://llama.meta.com/)
