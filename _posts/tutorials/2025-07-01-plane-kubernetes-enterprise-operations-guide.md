---
title: "Plane ì—”í„°í”„ë¼ì´ì¦ˆ ì¿ ë²„ë„¤í‹°ìŠ¤ ìš´ì˜ ê°€ì´ë“œ - Helmë¶€í„° ìì²´ í´ë¼ìš°ë“œê¹Œì§€"
excerpt: "Plane ì¿ ë²„ë„¤í‹°ìŠ¤ ìš´ì˜ì˜ ëª¨ë“  ê²ƒì„ ë‹¤ë£¨ëŠ” ì™„ì „í•œ ì—”í„°í”„ë¼ì´ì¦ˆ ê°€ì´ë“œ. Helm ì°¨íŠ¸, AWS/GCP/ìì²´ í´ë¼ìš°ë“œ ë°°í¬, ê·œëª¨ë³„ í•˜ë“œì›¨ì–´ ìŠ¤í™, ëª¨ë‹ˆí„°ë§, CI/CDê¹Œì§€ ì‹¤ì „ ìš´ì˜ ë…¸í•˜ìš°ë¥¼ ëª¨ë‘ ë‹´ì•˜ìŠµë‹ˆë‹¤."
seo_title: "Plane ì—”í„°í”„ë¼ì´ì¦ˆ ì¿ ë²„ë„¤í‹°ìŠ¤ ìš´ì˜ ê°€ì´ë“œ - Helm ìì²´í´ë¼ìš°ë“œ - Thaki Cloud"
seo_description: "Plane ì—”í„°í”„ë¼ì´ì¦ˆ ì¿ ë²„ë„¤í‹°ìŠ¤ ì™„ì „ ìš´ì˜ ê°€ì´ë“œ. Helm ì°¨íŠ¸ AWS EKS GCP GKE ìì²´í´ë¼ìš°ë“œ êµ¬ì¶• ê·œëª¨ë³„ í•˜ë“œì›¨ì–´ ìŠ¤í™ ëª¨ë‹ˆí„°ë§ CI/CD ë°±ì—… ë³´ì•ˆ"
date: 2025-07-01
last_modified_at: 2025-07-01
categories:
  - tutorials
tags:
  - plane
  - kubernetes
  - enterprise
  - helm
  - aws-eks
  - gcp-gke
  - private-cloud
  - self-hosted
  - monitoring
  - prometheus
  - grafana
  - cicd
  - gitops
  - argocd
  - backup
  - security
  - hardware-specs
  - production
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
header:
  teaser: "/assets/images/thumbnails/post-thumbnail.jpg"
  overlay_image: "/assets/images/headers/post-header.jpg"
  overlay_filter: 0.5
canonical_url: "https://thakicloud.github.io/tutorials/plane-kubernetes-enterprise-operations-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 30ë¶„

## ì„œë¡ 

[ì´ì „ ê¸€](#)ì—ì„œ OrbStack ê°œë°œ í™˜ê²½ê³¼ ê¸°ë³¸ ì¿ ë²„ë„¤í‹°ìŠ¤ ë°°í¬ë¥¼ ë‹¤ë¤˜ë‹¤ë©´, ì´ë²ˆì—ëŠ” **ì‹¤ì œ ì—”í„°í”„ë¼ì´ì¦ˆ ìš´ì˜**ì— í•„ìš”í•œ ëª¨ë“  ê²ƒì„ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

ì´ ê°€ì´ë“œëŠ” **ì§„ì§œ í˜„ì—…ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”** ì™„ì „í•œ ìš´ì˜ ë§¤ë‰´ì–¼ì…ë‹ˆë‹¤:

- ğŸ“¦ **Helm ì°¨íŠ¸ ë§ˆìŠ¤í„°**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë°°í¬ íŒ¨í‚¤ì§€ ì™„ì „ ì •ë³µ
- â˜ï¸ **ë©€í‹° í´ë¼ìš°ë“œ ì „ëµ**: AWS, GCP, ìì²´ í´ë¼ìš°ë“œ ëª¨ë“  í™˜ê²½ ì»¤ë²„
- ğŸ—ï¸ **ìì²´ í´ë¼ìš°ë“œ êµ¬ì¶•**: í•˜ë“œì›¨ì–´ë¶€í„° k8s í´ëŸ¬ìŠ¤í„°ê¹Œì§€ A-Z
- ğŸ“ **ê·œëª¨ë³„ ìŠ¤í™ ê°€ì´ë“œ**: 10ëª…~10,000ëª… íŒ€ê¹Œì§€ ìµœì í™”ëœ êµ¬ì„±
- ğŸ“Š **ì™„ì „í•œ ëª¨ë‹ˆí„°ë§**: Prometheus, Grafana, ì•Œë¦¼ ì‹œìŠ¤í…œ
- ğŸ”’ **ì—”í„°í”„ë¼ì´ì¦ˆ ë³´ì•ˆ**: RBAC, ë„¤íŠ¸ì›Œí¬ ì •ì±…, ì»´í”Œë¼ì´ì–¸ìŠ¤
- ğŸš€ **GitOps CI/CD**: ArgoCDë¡œ ì™„ì „ ìë™í™”ëœ ë°°í¬ íŒŒì´í”„ë¼ì¸

## Helm ì°¨íŠ¸ ì™„ì „ ì •ë³µ

### 1. Plane Helm ì°¨íŠ¸ êµ¬ì¡°

```bash
# Helm ì°¨íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°
plane-helm/
â”œâ”€â”€ Chart.yaml                 # ì°¨íŠ¸ ë©”íƒ€ë°ì´í„°
â”œâ”€â”€ values.yaml               # ê¸°ë³¸ ì„¤ì •ê°’
â”œâ”€â”€ values-dev.yaml           # ê°œë°œ í™˜ê²½ ì„¤ì •
â”œâ”€â”€ values-staging.yaml       # ìŠ¤í…Œì´ì§• í™˜ê²½ ì„¤ì •  
â”œâ”€â”€ values-production.yaml    # ìš´ì˜ í™˜ê²½ ì„¤ì •
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ _helpers.tpl         # í—¬í¼ í…œí”Œë¦¿
â”‚   â”œâ”€â”€ configmap.yaml       # ConfigMap í…œí”Œë¦¿
â”‚   â”œâ”€â”€ secret.yaml          # Secret í…œí”Œë¦¿
â”‚   â”œâ”€â”€ pvc.yaml             # PersistentVolumeClaim
â”‚   â”œâ”€â”€ postgresql/
â”‚   â”‚   â”œâ”€â”€ statefulset.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â””â”€â”€ init-job.yaml
â”‚   â”œâ”€â”€ redis/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â””â”€â”€ service.yaml
â”‚   â”œâ”€â”€ minio/
â”‚   â”‚   â”œâ”€â”€ statefulset.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â””â”€â”€ init-job.yaml
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â””â”€â”€ hpa.yaml
â”‚   â”œâ”€â”€ worker/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â””â”€â”€ hpa.yaml
â”‚   â”œâ”€â”€ web/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â””â”€â”€ hpa.yaml
â”‚   â”œâ”€â”€ admin/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â””â”€â”€ service.yaml
â”‚   â”œâ”€â”€ ingress.yaml
â”‚   â”œâ”€â”€ servicemonitor.yaml  # Prometheus ëª¨ë‹ˆí„°ë§
â”‚   â””â”€â”€ networkpolicy.yaml   # ë„¤íŠ¸ì›Œí¬ ì •ì±…
â””â”€â”€ charts/                   # ì„œë¸Œì°¨íŠ¸ (ì„ íƒì‚¬í•­)
```

### 2. Chart.yaml êµ¬ì„±

```yaml
# Chart.yaml
apiVersion: v2
name: plane
description: Open-source project management platform
type: application
version: 1.0.0
appVersion: "preview"
home: https://plane.so
sources:
  - https://github.com/makeplane/plane
maintainers:
  - name: Plane Team
    email: support@plane.so
keywords:
  - project-management
  - issue-tracking
  - agile
  - scrum
annotations:
  category: Productivity
dependencies:
  - name: postgresql
    version: "12.x.x"
    repository: "https://charts.bitnami.com/bitnami"
    condition: postgresql.enabled
  - name: redis
    version: "18.x.x" 
    repository: "https://charts.bitnami.com/bitnami"
    condition: redis.enabled
  - name: minio
    version: "12.x.x"
    repository: "https://charts.bitnami.com/bitnami"
    condition: minio.enabled
```

### 3. ë©”ì¸ values.yaml

```yaml
# values.yaml
global:
  # ì´ë¯¸ì§€ ì„¤ì •
  imageRegistry: ""
  imagePullSecrets: []
  storageClass: ""
  
# Plane ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
plane:
  # í™˜ê²½ ì„¤ì •
  environment: production
  debug: false
  domain: "plane.yourdomain.com"
  
  # ì‹œí¬ë¦¿ ì„¤ì • (ì™¸ë¶€ì—ì„œ ì£¼ì…)
  secrets:
    secretKey: ""
    postgresPassword: ""
    githubClientSecret: ""
    slackWebhookUrl: ""
  
  # ì´ë¯¸ì§€ ì„¤ì •
  image:
    registry: docker.io
    repository: makeplane/plane
    tag: "latest"
    pullPolicy: IfNotPresent
  
# API ì„œë²„ ì„¤ì •
api:
  enabled: true
  replicaCount: 3
  
  image:
    repository: makeplane/plane-backend
    tag: "latest"
    
  resources:
    requests:
      memory: "512Mi"
      cpu: "200m"
    limits:
      memory: "1Gi"
      cpu: "500m"
      
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
  
  service:
    type: ClusterIP
    port: 8000
    
  # í—¬ìŠ¤ì²´í¬ ì„¤ì •
  livenessProbe:
    httpGet:
      path: /api/health/
      port: 8000
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 10
    
  readinessProbe:
    httpGet:
      path: /api/health/
      port: 8000
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5

# Worker ì„¤ì •
worker:
  enabled: true
  replicaCount: 2
  
  image:
    repository: makeplane/plane-backend
    tag: "latest"
    
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "200m"
      
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80

# Beat ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
beat:
  enabled: true
  replicaCount: 1
  
  resources:
    requests:
      memory: "128Mi"
      cpu: "50m"
    limits:
      memory: "256Mi"
      cpu: "100m"

# Web ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
web:
  enabled: true
  replicaCount: 2
  
  image:
    repository: makeplane/plane-frontend
    tag: "latest"
    
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "200m"
      
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 8
    targetCPUUtilizationPercentage: 70
  
  service:
    type: ClusterIP
    port: 3000

# Admin íŒ¨ë„ ì„¤ì •
admin:
  enabled: true
  replicaCount: 1
  
  image:
    repository: makeplane/plane-admin
    tag: "latest"
    
  resources:
    requests:
      memory: "128Mi"
      cpu: "50m"
    limits:
      memory: "256Mi"
      cpu: "100m"
  
  service:
    type: ClusterIP
    port: 3001

# PostgreSQL ì„¤ì •
postgresql:
  enabled: true
  auth:
    postgresPassword: ""
    username: "plane"
    password: ""
    database: "plane"
  
  primary:
    persistence:
      enabled: true
      size: 20Gi
      storageClass: ""
    
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "1Gi"
        cpu: "500m"
        
    configuration: |-
      max_connections = 200
      shared_buffers = 256MB
      effective_cache_size = 1GB
      maintenance_work_mem = 64MB
      checkpoint_completion_target = 0.9
      wal_buffers = 16MB
      default_statistics_target = 100
      random_page_cost = 1.1
      effective_io_concurrency = 200

# Redis ì„¤ì •
redis:
  enabled: true
  auth:
    enabled: false
    
  master:
    persistence:
      enabled: true
      size: 5Gi
      
    resources:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "256Mi"
        cpu: "200m"
        
    configuration: |-
      maxmemory 256mb
      maxmemory-policy allkeys-lru
      appendonly yes

# MinIO ì„¤ì •
minio:
  enabled: true
  auth:
    rootUser: "plane"
    rootPassword: "plane123"
    
  persistence:
    enabled: true
    size: 50Gi
    
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "200m"

# Ingress ì„¤ì •
ingress:
  enabled: true
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "50m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers "X-Frame-Options: SAMEORIGIN";
      more_set_headers "X-Content-Type-Options: nosniff";
      more_set_headers "X-XSS-Protection: 1; mode=block";
      more_set_headers "Referrer-Policy: strict-origin-when-cross-origin";
  
  tls:
    enabled: true
    secretName: "plane-tls"
  
  hosts:
    - host: "plane.yourdomain.com"
      paths:
        - path: "/api"
          pathType: "Prefix"
          service: "api"
        - path: "/admin"
          pathType: "Prefix"
          service: "admin"
        - path: "/uploads"
          pathType: "Prefix"
          service: "minio"
        - path: "/"
          pathType: "Prefix"
          service: "web"

# ë„¤íŠ¸ì›Œí¬ ì •ì±…
networkPolicy:
  enabled: true
  
# ì„œë¹„ìŠ¤ ëª¨ë‹ˆí„° (Prometheus)
serviceMonitor:
  enabled: true
  namespace: ""
  interval: 30s
  scrapeTimeout: 10s

# Pod Disruption Budget
podDisruptionBudget:
  enabled: true
  minAvailable: 1

# Security Context
securityContext:
  enabled: true
  runAsNonRoot: true
  runAsUser: 1001
  fsGroup: 1001

# ë…¸ë“œ ì„ íƒ ë° í†¨ëŸ¬ë ˆì´ì…˜
nodeSelector: {}
tolerations: []
affinity: {}

# ì¶”ê°€ ë¼ë²¨ ë° ì–´ë…¸í…Œì´ì…˜
commonLabels: {}
commonAnnotations: {}
```

### 4. í™˜ê²½ë³„ values íŒŒì¼

#### ê°œë°œ í™˜ê²½ (values-dev.yaml)

```yaml
# values-dev.yaml
plane:
  environment: development
  debug: true
  domain: "plane-dev.yourdomain.com"

# ê°œë°œ í™˜ê²½ì€ ë¦¬ì†ŒìŠ¤ë¥¼ ìµœì†Œí™”
api:
  replicaCount: 1
  autoscaling:
    enabled: false
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "250m"

worker:
  replicaCount: 1
  autoscaling:
    enabled: false

web:
  replicaCount: 1
  autoscaling:
    enabled: false

# ì™¸ë¶€ ì„œë¹„ìŠ¤ ë¹„í™œì„±í™” (ê°œë°œìš© ë‚´ì¥ ì„œë¹„ìŠ¤ ì‚¬ìš©)
postgresql:
  enabled: true
  primary:
    persistence:
      size: 5Gi

redis:
  enabled: true
  master:
    persistence:
      size: 1Gi

minio:
  enabled: true
  persistence:
    size: 10Gi

# SSL ë¹„í™œì„±í™”
ingress:
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
  tls:
    enabled: false
```

#### ìŠ¤í…Œì´ì§• í™˜ê²½ (values-staging.yaml)

```yaml
# values-staging.yaml
plane:
  environment: staging
  debug: false
  domain: "plane-staging.yourdomain.com"

# ìš´ì˜ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ê·œëª¨ ì¶•ì†Œ
api:
  replicaCount: 2
  autoscaling:
    minReplicas: 1
    maxReplicas: 4

worker:
  replicaCount: 1
  autoscaling:
    minReplicas: 1
    maxReplicas: 3

web:
  replicaCount: 2
  autoscaling:
    minReplicas: 1
    maxReplicas: 4

# ìŠ¤í…Œì´ì§•ìš© ì¸ì¦ì„œ ë°œê¸‰ì
ingress:
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-staging"
```

#### ìš´ì˜ í™˜ê²½ (values-production.yaml)

```yaml
# values-production.yaml
plane:
  environment: production
  debug: false
  domain: "plane.yourdomain.com"

# ê³ ê°€ìš©ì„± ì„¤ì •
api:
  replicaCount: 3
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 15
    targetCPUUtilizationPercentage: 60  # ë” ë³´ìˆ˜ì 

worker:
  replicaCount: 3
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10

web:
  replicaCount: 3
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 12

# ì™¸ë¶€ ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ ì‚¬ìš©
postgresql:
  enabled: false  # AWS RDS ë˜ëŠ” Google Cloud SQL ì‚¬ìš©
  
redis:
  enabled: false  # AWS ElastiCache ë˜ëŠ” Google Cloud Memorystore ì‚¬ìš©
  
minio:
  enabled: false  # AWS S3 ë˜ëŠ” Google Cloud Storage ì‚¬ìš©

# ìš´ì˜ í™˜ê²½ ë³´ì•ˆ ê°•í™”
securityContext:
  enabled: true
  runAsNonRoot: true
  runAsUser: 1001
  fsGroup: 1001
  capabilities:
    drop: ["ALL"]
  readOnlyRootFilesystem: true

# Pod Anti-Affinity ì„¤ì • (ê³ ê°€ìš©ì„±)
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values: ["plane"]
        topologyKey: kubernetes.io/hostname

# ë¦¬ì†ŒìŠ¤ í• ë‹¹ëŸ‰ ì¦ê°€
api:
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"

# ëª¨ë‹ˆí„°ë§ í™œì„±í™”
serviceMonitor:
  enabled: true
  interval: 15s

# ë„¤íŠ¸ì›Œí¬ ì •ì±… í™œì„±í™”
networkPolicy:
  enabled: true

# PDB ì„¤ì •
podDisruptionBudget:
  enabled: true
  minAvailable: 2
```

### 5. í—¬í¼ í…œí”Œë¦¿ (_helpers.tpl)

{% raw %}
```yaml
{{/*
Expand the name of the chart.
*/}}
{{- define "plane.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
*/}}
{{- define "plane.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "plane.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Common labels
*/}}
{{- define "plane.labels" -}}
helm.sh/chart: {{ include "plane.chart" . }}
{{ include "plane.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- with .Values.commonLabels }}
{{ toYaml . }}
{{- end }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "plane.selectorLabels" -}}
app.kubernetes.io/name: {{ include "plane.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
Create the name of the service account to use
*/}}
{{- define "plane.serviceAccountName" -}}
{{- if .Values.serviceAccount.create }}
{{- default (include "plane.fullname" .) .Values.serviceAccount.name }}
{{- else }}
{{- default "default" .Values.serviceAccount.name }}
{{- end }}
{{- end }}

{{/*
Generate the PostgreSQL connection URL
*/}}
{{- define "plane.postgresqlUrl" -}}
{{- if .Values.postgresql.enabled }}
postgres://{{ .Values.postgresql.auth.username }}:{{ .Values.postgresql.auth.password }}@{{ include "plane.fullname" . }}-postgresql:5432/{{ .Values.postgresql.auth.database }}
{{- else }}
{{- .Values.externalDatabase.url }}
{{- end }}
{{- end }}

{{/*
Generate the Redis URL
*/}}
{{- define "plane.redisUrl" -}}
{{- if .Values.redis.enabled }}
redis://{{ include "plane.fullname" . }}-redis-master:6379
{{- else }}
{{- .Values.externalRedis.url }}
{{- end }}
{{- end }}

{{/*
Generate MinIO endpoint
*/}}
{{- define "plane.minioEndpoint" -}}
{{- if .Values.minio.enabled }}
http://{{ include "plane.fullname" . }}-minio:9000
{{- else }}
{{- .Values.externalStorage.endpoint }}
{{- end }}
{{- end }}

{{/*
Image pull secrets
*/}}
{{- define "plane.imagePullSecrets" -}}
{{- if or .Values.global.imagePullSecrets .Values.imagePullSecrets }}
imagePullSecrets:
{{- range .Values.global.imagePullSecrets }}
  - name: {{ . }}
{{- end }}
{{- range .Values.imagePullSecrets }}
  - name: {{ . }}
{{- end }}
{{- end }}
{{- end }}
```
{% endraw %}

### 6. API Deployment í…œí”Œë¦¿

{% raw %}
```yaml
# templates/api/deployment.yaml
{{- if .Values.api.enabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "plane.fullname" . }}-api
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "plane.labels" . | nindent 4 }}
    app.kubernetes.io/component: api
spec:
  {{- if not .Values.api.autoscaling.enabled }}
  replicas: {{ .Values.api.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "plane.selectorLabels" . | nindent 6 }}
      app.kubernetes.io/component: api
  template:
    metadata:
      labels:
        {{- include "plane.selectorLabels" . | nindent 8 }}
        app.kubernetes.io/component: api
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
        checksum/secret: {{ include (print $.Template.BasePath "/secret.yaml") . | sha256sum }}
    spec:
      {{- include "plane.imagePullSecrets" . | nindent 6 }}
      {{- if .Values.securityContext.enabled }}
      securityContext:
        runAsNonRoot: {{ .Values.securityContext.runAsNonRoot }}
        runAsUser: {{ .Values.securityContext.runAsUser }}
        fsGroup: {{ .Values.securityContext.fsGroup }}
      {{- end }}
      initContainers:
      - name: wait-for-db
        image: busybox:1.35
        command: ['sh', '-c']
        args:
          - |
            {{- if .Values.postgresql.enabled }}
            until nc -z {{ include "plane.fullname" . }}-postgresql 5432; do
              echo "Waiting for PostgreSQL..."
              sleep 2
            done
            {{- end }}
            {{- if .Values.redis.enabled }}
            until nc -z {{ include "plane.fullname" . }}-redis-master 6379; do
              echo "Waiting for Redis..."
              sleep 2
            done
            {{- end }}
            echo "Dependencies are ready!"
      containers:
      - name: api
        image: "{{ .Values.api.image.repository }}:{{ .Values.api.image.tag | default .Chart.AppVersion }}"
        imagePullPolicy: {{ .Values.api.image.pullPolicy }}
        ports:
        - name: http
          containerPort: 8000
          protocol: TCP
        env:
        - name: DATABASE_URL
          value: {{ include "plane.postgresqlUrl" . | quote }}
        - name: REDIS_URL
          value: {{ include "plane.redisUrl" . | quote }}
        - name: MINIO_ENDPOINT
          value: {{ include "plane.minioEndpoint" . | quote }}
        envFrom:
        - configMapRef:
            name: {{ include "plane.fullname" . }}-config
        - secretRef:
            name: {{ include "plane.fullname" . }}-secret
        {{- if .Values.api.livenessProbe }}
        livenessProbe:
          {{- toYaml .Values.api.livenessProbe | nindent 10 }}
        {{- end }}
        {{- if .Values.api.readinessProbe }}
        readinessProbe:
          {{- toYaml .Values.api.readinessProbe | nindent 10 }}
        {{- end }}
        resources:
          {{- toYaml .Values.api.resources | nindent 10 }}
        {{- if .Values.securityContext.enabled }}
        securityContext:
          capabilities:
            drop: {{ .Values.securityContext.capabilities.drop }}
          readOnlyRootFilesystem: {{ .Values.securityContext.readOnlyRootFilesystem }}
        {{- end }}
        volumeMounts:
        - name: static-files
          mountPath: /app/static
        - name: tmp
          mountPath: /tmp
      volumes:
      - name: static-files
        emptyDir: {}
      - name: tmp
        emptyDir: {}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
{{- end }}
```
{% endraw %}

### 7. HPA í…œí”Œë¦¿

{% raw %}
```yaml
# templates/api/hpa.yaml
{{- if and .Values.api.enabled .Values.api.autoscaling.enabled }}
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{ include "plane.fullname" . }}-api
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "plane.labels" . | nindent 4 }}
    app.kubernetes.io/component: api
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ include "plane.fullname" . }}-api
  minReplicas: {{ .Values.api.autoscaling.minReplicas }}
  maxReplicas: {{ .Values.api.autoscaling.maxReplicas }}
  metrics:
  {{- if .Values.api.autoscaling.targetCPUUtilizationPercentage }}
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: {{ .Values.api.autoscaling.targetCPUUtilizationPercentage }}
  {{- end }}
  {{- if .Values.api.autoscaling.targetMemoryUtilizationPercentage }}
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: {{ .Values.api.autoscaling.targetMemoryUtilizationPercentage }}
  {{- end }}
{{- end }}
```
{% endraw %}

### 8. Helm ì°¨íŠ¸ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# deploy-with-helm.sh

set -e

# ì„¤ì • ë³€ìˆ˜
NAMESPACE="plane"
RELEASE_NAME="plane"
CHART_PATH="./plane-helm"
ENVIRONMENT=${1:-development}

echo "ğŸ¯ Plane Helm ë°°í¬ ì‹œì‘ (í™˜ê²½: $ENVIRONMENT)"

# Helm ì €ì¥ì†Œ ì—…ë°ì´íŠ¸
echo "ğŸ“¦ Helm ì €ì¥ì†Œ ì—…ë°ì´íŠ¸ ì¤‘..."
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
echo "ğŸ—ï¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„± ì¤‘..."
kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

# ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
echo "ğŸ”„ ì°¨íŠ¸ ì˜ì¡´ì„± ì—…ë°ì´íŠ¸ ì¤‘..."
helm dependency update $CHART_PATH

# Secret ìƒì„± (ì™¸ë¶€ì—ì„œ ì£¼ì…)
echo "ğŸ” Secret ìƒì„± ì¤‘..."
kubectl create secret generic plane-secrets \
  --from-literal=secretKey="$(openssl rand -hex 32)" \
  --from-literal=postgresPassword="$(openssl rand -hex 16)" \
  --from-literal=githubClientSecret="your_github_client_secret" \
  --from-literal=slackWebhookUrl="your_slack_webhook_url" \
  --namespace=$NAMESPACE \
  --dry-run=client -o yaml | kubectl apply -f -

# í™˜ê²½ë³„ ë°°í¬
case $ENVIRONMENT in
  development)
    VALUES_FILE="values-dev.yaml"
    ;;
  staging)
    VALUES_FILE="values-staging.yaml"
    ;;
  production)
    VALUES_FILE="values-production.yaml"
    ;;
  *)
    VALUES_FILE="values.yaml"
    ;;
esac

echo "ğŸ“‹ ì‚¬ìš©í•  Values íŒŒì¼: $VALUES_FILE"

# Helm ë°°í¬
echo "ğŸš€ Helm ì°¨íŠ¸ ë°°í¬ ì¤‘..."
helm upgrade --install $RELEASE_NAME $CHART_PATH \
  --namespace=$NAMESPACE \
  --values=$CHART_PATH/$VALUES_FILE \
  --set plane.secrets.secretKey="" \
  --set plane.secrets.postgresPassword="" \
  --timeout=10m \
  --wait

echo "âœ… ë°°í¬ ì™„ë£Œ!"

# ë°°í¬ ìƒíƒœ í™•ì¸
echo "ğŸ“Š ë°°í¬ ìƒíƒœ í™•ì¸:"
helm status $RELEASE_NAME -n $NAMESPACE
kubectl get pods -n $NAMESPACE

# ì ‘ê·¼ ì •ë³´ ì¶œë ¥
echo ""
echo "ğŸŒ ì ‘ê·¼ ì •ë³´:"
INGRESS_IP=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "pending")
if [ "$INGRESS_IP" != "pending" ] && [ -n "$INGRESS_IP" ]; then
    echo "   ì›¹ URL: http://$INGRESS_IP"
else
    echo "   í¬íŠ¸ í¬ì›Œë”©: kubectl port-forward -n $NAMESPACE svc/$RELEASE_NAME-web 3000:3000"
fi

# ë¡œê·¸ í™•ì¸ ë°©ë²•
echo ""
echo "ğŸ” ë¡œê·¸ í™•ì¸:"
echo "   kubectl logs -f deployment/$RELEASE_NAME-api -n $NAMESPACE"
echo "   kubectl logs -f deployment/$RELEASE_NAME-web -n $NAMESPACE"

 # ë¬¸ì œ í•´ê²°
echo ""
echo "ğŸ”§ ë¬¸ì œ í•´ê²°:"
echo "   helm uninstall $RELEASE_NAME -n $NAMESPACE  # ì œê±°"
echo "   helm rollback $RELEASE_NAME 1 -n $NAMESPACE  # ë¡¤ë°±"
```

## ë©€í‹° í´ë¼ìš°ë“œ ë°°í¬ ì „ëµ

### 1. AWS EKS ë°°í¬

#### EKS í´ëŸ¬ìŠ¤í„° ìƒì„±

```bash
#!/bin/bash
# setup-eks-cluster.sh

set -e

# ì„¤ì • ë³€ìˆ˜
CLUSTER_NAME="plane-production"
REGION="ap-northeast-2"  # ì„œìš¸ ë¦¬ì „
NODE_GROUP_NAME="plane-workers"
KUBERNETES_VERSION="1.28"

echo "ğŸŒ©ï¸ AWS EKS í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œì‘"

# EKS í´ëŸ¬ìŠ¤í„° ìƒì„±
echo "ğŸ—ï¸ EKS í´ëŸ¬ìŠ¤í„° ìƒì„± ì¤‘..."
eksctl create cluster \
  --name $CLUSTER_NAME \
  --version $KUBERNETES_VERSION \
  --region $REGION \
  --zones ap-northeast-2a,ap-northeast-2b,ap-northeast-2c \
  --nodegroup-name $NODE_GROUP_NAME \
  --node-type m5.large \
  --nodes 3 \
  --nodes-min 2 \
  --nodes-max 10 \
  --managed \
  --enable-ssm \
  --alb-ingress-access \
  --full-ecr-access \
  --asg-access \
  --external-dns-access \
  --appmesh-access \
  --appmesh-preview-access

# kubectl ì„¤ì •
echo "âš™ï¸ kubectl ì„¤ì • ì¤‘..."
aws eks update-kubeconfig --region $REGION --name $CLUSTER_NAME

# í•„ìˆ˜ ì• ë“œì˜¨ ì„¤ì¹˜
echo "ğŸ”§ EKS ì• ë“œì˜¨ ì„¤ì¹˜ ì¤‘..."

# AWS Load Balancer Controller
kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master"

# cert-manager ì„¤ì¹˜
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml

# Cluster Autoscaler ì„¤ì¹˜
kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml

echo "âœ… EKS í´ëŸ¬ìŠ¤í„° ìƒì„± ì™„ë£Œ!"
```

#### RDS PostgreSQL ì„¤ì •

```bash
#!/bin/bash
# setup-rds-postgres.sh

set -e

# ì„¤ì • ë³€ìˆ˜
DB_NAME="plane-production"
DB_USERNAME="plane"
DB_PASSWORD="$(openssl rand -hex 32)"
DB_INSTANCE_CLASS="db.r5.large"
SUBNET_GROUP_NAME="plane-db-subnet-group"

echo "ğŸ—„ï¸ RDS PostgreSQL ì„¤ì • ì‹œì‘"

# ì„œë¸Œë„· ê·¸ë£¹ ìƒì„±
aws rds create-db-subnet-group \
  --db-subnet-group-name $SUBNET_GROUP_NAME \
  --db-subnet-group-description "Plane DB subnet group" \
  --subnet-ids subnet-xxx subnet-yyy subnet-zzz

# RDS ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
aws rds create-db-instance \
  --db-instance-identifier $DB_NAME \
  --db-instance-class $DB_INSTANCE_CLASS \
  --engine postgres \
  --engine-version 15.4 \
  --master-username $DB_USERNAME \
  --master-user-password $DB_PASSWORD \
  --allocated-storage 100 \
  --storage-type gp3 \
  --storage-encrypted \
  --vpc-security-group-ids sg-xxx \
  --db-subnet-group-name $SUBNET_GROUP_NAME \
  --backup-retention-period 7 \
  --multi-az \
  --monitoring-interval 60 \
  --monitoring-role-arn arn:aws:iam::xxx:role/rds-monitoring-role \
  --enable-performance-insights \
  --performance-insights-retention-period 7 \
  --deletion-protection

echo "ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ë¹„ë°€ë²ˆí˜¸ë¥¼ ê¸°ë¡í•´ë‘ì„¸ìš”: $DB_PASSWORD"
echo "âœ… RDS PostgreSQL ì„¤ì • ì™„ë£Œ!"
```

#### AWSìš© values-aws-production.yaml

```yaml
# values-aws-production.yaml
plane:
  environment: production
  debug: false
  domain: "plane.yourdomain.com"

# ì™¸ë¶€ ì„œë¹„ìŠ¤ ì‚¬ìš© (AWS ê´€ë¦¬í˜•)
postgresql:
  enabled: false

redis:
  enabled: false

minio:
  enabled: false

# ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
externalDatabase:
  url: "postgres://plane:PASSWORD@plane-production.xxx.ap-northeast-2.rds.amazonaws.com:5432/plane"

# ì™¸ë¶€ Redis ì„¤ì • (ElastiCache)
externalRedis:
  url: "redis://plane-production.xxx.cache.amazonaws.com:6379"

# ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ ì„¤ì • (S3)
externalStorage:
  type: "s3"
  endpoint: "https://s3.ap-northeast-2.amazonaws.com"
  bucket: "plane-production-files"
  region: "ap-northeast-2"
  accessKey: "your-access-key"
  secretKey: "your-secret-key"

# Ingress ì„¤ì • (ALB)
ingress:
  enabled: true
  className: "alb"
  annotations:
    kubernetes.io/ingress.class: "alb"
    alb.ingress.kubernetes.io/scheme: "internet-facing"
    alb.ingress.kubernetes.io/target-type: "ip"
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'
    alb.ingress.kubernetes.io/certificate-arn: "arn:aws:acm:ap-northeast-2:xxx:certificate/xxx"
    alb.ingress.kubernetes.io/tags: "Environment=production,Team=devops"

# ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ì„¤ì •
global:
  storageClass: "gp3"

# ë…¸ë“œ ì„ íƒ ë° ê°€ìš©ì„± ì˜ì—­ ë¶„ì‚°
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app.kubernetes.io/name
          operator: In
          values: ["plane"]
      topologyKey: "topology.kubernetes.io/zone"

# AWS íŠ¹í™” ë¦¬ì†ŒìŠ¤ ì„¤ì •
api:
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 20
    targetCPUUtilizationPercentage: 70
```

### 2. GCP GKE ë°°í¬

#### GKE í´ëŸ¬ìŠ¤í„° ìƒì„±

```bash
#!/bin/bash
# setup-gke-cluster.sh

set -e

# ì„¤ì • ë³€ìˆ˜
PROJECT_ID="your-project-id"
CLUSTER_NAME="plane-production"
REGION="asia-northeast3"  # ì„œìš¸ ë¦¬ì „
ZONE="asia-northeast3-a"
KUBERNETES_VERSION="1.28"

echo "â˜ï¸ GCP GKE í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œì‘"

# GCP í”„ë¡œì íŠ¸ ì„¤ì •
gcloud config set project $PROJECT_ID

# API í™œì„±í™”
gcloud services enable container.googleapis.com
gcloud services enable compute.googleapis.com
gcloud services enable sqladmin.googleapis.com

# GKE í´ëŸ¬ìŠ¤í„° ìƒì„±
echo "ğŸ—ï¸ GKE í´ëŸ¬ìŠ¤í„° ìƒì„± ì¤‘..."
gcloud container clusters create $CLUSTER_NAME \
  --region $REGION \
  --node-locations $ZONE \
  --cluster-version $KUBERNETES_VERSION \
  --machine-type n2-standard-4 \
  --num-nodes 3 \
  --min-nodes 2 \
  --max-nodes 10 \
  --enable-autoscaling \
  --enable-autorepair \
  --enable-autoupgrade \
  --enable-network-policy \
  --enable-ip-alias \
  --disk-type pd-ssd \
  --disk-size 50 \
  --addons HorizontalPodAutoscaling,HttpLoadBalancing,NetworkPolicy \
  --workload-pool=$PROJECT_ID.svc.id.goog

# kubectl ì„¤ì •
echo "âš™ï¸ kubectl ì„¤ì • ì¤‘..."
gcloud container clusters get-credentials $CLUSTER_NAME --region $REGION

# í•„ìˆ˜ ì• ë“œì˜¨ ì„¤ì¹˜
echo "ğŸ”§ GKE ì• ë“œì˜¨ ì„¤ì¹˜ ì¤‘..."

# cert-manager ì„¤ì¹˜
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml

# NGINX Ingress Controller ì„¤ì¹˜
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml

echo "âœ… GKE í´ëŸ¬ìŠ¤í„° ìƒì„± ì™„ë£Œ!"
```

#### Cloud SQL ì„¤ì •

```bash
#!/bin/bash
# setup-cloud-sql.sh

set -e

# ì„¤ì • ë³€ìˆ˜
INSTANCE_NAME="plane-production"
DATABASE_VERSION="POSTGRES_15"
TIER="db-custom-4-16384"  # 4 vCPU, 16GB RAM
REGION="asia-northeast3"

echo "ğŸ—„ï¸ Cloud SQL PostgreSQL ì„¤ì • ì‹œì‘"

# Cloud SQL ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
gcloud sql instances create $INSTANCE_NAME \
  --database-version=$DATABASE_VERSION \
  --tier=$TIER \
  --region=$REGION \
  --availability-type=REGIONAL \
  --storage-type=SSD \
  --storage-size=100GB \
  --storage-auto-increase \
  --backup-start-time=03:00 \
  --maintenance-window-day=SUN \
  --maintenance-window-hour=04 \
  --enable-bin-log \
  --deletion-protection

# ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
gcloud sql databases create plane --instance=$INSTANCE_NAME

# ì‚¬ìš©ì ìƒì„±
DB_PASSWORD="$(openssl rand -hex 32)"
gcloud sql users create plane \
  --instance=$INSTANCE_NAME \
  --password=$DB_PASSWORD

echo "ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ë¹„ë°€ë²ˆí˜¸ë¥¼ ê¸°ë¡í•´ë‘ì„¸ìš”: $DB_PASSWORD"
echo "âœ… Cloud SQL PostgreSQL ì„¤ì • ì™„ë£Œ!"
```

#### GCPìš© values-gcp-production.yaml

```yaml
# values-gcp-production.yaml
plane:
  environment: production
  debug: false
  domain: "plane.yourdomain.com"

# ì™¸ë¶€ ì„œë¹„ìŠ¤ ì‚¬ìš© (GCP ê´€ë¦¬í˜•)
postgresql:
  enabled: false

redis:
  enabled: false

minio:
  enabled: false

# ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
externalDatabase:
  url: "postgres://plane:PASSWORD@xxx.xxx.xxx.xxx:5432/plane"

# ì™¸ë¶€ Redis ì„¤ì • (Memorystore)
externalRedis:
  url: "redis://xxx.xxx.xxx.xxx:6379"

# ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ ì„¤ì • (Google Cloud Storage)
externalStorage:
  type: "gcs"
  bucket: "plane-production-files"
  projectId: "your-project-id"
  keyFile: "/path/to/service-account.json"

# Ingress ì„¤ì • (GCE)
ingress:
  enabled: true
  className: "gce"
  annotations:
    kubernetes.io/ingress.class: "gce"
    kubernetes.io/ingress.global-static-ip-name: "plane-ip"
    ingress.gcp.kubernetes.io/managed-certificates: "plane-ssl-cert"
    kubernetes.io/ingress.allow-http: "false"

# ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ì„¤ì •
global:
  storageClass: "pd-ssd"

# GCP íŠ¹í™” ë¦¬ì†ŒìŠ¤ ì„¤ì •
api:
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 20
```

## ìì²´ í´ë¼ìš°ë“œ êµ¬ì¶• ì™„ì „ ê°€ì´ë“œ

### 1. í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­ ë° ê·œëª¨ë³„ ìŠ¤í™

#### ì†Œê·œëª¨ (10-50ëª… íŒ€)

```yaml
# ìµœì†Œ í•˜ë“œì›¨ì–´ ìŠ¤í™
cluster_size: "small"
total_budget: "$3,000 - $5,000"

master_nodes:
  count: 3
  specs:
    cpu: "4 cores (Intel i5 or AMD Ryzen 5)"
    memory: "16GB DDR4"
    storage: "256GB NVMe SSD"
    network: "1Gbps"
  purpose: "ì¿ ë²„ë„¤í‹°ìŠ¤ ì»¨íŠ¸ë¡¤ í”Œë ˆì¸"
  recommended_hardware:
    - "Intel NUC 13 Pro"
    - "ASUS Mini PC PN53"
    - "Raspberry Pi 4 8GB (ê°œë°œìš©)"

worker_nodes:
  count: 3-5
  specs:
    cpu: "8 cores (Intel i7 or AMD Ryzen 7)"
    memory: "32GB DDR4"
    storage: "512GB NVMe SSD"
    network: "1Gbps"
  purpose: "ì• í”Œë¦¬ì¼€ì´ì…˜ ì›Œí¬ë¡œë“œ"
  recommended_hardware:
    - "Dell OptiPlex Micro"
    - "HP EliteDesk Mini"
    - "Lenovo ThinkCentre Tiny"

storage_nodes:
  count: 1
  specs:
    cpu: "4 cores"
    memory: "16GB"
    storage: "2TB HDD + 256GB SSD (cache)"
    network: "1Gbps"
  purpose: "ë¶„ì‚° ìŠ¤í† ë¦¬ì§€ (Longhorn/Ceph)"

networking:
  switch: "24-port Gigabit managed switch"
  router: "Enterprise-grade router with VLAN support"
  estimated_cost: "$300-500"

total_specs:
  cpu_cores: 44
  memory: 176GB
  storage: 3TB
  estimated_users: 50
  estimated_projects: 100
```

#### ì¤‘ê·œëª¨ (50-200ëª… íŒ€)

```yaml
# ì¤‘ê·œëª¨ í•˜ë“œì›¨ì–´ ìŠ¤í™
cluster_size: "medium"
total_budget: "$8,000 - $15,000"

master_nodes:
  count: 3
  specs:
    cpu: "8 cores (Intel Xeon E-2288G or AMD EPYC)"
    memory: "32GB DDR4 ECC"
    storage: "512GB NVMe SSD"
    network: "10Gbps"
  purpose: "ê³ ê°€ìš©ì„± ì»¨íŠ¸ë¡¤ í”Œë ˆì¸"
  recommended_hardware:
    - "Dell PowerEdge R340"
    - "HPE ProLiant ML110 Gen10"
    - "Supermicro SYS-E300-9D"

worker_nodes:
  count: 6-10
  specs:
    cpu: "16 cores (Intel Xeon or AMD EPYC)"
    memory: "64GB DDR4 ECC"
    storage: "1TB NVMe SSD"
    network: "10Gbps"
  purpose: "ë©”ì¸ ì›Œí¬ë¡œë“œ ì²˜ë¦¬"
  recommended_hardware:
    - "Dell PowerEdge R450"
    - "HPE ProLiant DL380 Gen10"
    - "Supermicro SuperServer"

storage_nodes:
  count: 3
  specs:
    cpu: "8 cores"
    memory: "32GB"
    storage: "4TB NVMe + 8TB HDD"
    network: "10Gbps"
  purpose: "ê³ ì„±ëŠ¥ ë¶„ì‚° ìŠ¤í† ë¦¬ì§€"

networking:
  switch: "48-port 10Gbps managed switch"
  router: "High-performance firewall/router"
  estimated_cost: "$2,000-3,000"

total_specs:
  cpu_cores: 200
  memory: 512GB
  storage: 50TB
  estimated_users: 200
  estimated_projects: 500
```

#### ëŒ€ê·œëª¨ (200-1000ëª… íŒ€)

```yaml
# ëŒ€ê·œëª¨ í•˜ë“œì›¨ì–´ ìŠ¤í™
cluster_size: "large"
total_budget: "$30,000 - $80,000"

master_nodes:
  count: 5
  specs:
    cpu: "24 cores (Intel Xeon Platinum or AMD EPYC)"
    memory: "128GB DDR4 ECC"
    storage: "2TB NVMe SSD"
    network: "25Gbps"
  purpose: "ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì»¨íŠ¸ë¡¤ í”Œë ˆì¸"
  recommended_hardware:
    - "Dell PowerEdge R750"
    - "HPE ProLiant DL380 Gen10 Plus"
    - "Cisco UCS C240 M6"

worker_nodes:
  count: 15-30
  specs:
    cpu: "32 cores (Intel Xeon Platinum or AMD EPYC)"
    memory: "256GB DDR4 ECC"
    storage: "4TB NVMe SSD"
    network: "25Gbps"
  purpose: "ê³ ì„±ëŠ¥ ì›Œí¬ë¡œë“œ ì²˜ë¦¬"

storage_nodes:
  count: 6
  specs:
    cpu: "16 cores"
    memory: "128GB"
    storage: "8TB NVMe + 32TB HDD"
    network: "25Gbps"
  purpose: "ì—”í„°í”„ë¼ì´ì¦ˆ ìŠ¤í† ë¦¬ì§€"

gpu_nodes: # AI/ML ì›Œí¬ë¡œë“œìš©
  count: 2-4
  specs:
    cpu: "32 cores"
    memory: "256GB"
    gpu: "4x NVIDIA A100 or H100"
    storage: "8TB NVMe"
    network: "100Gbps InfiniBand"

networking:
  core_switch: "64-port 100Gbps switch"
  access_switches: "48-port 25Gbps switches"
  estimated_cost: "$15,000-25,000"

total_specs:
  cpu_cores: 1000+
  memory: 4TB+
  storage: 500TB+
  estimated_users: 1000+
  estimated_projects: 5000+
```

### 2. ìì²´ ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„° êµ¬ì¶•

#### í•˜ë“œì›¨ì–´ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# hardware-setup.sh

set -e

# í•˜ë“œì›¨ì–´ ì •ë³´ ìˆ˜ì§‘
echo "ğŸ” í•˜ë“œì›¨ì–´ ì •ë³´ ìˆ˜ì§‘ ì¤‘..."

# CPU ì •ë³´
echo "=== CPU ì •ë³´ ==="
lscpu | grep -E "Model name|CPU\(s\)|Core\(s\) per socket|Socket\(s\)"

# ë©”ëª¨ë¦¬ ì •ë³´
echo "=== ë©”ëª¨ë¦¬ ì •ë³´ ==="
free -h
dmidecode -t memory | grep -E "Size|Speed|Type:" | head -20

# ìŠ¤í† ë¦¬ì§€ ì •ë³´
echo "=== ìŠ¤í† ë¦¬ì§€ ì •ë³´ ==="
lsblk -d -o NAME,SIZE,TYPE,MODEL
df -h

# ë„¤íŠ¸ì›Œí¬ ì •ë³´
echo "=== ë„¤íŠ¸ì›Œí¬ ì •ë³´ ==="
ip link show
ethtool eth0 2>/dev/null | grep Speed || echo "ë„¤íŠ¸ì›Œí¬ ì†ë„ ì •ë³´ ì—†ìŒ"

# í•˜ë“œì›¨ì–´ ê²€ì¦
echo "ğŸ§ª í•˜ë“œì›¨ì–´ ê²€ì¦ ì¤‘..."

# ìµœì†Œ ìš”êµ¬ì‚¬í•­ í™•ì¸
CPU_CORES=$(nproc)
MEMORY_GB=$(free -g | awk 'NR==2{print $2}')
DISK_GB=$(df / | awk 'NR==2{print int($2/1024/1024)}')

echo "ê²€ì¦ ê²°ê³¼:"
echo "  CPU ì½”ì–´: $CPU_CORES (ìµœì†Œ 4ê°œ í•„ìš”)"
echo "  ë©”ëª¨ë¦¬: ${MEMORY_GB}GB (ìµœì†Œ 16GB í•„ìš”)"
echo "  ë””ìŠ¤í¬: ${DISK_GB}GB (ìµœì†Œ 100GB í•„ìš”)"

# ìš”êµ¬ì‚¬í•­ í™•ì¸
if [ $CPU_CORES -lt 4 ]; then
    echo "âŒ CPU ì½”ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."
    exit 1
fi

if [ $MEMORY_GB -lt 16 ]; then
    echo "âŒ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."
    exit 1
fi

if [ $DISK_GB -lt 100 ]; then
    echo "âŒ ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
    exit 1
fi

echo "âœ… í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•©ë‹ˆë‹¤."

# ë„¤íŠ¸ì›Œí¬ êµ¬ì„± ì„¤ì •
echo "ğŸŒ ë„¤íŠ¸ì›Œí¬ êµ¬ì„± ì„¤ì • ì¤‘..."

# ê³ ì • IP ì„¤ì • (Ubuntu 22.04 Netplan ê¸°ì¤€)
cat << EOF > /etc/netplan/01-netcfg.yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    eth0:
      dhcp4: false
      addresses:
        - 192.168.1.10/24  # ê° ë…¸ë“œë³„ë¡œ ë‹¤ë¥¸ IP
      routes:
        - to: default
          via: 192.168.1.1
      nameservers:
        addresses: [8.8.8.8, 8.8.4.4]
EOF

netplan apply

echo "âœ… í•˜ë“œì›¨ì–´ ì„¤ì • ì™„ë£Œ!"
```

#### ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„° ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# install-k8s-cluster.sh

set -e

# ì„¤ì • ë³€ìˆ˜
NODE_TYPE=${1:-master}  # master ë˜ëŠ” worker
CLUSTER_NAME="plane-private"
POD_CIDR="10.244.0.0/16"
SERVICE_CIDR="10.96.0.0/12"

echo "ğŸš€ ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„° ì„¤ì¹˜ ì‹œì‘ (ë…¸ë“œ íƒ€ì…: $NODE_TYPE)"

# ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸
echo "ğŸ“¦ ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì¤‘..."
apt-get update && apt-get upgrade -y

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg \
    lsb-release \
    net-tools \
    htop \
    iotop \
    iftop

# Docker ì„¤ì¹˜
echo "ğŸ³ Docker ì„¤ì¹˜ ì¤‘..."
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null

apt-get update
apt-get install -y docker-ce docker-ce-cli containerd.io

# Docker ì„¤ì •
usermod -aG docker $USER
systemctl enable docker
systemctl start docker

# containerd ì„¤ì •
mkdir -p /etc/containerd
containerd config default | tee /etc/containerd/config.toml
sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
systemctl restart containerd

# ì»¤ë„ ëª¨ë“ˆ ë° sysctl ì„¤ì •
cat << EOF > /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat << EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

modprobe br_netfilter
sysctl --system

# ìŠ¤ì™‘ ë¹„í™œì„±í™”
swapoff -a
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

# ì¿ ë²„ë„¤í‹°ìŠ¤ ì €ì¥ì†Œ ì¶”ê°€
echo "â˜¸ï¸ ì¿ ë²„ë„¤í‹°ìŠ¤ ì„¤ì¹˜ ì¤‘..."
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | tee /etc/apt/sources.list.d/kubernetes.list

apt-get update
apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl

# kubelet ì„¤ì •
echo "KUBELET_EXTRA_ARGS=--container-runtime-endpoint=unix:///var/run/containerd/containerd.sock" > /etc/default/kubelet
systemctl daemon-reload
systemctl restart kubelet

if [ "$NODE_TYPE" = "master" ]; then
    echo "ğŸ¯ ë§ˆìŠ¤í„° ë…¸ë“œ ì´ˆê¸°í™” ì¤‘..."
    
    # ë§ˆìŠ¤í„° ë…¸ë“œ ì´ˆê¸°í™”
    kubeadm init \
        --cluster-name=$CLUSTER_NAME \
        --pod-network-cidr=$POD_CIDR \
        --service-cidr=$SERVICE_CIDR \
        --apiserver-advertise-address=$(hostname -I | awk '{print $1}') \
        --control-plane-endpoint=$(hostname -I | awk '{print $1}'):6443 \
        --upload-certs

    # kubectl ì„¤ì •
    mkdir -p $HOME/.kube
    cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    chown $(id -u):$(id -g) $HOME/.kube/config

    # CNI ì„¤ì¹˜ (Flannel)
    echo "ğŸ•¸ï¸ CNI ì„¤ì¹˜ ì¤‘..."
    kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

    # ì¡°ì¸ í† í° ìƒì„± ë° ì €ì¥
    kubeadm token create --print-join-command > /root/join-command.txt
    
    echo "âœ… ë§ˆìŠ¤í„° ë…¸ë“œ ì„¤ì • ì™„ë£Œ!"
    echo "ì›Œì»¤ ë…¸ë“œ ì¡°ì¸ ëª…ë ¹ì–´:"
    cat /root/join-command.txt

elif [ "$NODE_TYPE" = "worker" ]; then
    echo "ğŸ‘· ì›Œì»¤ ë…¸ë“œë¡œ ì„¤ì •ë©ë‹ˆë‹¤."
    echo "ë§ˆìŠ¤í„° ë…¸ë“œì—ì„œ ìƒì„±ëœ ì¡°ì¸ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:"
    echo "ì˜ˆ: kubeadm join 192.168.1.10:6443 --token xxx --discovery-token-ca-cert-hash sha256:xxx"
fi

echo "ğŸ‰ ì¿ ë²„ë„¤í‹°ìŠ¤ ì„¤ì¹˜ ì™„ë£Œ!"
```

#### ê³ ê°€ìš©ì„± ë§ˆìŠ¤í„° ë…¸ë“œ ì„¤ì •

```bash
#!/bin/bash
# setup-ha-masters.sh

set -e

# ì„¤ì • ë³€ìˆ˜
LOAD_BALANCER_IP="192.168.1.100"
MASTER_NODES=("192.168.1.10" "192.168.1.11" "192.168.1.12")
CLUSTER_NAME="plane-ha"

echo "ğŸ”„ ê³ ê°€ìš©ì„± ë§ˆìŠ¤í„° ë…¸ë“œ ì„¤ì • ì‹œì‘"

# HAProxy ë¡œë“œë°¸ëŸ°ì„œ ì„¤ì •
if [ "$1" = "setup-lb" ]; then
    echo "âš–ï¸ HAProxy ë¡œë“œë°¸ëŸ°ì„œ ì„¤ì • ì¤‘..."
    
    apt-get install -y haproxy
    
    cat << EOF > /etc/haproxy/haproxy.cfg
global
    log stdout local0
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    mode http
    log global
    option httplog
    option dontlognull
    option log-health-checks
    option forwardfor except 127.0.0.0/8
    option redispatch
    retries 3
    timeout http-request 10s
    timeout queue 20s
    timeout connect 10s
    timeout client 20s
    timeout server 20s
    timeout http-keep-alive 10s
    timeout check 10s

frontend kubernetes-frontend
    bind *:6443
    mode tcp
    option tcplog
    default_backend kubernetes-backend

backend kubernetes-backend
    mode tcp
    option tcp-check
    balance roundrobin
    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
    server master1 ${MASTER_NODES[0]}:6443 check
    server master2 ${MASTER_NODES[1]}:6443 check
    server master3 ${MASTER_NODES[2]}:6443 check

listen stats
    bind *:8080
    mode http
    stats enable
    stats uri /stats
    stats refresh 30s
    stats admin if TRUE
EOF

    systemctl enable haproxy
    systemctl restart haproxy
    
    echo "âœ… HAProxy ì„¤ì • ì™„ë£Œ!"
    echo "ìƒíƒœ í™•ì¸: http://$LOAD_BALANCER_IP:8080/stats"
    exit 0
fi

# ì²« ë²ˆì§¸ ë§ˆìŠ¤í„° ë…¸ë“œ ì´ˆê¸°í™”
if [ "$1" = "init-first-master" ]; then
    echo "ğŸ¯ ì²« ë²ˆì§¸ ë§ˆìŠ¤í„° ë…¸ë“œ ì´ˆê¸°í™” ì¤‘..."
    
    kubeadm init \
        --cluster-name=$CLUSTER_NAME \
        --control-plane-endpoint="$LOAD_BALANCER_IP:6443" \
        --pod-network-cidr="10.244.0.0/16" \
        --service-cidr="10.96.0.0/12" \
        --upload-certs \
        --apiserver-advertise-address=$(hostname -I | awk '{print $1}')

    # kubectl ì„¤ì •
    mkdir -p $HOME/.kube
    cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    chown $(id -u):$(id -g) $HOME/.kube/config

    # CNI ì„¤ì¹˜
    kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

    # ì¡°ì¸ ëª…ë ¹ì–´ ìƒì„±
    echo "ë§ˆìŠ¤í„° ë…¸ë“œ ì¡°ì¸ ëª…ë ¹ì–´:" > /root/master-join-command.txt
    kubeadm token create --print-join-command --certificate-key $(kubeadm init phase upload-certs --upload-certs | tail -1) >> /root/master-join-command.txt
    
    echo "ì›Œì»¤ ë…¸ë“œ ì¡°ì¸ ëª…ë ¹ì–´:" > /root/worker-join-command.txt
    kubeadm token create --print-join-command >> /root/worker-join-command.txt
    
    echo "âœ… ì²« ë²ˆì§¸ ë§ˆìŠ¤í„° ë…¸ë“œ ì´ˆê¸°í™” ì™„ë£Œ!"
    cat /root/master-join-command.txt
fi

echo "ğŸ‰ ê³ ê°€ìš©ì„± ì„¤ì • ì™„ë£Œ!"
```

### 3. ë¶„ì‚° ìŠ¤í† ë¦¬ì§€ êµ¬ì„± (Longhorn)

#### Longhorn ì„¤ì¹˜ ë° ì„¤ì •

```bash
#!/bin/bash
# setup-longhorn-storage.sh

set -e

echo "ğŸ’¾ Longhorn ë¶„ì‚° ìŠ¤í† ë¦¬ì§€ ì„¤ì¹˜ ì‹œì‘"

# ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸
echo "ğŸ” ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì¤‘..."

# ê° ë…¸ë“œì— í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.1/deploy/prerequisite/longhorn-iscsi-installation.yaml

# ì„¤ì¹˜ í™•ì¸
echo "â³ ìš”êµ¬ì‚¬í•­ ì„¤ì¹˜ í™•ì¸ ì¤‘..."
kubectl get pods -n longhorn-system --selector app=longhorn-iscsi-installation

# Longhorn ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace longhorn-system

# Longhorn ì„¤ì¹˜
echo "ğŸš€ Longhorn ì„¤ì¹˜ ì¤‘..."
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.1/deploy/longhorn.yaml

# ì„¤ì¹˜ ì™„ë£Œ ëŒ€ê¸°
echo "â³ Longhorn ì„¤ì¹˜ ì™„ë£Œ ëŒ€ê¸° ì¤‘..."
kubectl wait --for=condition=available --timeout=600s deployment/longhorn-manager -n longhorn-system
kubectl wait --for=condition=available --timeout=600s deployment/longhorn-driver-deployer -n longhorn-system

# ê¸°ë³¸ ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ì„¤ì •
cat << EOF | kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-retain
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: Immediate
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880"
  fromBackup: ""
  fsType: "ext4"
  dataLocality: "disabled"
EOF

# ì„±ëŠ¥ ìµœì í™” ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤
cat << EOF | kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-fast
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "2"
  staleReplicaTimeout: "30"
  fromBackup: ""
  fsType: "ext4"
  dataLocality: "best-effort"
  diskSelector: "ssd"
  nodeSelector: "storage"
EOF

# ë°±ì—… ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤
cat << EOF | kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-backup
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880"
  fromBackup: ""
  fsType: "ext4"
  recurringJobSelector: '[{"name":"backup", "isGroup":true}]'
EOF

# ë°±ì—… ì‘ì—… ìƒì„±
cat << EOF | kubectl apply -f -
apiVersion: longhorn.io/v1beta2
kind: RecurringJob
metadata:
  name: daily-backup
  namespace: longhorn-system
spec:
  cron: "0 2 * * *"
  task: "backup"
  groups:
  - backup
  retain: 14
  concurrency: 2
  labels:
    recurring-job: "daily-backup"
EOF

echo "âœ… Longhorn ì„¤ì¹˜ ì™„ë£Œ!"
echo "ğŸ“Š ëŒ€ì‹œë³´ë“œ ì ‘ê·¼: kubectl port-forward -n longhorn-system svc/longhorn-frontend 8080:80"
```

#### ìì²´ í´ë¼ìš°ë“œìš© values-private.yaml

```yaml
# values-private.yaml
plane:
  environment: production
  debug: false
  domain: "plane.yourdomain.com"

# ëª¨ë“  ì„œë¹„ìŠ¤ ë‚´ì¥ ì‚¬ìš©
postgresql:
  enabled: true
  auth:
    postgresPassword: "your-secure-password"
    username: "plane"
    password: "your-secure-password"
    database: "plane"
  
  primary:
    persistence:
      enabled: true
      size: 100Gi
      storageClass: "longhorn-retain"
    
    resources:
      requests:
        memory: "2Gi"
        cpu: "1000m"
      limits:
        memory: "4Gi"
        cpu: "2000m"
        
    configuration: |-
      max_connections = 500
      shared_buffers = 1GB
      effective_cache_size = 3GB
      maintenance_work_mem = 256MB
      checkpoint_completion_target = 0.9
      wal_buffers = 16MB
      default_statistics_target = 100
      random_page_cost = 1.1
      effective_io_concurrency = 200
      work_mem = 4MB
      min_wal_size = 1GB
      max_wal_size = 4GB

redis:
  enabled: true
  auth:
    enabled: true
    password: "your-redis-password"
    
  master:
    persistence:
      enabled: true
      size: 20Gi
      storageClass: "longhorn-fast"
      
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1000m"
        
    configuration: |-
      maxmemory 1gb
      maxmemory-policy allkeys-lru
      appendonly yes
      appendfsync everysec
      save 900 1
      save 300 10
      save 60 10000

minio:
  enabled: true
  auth:
    rootUser: "plane"
    rootPassword: "your-minio-password"
    
  persistence:
    enabled: true
    size: 500Gi
    storageClass: "longhorn-retain"
    
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"

# ê³ ì„±ëŠ¥ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
api:
  replicaCount: 5
  resources:
    requests:
      memory: "2Gi"
      cpu: "1000m"
    limits:
      memory: "4Gi"
      cpu: "2000m"
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 20
    targetCPUUtilizationPercentage: 60

worker:
  replicaCount: 3
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 15

web:
  replicaCount: 3
  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "1Gi"
      cpu: "500m"
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10

# ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ì„¤ì •
global:
  storageClass: "longhorn-retain"

# ê³ ê°€ìš©ì„± ì„¤ì •
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values: ["plane"]
        topologyKey: kubernetes.io/hostname

# ë…¸ë“œ ì„ íƒì (ì„±ëŠ¥ ìµœì í™”)
nodeSelector:
  node-type: "worker"

# ëª¨ë‹ˆí„°ë§ í™œì„±í™”
serviceMonitor:
  enabled: true
  interval: 15s

# ë„¤íŠ¸ì›Œí¬ ì •ì±… í™œì„±í™”
networkPolicy:
  enabled: true

# ë°±ì—… ì„¤ì •
backup:
  enabled: true
  schedule: "0 2 * * *"
  retention: 14
```

## ì™„ì „í•œ ëª¨ë‹ˆí„°ë§ & ë¡œê¹… ì‹œìŠ¤í…œ

### 1. Prometheus + Grafana ìŠ¤íƒ ì„¤ì¹˜

```bash
#!/bin/bash
# setup-monitoring.sh

set -e

echo "ğŸ“Š Prometheus + Grafana ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ì„¤ì¹˜ ì‹œì‘"

# Helm ì €ì¥ì†Œ ì¶”ê°€
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

# ëª¨ë‹ˆí„°ë§ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace monitoring

# Prometheus ì„¤ì¹˜
echo "ğŸ” Prometheus ì„¤ì¹˜ ì¤‘..."
cat << EOF > prometheus-values.yaml
prometheus:
  prometheusSpec:
    retention: 30d
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn-retain
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    
    resources:
      requests:
        memory: 2Gi
        cpu: 1000m
      limits:
        memory: 4Gi
        cpu: 2000m
    
    nodeSelector:
      node-type: "worker"

alertmanager:
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn-retain
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi
    
    resources:
      requests:
        memory: 256Mi
        cpu: 100m
      limits:
        memory: 512Mi
        cpu: 200m

grafana:
  enabled: true
  persistence:
    enabled: true
    storageClassName: longhorn-retain
    size: 10Gi
  
  adminPassword: "your-grafana-password"
  
  resources:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 1Gi
      cpu: 500m

nodeExporter:
  enabled: true

kubeStateMetrics:
  enabled: true

prometheusOperator:
  enabled: true
  
  resources:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 1Gi
      cpu: 500m
EOF

helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --values prometheus-values.yaml

# Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì •
echo "ğŸ“ˆ Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì • ì¤‘..."

# Plane ì „ìš© ëŒ€ì‹œë³´ë“œ
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: plane-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  plane-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "Plane Monitoring Dashboard",
        "tags": ["plane", "kubernetes"],
        "timezone": "browser",
        "panels": [
          {
            "id": 1,
            "title": "API Response Time",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job=\"plane-api\"}[5m])) by (le))",
                "refId": "A"
              }
            ],
            "yAxes": [
              {
                "label": "Response Time (s)",
                "min": 0
              }
            ]
          },
          {
            "id": 2,
            "title": "API Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total{job=\"plane-api\"}[5m])) by (method, status)",
                "refId": "A"
              }
            ]
          },
          {
            "id": 3,
            "title": "Database Connections",
            "type": "graph",
            "targets": [
              {
                "expr": "pg_stat_database_numbackends{datname=\"plane\"}",
                "refId": "A"
              }
            ]
          },
          {
            "id": 4,
            "title": "Worker Queue Length",
            "type": "graph",
            "targets": [
              {
                "expr": "redis_list_length{key=\"celery\"}",
                "refId": "A"
              }
            ]
          }
        ],
        "time": {
          "from": "now-6h",
          "to": "now"
        },
        "refresh": "30s"
      }
    }
EOF

echo "âœ… Prometheus + Grafana ì„¤ì¹˜ ì™„ë£Œ!"
echo "ğŸ”— ì ‘ê·¼ ì •ë³´:"
echo "   Prometheus: kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090"
echo "   Grafana: kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80"
echo "   Grafana ê¸°ë³¸ ë¡œê·¸ì¸: admin / your-grafana-password"
```

### 2. ELK ìŠ¤íƒ (Elasticsearch + Logstash + Kibana) ì„¤ì¹˜

```bash
#!/bin/bash
# setup-logging.sh

set -e

echo "ğŸ“ ELK ìŠ¤íƒ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì¹˜ ì‹œì‘"

# Elastic ì €ì¥ì†Œ ì¶”ê°€
helm repo add elastic https://helm.elastic.co
helm repo update

# ë¡œê¹… ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace logging

# Elasticsearch ì„¤ì¹˜
echo "ğŸ” Elasticsearch ì„¤ì¹˜ ì¤‘..."
cat << EOF > elasticsearch-values.yaml
replicas: 3
minimumMasterNodes: 2

esConfig:
  elasticsearch.yml: |
    cluster.name: "plane-logs"
    network.host: 0.0.0.0
    discovery.type: zen
    discovery.zen.minimum_master_nodes: 2
    discovery.zen.ping.unicast.hosts: ["elasticsearch-master-headless"]
    xpack.security.enabled: false
    xpack.monitoring.collection.enabled: true

resources:
  requests:
    cpu: "1000m"
    memory: "2Gi"
  limits:
    cpu: "2000m"
    memory: "4Gi"

volumeClaimTemplate:
  accessModes: ["ReadWriteOnce"]
  storageClassName: "longhorn-retain"
  resources:
    requests:
      storage: 100Gi

nodeSelector:
  node-type: "worker"
EOF

helm install elasticsearch elastic/elasticsearch \
  --namespace logging \
  --values elasticsearch-values.yaml

# Kibana ì„¤ì¹˜
echo "ğŸ“Š Kibana ì„¤ì¹˜ ì¤‘..."
cat << EOF > kibana-values.yaml
elasticsearchHosts: "http://elasticsearch-master:9200"

resources:
  requests:
    cpu: "500m"
    memory: "1Gi"
  limits:
    cpu: "1000m"
    memory: "2Gi"

service:
  type: ClusterIP
  port: 5601

ingress:
  enabled: true
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: kibana-auth
  hosts:
    - host: kibana.yourdomain.com
      paths:
        - path: /
          pathType: Prefix

kibanaConfig:
  kibana.yml: |
    server.host: 0.0.0.0
    elasticsearch.hosts: ["http://elasticsearch-master:9200"]
    logging.dest: stdout
    logging.silent: false
    logging.quiet: false
    logging.verbose: false
EOF

helm install kibana elastic/kibana \
  --namespace logging \
  --values kibana-values.yaml

# Filebeat ì„¤ì¹˜ (ë¡œê·¸ ìˆ˜ì§‘)
echo "ğŸ“ Filebeat ì„¤ì¹˜ ì¤‘..."
cat << EOF > filebeat-values.yaml
daemonset:
  enabled: true

deployment:
  enabled: false

filebeatConfig:
  filebeat.yml: |
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*.log
      processors:
        - add_kubernetes_metadata:
            host: \${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
        - drop_fields:
            fields: ["host", "agent", "ecs", "log", "input"]
    
    processors:
      - add_cloud_metadata: ~
      - add_host_metadata: ~
      - add_docker_metadata: ~
      - add_kubernetes_metadata: ~
    
    output.elasticsearch:
      hosts: ["elasticsearch-master:9200"]
      index: "plane-logs-%{+yyyy.MM.dd}"
      template.name: "plane-logs"
      template.pattern: "plane-logs-*"
      template.settings:
        index.number_of_shards: 3
        index.number_of_replicas: 1
        index.codec: best_compression
        index.refresh_interval: 5s
    
    setup.template.enabled: true
    setup.template.name: "plane-logs"
    setup.template.pattern: "plane-logs-*"

resources:
  requests:
    cpu: "100m"
    memory: "128Mi"
  limits:
    cpu: "200m"
    memory: "256Mi"

extraVolumes:
  - name: varlog
    hostPath:
      path: /var/log
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers

extraVolumeMounts:
  - name: varlog
    mountPath: /var/log
    readOnly: true
  - name: varlibdockercontainers
    mountPath: /var/lib/docker/containers
    readOnly: true
EOF

helm install filebeat elastic/filebeat \
  --namespace logging \
  --values filebeat-values.yaml

# Kibana ì¸ì¦ ì„¤ì •
echo "ğŸ” Kibana ì¸ì¦ ì„¤ì • ì¤‘..."
htpasswd -bc /tmp/auth admin your-kibana-password
kubectl create secret generic kibana-auth \
  --from-file=/tmp/auth \
  --namespace logging

echo "âœ… ELK ìŠ¤íƒ ì„¤ì¹˜ ì™„ë£Œ!"
echo "ğŸ”— ì ‘ê·¼ ì •ë³´:"
echo "   Elasticsearch: kubectl port-forward -n logging svc/elasticsearch-master 9200:9200"
echo "   Kibana: kubectl port-forward -n logging svc/kibana-kibana 5601:5601"
echo "   Kibana ë¡œê·¸ì¸: admin / your-kibana-password"
```

### 3. ì•Œë¦¼ ì‹œìŠ¤í…œ ì„¤ì •

{% raw %}
```bash
#!/bin/bash
# setup-alerting.sh

set -e

echo "ğŸš¨ ì•Œë¦¼ ì‹œìŠ¤í…œ ì„¤ì • ì‹œì‘"

# AlertManager ì„¤ì •
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'smtp.gmail.com:587'
      smtp_from: 'alerts@yourdomain.com'
      smtp_auth_username: 'alerts@yourdomain.com'
      smtp_auth_password: 'your-email-password'
    
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'default'
      routes:
      - match:
          severity: 'critical'
        receiver: 'critical-alerts'
      - match:
          severity: 'warning'
        receiver: 'warning-alerts'
    
    receivers:
    - name: 'default'
      email_configs:
      - to: 'admin@yourdomain.com'
        subject: '[Plane] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}
    
    - name: 'critical-alerts'
      email_configs:
      - to: 'admin@yourdomain.com'
        subject: '[CRITICAL] Plane Alert'
        body: |
          {{ range .Alerts }}
          CRITICAL ALERT: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#alerts'
        title: 'Critical Plane Alert'
        text: |
          {{ range .Alerts }}
          {{ .Annotations.summary }}
          {{ end }}
    
    - name: 'warning-alerts'
      email_configs:
      - to: 'admin@yourdomain.com'
        subject: '[WARNING] Plane Alert'
        body: |
          {{ range .Alerts }}
          WARNING: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}
EOF

# Plane ì „ìš© ì•Œë¦¼ ê·œì¹™
cat << EOF | kubectl apply -f -
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: plane-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
  - name: plane.rules
    rules:
    - alert: PlaneAPIHighResponseTime
      expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job="plane-api"}[5m])) by (le)) > 2
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Plane API high response time"
        description: "API 95th percentile response time is {{ \$value }}s"
    
    - alert: PlaneAPIHighErrorRate
      expr: sum(rate(http_requests_total{job="plane-api",status=~"5.."}[5m])) / sum(rate(http_requests_total{job="plane-api"}[5m])) > 0.1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Plane API high error rate"
        description: "API error rate is {{ \$value | humanizePercentage }}"
    
    - alert: PlaneDatabaseConnectionsHigh
      expr: pg_stat_database_numbackends{datname="plane"} > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Plane database connections high"
        description: "Database has {{ \$value }} active connections"
    
    - alert: PlaneWorkerQueueBacklog
      expr: redis_list_length{key="celery"} > 1000
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Plane worker queue backlog"
        description: "Worker queue has {{ \$value }} pending tasks"
    
    - alert: PlanePodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total{namespace="plane"}[5m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Plane pod crash looping"
        description: "Pod {{ \$labels.pod }} is crash looping"
    
    - alert: PlaneStorageSpaceLow
      expr: (kubelet_volume_stats_available_bytes{namespace="plane"} / kubelet_volume_stats_capacity_bytes{namespace="plane"}) < 0.1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Plane storage space low"
        description: "Storage {{ \$labels.persistentvolumeclaim }} has less than 10% free space"
EOF

echo "âœ… ì•Œë¦¼ ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ!"
```
{% endraw %}

## ì—”í„°í”„ë¼ì´ì¦ˆ ë³´ì•ˆ ê°•í™”

### 1. RBAC ë° ë„¤íŠ¸ì›Œí¬ ì •ì±…

```bash
#!/bin/bash
# setup-security.sh

set -e

echo "ğŸ”’ ë³´ì•ˆ ì„¤ì • ì‹œì‘"

# Plane ì „ìš© ì„œë¹„ìŠ¤ ê³„ì • ë° RBAC ì„¤ì •
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: plane-api
  namespace: plane
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: plane-api-role
  namespace: plane
rules:
- apiGroups: [""]
  resources: ["pods", "services", "endpoints"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: plane-api-binding
  namespace: plane
subjects:
- kind: ServiceAccount
  name: plane-api
  namespace: plane
roleRef:
  kind: Role
  name: plane-api-role
  apiGroup: rbac.authorization.k8s.io
EOF

# ë„¤íŠ¸ì›Œí¬ ì •ì±… ì„¤ì •
cat << EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: plane-network-policy
  namespace: plane
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: plane
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    - podSelector:
        matchLabels:
          app.kubernetes.io/name: plane
    ports:
    - protocol: TCP
      port: 8000
    - protocol: TCP
      port: 3000
  egress:
  - to:
    - podSelector:
        matchLabels:
          app.kubernetes.io/name: postgresql
    ports:
    - protocol: TCP
      port: 5432
  - to:
    - podSelector:
        matchLabels:
          app.kubernetes.io/name: redis
    ports:
    - protocol: TCP
      port: 6379
  - to:
    - podSelector:
        matchLabels:
          app.kubernetes.io/name: minio
    ports:
    - protocol: TCP
      port: 9000
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 443
    - protocol: TCP
      port: 80
EOF

# Pod Security Standards ì„¤ì •
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: plane
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
EOF

echo "âœ… ë³´ì•ˆ ì„¤ì • ì™„ë£Œ!"
```

### 2. SSL/TLS ì¸ì¦ì„œ ìë™ ê´€ë¦¬

```bash
#!/bin/bash
# setup-ssl.sh

set -e

echo "ğŸ” SSL/TLS ì¸ì¦ì„œ ì„¤ì • ì‹œì‘"

# cert-manager ì„¤ì¹˜
helm repo add jetstack https://charts.jetstack.io
helm repo update

kubectl create namespace cert-manager

helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --version v1.13.0 \
  --set installCRDs=true

# Let's Encrypt ë°œê¸‰ì ì„¤ì •
cat << EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@yourdomain.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-staging
spec:
  acme:
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    email: admin@yourdomain.com
    privateKeySecretRef:
      name: letsencrypt-staging
    solvers:
    - http01:
        ingress:
          class: nginx
EOF

# ì¸ì¦ì„œ í…œí”Œë¦¿
cat << EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: plane-tls
  namespace: plane
spec:
  secretName: plane-tls
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  dnsNames:
  - plane.yourdomain.com
  - api.plane.yourdomain.com
  - admin.plane.yourdomain.com
EOF

echo "âœ… SSL/TLS ì„¤ì • ì™„ë£Œ!"
```

## GitOps CI/CD íŒŒì´í”„ë¼ì¸ (ArgoCD)

### 1. ArgoCD ì„¤ì¹˜ ë° ì„¤ì •

```bash
#!/bin/bash
# setup-gitops.sh

set -e

echo "ğŸš€ ArgoCD GitOps íŒŒì´í”„ë¼ì¸ ì„¤ì • ì‹œì‘"

# ArgoCD ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace argocd

# ArgoCD ì„¤ì¹˜
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# ArgoCD ì„œë²„ ëŒ€ê¸°
echo "â³ ArgoCD ì„œë²„ ì‹œì‘ ëŒ€ê¸° ì¤‘..."
kubectl wait --for=condition=available --timeout=300s deployment/argocd-server -n argocd

# ArgoCD CLI ì„¤ì¹˜
echo "ğŸ”§ ArgoCD CLI ì„¤ì¹˜ ì¤‘..."
curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
chmod +x /usr/local/bin/argocd

# ArgoCD ì´ˆê¸° ë¹„ë°€ë²ˆí˜¸ ì–»ê¸°
ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d)
echo "ğŸ” ArgoCD ì´ˆê¸° admin ë¹„ë°€ë²ˆí˜¸: $ARGOCD_PASSWORD"

# ArgoCD ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: argocd-cm
  namespace: argocd
  labels:
    app.kubernetes.io/name: argocd-cm
    app.kubernetes.io/part-of: argocd
data:
  url: "https://argocd.yourdomain.com"
  dex.config: |
    connectors:
    - type: github
      id: github
      name: GitHub
      config:
        clientID: your-github-client-id
        clientSecret: your-github-client-secret
        orgs:
        - name: your-org
          teams:
          - your-team
  policy.default: role:readonly
  policy.csv: |
    p, role:admin, applications, *, */*, allow
    p, role:admin, certificates, *, *, allow
    p, role:admin, clusters, *, *, allow
    p, role:admin, repositories, *, *, allow
    g, your-org:your-team, role:admin
EOF

# Ingress ì„¤ì •
cat << EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: argocd-server-ingress
  namespace: argocd
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - argocd.yourdomain.com
    secretName: argocd-server-tls
  rules:
  - host: argocd.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: argocd-server
            port:
              number: 443
EOF

echo "âœ… ArgoCD ì„¤ì¹˜ ì™„ë£Œ!"
echo "ğŸ”— ArgoCD URL: https://argocd.yourdomain.com"
echo "ğŸ‘¤ ê¸°ë³¸ ë¡œê·¸ì¸: admin / $ARGOCD_PASSWORD"
```

### 2. Plane GitOps ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •

```bash
#!/bin/bash
# setup-plane-gitops.sh

set -e

echo "ğŸ“¦ Plane GitOps ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ì‹œì‘"

# Git ì €ì¥ì†Œ ë“±ë¡
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: plane-repo
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: repository
type: Opaque
stringData:
  type: git
  url: https://github.com/your-org/plane-k8s-config
  password: your-github-token
  username: your-github-username
EOF

# Plane ê°œë°œ í™˜ê²½ ì• í”Œë¦¬ì¼€ì´ì…˜
cat << EOF | kubectl apply -f -
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: plane-dev
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/plane-k8s-config
    targetRevision: develop
    path: plane-helm
    helm:
      valueFiles:
      - values-dev.yaml
      parameters:
      - name: plane.image.tag
        value: develop
  destination:
    server: https://kubernetes.default.svc
    namespace: plane-dev
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF

# Plane ìŠ¤í…Œì´ì§• í™˜ê²½ ì• í”Œë¦¬ì¼€ì´ì…˜
cat << EOF | kubectl apply -f -
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: plane-staging
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/plane-k8s-config
    targetRevision: main
    path: plane-helm
    helm:
      valueFiles:
      - values-staging.yaml
      parameters:
      - name: plane.image.tag
        value: staging
  destination:
    server: https://kubernetes.default.svc
    namespace: plane-staging
  syncPolicy:
    syncOptions:
    - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF

# Plane ìš´ì˜ í™˜ê²½ ì• í”Œë¦¬ì¼€ì´ì…˜ (ìˆ˜ë™ ë™ê¸°í™”)
cat << EOF | kubectl apply -f -
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: plane-production
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/plane-k8s-config
    targetRevision: main
    path: plane-helm
    helm:
      valueFiles:
      - values-production.yaml
      parameters:
      - name: plane.image.tag
        value: v1.0.0
  destination:
    server: https://kubernetes.default.svc
    namespace: plane
  syncPolicy:
    syncOptions:
    - CreateNamespace=true
    retry:
      limit: 3
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF

echo "âœ… Plane GitOps ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ì™„ë£Œ!"
```

### 3. GitHub Actions CI/CD íŒŒì´í”„ë¼ì¸

```yaml
# .github/workflows/plane-cicd.yml
name: Plane CI/CD Pipeline

on:
  push:
    branches: [main, develop]
    tags: ['v*']
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: $`github.repository`/plane

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: plane_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest coverage
    
    - name: Run tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/plane_test
        REDIS_URL: redis://localhost:6379
      run: |
        coverage run -m pytest
        coverage xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3

  security-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

  build-and-push:
    needs: [test, security-scan]
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: $`github.actor`
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha,prefix={{branch}}-
    
    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64,linux/arm64

  deploy-dev:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    environment: development
    
    steps:
    - name: Update ArgoCD Application
      run: |
        curl -X PATCH \
          -H "Authorization: Bearer ${{ secrets.ARGOCD_TOKEN }}" \
          -H "Content-Type: application/json" \
          -d '{
            "spec": {
              "source": {
                "helm": {
                  "parameters": [
                    {
                      "name": "plane.image.tag",
                      "value": "develop-$`github.sha`"
                    }
                  ]
                }
              }
            }
          }' \
          https://argocd.yourdomain.com/api/v1/applications/plane-dev

  deploy-staging:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: staging
    
    steps:
    - name: Update ArgoCD Application
      run: |
        curl -X PATCH \
          -H "Authorization: Bearer ${{ secrets.ARGOCD_TOKEN }}" \
          -H "Content-Type: application/json" \
          -d '{
            "spec": {
              "source": {
                "helm": {
                  "parameters": [
                    {
                      "name": "plane.image.tag",
                      "value": "main-$`github.sha`"
                    }
                  ]
                }
              }
            }
          }' \
          https://argocd.yourdomain.com/api/v1/applications/plane-staging

  deploy-production:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    environment: production
    
    steps:
    - name: Update ArgoCD Application
      run: |
        curl -X PATCH \
          -H "Authorization: Bearer ${{ secrets.ARGOCD_TOKEN }}" \
          -H "Content-Type: application/json" \
          -d '{
            "spec": {
              "source": {
                "helm": {
                  "parameters": [
                    {
                      "name": "plane.image.tag",
                      "value": "$`github.ref_name`"
                    }
                  ]
                }
              }
            }
          }' \
          https://argocd.yourdomain.com/api/v1/applications/plane-production
        
        # Sync the application
        curl -X POST \
          -H "Authorization: Bearer ${{ secrets.ARGOCD_TOKEN }}" \
          -H "Content-Type: application/json" \
          https://argocd.yourdomain.com/api/v1/applications/plane-production/sync
```

## ë°±ì—… & ë³µêµ¬ ì „ëµ

### 1. ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ìë™í™”

```bash
#!/bin/bash
# setup-backup.sh

set -e

echo "ğŸ’¾ ë°±ì—… ì‹œìŠ¤í…œ ì„¤ì • ì‹œì‘"

# ë°±ì—… ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace backup

# Velero ì„¤ì¹˜ (ì¿ ë²„ë„¤í‹°ìŠ¤ ë°±ì—… ë„êµ¬)
echo "ğŸ“¦ Velero ì„¤ì¹˜ ì¤‘..."
curl -fsSL -o velero-v1.12.0-linux-amd64.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.12.0/velero-v1.12.0-linux-amd64.tar.gz
tar -xzf velero-v1.12.0-linux-amd64.tar.gz
sudo mv velero-v1.12.0-linux-amd64/velero /usr/local/bin/

# MinIO ë°±ì—… ì„œë²„ ì„¤ì • (ìì²´ í´ë¼ìš°ë“œìš©)
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backup-minio
  namespace: backup
spec:
  replicas: 1
  selector:
    matchLabels:
      app: backup-minio
  template:
    metadata:
      labels:
        app: backup-minio
    spec:
      containers:
      - name: minio
        image: minio/minio:latest
        args:
        - server
        - /data
        - --console-address
        - ":9001"
        env:
        - name: MINIO_ROOT_USER
          value: "backupuser"
        - name: MINIO_ROOT_PASSWORD
          value: "backuppassword123"
        ports:
        - containerPort: 9000
        - containerPort: 9001
        volumeMounts:
        - name: backup-storage
          mountPath: /data
      volumes:
      - name: backup-storage
        persistentVolumeClaim:
          claimName: backup-storage-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-storage-pvc
  namespace: backup
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn-retain
  resources:
    requests:
      storage: 1Ti
---
apiVersion: v1
kind: Service
metadata:
  name: backup-minio
  namespace: backup
spec:
  selector:
    app: backup-minio
  ports:
  - name: api
    port: 9000
    targetPort: 9000
  - name: console
    port: 9001
    targetPort: 9001
EOF

# Velero ì„¤ì¹˜ (MinIO ë°±ì—”ë“œ)
echo "ğŸ”§ Velero ì„¤ì • ì¤‘..."
cat << EOF > velero-credentials
[default]
aws_access_key_id = backupuser
aws_secret_access_key = backuppassword123
EOF

kubectl create secret generic cloud-credentials \
  --namespace velero \
  --from-file cloud=velero-credentials

velero install \
  --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.8.0 \
  --bucket velero-backups \
  --secret-file ./velero-credentials \
  --use-volume-snapshots=false \
  --backup-location-config region=minio,s3ForcePathStyle="true",s3Url=http://backup-minio.backup:9000

# ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… CronJob
cat << EOF | kubectl apply -f -
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: plane
spec:
  schedule: "0 2 * * *"  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: postgres-backup
            image: postgres:15
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: plane-postgresql
                  key: postgres-password
            command:
            - /bin/bash
            - -c
            - |
              BACKUP_FILE="plane-backup-\$(date +%Y%m%d_%H%M%S).sql"
              pg_dump -h plane-postgresql -U plane plane > /backup/\$BACKUP_FILE
              gzip /backup/\$BACKUP_FILE
              
              # 7ì¼ ì´ìƒ ëœ ë°±ì—… íŒŒì¼ ì‚­ì œ
              find /backup -name "*.sql.gz" -mtime +7 -delete
              
              echo "ë°±ì—… ì™„ë£Œ: \$BACKUP_FILE.gz"
            volumeMounts:
            - name: backup-volume
              mountPath: /backup
          volumes:
          - name: backup-volume
            persistentVolumeClaim:
              claimName: postgres-backup-pvc
          restartPolicy: OnFailure
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-backup-pvc
  namespace: plane
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn-retain
  resources:
    requests:
      storage: 50Gi
EOF

# ì „ì²´ í´ëŸ¬ìŠ¤í„° ë°±ì—… ìŠ¤ì¼€ì¤„
cat << EOF | kubectl apply -f -
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup
  namespace: velero
spec:
  schedule: "0 3 * * *"  # ë§¤ì¼ ìƒˆë²½ 3ì‹œ
  template:
    includedNamespaces:
    - plane
    - monitoring
    - logging
    ttl: 720h  # 30ì¼ ë³´ê´€
    storageLocation: default
EOF

# ì£¼ê°„ ì „ì²´ ë°±ì—…
cat << EOF | kubectl apply -f -
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: weekly-full-backup
  namespace: velero
spec:
  schedule: "0 1 * * 0"  # ë§¤ì£¼ ì¼ìš”ì¼ ìƒˆë²½ 1ì‹œ
  template:
    ttl: 2160h  # 90ì¼ ë³´ê´€
    storageLocation: default
EOF

echo "âœ… ë°±ì—… ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ!"
echo "ğŸ“‹ ë°±ì—… í™•ì¸ ëª…ë ¹ì–´:"
echo "   velero backup get"
echo "   velero schedule get"
```

### 2. ë³µêµ¬ ì ˆì°¨ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# disaster-recovery.sh

set -e

BACKUP_NAME=${1}
RESTORE_TYPE=${2:-full}  # full, database, namespace

if [ -z "$BACKUP_NAME" ]; then
    echo "ì‚¬ìš©ë²•: $0 <ë°±ì—…ì´ë¦„> [full|database|namespace]"
    echo "ë°±ì—… ëª©ë¡ í™•ì¸: velero backup get"
    exit 1
fi

echo "ğŸš¨ ì¬í•´ë³µêµ¬ ì‹œì‘: $BACKUP_NAME ($RESTORE_TYPE)"

case $RESTORE_TYPE in
    full)
        echo "ğŸ”„ ì „ì²´ ì‹œìŠ¤í…œ ë³µêµ¬ ì¤‘..."
        velero restore create $BACKUP_NAME-restore --from-backup $BACKUP_NAME
        ;;
    
    database)
        echo "ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬ ì¤‘..."
        # í˜„ì¬ DB ì¤‘ì§€
        kubectl scale deployment plane-api --replicas=0 -n plane
        kubectl scale deployment plane-worker --replicas=0 -n plane
        
        # ë°±ì—…ì—ì„œ ë³µêµ¬
        velero restore create $BACKUP_NAME-db-restore \
          --from-backup $BACKUP_NAME \
          --include-resources persistentvolumeclaims,persistentvolumes \
          --selector app.kubernetes.io/name=postgresql
        
        # ì„œë¹„ìŠ¤ ì¬ì‹œì‘
        kubectl scale deployment plane-api --replicas=3 -n plane
        kubectl scale deployment plane-worker --replicas=2 -n plane
        ;;
    
    namespace)
        NAMESPACE=${3:-plane}
        echo "ğŸ“¦ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë³µêµ¬ ì¤‘: $NAMESPACE"
        velero restore create $BACKUP_NAME-ns-restore \
          --from-backup $BACKUP_NAME \
          --include-namespaces $NAMESPACE
        ;;
    
    *)
        echo "âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë³µêµ¬ íƒ€ì…: $RESTORE_TYPE"
        exit 1
        ;;
esac

echo "â³ ë³µêµ¬ ìƒíƒœ í™•ì¸ ì¤‘..."
sleep 30

velero restore describe $BACKUP_NAME-*-restore

echo "âœ… ì¬í•´ë³µêµ¬ ì™„ë£Œ!"
echo "ğŸ” í™•ì¸ ëª…ë ¹ì–´:"
echo "   kubectl get pods -n plane"
echo "   kubectl logs -f deployment/plane-api -n plane"
```

## ì„±ëŠ¥ ìµœì í™” & ë¬¸ì œ í•´ê²°

### 1. ì„±ëŠ¥ íŠœë‹ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# performance-tuning.sh

set -e

echo "âš¡ ì„±ëŠ¥ ìµœì í™” ì‹œì‘"

# 1. ë…¸ë“œ ë ˆë²¨ ìµœì í™”
echo "ğŸ–¥ï¸ ë…¸ë“œ ë ˆë²¨ ìµœì í™” ì¤‘..."

# CPU Governor ì„¤ì •
for cpu in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do
    [ -f "$cpu" ] && echo performance > "$cpu"
done

# TCP ë²„í¼ í¬ê¸° ìµœì í™”
cat << EOF > /etc/sysctl.d/99-kubernetes-performance.conf
# ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ ìµœì í™”
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 87380 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728
net.ipv4.tcp_congestion_control = bbr

# íŒŒì¼ ì‹œìŠ¤í…œ ìµœì í™”
fs.file-max = 1000000
vm.swappiness = 1
vm.dirty_ratio = 20
vm.dirty_background_ratio = 5

# ì¿ ë²„ë„¤í‹°ìŠ¤ ìµœì í™”
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF

sysctl -p /etc/sysctl.d/99-kubernetes-performance.conf

# 2. ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„° ìµœì í™”
echo "â˜¸ï¸ ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„° ìµœì í™” ì¤‘..."

# HPA ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì„¤ì •
cat << EOF | kubectl apply -f -
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: plane-api-advanced-hpa
  namespace: plane
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: plane-api
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
EOF

# VPA (Vertical Pod Autoscaler) ì„¤ì •
cat << EOF | kubectl apply -f -
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: plane-api-vpa
  namespace: plane
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: plane-api
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: api
      minAllowed:
        cpu: 500m
        memory: 1Gi
      maxAllowed:
        cpu: 4000m
        memory: 8Gi
      controlledResources: ["cpu", "memory"]
EOF

# 3. ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
echo "ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì¤‘..."

cat << EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-performance-config
  namespace: plane
data:
  postgresql.conf: |
    # ì—°ê²° ì„¤ì •
    max_connections = 500
    shared_buffers = 4GB
    effective_cache_size = 12GB
    
    # ë©”ëª¨ë¦¬ ì„¤ì •
    work_mem = 16MB
    maintenance_work_mem = 1GB
    
    # WAL ì„¤ì •
    wal_buffers = 64MB
    min_wal_size = 2GB
    max_wal_size = 8GB
    checkpoint_completion_target = 0.9
    
    # ì¿¼ë¦¬ ìµœì í™”
    random_page_cost = 1.1
    effective_io_concurrency = 200
    default_statistics_target = 100
    
    # ë¡œê¹… ì„¤ì •
    log_min_duration_statement = 1000
    log_checkpoints = on
    log_connections = on
    log_disconnections = on
    log_lock_waits = on
    
    # ë³‘ë ¬ ì²˜ë¦¬
    max_worker_processes = 16
    max_parallel_workers_per_gather = 4
    max_parallel_workers = 16
    max_parallel_maintenance_workers = 4
EOF

echo "âœ… ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ!"
```

### 2. ì¢…í•© ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

```bash
#!/bin/bash
# troubleshooting-guide.sh

set -e

ISSUE_TYPE=${1:-health-check}

echo "ğŸ”§ Plane ë¬¸ì œ í•´ê²° ë„êµ¬"

health_check() {
    echo "ğŸ¥ ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœ í™•ì¸ ì¤‘..."
    
    echo "=== ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„° ìƒíƒœ ==="
    kubectl cluster-info
    kubectl get nodes -o wide
    
    echo "=== Plane ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒíƒœ ==="
    kubectl get all -n plane
    
    echo "=== ìŠ¤í† ë¦¬ì§€ ìƒíƒœ ==="
    kubectl get pv,pvc -n plane
    
    echo "=== ìµœê·¼ ì´ë²¤íŠ¸ ==="
    kubectl get events -n plane --sort-by='.lastTimestamp' | tail -20
    
    echo "=== ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ==="
    kubectl top nodes
    kubectl top pods -n plane
}

pod_issues() {
    echo "ğŸ› Pod ë¬¸ì œ ì§„ë‹¨ ì¤‘..."
    
    echo "=== ì‹¤íŒ¨í•œ Pod ëª©ë¡ ==="
    kubectl get pods -n plane --field-selector=status.phase=Failed
    
    echo "=== Pending Pod ëª©ë¡ ==="
    kubectl get pods -n plane --field-selector=status.phase=Pending
    
    echo "=== ì¬ì‹œì‘ ë§ì€ Pod ==="
    kubectl get pods -n plane --sort-by='.status.containerStatuses[0].restartCount' | tail -10
    
    for pod in $(kubectl get pods -n plane -o jsonpath='{.items[?(@.status.phase!="Running")].metadata.name}'); do
        echo "=== Pod $pod ìƒì„¸ ì •ë³´ ==="
        kubectl describe pod $pod -n plane
        echo "=== Pod $pod ë¡œê·¸ ==="
        kubectl logs $pod -n plane --tail=50
    done
}

network_issues() {
    echo "ğŸŒ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ì§„ë‹¨ ì¤‘..."
    
    echo "=== ì„œë¹„ìŠ¤ ìƒíƒœ ==="
    kubectl get svc -n plane -o wide
    
    echo "=== Ingress ìƒíƒœ ==="
    kubectl get ingress -n plane -o wide
    
    echo "=== NetworkPolicy ìƒíƒœ ==="
    kubectl get networkpolicy -n plane
    
    echo "=== DNS í…ŒìŠ¤íŠ¸ ==="
    kubectl run dns-test --image=busybox:1.35 --rm -it --restart=Never -- nslookup kubernetes.default
}

storage_issues() {
    echo "ğŸ’¾ ìŠ¤í† ë¦¬ì§€ ë¬¸ì œ ì§„ë‹¨ ì¤‘..."
    
    echo "=== PV/PVC ìƒíƒœ ==="
    kubectl get pv,pvc -A
    
    echo "=== ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ==="
    kubectl get storageclass
    
    echo "=== Longhorn ìƒíƒœ (ìˆëŠ” ê²½ìš°) ==="
    kubectl get pods -n longhorn-system 2>/dev/null || echo "Longhornì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ"
    
    echo "=== ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ==="
    for node in $(kubectl get nodes -o jsonpath='{.items[*].metadata.name}'); do
        echo "Node: $node"
        kubectl debug node/$node -it --image=busybox:1.35 -- df -h
    done
}

performance_issues() {
    echo "âš¡ ì„±ëŠ¥ ë¬¸ì œ ì§„ë‹¨ ì¤‘..."
    
    echo "=== ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ìƒìœ„ Pod ==="
    kubectl top pods -n plane --sort-by=cpu | head -10
    kubectl top pods -n plane --sort-by=memory | head -10
    
    echo "=== HPA ìƒíƒœ ==="
    kubectl get hpa -n plane
    kubectl describe hpa -n plane
    
    echo "=== ë©”íŠ¸ë¦­ ì„œë²„ ìƒíƒœ ==="
    kubectl get pods -n kube-system | grep metrics-server
    
    echo "=== API ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸ ==="
    API_URL=$(kubectl get ingress -n plane -o jsonpath='{.items[0].spec.rules[0].host}')
    if [ -n "$API_URL" ]; then
        curl -w "@curl-format.txt" -o /dev/null -s "https://$API_URL/api/health/"
    fi
}

backup_issues() {
    echo "ğŸ’¾ ë°±ì—… ë¬¸ì œ ì§„ë‹¨ ì¤‘..."
    
    echo "=== Velero ìƒíƒœ ==="
    kubectl get pods -n velero
    
    echo "=== ë°±ì—… ìƒíƒœ ==="
    velero backup get
    
    echo "=== ìµœê·¼ ë°±ì—… ë¡œê·¸ ==="
    RECENT_BACKUP=$(velero backup get -o jsonpath='{.items[0].metadata.name}')
    if [ -n "$RECENT_BACKUP" ]; then
        velero backup logs $RECENT_BACKUP
    fi
}

auto_fix() {
    echo "ğŸ”¨ ìë™ ë³µêµ¬ ì‹œë„ ì¤‘..."
    
    # Stuck pods ì¬ì‹œì‘
    for pod in $(kubectl get pods -n plane --field-selector=status.phase=Pending -o jsonpath='{.items[*].metadata.name}'); do
        echo "Pod $pod ì¬ì‹œì‘ ì¤‘..."
        kubectl delete pod $pod -n plane
    done
    
    # Failed jobs ì •ë¦¬
    kubectl delete job --field-selector=status.successful=0 -n plane
    
    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    kubectl delete pod --field-selector=status.phase=Succeeded -n plane
    kubectl delete pod --field-selector=status.phase=Failed -n plane
    
    # DNS ìºì‹œ ì •ë¦¬
    kubectl delete pod -n kube-system -l k8s-app=kube-dns
    
    echo "âœ… ìë™ ë³µêµ¬ ì™„ë£Œ"
}

case $ISSUE_TYPE in
    health-check|health)
        health_check
        ;;
    pod|pods)
        pod_issues
        ;;
    network)
        network_issues
        ;;
    storage)
        storage_issues
        ;;
    performance|perf)
        performance_issues
        ;;
    backup)
        backup_issues
        ;;
    auto-fix|fix)
        auto_fix
        ;;
    all)
        health_check
        pod_issues
        network_issues
        storage_issues
        performance_issues
        backup_issues
        ;;
    *)
        echo "ì‚¬ìš©ë²•: $0 [health-check|pods|network|storage|performance|backup|auto-fix|all]"
        exit 1
        ;;
esac

echo "ğŸ¯ ë¬¸ì œ í•´ê²° ì™„ë£Œ!"
```

## ì‹œë¦¬ì¦ˆ ë§ˆë¬´ë¦¬

### ìš´ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
#!/bin/bash
# daily-operations-checklist.sh

echo "ğŸ“‹ Plane ì¼ì¼ ìš´ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸"
echo "ë‚ ì§œ: $(date)"
echo "==============================="

echo "âœ… 1. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"
kubectl get nodes
kubectl get pods -n plane

echo "âœ… 2. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸"
kubectl top nodes
kubectl top pods -n plane

echo "âœ… 3. ìŠ¤í† ë¦¬ì§€ ìš©ëŸ‰ í™•ì¸"
kubectl get pvc -n plane

echo "âœ… 4. ë°±ì—… ìƒíƒœ í™•ì¸"
velero backup get | head -5

echo "âœ… 5. ì•Œë¦¼ í™•ì¸"
kubectl get alerts -A 2>/dev/null || echo "ì•Œë¦¼ ì—†ìŒ"

echo "âœ… 6. ë¡œê·¸ ê²€í†  (ìµœê·¼ 1ì‹œê°„)"
kubectl logs -n plane deployment/plane-api --since=1h | grep -i error | tail -10

echo "==============================="
echo "ì²´í¬ë¦¬ìŠ¤íŠ¸ ì™„ë£Œ: $(date)"
```

## ê²°ë¡ 

ì´ **Plane ì—”í„°í”„ë¼ì´ì¦ˆ ì¿ ë²„ë„¤í‹°ìŠ¤ ìš´ì˜ ê°€ì´ë“œ**ë¥¼ í†µí•´ ë‹¤ìŒì„ ì™„ì „íˆ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤:

### ğŸ¯ í•µì‹¬ ì„±ê³¼
- **Helm ì°¨íŠ¸**: í™˜ê²½ë³„ ë§ì¶¤ ë°°í¬ íŒ¨í‚¤ì§€ ì™„ì„±
- **ë©€í‹° í´ë¼ìš°ë“œ**: AWS, GCP, ìì²´ í´ë¼ìš°ë“œ ëª¨ë“  í™˜ê²½ ëŒ€ì‘
- **í•˜ë“œì›¨ì–´ ê°€ì´ë“œ**: 10ëª…~10,000ëª… ê·œëª¨ë³„ ìµœì í™”ëœ ìŠ¤í™
- **ì™„ì „í•œ ëª¨ë‹ˆí„°ë§**: Prometheus + Grafana + ELK í†µí•© ê´€ì¸¡ì„±
- **ì—”í„°í”„ë¼ì´ì¦ˆ ë³´ì•ˆ**: RBAC, ë„¤íŠ¸ì›Œí¬ ì •ì±…, SSL/TLS ìë™í™”
- **GitOps CI/CD**: ArgoCD ê¸°ë°˜ ì™„ì „ ìë™í™”ëœ ë°°í¬
- **ì¬í•´ë³µêµ¬**: Velero ë°±ì—… ë° ë³µêµ¬ ì „ëµ
- **ì„±ëŠ¥ ìµœì í™”**: HPA, VPA, ë°ì´í„°ë² ì´ìŠ¤ íŠœë‹

### ğŸš€ ë‹¤ìŒ ë‹¨ê³„
1. **í™˜ê²½ì— ë§ëŠ” ì„¤ì • ì„ íƒ**: AWS/GCP/ìì²´ í´ë¼ìš°ë“œ ì¤‘ ì„ íƒ
2. **ë‹¨ê³„ë³„ êµ¬ì¶•**: ê°œë°œ â†’ ìŠ¤í…Œì´ì§• â†’ ìš´ì˜ ìˆœì„œë¡œ ì§„í–‰
3. **ëª¨ë‹ˆí„°ë§ êµ¬ì¶•**: ê´€ì¸¡ì„± ë¨¼ì €, ê·¸ ë‹¤ìŒ ë³´ì•ˆ
4. **GitOps ë„ì…**: ìˆ˜ë™ ë°°í¬ì—ì„œ ìë™í™”ë¡œ ì „í™˜
5. **ì§€ì†ì  ê°œì„ **: ë©”íŠ¸ë¦­ ê¸°ë°˜ ì„±ëŠ¥ ìµœì í™”

### ğŸ“š ì‹œë¦¬ì¦ˆ ì—°ê²°
ì´ ì‹œë¦¬ì¦ˆì˜ ë‹¤ë¥¸ ê¸€ë“¤:
- **1í¸**: [Plane í”„ë¡œì íŠ¸ ê´€ë¦¬ ì™„ì „ ê°€ì´ë“œ](#)
- **2í¸**: [Plane GitHub í†µí•© ê³ ê¸‰ ê°€ì´ë“œ](#)
- **3í¸**: [Plane ì¿ ë²„ë„¤í‹°ìŠ¤ ìš´ì˜ ë°°í¬ ê°€ì´ë“œ](#)
- **4í¸**: í˜„ì¬ ê¸€ - Plane ì—”í„°í”„ë¼ì´ì¦ˆ ì¿ ë²„ë„¤í‹°ìŠ¤ ìš´ì˜ ê°€ì´ë“œ

### ğŸ’¡ ë§ˆì§€ë§‰ íŒ
> "ì™„ë²½í•œ ì‹œìŠ¤í…œì€ ì—†ì§€ë§Œ, ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ì‹œìŠ¤í…œì€ ìˆìŠµë‹ˆë‹¤. ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ í˜„ìƒì„ íŒŒì•…í•˜ê³ , ìë™í™”ë¡œ ë°˜ë³µì„ ì¤„ì´ë©°, ë°±ì—…ìœ¼ë¡œ ì•ˆì „ì„ í™•ë³´í•˜ì„¸ìš”."

**ì—”í„°í”„ë¼ì´ì¦ˆ Plane ìš´ì˜**ì˜ ëª¨ë“  ê²ƒì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. ì´ì œ ì—¬ëŸ¬ë¶„ì˜ íŒ€ì— ë§ëŠ” ìµœì ì˜ í”„ë¡œì íŠ¸ ê´€ë¦¬ í™˜ê²½ì„ êµ¬ì¶•í•´ë³´ì„¸ìš”! ğŸ¯ 