---
title: "NVIDIA NeMo RL ì™„ì „ ê°€ì´ë“œ: LLM ê°•í™”í•™ìŠµ post-training ë§ˆìŠ¤í„°í•˜ê¸°"
excerpt: "NVIDIA NeMo RLì„ í™œìš©í•œ LLM ê°•í™”í•™ìŠµ íŠœí† ë¦¬ì–¼ - SFT, DPO, RM í›ˆë ¨ë¶€í„° ë©€í‹°ë…¸ë“œ í™•ì¥ê¹Œì§€ ì™„ì „ ì •ë³µ"
seo_title: "NVIDIA NeMo RL ì™„ì „ ê°€ì´ë“œ - LLM ê°•í™”í•™ìŠµ post-training íŠœí† ë¦¬ì–¼ - Thaki Cloud"
seo_description: "NVIDIA NeMo RLë¡œ LLM ê°•í™”í•™ìŠµì„ ë§ˆìŠ¤í„°í•˜ì„¸ìš”. SFT, DPO, RM í›ˆë ¨ ë°©ë²•ê³¼ ë©€í‹°ë…¸ë“œ í™•ì¥ ê°€ì´ë“œê¹Œì§€ ì™„ì „ ì •ë³µ íŠœí† ë¦¬ì–¼"
date: 2025-08-22
last_modified_at: 2025-08-22
categories:
  - tutorials
  - llmops
tags:
  - nvidia
  - nemo-rl
  - reinforcement-learning
  - llm
  - sft
  - dpo
  - reward-model
  - post-training
  - machine-learning
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/tutorials/nvidia-nemo-rl-complete-tutorial-llm-reinforcement-learning/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 25ë¶„

## ì„œë¡ 

Large Language Model(LLM)ì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ë ¤ë©´ ì´ˆê¸° ì‚¬ì „ í›ˆë ¨(pre-training) ì´í›„ì˜ **post-training** ê³¼ì •ì´ í•µì‹¬ì…ë‹ˆë‹¤. NVIDIA NeMo RLì€ ì´ëŸ¬í•œ post-trainingì„ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•œ íˆ´í‚·ìœ¼ë¡œ, **Supervised Fine-Tuning(SFT)**, **Direct Preference Optimization(DPO)**, **Reward Model(RM)** í›ˆë ¨ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì§€ì›í•©ë‹ˆë‹¤.

ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” NVIDIA NeMo RLì„ í™œìš©í•˜ì—¬ ì‹¤ì œ LLM ê°•í™”í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ê³  ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ì™„ì „íˆ ë§ˆìŠ¤í„°í•´ë³´ê² ìŠµë‹ˆë‹¤.

## ğŸ“‹ ì£¼ìš” í•™ìŠµ ë‚´ìš©

- **NeMo RL í•µì‹¬ ê°œë…**: SFT, DPO, RM, GRPO ì´í•´
- **í™˜ê²½ ì„¤ì •**: ë¡œì»¬ macOS ê°œë°œí™˜ê²½ êµ¬ì¶•
- **ë‹¨ì¼ ë…¸ë“œ í›ˆë ¨**: ê¸°ë³¸ ëª¨ë¸ í›ˆë ¨ ì‹¤ìŠµ
- **ë©€í‹° ë…¸ë“œ í™•ì¥**: ëŒ€ê·œëª¨ ë¶„ì‚° í›ˆë ¨ ì„¤ì •
- **ëª¨ë¸ í‰ê°€**: í›ˆë ¨ëœ ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦
- **ì‹¤ì „ í™œìš©**: í”„ë¡œë•ì…˜ í™˜ê²½ ë°°í¬ ì¤€ë¹„

## ğŸ”§ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **GPU**: NVIDIA GPU (CUDA ì§€ì›) - RTX 3080 ì´ìƒ ê¶Œì¥
- **ë©”ëª¨ë¦¬**: ìµœì†Œ 16GB RAM, 32GB ê¶Œì¥
- **ì €ì¥ê³µê°„**: ìµœì†Œ 50GB ì—¬ìœ  ê³µê°„

### ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **ìš´ì˜ì²´ì œ**: macOS 12.0+ ë˜ëŠ” Linux
- **Python**: 3.8-3.11
- **CUDA**: 11.8 ì´ìƒ (GPU ì‚¬ìš© ì‹œ)
- **Docker**: ìµœì‹  ë²„ì „ (ì„ íƒì‚¬í•­)

## NeMo RL í•µì‹¬ ê°œë… ì´í•´

### 1. Supervised Fine-Tuning (SFT)

**SFT**ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ íŠ¹ì • ì‘ì—…ì— ë§ê²Œ ì¡°ì •í•˜ëŠ” ì§€ë„í•™ìŠµ ë°©ì‹ì…ë‹ˆë‹¤.

```python
# SFT ê¸°ë³¸ ì„¤ì • ì˜ˆì‹œ
sft_config = {
    "model_name": "meta-llama/Llama-3.2-1B-Instruct",
    "dataset": "helpsteer_data",
    "batch_size": 32,
    "learning_rate": 5e-5,
    "epochs": 3
}
```

### 2. Direct Preference Optimization (DPO)

**DPO**ëŠ” ì¸ê°„ì˜ ì„ í˜¸ë„ë¥¼ ì§ì ‘ ëª¨ë¸ì— ë°˜ì˜í•˜ëŠ” í˜ì‹ ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.

```python
# DPO ì„¤ì • ì˜ˆì‹œ
dpo_config = {
    "preference_dataset": "helpsteer3",
    "beta": 0.1,  # KL divergence ê°€ì¤‘ì¹˜
    "sft_loss_weight": 0.1,
    "preference_average_log_probs": True
}
```

### 3. Reward Model (RM)

**RM**ì€ ëª¨ë¸ ì¶œë ¥ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë³´ì¡° ëª¨ë¸ì…ë‹ˆë‹¤.

```python
# RM í›ˆë ¨ ì„¤ì •
rm_config = {
    "base_model": "meta-llama/Llama-3.2-1B-Instruct",
    "preference_data": "helpsteer3",
    "ranking_loss": "cross_entropy"
}
```

## í™˜ê²½ ì„¤ì • ë° ì„¤ì¹˜

### 1. ë ˆí¬ì§€í† ë¦¬ í´ë¡  ë° ì„œë¸Œëª¨ë“ˆ ì´ˆê¸°í™”

```bash
# NeMo RL ë ˆí¬ì§€í† ë¦¬ í´ë¡ 
git clone https://github.com/NVIDIA-NeMo/RL.git
cd RL

# ì„œë¸Œëª¨ë“ˆ ì´ˆê¸°í™” (ì¤‘ìš”!)
git submodule update --init --recursive
```

### 2. íŒŒì´ì¬ í™˜ê²½ ì„¤ì •

```bash
# Python ë²„ì „ í™•ì¸
python --version  # 3.8-3.11 í™•ì¸

# UV íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì„¤ì¹˜ (ê¶Œì¥)
curl -LsSf https://astral.sh/uv/install.sh | sh

# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
uv venv nemo-rl-env
source nemo-rl-env/bin/activate
```

### 3. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# ê¸°ë³¸ ì˜ì¡´ì„± ì„¤ì¹˜
uv pip install -e .

# GPU ì§€ì›ì„ ìœ„í•œ ì¶”ê°€ íŒ¨í‚¤ì§€
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# í‰ê°€ ë„êµ¬ ì„¤ì¹˜
uv pip install --extra eval
```

### 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# ~/.zshrc ë˜ëŠ” ~/.bashrcì— ì¶”ê°€
export NEMO_RL_HOME="$HOME/RL"
export CUDA_VISIBLE_DEVICES=0  # ì‚¬ìš©í•  GPU ì§€ì •
export PYTHONPATH="$NEMO_RL_HOME:$PYTHONPATH"

# í™˜ê²½ ë³€ìˆ˜ ì ìš©
source ~/.zshrc
```

## SFT (Supervised Fine-Tuning) ì‹¤ìŠµ

### 1. ê¸°ë³¸ SFT ì‹¤í–‰

ë‹¨ì¼ GPUì—ì„œ Llama 3.2 1B ëª¨ë¸ì„ í›ˆë ¨í•´ë³´ê² ìŠµë‹ˆë‹¤.

```bash
# ê¸°ë³¸ SFT ì‹¤í–‰
uv run python examples/run_sft.py
```

### 2. ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

ë” ë§ì€ GPUë¥¼ í™œìš©í•˜ì—¬ ë” í° ëª¨ë¸ì„ í›ˆë ¨í•´ë³´ê² ìŠµë‹ˆë‹¤.

```bash
# 8B ëª¨ë¸ë¡œ 8GPU í›ˆë ¨
uv run python examples/run_sft.py \
  policy.model_name="meta-llama/Llama-3.1-8B-Instruct" \
  policy.train_global_batch_size=128 \
  sft.val_global_batch_size=128 \
  cluster.gpus_per_node=8 \
  checkpointing.checkpoint_dir="results/sft_llama8b" \
  logger.wandb_enabled=True \
  logger.wandb.name="sft-llama8b-tutorial"
```

### 3. ë°ì´í„°ì…‹ ì»¤ìŠ¤í„°ë§ˆì´ì§•

ìì²´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•©ë‹ˆë‹¤.

```yaml
# custom_sft_config.yaml
data:
  dataset_name: "custom_dataset"
  dataset_path: "/path/to/your/dataset"
  prompt_template: "Human: {input}\n\nAssistant: {output}"
  
sft:
  max_epochs: 5
  save_interval: 500
  val_check_interval: 100
```

```bash
# ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
uv run python examples/run_sft.py --config custom_sft_config.yaml
```

## DPO (Direct Preference Optimization) ì‹¤ìŠµ

### 1. ê¸°ë³¸ DPO ì‹¤í–‰

HelpSteer3 ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ì„ í˜¸ë„ ê¸°ë°˜ í›ˆë ¨ì„ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤.

```bash
# ë‹¨ì¼ GPU DPO ì‹¤í–‰
uv run python examples/run_dpo.py
```

### 2. ê³ ê¸‰ DPO ì„¤ì •

```bash
# ê³ ê¸‰ DPO ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
uv run python examples/run_dpo.py \
  policy.model_name="meta-llama/Llama-3.1-8B-Instruct" \
  policy.train_global_batch_size=256 \
  dpo.sft_loss_weight=0.1 \
  dpo.preference_average_log_probs=True \
  dpo.beta=0.1 \
  cluster.gpus_per_node=8 \
  checkpointing.checkpoint_dir="results/dpo_llama8b" \
  logger.wandb_enabled=True \
  logger.wandb.name="dpo-llama8b-tutorial"
```

### 3. DPO ëª¨ë‹ˆí„°ë§

```bash
# TensorBoardë¡œ í›ˆë ¨ ê³¼ì • ëª¨ë‹ˆí„°ë§
tensorboard --logdir results/dpo_llama8b/tensorboard

# Weights & Biases ëŒ€ì‹œë³´ë“œ í™•ì¸
# https://wandb.ai ì—ì„œ í”„ë¡œì íŠ¸ í™•ì¸
```

## Reward Model (RM) í›ˆë ¨

### 1. ê¸°ë³¸ RM í›ˆë ¨

```bash
# ë‹¨ì¼ GPU RM í›ˆë ¨
uv run python examples/run_rm.py
```

### 2. ë©€í‹° GPU RM í›ˆë ¨

```bash
# 8GPU RM í›ˆë ¨
uv run python examples/run_rm.py \
  cluster.gpus_per_node=8 \
  checkpointing.checkpoint_dir="results/rm_llama1b" \
  logger.wandb_enabled=True \
  logger.wandb.name="rm-llama1b-tutorial"
```

### 3. RM ì„±ëŠ¥ í‰ê°€

```bash
# í›ˆë ¨ëœ RM í‰ê°€
uv run python examples/eval_rm.py \
  --model_path results/rm_llama1b/final \
  --test_dataset helpsteer3_test
```

## ëª¨ë¸ í‰ê°€ ë° ë³€í™˜

### 1. ì²´í¬í¬ì¸íŠ¸ ë³€í™˜

PyTorch DCP í˜•ì‹ì—ì„œ Hugging Face í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

```bash
# DCP â†’ Hugging Face ë³€í™˜
uv run python examples/converters/convert_dcp_to_hf.py \
    --config results/dpo_llama8b/step_170/config.yaml \
    --dcp-ckpt-path results/dpo_llama8b/step_170/policy/weights/ \
    --hf-ckpt-path results/dpo_llama8b/hf
```

### 2. ëª¨ë¸ í‰ê°€ ì‹¤í–‰

```bash
# ë³€í™˜ëœ ëª¨ë¸ í‰ê°€
uv run python examples/run_eval.py \
    generation.model_name=$PWD/results/dpo_llama8b/hf \
    generation.temperature=0.7 \
    generation.top_p=0.9 \
    data.dataset_name=math500 \
    eval.num_tests_per_prompt=16 \
    cluster.gpus_per_node=8
```

### 3. ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸

```bash
# MATH-500 ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
uv run python examples/run_eval.py \
    --config examples/configs/evals/math_eval.yaml \
    generation.model_name=agentica-org/DeepScaleR-1.5B-Preview \
    generation.temperature=0.6 \
    generation.top_p=0.95 \
    generation.vllm_cfg.max_model_len=32768 \
    data.dataset_name=math500 \
    eval.num_tests_per_prompt=16 \
    cluster.gpus_per_node=8
```

## ë©€í‹°ë…¸ë“œ ë¶„ì‚° í›ˆë ¨

### 1. Slurm í´ëŸ¬ìŠ¤í„° ì„¤ì •

```bash
# multi_node_sft.sh
#!/bin/bash
NUM_ACTOR_NODES=2

COMMAND="uv run ./examples/run_sft.py \
  --config examples/configs/sft.yaml \
  cluster.num_nodes=2 \
  cluster.gpus_per_node=8 \
  checkpointing.checkpoint_dir='results/sft_llama8b_2nodes' \
  logger.wandb_enabled=True \
  logger.wandb.name='sft-llama8b-multinode'" \
CONTAINER=YOUR_CONTAINER \
MOUNTS="$PWD:$PWD" \
sbatch \
    --nodes=${NUM_ACTOR_NODES} \
    --account=YOUR_ACCOUNT \
    --job-name=nemo-rl-sft \
    --partition=YOUR_PARTITION \
    --time=4:0:0 \
    --gres=gpu:8 \
    ray.sub
```

### 2. Kubernetes í´ëŸ¬ìŠ¤í„° ì„¤ì •

```yaml
# k8s-nemo-rl-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: nemo-rl-training
spec:
  template:
    spec:
      containers:
      - name: nemo-rl
        image: nvcr.io/nvidia/nemo:24.01.01
        command: ["uv", "run", "python", "examples/run_sft.py"]
        args:
          - "--config"
          - "examples/configs/sft.yaml"
          - "cluster.num_nodes=2"
          - "cluster.gpus_per_node=8"
        resources:
          limits:
            nvidia.com/gpu: 8
        volumeMounts:
        - name: data-volume
          mountPath: /workspace/data
      volumes:
      - name: data-volume
        persistentVolumeClaim:
          claimName: nemo-rl-data
      restartPolicy: Never
```

## ì‹¤ì „ í™œìš© ë° ìµœì í™”

### 1. ì„±ëŠ¥ ìµœì í™” íŒ

```python
# config/optimization.yaml
trainer:
  gradient_clip_val: 1.0
  accumulate_grad_batches: 4
  precision: 16-mixed  # Mixed precision ì‚¬ìš©
  
model:
  micro_batch_size: 8
  global_batch_size: 256
  activation_checkpointing: true
  
optimizer:
  name: adamw
  lr: 3e-4
  weight_decay: 0.01
  betas: [0.9, 0.95]
```

### 2. ë©”ëª¨ë¦¬ ìµœì í™”

```bash
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í›ˆë ¨
uv run python examples/run_sft.py \
  model.activation_checkpointing=true \
  model.sequence_parallel=true \
  model.tensor_model_parallel_size=2 \
  trainer.precision=16-mixed
```

### 3. ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìµœì í™”

```python
# data_config.py
data_config = {
    "num_workers": 8,
    "pin_memory": True,
    "persistent_workers": True,
    "prefetch_factor": 2,
    "drop_last": True
}
```

## ë¬¸ì œ í•´ê²° ë° ë””ë²„ê¹…

### 1. ì¼ë°˜ì ì¸ ì˜¤ë¥˜ í•´ê²°

```bash
# ì„œë¸Œëª¨ë“ˆ ì´ˆê¸°í™” ì˜¤ë¥˜
ModuleNotFoundError: No module named 'megatron'

# í•´ê²°ë°©ë²•
git submodule update --init --recursive
NRL_FORCE_REBUILD_VENVS=true uv run examples/run_sft.py
```

### 2. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

```bash
# ë°°ì¹˜ í¬ê¸° ì¡°ì •
uv run python examples/run_sft.py \
  model.micro_batch_size=4 \
  trainer.accumulate_grad_batches=8
```

### 3. ë””ë²„ê·¸ ëª¨ë“œ ì‹¤í–‰

```bash
# ìƒì„¸ ë¡œê·¸ì™€ í•¨ê»˜ ì‹¤í–‰
NEMO_LOG_LEVEL=DEBUG uv run python examples/run_sft.py \
  trainer.limit_train_batches=10 \
  trainer.limit_val_batches=5 \
  trainer.max_epochs=1
```

## ê³ ê¸‰ í™œìš© ì‚¬ë¡€

### 1. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ êµ¬ì„±

```python
# custom_dataset.py
from torch.utils.data import Dataset
import json

class CustomPreferenceDataset(Dataset):
    def __init__(self, data_path):
        with open(data_path, 'r') as f:
            self.data = json.load(f)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        return {
            'prompt': item['prompt'],
            'chosen': item['chosen'],
            'rejected': item['rejected']
        }
```

### 2. ì»¤ìŠ¤í…€ í‰ê°€ ë©”íŠ¸ë¦­

```python
# custom_metrics.py
def calculate_preference_accuracy(predictions, labels):
    """ì„ í˜¸ë„ ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°"""
    correct = sum(pred == label for pred, label in zip(predictions, labels))
    return correct / len(predictions)

def calculate_reward_correlation(predicted_rewards, human_ratings):
    """ì˜ˆì¸¡ ë³´ìƒê³¼ ì¸ê°„ í‰ê°€ ê°„ ìƒê´€ê´€ê³„"""
    import numpy as np
    return np.corrcoef(predicted_rewards, human_ratings)[0, 1]
```

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```python
# hyperparameter_search.py
import optuna

def objective(trial):
    lr = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    beta = trial.suggest_float('dpo_beta', 0.01, 1.0)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])
    
    # ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
    score = train_and_evaluate(lr=lr, beta=beta, batch_size=batch_size)
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)
```

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ë¹„êµ

### 1. ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ê³¼ ë¹„êµ

```python
# benchmark.py
models = {
    'baseline': 'meta-llama/Llama-3.2-1B-Instruct',
    'sft_trained': 'results/sft_llama1b/hf',
    'dpo_trained': 'results/dpo_llama1b/hf',
    'rm_trained': 'results/rm_llama1b/hf'
}

for name, model_path in models.items():
    score = evaluate_model(model_path)
    print(f"{name}: {score}")
```

### 2. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„

```python
# metrics_analysis.py
import matplotlib.pyplot as plt
import pandas as pd

def plot_training_curves(log_dir):
    """í›ˆë ¨ ê³¡ì„  ì‹œê°í™”"""
    df = pd.read_csv(f"{log_dir}/metrics.csv")
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # Loss curves
    axes[0,0].plot(df['step'], df['train_loss'], label='Train')
    axes[0,0].plot(df['step'], df['val_loss'], label='Validation')
    axes[0,0].set_title('Loss Curves')
    axes[0,0].legend()
    
    # Reward curves (for DPO/RM)
    axes[0,1].plot(df['step'], df['reward_accuracy'])
    axes[0,1].set_title('Reward Accuracy')
    
    # Learning rate
    axes[1,0].plot(df['step'], df['learning_rate'])
    axes[1,0].set_title('Learning Rate Schedule')
    
    # GPU utilization
    axes[1,1].plot(df['step'], df['gpu_utilization'])
    axes[1,1].set_title('GPU Utilization')
    
    plt.tight_layout()
    plt.savefig(f"{log_dir}/training_analysis.png")
```

## í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„

### 1. ëª¨ë¸ ìµœì í™”

```bash
# ì¶”ë¡  ìµœì í™”ëœ ëª¨ë¸ ìƒì„±
uv run python examples/optimize_for_inference.py \
  --model_path results/dpo_llama8b/hf \
  --output_path results/dpo_llama8b/optimized \
  --optimization_level 3
```

### 2. Docker ì»¨í…Œì´ë„ˆí™”

```dockerfile
# Dockerfile
FROM nvcr.io/nvidia/nemo:24.01.01

WORKDIR /workspace
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
RUN pip install -e .

EXPOSE 8000
CMD ["python", "serve_model.py"]
```

### 3. ì„œë¹„ìŠ¤ API êµ¬ì„±

```python
# serve_model.py
from fastapi import FastAPI
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

app = FastAPI()

# ëª¨ë¸ ë¡œë“œ
model_path = "results/dpo_llama8b/hf"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)

@app.post("/generate")
async def generate_text(prompt: str, max_length: int = 512):
    inputs = tokenizer(prompt, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"response": response}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## ê²°ë¡ 

NVIDIA NeMo RLì€ LLMì˜ post-trainingì„ ìœ„í•œ ê°•ë ¥í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ í”Œë«í¼ì…ë‹ˆë‹¤. ë³¸ íŠœí† ë¦¬ì–¼ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤:

### ğŸ¯ ì£¼ìš” ì„±ê³¼
- **SFT, DPO, RM** í›ˆë ¨ ë°©ë²•ë¡  ì´í•´ ë° ì‹¤ìŠµ
- **ë‹¨ì¼/ë©€í‹° ë…¸ë“œ** í™˜ê²½ì—ì„œì˜ í™•ì¥ ê°€ëŠ¥í•œ í›ˆë ¨
- **ëª¨ë¸ í‰ê°€** ë° **ì„±ëŠ¥ ìµœì í™”** ê¸°ë²•
- **í”„ë¡œë•ì…˜ ë°°í¬**ë¥¼ ìœ„í•œ ëª¨ë¸ ìµœì í™”

### ğŸš€ ë‹¤ìŒ ë‹¨ê³„
- **GRPO (Generalized Preference Optimization)** ê³ ê¸‰ ê¸°ë²• íƒêµ¬
- **ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹** êµ¬ì„± ë° ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ ê°œë°œ
- **ëŒ€ê·œëª¨ í´ëŸ¬ìŠ¤í„°** í™˜ê²½ì—ì„œì˜ ë¶„ì‚° í›ˆë ¨ ìµœì í™”
- **A/B í…ŒìŠ¤íŒ…**ì„ í†µí•œ ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦

### ğŸ’¡ ì¶”ê°€ ë¦¬ì†ŒìŠ¤
- [NVIDIA NeMo RL ê³µì‹ ë¬¸ì„œ](https://docs.nvidia.com/nemo/rl/latest/index.html)
- [GitHub ë ˆí¬ì§€í† ë¦¬](https://github.com/NVIDIA-NeMo/RL)
- [ì»¤ë®¤ë‹ˆí‹° í¬ëŸ¼](https://github.com/NVIDIA-NeMo/RL/discussions)

NVIDIA NeMo RLì„ í™œìš©í•˜ì—¬ ì—¬ëŸ¬ë¶„ë§Œì˜ ê³ ì„±ëŠ¥ LLMì„ êµ¬ì¶•í•´ë³´ì„¸ìš”! ğŸš€

---

## ğŸ§ª ì‹¤ìŠµ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ë³¸ íŠœí† ë¦¬ì–¼ê³¼ í•¨ê»˜ ì œê³µë˜ëŠ” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ í™œìš©í•˜ì—¬ NeMo RL í™˜ê²½ì„ ì‰½ê²Œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 1. ê¸°ë³¸ ì„¤ì¹˜ í…ŒìŠ¤íŠ¸

```bash
# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰
curl -O https://raw.githubusercontent.com/thakicloud/tutorials/main/nemo-rl-test/test-nemo-rl-setup.sh
chmod +x test-nemo-rl-setup.sh
./test-nemo-rl-setup.sh
```

### 2. Alias ì„¤ì •

```bash
# í¸ë¦¬í•œ ë‹¨ì¶• ëª…ë ¹ì–´ ì„¤ì •
curl -O https://raw.githubusercontent.com/thakicloud/tutorials/main/nemo-rl-test/setup-nemo-rl-aliases.sh
chmod +x setup-nemo-rl-aliases.sh
./setup-nemo-rl-aliases.sh

# ì„¤ì • ì ìš©
source ~/.zshrc

# ì‚¬ìš©ë²• í™•ì¸
nemo-help
```

### 3. ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼

**macOS Sonoma 14.7 í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ**

```bash
# ì‹œìŠ¤í…œ í™˜ê²½
macOS Sonoma 14.7
Python 3.11.8
UV 0.4.15

# í…ŒìŠ¤íŠ¸ ê²°ê³¼
âœ“ Python ë²„ì „ì´ í˜¸í™˜ë©ë‹ˆë‹¤ (3.8-3.11)
âœ“ Gitì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤
âœ“ UV íŒ¨í‚¤ì§€ ë§¤ë‹ˆì €ê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤
âœ“ ë ˆí¬ì§€í† ë¦¬ í´ë¡  ë° ì„œë¸Œëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ
âœ“ ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™” ì™„ë£Œ
âœ“ ê¸°ë³¸ ì˜ì¡´ì„± ì„¤ì¹˜ ì™„ë£Œ
âœ“ PyTorch CPU ë²„ì „ ì„¤ì¹˜ ì™„ë£Œ
âœ“ NeMo RL ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ
âœ“ ì„¤ì • ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ
âœ“ ëª¨ë“  ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ ë¬¸ë²• ê²€ì‚¬ í†µê³¼
```

### 4. ê°œë°œí™˜ê²½ ë²„ì „ ì •ë³´

```yaml
ê°œë°œí™˜ê²½:
  OS: macOS Sonoma 14.7
  Python: 3.11.8
  UV: 0.4.15
  PyTorch: 2.1.0+cpu
  CUDA: N/A (CPU í…ŒìŠ¤íŠ¸)

í…ŒìŠ¤íŠ¸ í†µê³¼ í•­ëª©:
  - í™˜ê²½ ìš”êµ¬ì‚¬í•­ ê²€ì¦
  - ë ˆí¬ì§€í† ë¦¬ í´ë¡  ë° ì„œë¸Œëª¨ë“ˆ
  - ê°€ìƒí™˜ê²½ ìƒì„±
  - ì˜ì¡´ì„± ì„¤ì¹˜
  - ëª¨ë“ˆ ì„í¬íŠ¸
  - ì„¤ì • íŒŒì¼ í™•ì¸
  - ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ ë¬¸ë²• ê²€ì‚¬
```

### 5. GPU í™˜ê²½ ì¶”ê°€ ì„¤ì •

GPUê°€ ìˆëŠ” í™˜ê²½ì—ì„œëŠ” ë‹¤ìŒ ì¶”ê°€ ì„¤ì •ì„ ì§„í–‰í•˜ì„¸ìš”:

```bash
# CUDA ë²„ì „ í™•ì¸
nvidia-smi

# GPU ì§€ì› PyTorch ì„¤ì¹˜
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# ì‹¤ì œ ëª¨ë¸ í›ˆë ¨ í…ŒìŠ¤íŠ¸
nemo-sft-1b trainer.max_epochs=1 trainer.limit_train_batches=10
```

**ë‹¤ìŒ í¬ìŠ¤íŠ¸ ì˜ˆê³ **: NeMo RLê³¼ Kubernetesë¥¼ í™œìš©í•œ ëŒ€ê·œëª¨ ë¶„ì‚° í›ˆë ¨ ì•„í‚¤í…ì²˜ êµ¬ì¶• ê°€ì´ë“œë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. êµ¬ë…í•˜ê³  ìµœì‹  íŠœí† ë¦¬ì–¼ì„ ë°›ì•„ë³´ì„¸ìš”!
