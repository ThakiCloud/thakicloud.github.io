---
title: "Gemma3n FineVideo íŒŒì¸íŠœë‹ ì™„ì „ ê°€ì´ë“œ: ë©€í‹°ëª¨ë‹¬ ë¹„ë””ì˜¤ ì´í•´ AI êµ¬ì¶•í•˜ê¸°"
excerpt: "Googleì˜ ìµœì‹  ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ Gemma3nì„ FineVideo ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ì—¬ ê³ í’ˆì§ˆ ë¹„ë””ì˜¤ ì´í•´ AIë¥¼ êµ¬ì¶•í•˜ëŠ” ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤."
seo_title: "Gemma3n FineVideo íŒŒì¸íŠœë‹ ê°€ì´ë“œ - ë©€í‹°ëª¨ë‹¬ AI í›ˆë ¨ - Thaki Cloud"
seo_description: "Gemma3nì„ FineVideo ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ë‹¨ê³„ë³„ ê°€ì´ë“œ. Unsloth, LoRA ê¸°ë²•ì„ í™œìš©í•œ íš¨ìœ¨ì ì¸ ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸ í›ˆë ¨ ë°©ë²•ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤."
date: 2025-07-17
last_modified_at: 2025-07-17
categories:
  - tutorials
  - llmops
tags:
  - Gemma3n
  - FineVideo
  - íŒŒì¸íŠœë‹
  - ë©€í‹°ëª¨ë‹¬
  - ë¹„ë””ì˜¤AI
  - LoRA
  - Unsloth
  - Google
  - HuggingFace
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/tutorials/gemma3n-finevideo-finetuning-complete-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 25ë¶„

## ì„œë¡ 

Googleì˜ ìµœì‹  ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì¸ **Gemma3n**ê³¼ HuggingFaceì˜ ê³ í’ˆì§ˆ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ **FineVideo**ë¥¼ ê²°í•©í•˜ì—¬ ê°•ë ¥í•œ ë¹„ë””ì˜¤ ì´í•´ AIë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì´ ê°€ì´ë“œëŠ” ì‹¤ì œ macOS í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ëœ ì½”ë“œì™€ í•¨ê»˜ ë‹¨ê³„ë³„ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

- **Gemma3n**: 1B~27B íŒŒë¼ë¯¸í„°, 128K ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°, ë©€í‹°ëª¨ë‹¬ ì§€ì›
- **FineVideo**: 43,751ê°œ ë¹„ë””ì˜¤, 3,425ì‹œê°„ ì½˜í…ì¸ , ìƒì„¸í•œ ë©”íƒ€ë°ì´í„°
- **íš¨ìœ¨ì  í›ˆë ¨**: LoRA, QLoRA, Unslothë¥¼ í™œìš©í•œ ë©”ëª¨ë¦¬ ìµœì í™”

## ê°œë°œ í™˜ê²½ ì„¤ì •

### 1. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

```bash
# macOS ë²„ì „ í™•ì¸
sw_vers

# Python ë²„ì „ í™•ì¸ (3.9+ ê¶Œì¥)
python --version

# GPU ë©”ëª¨ë¦¬ í™•ì¸ (Apple Siliconì˜ ê²½ìš°)
system_profiler SPDisplaysDataType | grep "Chipset Model"
```

### 2. ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv gemma3n_env
source gemma3n_env/bin/activate

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install --upgrade pip
pip install torch torchvision torchaudio
pip install transformers datasets accelerate
pip install huggingface_hub wandb
pip install unsloth
pip install bitsandbytes
pip install peft
```

### 3. Unsloth ìµœì‹  ë²„ì „ ì„¤ì¹˜

```bash
# Unsloth ìµœì‹  ë²„ì „ (Gemma3n ì§€ì›)
pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo
```

## FineVideo ë°ì´í„°ì…‹ ì¤€ë¹„

### 1. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

```python
from datasets import load_dataset
from huggingface_hub import login
import os

# HuggingFace í† í°ìœ¼ë¡œ ë¡œê·¸ì¸
login(token="your_hf_token_here")

# FineVideo ë°ì´í„°ì…‹ ìŠ¤íŠ¸ë¦¬ë° ë¡œë“œ
dataset = load_dataset(
    "HuggingFaceFV/finevideo", 
    split="train", 
    streaming=True
)

print("ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!")
```

### 2. ë°ì´í„° ì „ì²˜ë¦¬

```python
import json
import cv2
import numpy as np
from PIL import Image
import torch
from transformers import AutoProcessor

def preprocess_video_sample(sample, max_frames=8):
    """
    FineVideo ìƒ˜í”Œì„ Gemma3n ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬
    """
    # ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° íŒŒì‹±
    metadata = json.loads(sample['json'])
    video_data = sample['mp4']
    
    # ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì¶œ
    frames = extract_frames_from_video(video_data, max_frames)
    
    # í…ìŠ¤íŠ¸ ë°ì´í„° êµ¬ì„±
    title = metadata.get('youtube_title', '')
    description = metadata.get('content_metadata', {}).get('description', '')
    scenes = metadata.get('content_metadata', {}).get('scenes', [])
    
    # í›ˆë ¨ìš© ëŒ€í™” í¬ë§· ìƒì„±
    conversation = create_training_conversation(title, description, scenes, frames)
    
    return {
        'frames': frames,
        'conversation': conversation,
        'metadata': metadata
    }

def extract_frames_from_video(video_data, max_frames=8):
    """
    ë¹„ë””ì˜¤ì—ì„œ ê· ë“±í•˜ê²Œ í”„ë ˆì„ ì¶”ì¶œ
    """
    # ì„ì‹œ íŒŒì¼ë¡œ ë¹„ë””ì˜¤ ì €ì¥
    temp_path = "/tmp/temp_video.mp4"
    with open(temp_path, 'wb') as f:
        f.write(video_data)
    
    # OpenCVë¡œ í”„ë ˆì„ ì¶”ì¶œ
    cap = cv2.VideoCapture(temp_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    frame_indices = np.linspace(0, total_frames-1, max_frames, dtype=int)
    frames = []
    
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            # BGRì„ RGBë¡œ ë³€í™˜
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(Image.fromarray(frame_rgb))
    
    cap.release()
    os.remove(temp_path)
    
    return frames

def create_training_conversation(title, description, scenes, frames):
    """
    í›ˆë ¨ìš© ëŒ€í™” ë°ì´í„° ìƒì„±
    """
    # ì¥ë©´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„±
    conversations = []
    
    # ê¸°ë³¸ ë¹„ë””ì˜¤ ì„¤ëª… íƒœìŠ¤í¬
    user_message = f"ì´ ë¹„ë””ì˜¤ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    assistant_message = f"ì œëª©: {title}\n\nì„¤ëª…: {description}"
    
    if scenes:
        scene_descriptions = []
        for scene in scenes[:3]:  # ì²« 3ê°œ ì¥ë©´ë§Œ ì‚¬ìš©
            scene_desc = f"ì¥ë©´ {scene.get('sceneId', '')}: {scene.get('title', '')}"
            if 'activities' in scene:
                activities = [act.get('description', '') for act in scene['activities'][:2]]
                scene_desc += f" - í™œë™: {', '.join(activities)}"
            scene_descriptions.append(scene_desc)
        
        assistant_message += f"\n\nì£¼ìš” ì¥ë©´:\n" + "\n".join(scene_descriptions)
    
    conversation = [
        {"role": "user", "content": user_message},
        {"role": "assistant", "content": assistant_message}
    ]
    
    return conversation
```

### 3. ë°ì´í„°ì…‹ í•„í„°ë§ ë° ìƒ˜í”Œë§

```python
def filter_dataset_for_training(dataset, num_samples=1000):
    """
    í›ˆë ¨ì— ì í•©í•œ ìƒ˜í”Œë§Œ í•„í„°ë§
    """
    filtered_samples = []
    count = 0
    
    for sample in dataset:
        if count >= num_samples:
            break
            
        try:
            # ë©”íƒ€ë°ì´í„° í™•ì¸
            metadata = json.loads(sample['json'])
            
            # í’ˆì§ˆ ê¸°ì¤€ í™•ì¸
            duration = metadata.get('duration_seconds', 0)
            resolution = metadata.get('resolution', '')
            
            # í•„í„° ì¡°ê±´
            if (duration > 30 and duration < 600 and  # 30ì´ˆ~10ë¶„
                'content_metadata' in metadata and
                resolution and 'x' in resolution):
                
                processed_sample = preprocess_video_sample(sample)
                filtered_samples.append(processed_sample)
                count += 1
                
                if count % 100 == 0:
                    print(f"ì²˜ë¦¬ëœ ìƒ˜í”Œ: {count}/{num_samples}")
                    
        except Exception as e:
            print(f"ìƒ˜í”Œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            continue
    
    return filtered_samples

# ë°ì´í„°ì…‹ í•„í„°ë§ ì‹¤í–‰
print("ë°ì´í„°ì…‹ í•„í„°ë§ ì‹œì‘...")
training_samples = filter_dataset_for_training(dataset, num_samples=1000)
print(f"í›ˆë ¨ìš© ìƒ˜í”Œ {len(training_samples)}ê°œ ì¤€ë¹„ ì™„ë£Œ!")
```

## Gemma3n ëª¨ë¸ ì„¤ì •

### 1. ëª¨ë¸ ë¡œë“œ (Unsloth í™œìš©)

```python
from unsloth import FastVisionModel
import torch

# ëª¨ë¸ ì„¤ì •
model_name = "unsloth/Gemma-3-12B-Instruct-bnb-4bit"
max_seq_length = 4096
dtype = None  # None = auto detection
load_in_4bit = True

# Unslothë¡œ ëª¨ë¸ ë¡œë“œ
model, tokenizer = FastVisionModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
    trust_remote_code=True,
)

print("Gemma3n ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {model.get_nb_trainable_parameters():,}")
```

### 2. LoRA ì–´ëŒ‘í„° ì„¤ì •

```python
from peft import LoraConfig, get_peft_model

# LoRA ì„¤ì •
model = FastVisionModel.get_peft_model(
    model,
    r=16,  # LoRA rank
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
        "vision_tower"  # ë¹„ì „ ëª¨ë“ˆë„ í¬í•¨
    ],
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=42,
    use_rslora=False,
    loftq_config=None,
)

# í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¶œë ¥
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
```

### 3. í† í¬ë‚˜ì´ì € ì„¤ì •

```python
# íŠ¹ìˆ˜ í† í° ì¶”ê°€
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# ì±„íŒ… í…œí”Œë¦¿ í™•ì¸
if tokenizer.chat_template is None:
    tokenizer.chat_template = """<start_of_turn>user
{{ messages[0]['content'] }}<end_of_turn>
<start_of_turn>model
{{ messages[1]['content'] }}<end_of_turn>"""

print("í† í¬ë‚˜ì´ì € ì„¤ì • ì™„ë£Œ!")
```

## í›ˆë ¨ ë°ì´í„° ì¤€ë¹„

### 1. ë°ì´í„° í¬ë§·íŒ…

```python
from transformers import AutoProcessor
import torch.nn.functional as F

# í”„ë¡œì„¸ì„œ ë¡œë“œ
processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)

def format_training_data(samples, processor, tokenizer, max_length=4096):
    """
    í›ˆë ¨ ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    """
    formatted_data = []
    
    for sample in samples:
        try:
            frames = sample['frames']
            conversation = sample['conversation']
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if frames:
                # í”„ë ˆì„ì„ í…ì„œë¡œ ë³€í™˜
                pixel_values = processor.image_processor(
                    frames, 
                    return_tensors="pt"
                )['pixel_values']
            else:
                continue
            
            # ëŒ€í™”ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            formatted_text = tokenizer.apply_chat_template(
                conversation,
                add_generation_prompt=False,
                tokenize=False
            )
            
            # í† í¬ë‚˜ì´ì§•
            tokens = tokenizer(
                formatted_text,
                truncation=True,
                max_length=max_length,
                padding="max_length",
                return_tensors="pt"
            )
            
            formatted_data.append({
                'input_ids': tokens['input_ids'].squeeze(),
                'attention_mask': tokens['attention_mask'].squeeze(),
                'pixel_values': pixel_values.squeeze() if pixel_values.dim() > 4 else pixel_values,
                'labels': tokens['input_ids'].squeeze()
            })
            
        except Exception as e:
            print(f"ë°ì´í„° í¬ë§·íŒ… ì˜¤ë¥˜: {e}")
            continue
    
    return formatted_data

# ë°ì´í„° í¬ë§·íŒ… ì‹¤í–‰
print("í›ˆë ¨ ë°ì´í„° í¬ë§·íŒ… ì‹œì‘...")
formatted_data = format_training_data(training_samples, processor, tokenizer)
print(f"í¬ë§·íŒ… ì™„ë£Œ: {len(formatted_data)}ê°œ ìƒ˜í”Œ")
```

### 2. ë°ì´í„°ë¡œë” ìƒì„±

```python
from torch.utils.data import Dataset, DataLoader
import torch

class VideoTextDataset(Dataset):
    def __init__(self, formatted_data):
        self.data = formatted_data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

def collate_fn(batch):
    """
    ë°°ì¹˜ ë°ì´í„° ì½œë ˆì´í„°
    """
    input_ids = torch.stack([item['input_ids'] for item in batch])
    attention_mask = torch.stack([item['attention_mask'] for item in batch])
    labels = torch.stack([item['labels'] for item in batch])
    
    # í”½ì…€ ê°’ ì²˜ë¦¬ (í¬ê¸°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
    pixel_values = [item['pixel_values'] for item in batch]
    if pixel_values[0].dim() == 3:  # (C, H, W)
        pixel_values = torch.stack(pixel_values)
    else:  # (N, C, H, W)
        # í”„ë ˆì„ ìˆ˜ê°€ ë‹¤ë¥¸ ê²½ìš° íŒ¨ë”© ë˜ëŠ” ì˜ë¼ë‚´ê¸°
        max_frames = max(pv.size(0) for pv in pixel_values)
        padded_pixel_values = []
        for pv in pixel_values:
            if pv.size(0) < max_frames:
                # íŒ¨ë”©
                pad_size = max_frames - pv.size(0)
                padding = torch.zeros(pad_size, *pv.shape[1:])
                pv = torch.cat([pv, padding], dim=0)
            elif pv.size(0) > max_frames:
                # ì˜ë¼ë‚´ê¸°
                pv = pv[:max_frames]
            padded_pixel_values.append(pv)
        pixel_values = torch.stack(padded_pixel_values)
    
    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'pixel_values': pixel_values,
        'labels': labels
    }

# ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±
train_dataset = VideoTextDataset(formatted_data)
train_dataloader = DataLoader(
    train_dataset,
    batch_size=1,  # ë©”ëª¨ë¦¬ ì œì•½ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° 1
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=0  # macOSì—ì„œ ë©€í‹°í”„ë¡œì„¸ì‹± ì´ìŠˆ ë°©ì§€
)

print(f"ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ: {len(train_dataloader)} ë°°ì¹˜")
```

## ëª¨ë¸ í›ˆë ¨

### 1. í›ˆë ¨ ì„¤ì •

```python
from transformers import TrainingArguments, Trainer
import wandb

# WandB ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)
wandb.init(
    project="gemma3n-finevideo",
    name="gemma3n-12b-finevideo-lora",
    config={
        "model": "Gemma-3-12B",
        "dataset": "FineVideo",
        "technique": "LoRA",
        "rank": 16,
        "batch_size": 1,
        "learning_rate": 2e-4
    }
)

# í›ˆë ¨ ì¸ì ì„¤ì •
training_args = TrainingArguments(
    output_dir="./gemma3n-finevideo-checkpoints",
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    warmup_steps=100,
    learning_rate=2e-4,
    fp16=False,  # Apple Siliconì—ì„œëŠ” False
    bf16=True,   # bfloat16 ì‚¬ìš©
    logging_steps=10,
    save_steps=500,
    eval_steps=500,
    save_total_limit=2,
    remove_unused_columns=False,
    push_to_hub=False,
    report_to="wandb",
    load_best_model_at_end=True,
    ddp_find_unused_parameters=False,
    group_by_length=True,
    dataloader_pin_memory=False,
)

print("í›ˆë ¨ ì„¤ì • ì™„ë£Œ!")
```

### 2. ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ ì •ì˜

```python
class MultimodalTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        ë©€í‹°ëª¨ë‹¬ ì…ë ¥ì— ëŒ€í•œ ì†ì‹¤ ê³„ì‚°
        """
        labels = inputs.pop("labels")
        
        # ëª¨ë¸ forward
        outputs = model(**inputs)
        logits = outputs.get('logits')
        
        if logits is not None:
            # ì†ì‹¤ ê³„ì‚°
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            
            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1)
            )
        else:
            loss = outputs.loss
        
        return (loss, outputs) if return_outputs else loss
```

### 3. í›ˆë ¨ ì‹¤í–‰

```python
# íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
trainer = MultimodalTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=collate_fn,
    tokenizer=tokenizer,
)

# í›ˆë ¨ ì‹œì‘
print("í›ˆë ¨ ì‹œì‘...")
try:
    trainer.train()
    print("í›ˆë ¨ ì™„ë£Œ!")
except Exception as e:
    print(f"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘ ê°€ëŠ¥
    trainer.train(resume_from_checkpoint=True)
```

### 4. ëª¨ë¸ ì €ì¥

```python
# ìµœì¢… ëª¨ë¸ ì €ì¥
trainer.save_model("./gemma3n-finevideo-final")
tokenizer.save_pretrained("./gemma3n-finevideo-final")

# LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥ (ìš©ëŸ‰ ì ˆì•½)
model.save_pretrained("./gemma3n-finevideo-lora")

print("ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
```

## ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° í‰ê°€

### 1. ì¶”ë¡  í•¨ìˆ˜ ì •ì˜

```python
def generate_video_description(model, processor, tokenizer, video_frames, prompt="ì´ ë¹„ë””ì˜¤ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."):
    """
    ë¹„ë””ì˜¤ í”„ë ˆì„ì— ëŒ€í•œ ì„¤ëª… ìƒì„±
    """
    # ì…ë ¥ ì¤€ë¹„
    pixel_values = processor.image_processor(video_frames, return_tensors="pt")['pixel_values']
    
    # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì…ë ¥ êµ¬ì„±
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
    
    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(text, return_tensors="pt")
    
    # ì¶”ë¡ 
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs['input_ids'],
            pixel_values=pixel_values,
            max_new_tokens=256,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # ê²°ê³¼ ë””ì½”ë”©
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("<start_of_turn>model\n")[-1]

# í…ŒìŠ¤íŠ¸ ì˜ˆì œ
def test_model(test_samples, num_tests=5):
    """
    ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    """
    print("ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    for i, sample in enumerate(test_samples[:num_tests]):
        print(f"\n=== í…ŒìŠ¤íŠ¸ {i+1} ===")
        
        frames = sample['frames'][:4]  # ì²« 4ê°œ í”„ë ˆì„ë§Œ ì‚¬ìš©
        original_conversation = sample['conversation']
        
        # ëª¨ë¸ ì˜ˆì¸¡
        prediction = generate_video_description(model, processor, tokenizer, frames)
        
        print(f"ì›ë³¸ ë‹µë³€: {original_conversation[1]['content'][:200]}...")
        print(f"ëª¨ë¸ ì˜ˆì¸¡: {prediction[:200]}...")
        
        # ê°„ë‹¨í•œ ìœ ì‚¬ë„ í‰ê°€ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í‰ê°€ í•„ìš”)
        original_text = original_conversation[1]['content'].lower()
        pred_text = prediction.lower()
        
        common_words = set(original_text.split()) & set(pred_text.split())
        similarity = len(common_words) / max(len(original_text.split()), len(pred_text.split()))
        print(f"ë‹¨ì–´ ìœ ì‚¬ë„: {similarity:.2f}")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
test_model(training_samples)
```

### 2. í‰ê°€ ë©”íŠ¸ë¦­

```python
from sklearn.metrics import accuracy_score
import numpy as np

def evaluate_model_performance(model, test_data, metrics=['bleu', 'rouge']):
    """
    ëª¨ë¸ ì„±ëŠ¥ì„ ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ìœ¼ë¡œ í‰ê°€
    """
    predictions = []
    references = []
    
    for sample in test_data:
        try:
            frames = sample['frames']
            ground_truth = sample['conversation'][1]['content']
            
            prediction = generate_video_description(model, processor, tokenizer, frames)
            
            predictions.append(prediction)
            references.append(ground_truth)
            
        except Exception as e:
            print(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
    
    # BLEU ìŠ¤ì½”ì–´ ê³„ì‚°
    try:
        from nltk.translate.bleu_score import corpus_bleu
        import nltk
        nltk.download('punkt', quiet=True)
        
        bleu_score = corpus_bleu(
            [[ref.split()] for ref in references],
            [pred.split() for pred in predictions]
        )
        print(f"BLEU Score: {bleu_score:.4f}")
        
    except ImportError:
        print("NLTKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ BLEU ìŠ¤ì½”ì–´ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ê°„ë‹¨í•œ ì •í™•ë„ ë©”íŠ¸ë¦­
    word_overlaps = []
    for pred, ref in zip(predictions, references):
        pred_words = set(pred.lower().split())
        ref_words = set(ref.lower().split())
        overlap = len(pred_words & ref_words) / max(len(pred_words), len(ref_words))
        word_overlaps.append(overlap)
    
    avg_overlap = np.mean(word_overlaps)
    print(f"í‰ê·  ë‹¨ì–´ ì¤‘ë³µë„: {avg_overlap:.4f}")
    
    return {
        'bleu': bleu_score if 'bleu_score' in locals() else None,
        'word_overlap': avg_overlap,
        'predictions': predictions,
        'references': references
    }

# í‰ê°€ ì‹¤í–‰
evaluation_results = evaluate_model_performance(model, training_samples[-100:])
```

## ì„±ëŠ¥ ìµœì í™” ë° íŒ

### 1. ë©”ëª¨ë¦¬ ìµœì í™”

```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
export TOKENIZERS_PARALLELISM=false

# Swap ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
watch -n 2 'vm_stat | grep "Pages free\|Pages active\|Pages inactive\|Pages speculative\|Pages wired down"'
```

### 2. í›ˆë ¨ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸

```python
import psutil
import GPUtil
import time
import matplotlib.pyplot as plt

def monitor_training_resources(log_file="training_monitor.log"):
    """
    í›ˆë ¨ ì¤‘ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
    """
    cpu_usage = []
    memory_usage = []
    timestamps = []
    
    start_time = time.time()
    
    while True:
        try:
            # CPU ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_percent = psutil.virtual_memory().percent
            
            current_time = time.time() - start_time
            
            cpu_usage.append(cpu_percent)
            memory_usage.append(memory_percent)
            timestamps.append(current_time)
            
            # ë¡œê·¸ ê¸°ë¡
            with open(log_file, 'a') as f:
                f.write(f"{current_time:.1f},{cpu_percent:.1f},{memory_percent:.1f}\n")
            
            # ì‹¤ì‹œê°„ ì¶œë ¥
            print(f"Time: {current_time:.1f}s, CPU: {cpu_percent:.1f}%, Memory: {memory_percent:.1f}%")
            
            time.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì²´í¬
            
        except KeyboardInterrupt:
            break
    
    # ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    plt.plot(timestamps, cpu_usage)
    plt.title('CPU Usage')
    plt.xlabel('Time (seconds)')
    plt.ylabel('CPU %')
    
    plt.subplot(1, 2, 2)
    plt.plot(timestamps, memory_usage)
    plt.title('Memory Usage')
    plt.xlabel('Time (seconds)')
    plt.ylabel('Memory %')
    
    plt.tight_layout()
    plt.savefig('training_monitor.png')
    plt.show()
```

### 3. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

```python
import os
import shutil
from datetime import datetime

def manage_checkpoints(checkpoint_dir, max_checkpoints=3):
    """
    ì²´í¬í¬ì¸íŠ¸ ìë™ ê´€ë¦¬
    """
    if not os.path.exists(checkpoint_dir):
        return
    
    # ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]
    
    # ë²ˆí˜¸ë¡œ ì •ë ¬
    checkpoints.sort(key=lambda x: int(x.split('-')[1]))
    
    # ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
    if len(checkpoints) > max_checkpoints:
        for old_checkpoint in checkpoints[:-max_checkpoints]:
            old_path = os.path.join(checkpoint_dir, old_checkpoint)
            print(f"ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ: {old_path}")
            shutil.rmtree(old_path)

def backup_best_model(model_path, backup_dir="./model_backups"):
    """
    ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë°±ì—…
    """
    if not os.path.exists(backup_dir):
        os.makedirs(backup_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = os.path.join(backup_dir, f"gemma3n_finevideo_{timestamp}")
    
    shutil.copytree(model_path, backup_path)
    print(f"ëª¨ë¸ ë°±ì—… ì™„ë£Œ: {backup_path}")

# ì •ê¸°ì ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
manage_checkpoints("./gemma3n-finevideo-checkpoints")
backup_best_model("./gemma3n-finevideo-final")
```

## ë°°í¬ ë° ì‹¤ì œ ì‚¬ìš©

### 1. FastAPI ì„œë¹™ ì„œë²„

```python
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
import uvicorn
import tempfile
import os

app = FastAPI(title="Gemma3n FineVideo API")

# ì „ì—­ ëª¨ë¸ ë¡œë“œ
global_model = None
global_processor = None
global_tokenizer = None

@app.on_event("startup")
async def load_model():
    global global_model, global_processor, global_tokenizer
    
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    global_model, global_tokenizer = FastVisionModel.from_pretrained(
        "./gemma3n-finevideo-final",
        max_seq_length=4096,
        dtype=None,
        load_in_4bit=True,
    )
    global_processor = AutoProcessor.from_pretrained("./gemma3n-finevideo-final")
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

@app.post("/analyze_video")
async def analyze_video(file: UploadFile = File(...), prompt: str = "ì´ ë¹„ë””ì˜¤ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."):
    try:
        # ì„ì‹œ íŒŒì¼ë¡œ ë¹„ë””ì˜¤ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_path = tmp_file.name
        
        # ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ
        frames = extract_frames_from_video(open(tmp_path, 'rb').read(), max_frames=8)
        
        # ë¶„ì„ ì‹¤í–‰
        description = generate_video_description(
            global_model, 
            global_processor, 
            global_tokenizer, 
            frames, 
            prompt
        )
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(tmp_path)
        
        return JSONResponse({
            "status": "success",
            "description": description,
            "frame_count": len(frames)
        })
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model_loaded": global_model is not None}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2. í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ì˜ˆì œ

```python
import requests
import json

def test_api_client():
    """
    API ì„œë²„ í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸
    """
    base_url = "http://localhost:8000"
    
    # í—¬ìŠ¤ ì²´í¬
    health_response = requests.get(f"{base_url}/health")
    print(f"ì„œë²„ ìƒíƒœ: {health_response.json()}")
    
    # ë¹„ë””ì˜¤ ë¶„ì„ í…ŒìŠ¤íŠ¸
    video_path = "test_video.mp4"  # í…ŒìŠ¤íŠ¸ ë¹„ë””ì˜¤ ê²½ë¡œ
    
    with open(video_path, "rb") as video_file:
        files = {"file": ("test_video.mp4", video_file, "video/mp4")}
        data = {"prompt": "ì´ ë¹„ë””ì˜¤ì˜ ì£¼ìš” ë‚´ìš©ê³¼ ë“±ì¥ì¸ë¬¼ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."}
        
        response = requests.post(f"{base_url}/analyze_video", files=files, data=data)
        
        if response.status_code == 200:
            result = response.json()
            print(f"ë¶„ì„ ê²°ê³¼: {result['description']}")
        else:
            print(f"ì˜¤ë¥˜: {response.status_code} - {response.text}")

# API í…ŒìŠ¤íŠ¸ ì‹¤í–‰
test_api_client()
```

## ì¶”ê°€ ìµœì í™” ê¸°ë²•

### 1. ì–‘ìí™” (Quantization)

```python
from transformers import BitsAndBytesConfig

# 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

# ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
quantized_model, _ = FastVisionModel.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    max_seq_length=4096,
    dtype=None,
    load_in_4bit=True,
)

print("ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
```

### 2. ë¶„ì‚° í›ˆë ¨ (Multi-GPU)

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup_distributed_training():
    """
    ë¶„ì‚° í›ˆë ¨ í™˜ê²½ ì„¤ì • (ì—¬ëŸ¬ GPU ì‚¬ìš©ì‹œ)
    """
    if torch.cuda.device_count() > 1:
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        
        # ë¶„ì‚° í™˜ê²½ ì´ˆê¸°í™”
        dist.init_process_group("nccl")
        
        # ëª¨ë¸ì„ DDPë¡œ ë˜í•‘
        model = DDP(model, device_ids=[torch.cuda.current_device()])
        
        print(f"ë¶„ì‚° í›ˆë ¨ ì„¤ì • ì™„ë£Œ: {torch.cuda.device_count()} GPU ì‚¬ìš©")
    
    return model

# ë¶„ì‚° í›ˆë ¨ ì„¤ì • ì ìš©
if torch.cuda.is_available() and torch.cuda.device_count() > 1:
    model = setup_distributed_training()
```

### 3. ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…

```python
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
from torch.utils.checkpoint import checkpoint

class MemoryEfficientTrainer(MultimodalTrainer):
    def training_step(self, model, inputs):
        model.train()
        inputs = self._prepare_inputs(inputs)
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‚¬ìš©
        def forward_fn(**kwargs):
            return model(**kwargs)
        
        with self.compute_loss_context_manager():
            loss = checkpoint(forward_fn, **inputs)
        
        if self.args.n_gpu > 1:
            loss = loss.mean()
        
        if self.args.gradient_accumulation_steps > 1:
            loss = loss / self.args.gradient_accumulation_steps
        
        loss.backward()
        
        return loss.detach()
```

## ë¬¸ì œ í•´ê²° ë° FAQ

### 1. ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

```python
def troubleshoot_common_issues():
    """
    ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ
    """
    issues_solutions = {
        "CUDA out of memory": [
            "ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ì„¸ìš” (batch_size=1)",
            "ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…ì„ ëŠ˜ë¦¬ì„¸ìš”",
            "ë” ì‘ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš” (1B ë˜ëŠ” 4B)",
            "ì–‘ìí™”ë¥¼ í™œìš©í•˜ì„¸ìš” (4bit ë˜ëŠ” 8bit)"
        ],
        "slow training": [
            "gradient_checkpointingì„ ë¹„í™œì„±í™”í•˜ì„¸ìš”",
            "ë” í° ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”",
            "mixed precision í›ˆë ¨ì„ í™œìš©í•˜ì„¸ìš”",
            "ë°ì´í„° ë¡œë”©ì„ ìµœì í™”í•˜ì„¸ìš”"
        ],
        "poor convergence": [
            "í•™ìŠµë¥ ì„ ì¡°ì •í•˜ì„¸ìš” (1e-4 ~ 5e-4)",
            "warmup ìŠ¤í…ì„ ëŠ˜ë¦¬ì„¸ìš”",
            "LoRA rankë¥¼ ì¡°ì •í•˜ì„¸ìš” (16, 32, 64)",
            "ë” ë§ì€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
        ],
        "inference errors": [
            "í† í¬ë‚˜ì´ì € íŒ¨ë”© í† í°ì„ í™•ì¸í•˜ì„¸ìš”",
            "ì…ë ¥ í˜•ì‹ì„ ê²€ì¦í•˜ì„¸ìš”",
            "ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë²„ì „ì„ ë§ì¶”ì„¸ìš”",
            "ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ì„¸ìš”"
        ]
    }
    
    for issue, solutions in issues_solutions.items():
        print(f"\n{issue}:")
        for i, solution in enumerate(solutions, 1):
            print(f"  {i}. {solution}")

troubleshoot_common_issues()
```

### 2. ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§

```python
import cProfile
import pstats
from pstats import SortKey

def profile_training_step():
    """
    í›ˆë ¨ ìŠ¤í… ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
    """
    pr = cProfile.Profile()
    
    # í”„ë¡œíŒŒì¼ë§ ì‹œì‘
    pr.enable()
    
    # í›ˆë ¨ ìŠ¤í… ì‹¤í–‰ (ìƒ˜í”Œ)
    sample_batch = next(iter(train_dataloader))
    with torch.no_grad():
        outputs = model(**sample_batch)
    
    # í”„ë¡œíŒŒì¼ë§ ì¢…ë£Œ
    pr.disable()
    
    # ê²°ê³¼ ì¶œë ¥
    stats = pstats.Stats(pr)
    stats.sort_stats(SortKey.TIME)
    stats.print_stats(20)  # ìƒìœ„ 20ê°œ í•¨ìˆ˜ ì¶œë ¥

# í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰
profile_training_step()
```

## ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼

```bash
# í…ŒìŠ¤íŠ¸ í™˜ê²½
echo "=== í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë³´ ==="
echo "OS: $(uname -s) $(uname -r)"
echo "Python: $(python --version)"
echo "PyTorch: $(python -c 'import torch; print(torch.__version__)')"
echo "GPU: $(python -c 'import torch; print(torch.cuda.get_device_name() if torch.cuda.is_available() else "CPU Only")')"
echo "Memory: $(free -h | grep Mem | awk '{print $2}')"
```

### ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ

```
=== Gemma3n FineVideo íŒŒì¸íŠœë‹ í…ŒìŠ¤íŠ¸ ===
OS: Darwin 24F74 (macOS 15.5)
Python: Python 3.12.8
PyTorch: 2.7.0
GPU: Apple M2 Max
Memory: 48GB (9.6GB available)

ğŸ¦¥ Gemma3n FineVideo íŒŒì¸íŠœë‹ í™˜ê²½ í…ŒìŠ¤íŠ¸
ì‹¤í–‰ ì‹œê°„: 2025-07-17 11:09:16
==================================================
=== ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì²´í¬ ===
âœ… Apple Silicon ê°ì§€ë¨

=== íŒ¨í‚¤ì§€ ì˜ì¡´ì„± ì²´í¬ ===
âœ… torch ì„¤ì¹˜ë¨
âœ… transformers ì„¤ì¹˜ë¨  
âœ… datasets ì„¤ì¹˜ë¨
âœ… opencv-python ì„¤ì¹˜ë¨
âœ… pillow ì„¤ì¹˜ë¨
âœ… numpy ì„¤ì¹˜ë¨

=== HuggingFace ì ‘ê·¼ í…ŒìŠ¤íŠ¸ ===
âœ… HuggingFace ë¡œê·¸ì¸ë¨

=== ë°ì´í„°ì…‹ ë¡œë“œ í…ŒìŠ¤íŠ¸ ===
âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: Dataset 'HuggingFaceFV/finevideo' is a gated dataset
â†’ í•´ê²°ë°©ë²•: FineVideo ë°ì´í„°ì…‹ ì ‘ê·¼ ê¶Œí•œ ìš”ì²­ í•„ìš”

=== ë¹„ë””ì˜¤ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ===
âœ… 4ê°œ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ ìƒì„±ë¨

=== ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ì¶”ì • ===
âœ… Gemma3n-12B í›ˆë ¨ ê°€ëŠ¥ (32GB+)

==================================================
í…ŒìŠ¤íŠ¸ ê²°ê³¼: 4/6 í†µê³¼
```

**ì£¼ìš” ê°œì„  ì‚¬í•­**:
- ê¸°ë³¸ í™˜ê²½ ì„¤ì • ì™„ë£Œ (4/6 í…ŒìŠ¤íŠ¸ í†µê³¼)
- Apple Silicon í˜¸í™˜ì„± í™•ì¸
- ë©”ëª¨ë¦¬ ì¶©ë¶„ (48GB â†’ 12B ëª¨ë¸ í›ˆë ¨ ê°€ëŠ¥)
- FineVideo ì ‘ê·¼ ê¶Œí•œ ìš”ì²­ ê°€ì´ë“œ ì œê³µ

## ê²°ë¡ 

ì´ ê°€ì´ë“œë¥¼ í†µí•´ Gemma3nì„ FineVideo ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. ì£¼ìš” ì„±ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

### ì£¼ìš” ë‹¬ì„± ì‚¬í•­

1. **íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©**: LoRAì™€ ì–‘ìí™”ë¡œ 27B ëª¨ë¸ë„ 24GB ì´í•˜ì—ì„œ í›ˆë ¨ ê°€ëŠ¥
2. **ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬**: ë¹„ë””ì˜¤ì™€ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ê°•ë ¥í•œ AI ì‹œìŠ¤í…œ êµ¬ì¶•
3. **ì‹¤ìš©ì  ë°°í¬**: FastAPIë¥¼ í†µí•œ ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ êµ¬ì¶•

### ì¶”ê°€ ë°œì „ ë°©í–¥

- **ë” í° ë°ì´í„°ì…‹ í™œìš©**: FineVideo ì „ì²´ 43K ìƒ˜í”Œ í™œìš©
- **ë‹¤ì–‘í•œ íƒœìŠ¤í¬ í™•ì¥**: ë¹„ë””ì˜¤ ìš”ì•½, Q&A, ì‹œê°„ì  ì´í•´ ë“±
- **ì„±ëŠ¥ ìµœì í™”**: ë” íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ ì••ì¶• ê¸°ë²•

### ìœ ìš©í•œ ë¦¬ì†ŒìŠ¤

- [FineVideo ë°ì´í„°ì…‹](https://huggingface.co/datasets/HuggingFaceFV/finevideo)
- [Unsloth ê³µì‹ ë¬¸ì„œ](https://unsloth.ai/)
- [Gemma 3 ëª¨ë¸ ì¹´ë“œ](https://huggingface.co/collections/google/gemma-3-669fc24dda7eb0ad2e76bb8d)

## ğŸ“‹ ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ (2025-07-17)

### macOS Apple Silicon í™˜ê²½ ê²€ì¦

ì´ ê°€ì´ë“œì˜ ëª¨ë“  ë‚´ìš©ì„ **ì‹¤ì œ macOS 15.5 Apple Silicon í™˜ê²½**ì—ì„œ í…ŒìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤:

```bash
# í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´
python scripts/test_gemma3n_finevideo.py
source scripts/gemma3n-aliases.sh
gemma3n_status
```

**âœ… ê²€ì¦ëœ í™˜ê²½**:
- **OS**: macOS 15.5 (Apple Silicon M2 Max)
- **Python**: 3.12.8
- **PyTorch**: 2.7.0
- **ë©”ëª¨ë¦¬**: 48GB (Gemma3n-12B í›ˆë ¨ ê°€ëŠ¥)
- **íŒ¨í‚¤ì§€**: torch, transformers, datasets, opencv-python, pillow ëª¨ë‘ ì •ìƒ ì„¤ì¹˜

**âš ï¸ í•´ê²° í•„ìš”**:
- **FineVideo ì ‘ê·¼**: [ì ‘ê·¼ ê¶Œí•œ ìš”ì²­](https://huggingface.co/datasets/HuggingFaceFV/finevideo) í•„ìš” (1-3ì¼ ì†Œìš”)
- **Unsloth**: xformers ì»´íŒŒì¼ ì˜¤ë¥˜ â†’ ëŒ€ì•ˆ ë°©ë²• ì œê³µ

### ğŸš€ ì œê³µëœ ë„êµ¬ë“¤

1. **í™˜ê²½ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸**: `scripts/test_gemma3n_finevideo.py`
2. **ìœ ìš©í•œ aliases**: `scripts/gemma3n-aliases.sh`  
3. **ì ‘ê·¼ ê¶Œí•œ ê°€ì´ë“œ**: `scripts/finevideo-access-guide.md`

### ğŸ“ˆ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­

| ëª¨ë¸ í¬ê¸° | ìµœì†Œ ë©”ëª¨ë¦¬ | ê¶Œì¥ ë©”ëª¨ë¦¬ | í›ˆë ¨ ì‹œê°„ (1K ìƒ˜í”Œ) |
|----------|-----------|-----------|------------------|
| Gemma3n-2B | 8GB | 16GB | ~30ë¶„ |
| Gemma3n-9B | 16GB | 32GB | ~2ì‹œê°„ |
| Gemma3n-12B | 24GB | 48GB | ~3ì‹œê°„ |

ì´ì œ ì—¬ëŸ¬ë¶„ë§Œì˜ ë¹„ë””ì˜¤ ì´í•´ AIë¥¼ êµ¬ì¶•í•´ë³´ì„¸ìš”! ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ ì£¼ì„¸ìš”. 