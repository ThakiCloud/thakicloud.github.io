---
title: "RAGLight ì™„ë²½ ê°€ì´ë“œ: ê¸°ë³¸ RAGë¶€í„° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ê¹Œì§€"
excerpt: "RAGLight í”„ë ˆìž„ì›Œí¬ë¥¼ ë§ˆìŠ¤í„°í•˜ì„¸ìš”. RAG, Agentic RAG, RAT íŒŒì´í”„ë¼ì¸, MCP í†µí•©ì„ í†µí•´ ê°•ë ¥í•œ ê²€ìƒ‰ ì¦ê°• ìƒì„± ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ì‹¤ì „ ê°€ì´ë“œìž…ë‹ˆë‹¤."
seo_title: "RAGLight íŠœí† ë¦¬ì–¼: ì™„ì „í•œ RAG í”„ë ˆìž„ì›Œí¬ ê°€ì´ë“œ - Thaki Cloud"
seo_description: "RAGLight í”„ë ˆìž„ì›Œí¬ë¡œ ì‹¤ì „ ì˜ˆì œë¥¼ í†µí•´ í•™ìŠµí•˜ì„¸ìš”. macOSì—ì„œ Ollama, OpenAI, Mistralì„ ì‚¬ìš©í•˜ì—¬ RAG, Agentic RAG, RAT íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."
date: 2025-10-14
categories:
  - tutorials
tags:
  - raglight
  - rag
  - agentic-rag
  - ollama
  - python
  - llm
  - vector-database
  - mcp
  - huggingface
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
lang: ko
permalink: /ko/tutorials/raglight-comprehensive-guide/
canonical_url: "https://thakicloud.github.io/ko/tutorials/raglight-comprehensive-guide/"
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 15ë¶„

## ì†Œê°œ

**RAGLight**ëŠ” **ê²€ìƒ‰ ì¦ê°• ìƒì„±(Retrieval-Augmented Generation, RAG)** êµ¬í˜„ì„ ê°„ì†Œí™”í•˜ë„ë¡ ì„¤ê³„ëœ ê²½ëŸ‰ ëª¨ë“ˆí˜• Python í”„ë ˆìž„ì›Œí¬ìž…ë‹ˆë‹¤. ë¬¸ì„œ ê²€ìƒ‰ê³¼ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(Large Language Model, LLM)ì„ ê²°í•©í•˜ì—¬, ì‚¬ìš©ìžì˜ ë¬¸ì„œì™€ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìžˆëŠ” ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ì´ ì¢…í•© íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ë‹¤ìŒì„ í•™ìŠµí•©ë‹ˆë‹¤:

- ë‹¤ì–‘í•œ LLM ì œê³µìž(Ollama, OpenAI, Mistral)ì™€ RAGLight ì„¤ì •
- ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µì„ ìœ„í•œ ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- ë‹¤ë‹¨ê³„ ì¶”ë¡  ìž‘ì—…ì„ ìœ„í•œ Agentic RAG êµ¬í˜„
- í–¥ìƒëœ ì¶”ë¡ ì„ ìœ„í•œ RAT(Retrieval-Augmented Thinking) ì‚¬ìš©
- MCP(Model Context Protocol)ë¥¼ ì‚¬ìš©í•œ ì™¸ë¶€ ë„êµ¬ í†µí•©

### RAGLightì˜ íŠ¹ë³„í•œ ì 

RAGLightì˜ ìž¥ì :

- **ëª¨ë“ˆí˜• ì•„í‚¤í…ì²˜**: LLM, ìž„ë² ë”©, ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‰½ê²Œ êµì²´ ê°€ëŠ¥
- **ë‹¤ì¤‘ ì œê³µìž ì§€ì›**: Ollama, OpenAI, Mistral, LMStudio, vLLM, Google AI
- **ê³ ê¸‰ íŒŒì´í”„ë¼ì¸**: ê¸°ë³¸ RAG, Agentic RAG, ì¶”ë¡  ë ˆì´ì–´ê°€ ìžˆëŠ” RAT
- **MCP í†µí•©**: ì™¸ë¶€ ë„êµ¬ ë° ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì›í™œí•˜ê²Œ ì—°ê²°
- **ìœ ì—°í•œ êµ¬ì„±**: RAG íŒŒì´í”„ë¼ì¸ì˜ ëª¨ë“  ì¸¡ë©´ì„ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥

## ì‚¬ì „ ìš”êµ¬ì‚¬í•­

ì´ íŠœí† ë¦¬ì–¼ì„ ì‹œìž‘í•˜ê¸° ì „ì— ë‹¤ìŒì„ ì¤€ë¹„í•˜ì„¸ìš”:

### 1. Python í™˜ê²½

```bash
# Python ë²„ì „ í™•ì¸ (3.8 ì´ìƒ í•„ìš”)
python3 --version

# ê°€ìƒ í™˜ê²½ ìƒì„± (ê¶Œìž¥)
python3 -m venv raglight-env
source raglight-env/bin/activate  # macOS/Linux
# raglight-env\Scripts\activate  # Windows
```

### 2. Ollama ì„¤ì¹˜ (ë¡œì»¬ LLMìš©)

```bash
# macOS
brew install ollama

# ë˜ëŠ” https://ollama.ai/download ì—ì„œ ë‹¤ìš´ë¡œë“œ

# Ollama ì„œë¹„ìŠ¤ ì‹œìž‘
ollama serve

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìƒˆ í„°ë¯¸ë„ì—ì„œ)
ollama pull llama3.2:3b
```

**ëŒ€ì•ˆ**: í´ë¼ìš°ë“œ ê¸°ë°˜ LLMì„ ì„ í˜¸í•˜ëŠ” ê²½ìš° OpenAI ë˜ëŠ” Mistral APIë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

### 3. RAGLight ì„¤ì¹˜

```bash
pip install raglight
```

## ì„¤ì¹˜ ë° ì„¤ì •

### í™˜ê²½ êµ¬ì„±

API í‚¤ë¥¼ ì €ìž¥í•  `.env` íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš” (í´ë¼ìš°ë“œ ì œê³µìž ì‚¬ìš© ì‹œ):

```bash
# .env íŒŒì¼
OPENAI_API_KEY=your_openai_key_here
MISTRAL_API_KEY=your_mistral_key_here
```

### í”„ë¡œì íŠ¸ êµ¬ì¡°

í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•˜ì„¸ìš”:

```bash
mkdir raglight-tutorial
cd raglight-tutorial
mkdir data
mkdir knowledge_base
```

### ìƒ˜í”Œ ë°ì´í„° ìƒì„±

í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë¬¸ì„œë¥¼ ìƒì„±í•˜ì„¸ìš”:

```bash
# data/document1.txt
cat > data/document1.txt << 'EOF'
RAGLightëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì„ ìœ„í•œ ëª¨ë“ˆí˜• Python í”„ë ˆìž„ì›Œí¬ìž…ë‹ˆë‹¤.
Ollama, OpenAI, Mistralì„ í¬í•¨í•œ ì—¬ëŸ¬ LLM ì œê³µìžë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
ì£¼ìš” ê¸°ëŠ¥ìœ¼ë¡œ ChromaDB ë° FAISSì™€ì˜ ìœ ì—°í•œ ë²¡í„° ìŠ¤í† ì–´ í†µí•©ì´ ìžˆìŠµë‹ˆë‹¤.
EOF

# data/document2.txt
cat > data/document2.txt << 'EOF'
Agentic RAGëŠ” ìžìœ¨ì ì¸ ì—ì´ì „íŠ¸ë¥¼ í†µí•©í•˜ì—¬ ì „í†µì ì¸ RAGë¥¼ í™•ìž¥í•©ë‹ˆë‹¤.
ì´ëŸ¬í•œ ì—ì´ì „íŠ¸ëŠ” ë‹¤ë‹¨ê³„ ì¶”ë¡ ê³¼ ë™ì  ì •ë³´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
ì‚¬ìš© ì‚¬ë¡€ì—ëŠ” ë³µìž¡í•œ ì§ˆì˜ì‘ë‹µê³¼ ì—°êµ¬ ë³´ì¡°ê°€ í¬í•¨ë©ë‹ˆë‹¤.
EOF

# data/document3.txt
cat > data/document3.txt << 'EOF'
RAT(Retrieval-Augmented Thinking)ëŠ” íŠ¹í™”ëœ ì¶”ë¡  ë ˆì´ì–´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
ì¶”ë¡  LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ í’ˆì§ˆê³¼ ë¶„ì„ ê¹Šì´ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
RATëŠ” ê¹Šì€ ì‚¬ê³ ì™€ ë‹¤ì¤‘ í™‰ ì¶”ë¡ ì´ í•„ìš”í•œ ìž‘ì—…ì— ì´ìƒì ìž…ë‹ˆë‹¤.
EOF
```

## ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸

### RAG ì•„í‚¤í…ì²˜ ì´í•´

ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸ì€ ì„¸ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤:

1. **ë¬¸ì„œ ìˆ˜ì§‘(Document Ingestion)**: ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ìž„ë² ë”©ìœ¼ë¡œ ë³€í™˜
2. **ë²¡í„° ì €ìž¥(Vector Storage)**: ìž„ë² ë”©ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤(ChromaDB, FAISS ë“±)ì— ì €ìž¥
3. **ê²€ìƒ‰ ë° ìƒì„±(Retrieval & Generation)**: ì¿¼ë¦¬ ì‹œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ LLMì— ì „ë‹¬

### êµ¬í˜„

ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸ì˜ ì™„ì „í•œ ì˜ˆì œìž…ë‹ˆë‹¤:

```python
#!/usr/bin/env python3
"""RAGLightë¥¼ ì‚¬ìš©í•œ ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸"""

from raglight.rag.simple_rag_api import RAGPipeline
from raglight.config.rag_config import RAGConfig
from raglight.config.vector_store_config import VectorStoreConfig
from raglight.config.settings import Settings
from raglight.models.data_source_model import FolderSource
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
Settings.setup_logging()

# ë²¡í„° ìŠ¤í† ì–´ êµ¬ì„±
vector_store_config = VectorStoreConfig(
    embedding_model=Settings.DEFAULT_EMBEDDINGS_MODEL,
    api_base=Settings.DEFAULT_OLLAMA_CLIENT,
    provider=Settings.HUGGINGFACE,
    database=Settings.CHROMA,
    persist_directory="./chroma_db",
    collection_name="my_knowledge_base"
)

# RAG êµ¬ì„±
config = RAGConfig(
    llm="llama3.2:3b",  # Ollama ëª¨ë¸
    k=5,  # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
    provider=Settings.OLLAMA,
    system_prompt=Settings.DEFAULT_SYSTEM_PROMPT,
    knowledge_base=[FolderSource(path="./data")]
)

# íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ë° êµ¬ì¶•
print("RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
pipeline = RAGPipeline(config, vector_store_config)

print("ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")
pipeline.build()

# íŒŒì´í”„ë¼ì¸ì— ì¿¼ë¦¬
query = "RAGLightì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?"
print(f"\nì¿¼ë¦¬: {query}")

response = pipeline.generate(query)
print(f"\nì‘ë‹µ:\n{response}")
```

### ì£¼ìš” êµ¬ì„± ì˜µì…˜

**ë²¡í„° ìŠ¤í† ì–´ ì˜µì…˜:**
- `database`: CHROMA, FAISS, ë˜ëŠ” QDRANT
- `provider`: ìž„ë² ë”©ì„ ìœ„í•œ HUGGINGFACE, OLLAMA, ë˜ëŠ” OPENAI
- `persist_directory`: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì €ìž¥í•  ìœ„ì¹˜

**RAG ì˜µì…˜:**
- `llm`: ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "llama3.2:3b", "gpt-4", "mistral-large-2411")
- `k`: ê²€ìƒ‰í•  ê´€ë ¨ ë¬¸ì„œ ìˆ˜
- `provider`: OLLAMA, OPENAI, MISTRAL, LMSTUDIO, GOOGLE

### ë‹¤ì–‘í•œ LLM ì œê³µìž ì‚¬ìš©

**OpenAI:**
```python
config = RAGConfig(
    llm="gpt-4",
    k=5,
    provider=Settings.OPENAI,
    api_key=Settings.OPENAI_API_KEY,
    knowledge_base=[FolderSource(path="./data")]
)
```

**Mistral:**
```python
config = RAGConfig(
    llm="mistral-large-2411",
    k=5,
    provider=Settings.MISTRAL,
    api_key=Settings.MISTRAL_API_KEY,
    knowledge_base=[FolderSource(path="./data")]
)
```

## Agentic RAG íŒŒì´í”„ë¼ì¸

### Agentic RAGëž€?

Agentic RAGëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆëŠ” ìžìœ¨ì ì¸ ì—ì´ì „íŠ¸(Agent)ë¥¼ í†µí•©í•˜ì—¬ ì „í†µì ì¸ RAGë¥¼ í™•ìž¥í•©ë‹ˆë‹¤:

- ë‹¤ë‹¨ê³„ ì¶”ë¡  ìˆ˜í–‰
- ì¶”ê°€ ì •ë³´ ê²€ìƒ‰ ì‹œì  ê²°ì •
- ì—¬ëŸ¬ ê²€ìƒ‰-ìƒì„± ì‚¬ì´í´ì„ í†µí•œ ë°˜ë³µ
- ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ê°€ í•„ìš”í•œ ë³µìž¡í•œ ì§ˆë¬¸ ì²˜ë¦¬

### êµ¬í˜„

```python
"""RAGLightë¥¼ ì‚¬ìš©í•œ Agentic RAG íŒŒì´í”„ë¼ì¸"""

from raglight.rag.simple_agentic_rag_api import AgenticRAGPipeline
from raglight.config.agentic_rag_config import AgenticRAGConfig
from raglight.config.vector_store_config import VectorStoreConfig
from raglight.config.settings import Settings
from raglight.models.data_source_model import FolderSource
from dotenv import load_dotenv

load_dotenv()
Settings.setup_logging()

# ë²¡í„° ìŠ¤í† ì–´ êµ¬ì„±
vector_store_config = VectorStoreConfig(
    embedding_model=Settings.DEFAULT_EMBEDDINGS_MODEL,
    api_base=Settings.DEFAULT_OLLAMA_CLIENT,
    provider=Settings.HUGGINGFACE,
    database=Settings.CHROMA,
    persist_directory="./agentic_chroma_db",
    collection_name="agentic_knowledge_base"
)

# Agentic RAG êµ¬ì„±
config = AgenticRAGConfig(
    provider=Settings.MISTRAL,
    model="mistral-large-2411",
    k=10,
    system_prompt=Settings.DEFAULT_AGENT_PROMPT,
    max_steps=4,  # ìµœëŒ€ ì¶”ë¡  ë‹¨ê³„
    api_key=Settings.MISTRAL_API_KEY,
    knowledge_base=[FolderSource(path="./data")]
)

# ì´ˆê¸°í™” ë° êµ¬ì¶•
print("Agentic RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
agentic_rag = AgenticRAGPipeline(config, vector_store_config)

print("ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")
agentic_rag.build()

# ì—¬ëŸ¬ ë‹¨ê³„ê°€ í•„ìš”í•œ ë³µìž¡í•œ ì¿¼ë¦¬
query = """
ê¸°ë³¸ RAGì™€ Agentic RAGì˜ ê¸°ëŠ¥ì„ ë¹„êµí•˜ì„¸ìš”.
Agentic RAGê°€ ë” ìœ ë¦¬í•œ êµ¬ì²´ì ì¸ ì‚¬ìš© ì‚¬ë¡€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
"""

print(f"\nì¿¼ë¦¬: {query}")
response = agentic_rag.generate(query)
print(f"\nì‘ë‹µ:\n{response}")
```

### Agentic RAGì˜ ì£¼ìš” ê¸°ëŠ¥

**max_steps**: ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•  ìˆ˜ ìžˆëŠ” ì¶”ë¡  ë°˜ë³µ íšŸìˆ˜ ì œì–´
```python
# ê°„ë‹¨í•œ ì¿¼ë¦¬: ì ì€ ë‹¨ê³„ í•„ìš”
config = AgenticRAGConfig(max_steps=2, ...)

# ë³µìž¡í•œ ë¶„ì„: ë” ë§Žì€ ë‹¨ê³„ í—ˆìš©
config = AgenticRAGConfig(max_steps=10, ...)
```

**ì»¤ìŠ¤í…€ ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸**: ì—ì´ì „íŠ¸ì˜ ë™ìž‘ ë§žì¶¤í™”
```python
custom_agent_prompt = """
ë‹¹ì‹ ì€ ì—°êµ¬ ë³´ì¡°ìž…ë‹ˆë‹¤. ì§ˆë¬¸ì— ë‹µí•  ë•Œ:
1. ë³µìž¡í•œ ì¿¼ë¦¬ë¥¼ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´
2. ê° í•˜ìœ„ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
3. ë°œê²¬ ì‚¬í•­ì„ ì¢…í•©í•˜ì—¬ í¬ê´„ì ì¸ ë‹µë³€ ìž‘ì„±
4. ê°€ëŠ¥í•œ ê²½ìš° ì¶œì²˜ ì¸ìš©
"""

config = AgenticRAGConfig(
    system_prompt=custom_agent_prompt,
    ...
)
```

## RAT (ê²€ìƒ‰ ì¦ê°• ì‚¬ê³ )

### RAT ì´í•´í•˜ê¸°

RATëŠ” RAG íŒŒì´í”„ë¼ì¸ì— íŠ¹í™”ëœ ì¶”ë¡  ë ˆì´ì–´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤:

1. **ê²€ìƒ‰(Retrieval)**: ê´€ë ¨ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
2. **ì¶”ë¡ (Reasoning)**: ì¶”ë¡  LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ëœ ë‚´ìš© ë¶„ì„
3. **ì‚¬ê³ (Thinking)**: ì¤‘ê°„ ì¶”ë¡  ë‹¨ê³„ ìƒì„±
4. **ìƒì„±(Generation)**: í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±

### êµ¬í˜„

```python
"""RAGLightë¥¼ ì‚¬ìš©í•œ RAT íŒŒì´í”„ë¼ì¸"""

from raglight.rat.simple_rat_api import RATPipeline
from raglight.config.rat_config import RATConfig
from raglight.config.vector_store_config import VectorStoreConfig
from raglight.config.settings import Settings
from raglight.models.data_source_model import FolderSource

Settings.setup_logging()

# ë²¡í„° ìŠ¤í† ì–´ êµ¬ì„±
vector_store_config = VectorStoreConfig(
    embedding_model=Settings.DEFAULT_EMBEDDINGS_MODEL,
    api_base=Settings.DEFAULT_OLLAMA_CLIENT,
    provider=Settings.HUGGINGFACE,
    database=Settings.CHROMA,
    persist_directory="./rat_chroma_db",
    collection_name="rat_knowledge_base"
)

# RAT êµ¬ì„±
config = RATConfig(
    cross_encoder_model=Settings.DEFAULT_CROSS_ENCODER_MODEL,
    llm="llama3.2:3b",
    k=Settings.DEFAULT_K,
    provider=Settings.OLLAMA,
    system_prompt=Settings.DEFAULT_SYSTEM_PROMPT,
    reasoning_llm=Settings.DEFAULT_REASONING_LLM,
    reflection=3,  # ì¶”ë¡  ë°˜ë³µ íšŸìˆ˜
    knowledge_base=[FolderSource(path="./data")]
)

# ì´ˆê¸°í™” ë° êµ¬ì¶•
print("RAT íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
pipeline = RATPipeline(config, vector_store_config)

print("ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")
pipeline.build()

# ê¹Šì€ ì¶”ë¡ ì´ í•„ìš”í•œ ì¿¼ë¦¬
query = """
RAG, Agentic RAG, RATì˜ ì•„í‚¤í…ì²˜ ì°¨ì´ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
ë³µìž¡ì„±, ì„±ëŠ¥, ì¶œë ¥ í’ˆì§ˆ ì¸¡ë©´ì—ì„œ ê°ê°ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
"""

print(f"\nì¿¼ë¦¬: {query}")
response = pipeline.generate(query)
print(f"\nì‘ë‹µ:\n{response}")
```

### RAT êµ¬ì„± ì˜µì…˜

**reflection**: ì¶”ë¡  ë°˜ë³µ íšŸìˆ˜
```python
# ë¹ ë¥¸ ì¶”ë¡ 
config = RATConfig(reflection=1, ...)

# ê¹Šì€ ë¶„ì„ì  ì‚¬ê³ 
config = RATConfig(reflection=5, ...)
```

**cross_encoder_model**: ë” ë‚˜ì€ ê²€ìƒ‰ì„ ìœ„í•œ ë¦¬ëž­í‚¹ ëª¨ë¸
```python
config = RATConfig(
    cross_encoder_model="cross-encoder/ms-marco-MiniLM-L-12-v2",
    ...
)
```

## MCP í†µí•©

### MCPëž€?

Model Context Protocol(MCP)ì„ ì‚¬ìš©í•˜ë©´ RAG íŒŒì´í”„ë¼ì¸ì´ ì™¸ë¶€ ë„êµ¬ ë° ì„œë¹„ìŠ¤ì™€ ìƒí˜¸ ìž‘ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë‹¤ìŒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:

- ì›¹ ê²€ìƒ‰ í†µí•©
- ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬
- ì™¸ë¶€ ì„œë¹„ìŠ¤ì— ëŒ€í•œ API í˜¸ì¶œ
- ì½”ë“œ ì‹¤í–‰ í™˜ê²½
- ì»¤ìŠ¤í…€ ë„êµ¬ í†µí•©

### MCP ì„œë²„ ì„¤ì •

ë¨¼ì € MCP ì„œë²„ë¥¼ êµ¬ì„±í•˜ì„¸ìš”(MCPClient ì‚¬ìš© ì˜ˆì œ):

```python
"""MCP ì„œë²„ êµ¬ì„±"""

from raglight.rag.simple_agentic_rag_api import AgenticRAGPipeline
from raglight.config.agentic_rag_config import AgenticRAGConfig
from raglight.config.settings import Settings

# MCP ì„œë²„ URL êµ¬ì„±
config = AgenticRAGConfig(
    provider=Settings.OPENAI,
    model="gpt-4o",
    k=10,
    mcp_config=[
        {% raw %}{"url": "http://127.0.0.1:8001/sse"}{% endraw %}  # MCP ì„œë²„ URL
    ],
    system_prompt=Settings.DEFAULT_AGENT_PROMPT,
    max_steps=4,
    api_key=Settings.OPENAI_API_KEY
)

# MCPë¡œ ì´ˆê¸°í™”
pipeline = AgenticRAGPipeline(config, vector_store_config)
pipeline.build()

# ì´ì œ ì—ì´ì „íŠ¸ê°€ ì™¸ë¶€ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤
query = "RAG í”„ë ˆìž„ì›Œí¬ì— ëŒ€í•œ ìµœê·¼ ì—…ë°ì´íŠ¸ë¥¼ ì›¹ì—ì„œ ê²€ìƒ‰í•˜ê³  ìš”ì•½í•˜ì„¸ìš”"
response = pipeline.generate(query)
```

### MCP ì‚¬ìš© ì‚¬ë¡€

**ì›¹ ê²€ìƒ‰ í†µí•©:**
```python
# ì—ì´ì „íŠ¸ê°€ ì›¹ ê²°ê³¼ë¥¼ ê²€ìƒ‰í•˜ê³  í†µí•© ê°€ëŠ¥
query = "2024ë…„ RAG ê¸°ìˆ ì˜ ìµœì‹  ë°œì „ ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?"
```

**ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬:**
```python
# ì—ì´ì „íŠ¸ê°€ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ìœ„í•´ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ê°€ëŠ¥
query = "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚¬ìš©ìž í†µê³„ë¥¼ ê²€ìƒ‰í•˜ê³  ì¶”ì„¸ë¥¼ ë¶„ì„í•˜ì„¸ìš”"
```

**API í†µí•©:**
```python
# ì—ì´ì „íŠ¸ê°€ ì™¸ë¶€ API í˜¸ì¶œ ê°€ëŠ¥
query = "ë‚ ì”¨ APIë¥¼ í™•ì¸í•˜ê³  ì˜ˆë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™œë™ì„ ì¶”ì²œí•˜ì„¸ìš”"
```

## ì„±ëŠ¥ ë¹„êµ

### íŒŒì´í”„ë¼ì¸ íŠ¹ì„±

| íŒŒì´í”„ë¼ì¸ íƒ€ìž… | ë³µìž¡ë„ | ì‘ë‹µ ì‹œê°„ | ì‚¬ìš© ì‚¬ë¡€ |
|--------------|--------|----------|----------|
| **ê¸°ë³¸ RAG** | ë‚®ìŒ | ë¹ ë¦„ (< 5ì´ˆ) | ê°„ë‹¨í•œ Q&A, ë¬¸ì„œ ì¡°íšŒ |
| **Agentic RAG** | ì¤‘ê°„ | ë³´í†µ (5-15ì´ˆ) | ë‹¤ë‹¨ê³„ ì¶”ë¡ , ì—°êµ¬ |
| **RAT** | ë†’ìŒ | ëŠë¦¼ (15-30ì´ˆ) | ê¹Šì€ ë¶„ì„, ë³µìž¡í•œ ì¶”ë¡  |
| **RAG + MCP** | ê°€ë³€ì  | ë„êµ¬ì— ë”°ë¼ ë‹¤ë¦„ | ì™¸ë¶€ ë„êµ¬ í†µí•© |

### ì˜¬ë°”ë¥¸ íŒŒì´í”„ë¼ì¸ ì„ íƒ

**ê¸°ë³¸ RAGë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°:**
- ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•  ë•Œ
- ì§ˆë¬¸ì´ ê°„ë‹¨í•  ë•Œ
- ë‹¨ì¼ ë¬¸ì„œ ì¡°íšŒë¡œ ì¶©ë¶„í•  ë•Œ

**Agentic RAGë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°:**
- ì§ˆë¬¸ì— ì—¬ëŸ¬ ë‹¨ê³„ê°€ í•„ìš”í•  ë•Œ
- ë™ì  ê²€ìƒ‰ì´ í•„ìš”í•  ë•Œ
- ìž‘ì—…ì— ì—°êµ¬ ë˜ëŠ” íƒìƒ‰ì´ í¬í•¨ë  ë•Œ

**RATë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°:**
- ê¹Šì€ ë¶„ì„ì  ì‚¬ê³ ê°€ í•„ìš”í•  ë•Œ
- ì†ë„ë³´ë‹¤ í’ˆì§ˆì´ ì¤‘ìš”í•  ë•Œ
- ë³µìž¡í•œ ë‹¤ì¤‘ í™‰ ì¶”ë¡ ì´ í•„ìš”í•  ë•Œ

**MCP í†µí•©ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°:**
- ì‹¤ì‹œê°„ ì™¸ë¶€ ë°ì´í„°ê°€ í•„ìš”í•  ë•Œ
- ìž‘ì—…ì— ë„êµ¬ ì‚¬ìš©ì´ í•„ìš”í•  ë•Œ
- ë™ì  ì •ë³´ê°€ í•„ìˆ˜ì ì¼ ë•Œ

## ëª¨ë²” ì‚¬ë¡€

### 1. ë¬¸ì„œ ì¤€ë¹„

**ì²­í¬ í¬ê¸° ìµœì í™”:**
```python
# ê¸°ìˆ  ë¬¸ì„œì˜ ê²½ìš°
chunk_size = 512

# ë‚´ëŸ¬í‹°ë¸Œ ì½˜í…ì¸ ì˜ ê²½ìš°
chunk_size = 1024
```

**í´ë” êµ¬ì„±:**
```
knowledge_base/
â”œâ”€â”€ technical_docs/
â”œâ”€â”€ user_manuals/
â”œâ”€â”€ api_reference/
â””â”€â”€ faq/
```

### 2. ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬

**ì˜êµ¬ ì €ìž¥:**
```python
# í”„ë¡œë•ì…˜ì—ì„œëŠ” í•­ìƒ ì˜êµ¬ ì €ìž¥ì†Œ ì‚¬ìš©
vector_store_config = VectorStoreConfig(
    persist_directory="./prod_vectordb",
    collection_name="production_kb"
)
```

**ì»¬ë ‰ì…˜ êµ¬ì„±:**
```python
# ë‹¤ì–‘í•œ ë„ë©”ì¸ì— ëŒ€í•´ ë³„ë„ ì»¬ë ‰ì…˜ ì‚¬ìš©
collections = {
    "technical": "tech_docs_collection",
    "business": "business_docs_collection",
    "general": "general_knowledge_collection"
}
```

### 3. LLM ì„ íƒ

**ê°œë°œ:**
```python
# ê°œë°œìš© ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
config = RAGConfig(
    llm="llama3.2:3b",
    provider=Settings.OLLAMA
)
```

**í”„ë¡œë•ì…˜:**
```python
# í”„ë¡œë•ì…˜ìš© ê°•ë ¥í•œ ëª¨ë¸ ì‚¬ìš©
config = RAGConfig(
    llm="gpt-4",
    provider=Settings.OPENAI
)
```

### 4. ì˜¤ë¥˜ ì²˜ë¦¬

```python
"""ê²¬ê³ í•œ ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ í¬í•¨í•œ RAG íŒŒì´í”„ë¼ì¸"""

try:
    pipeline = RAGPipeline(config, vector_store_config)
    pipeline.build()
    response = pipeline.generate(query)
except Exception as e:
    print(f"íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
    # RAG ì—†ì´ ê¸°ë³¸ LLMìœ¼ë¡œ í´ë°±
    response = fallback_generate(query)
```

### 5. ë¬´ì‹œ í´ë” êµ¬ì„±

ì½”ë“œ ì €ìž¥ì†Œë¥¼ ì¸ë±ì‹±í•  ë•Œ ë¶ˆí•„ìš”í•œ ë””ë ‰í† ë¦¬ë¥¼ ì œì™¸í•˜ì„¸ìš”:

```python
# ì»¤ìŠ¤í…€ ë¬´ì‹œ í´ë”
custom_ignore_folders = [
    ".venv",
    "venv",
    "node_modules",
    "__pycache__",
    ".git",
    "build",
    "dist",
    "my_custom_folder_to_ignore"
]

config = AgenticRAGConfig(
    ignore_folders=custom_ignore_folders,
    ...
)
```

### 6. ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

```python
"""ìƒì„¸ ë¡œê¹… í™œì„±í™”"""

import logging

# ë¡œê¹… ë ˆë²¨ êµ¬ì„±
logging.basicConfig(level=logging.INFO)

# ë˜ëŠ” RAGLightì˜ ì„¤ì • ì‚¬ìš©
Settings.setup_logging()

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
import time

start_time = time.time()
response = pipeline.generate(query)
elapsed_time = time.time() - start_time

print(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
```

## ê³ ê¸‰ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ì»¤ìŠ¤í…€ íŒŒì´í”„ë¼ì¸ ë¹Œë”

```python
"""ë¹Œë” íŒ¨í„´ì„ ì‚¬ìš©í•œ ì»¤ìŠ¤í…€ RAG íŒŒì´í”„ë¼ì¸"""

from raglight.rag.builder import Builder
from raglight.config.settings import Settings

# ì»¤ìŠ¤í…€ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
rag = Builder() \
    .with_embeddings(
        Settings.HUGGINGFACE,
        model_name=Settings.DEFAULT_EMBEDDINGS_MODEL
    ) \
    .with_vector_store(
        Settings.CHROMA,
        persist_directory="./custom_db",
        collection_name="custom_collection"
    ) \
    .with_llm(
        Settings.OLLAMA,
        model_name="llama3.2:3b",
        system_prompt_file="./custom_prompt.txt",
        provider=Settings.OLLAMA
    ) \
    .build_rag(k=5)

# ë¬¸ì„œ ìˆ˜ì§‘
rag.vector_store.ingest(data_path='./data')

# ì¿¼ë¦¬
response = rag.generate("ì—¬ê¸°ì— ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”")
```

### ì½”ë“œ ì €ìž¥ì†Œ ì¸ë±ì‹±

```python
"""ì½”ë“œ ì €ìž¥ì†Œ ì¸ë±ì‹±"""

# ì‹œê·¸ë‹ˆì²˜ ì¶”ì¶œë¡œ ì½”ë“œ ì¸ë±ì‹±
rag.vector_store.ingest(repos_path=['./repo1', './repo2'])

# ì½”ë“œ ê²€ìƒ‰
code_results = rag.vector_store.similarity_search("ì¸ì¦ ë¡œì§")

# í´ëž˜ìŠ¤ ì‹œê·¸ë‹ˆì²˜ ê²€ìƒ‰
class_results = rag.vector_store.similarity_search_class("User í´ëž˜ìŠ¤ ì •ì˜")
```

### GitHub ì €ìž¥ì†Œ í†µí•©

```python
"""GitHub ì €ìž¥ì†Œ ì§ì ‘ ì¸ë±ì‹±"""

from raglight.models.data_source_model import GitHubSource

knowledge_base = [
    GitHubSource(url="https://github.com/Bessouat40/RAGLight"),
    GitHubSource(url="https://github.com/your-org/your-repo")
]

config = RAGConfig(
    knowledge_base=knowledge_base,
    ...
)
```

## Docker ë°°í¬

### Dockerfile ì˜ˆì œ

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ë³µì‚¬
COPY . .

# Ollama/LMStudioë¥¼ ìœ„í•œ í˜¸ìŠ¤íŠ¸ ë§¤í•‘ ì¶”ê°€
# ì‹¤í–‰: docker run --add-host=host.docker.internal:host-gateway your-image

CMD ["python", "app.py"]
```

### ë¹Œë“œ ë° ì‹¤í–‰

```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t raglight-app .

# í˜¸ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ ì•¡ì„¸ìŠ¤ë¡œ ì‹¤í–‰ (Ollamaìš©)
docker run --add-host=host.docker.internal:host-gateway raglight-app
```

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

**1. Ollama ì—°ê²° ì˜¤ë¥˜:**
```python
# Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
# macOS/Linux:
ollama serve

# í•„ìš”ì‹œ API ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
vector_store_config = VectorStoreConfig(
    api_base="http://localhost:11434",  # ê¸°ë³¸ Ollama URL
    ...
)
```

**2. ë©”ëª¨ë¦¬ ë¬¸ì œ:**
```python
# ì²­í¬ í¬ê¸°ì™€ k ê°’ ì¤„ì´ê¸°
config = RAGConfig(
    k=3,  # ë” ì ì€ ë¬¸ì„œ ê²€ìƒ‰
    ...
)
```

**3. ëŠë¦° ì„±ëŠ¥:**
```python
# ë” ìž‘ì€ ìž„ë² ë”© ëª¨ë¸ ì‚¬ìš©
vector_store_config = VectorStoreConfig(
    embedding_model="all-MiniLM-L6-v2",  # ë” ìž‘ê³  ë¹ ë¥¸ ëª¨ë¸
    ...
)
```

**4. ë²¡í„° ìŠ¤í† ì–´ ì˜¤ë¥˜:**
```bash
# ì§€ìš°ê³  ìž¬êµ¬ì¶•
rm -rf ./chroma_db
python rebuild_kb.py
```

## ê²°ë¡ 

RAGLightëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ê°•ë ¥í•˜ê³  ìœ ì—°í•œ í”„ë ˆìž„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê°„ë‹¨í•œ ë¬¸ì„œ Q&Aë¶€í„° ì™¸ë¶€ ë„êµ¬ í†µí•©ì´ í¬í•¨ëœ ë³µìž¡í•œ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ê¹Œì§€, RAGLightì˜ ëª¨ë“ˆí˜• ì•„í‚¤í…ì²˜ë¥¼ í†µí•´ ì‰½ê²Œ êµ¬ì¶•í•˜ê³  í™•ìž¥í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

### í•µì‹¬ ìš”ì 

- **ê°„ë‹¨í•˜ê²Œ ì‹œìž‘**: ê¸°ë³¸ RAGë¡œ ì‹œìž‘í•˜ì—¬ í•„ìš”ì— ë”°ë¼ Agentic RAG ë˜ëŠ” RATë¡œ ì—…ê·¸ë ˆì´ë“œ
- **í˜„ëª…í•˜ê²Œ ì„ íƒ**: ì‚¬ìš© ì‚¬ë¡€ì™€ ì„±ëŠ¥ ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ ì˜¬ë°”ë¥¸ íŒŒì´í”„ë¼ì¸ ì„ íƒ
- **ê´‘ë²”ìœ„í•˜ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì§•**: RAGLightì˜ ëª¨ë“ˆí˜• ì„¤ê³„ë¥¼ í†µí•´ ì™„ì „í•œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
- **ì ì§„ì ìœ¼ë¡œ í™•ìž¥**: Ollamaë¡œ ë¡œì»¬ì—ì„œ ì‹œìž‘í•œ í›„ í”„ë¡œë•ì…˜ì„ ìœ„í•´ í´ë¼ìš°ë“œ ì œê³µìžë¡œ ì´ë™

### ë‹¤ìŒ ë‹¨ê³„

1. **ì‹¤í—˜**: ë‹¤ì–‘í•œ LLM ì œê³µìžì™€ ë²¡í„° ìŠ¤í† ì–´ ì‹œë„
2. **ìµœì í™”**: k ê°’, ì²­í¬ í¬ê¸°, ëª¨ë¸ ì„ íƒ ì¡°ì •
3. **í†µí•©**: ì™¸ë¶€ ë„êµ¬ ì•¡ì„¸ìŠ¤ë¥¼ ìœ„í•œ MCP ì„œë²„ ì¶”ê°€
4. **ë°°í¬**: í”„ë¡œë•ì…˜ ë°°í¬ë¥¼ ìœ„í•´ Dockerë¡œ ì»¨í…Œì´ë„ˆí™”

### ì°¸ê³  ìžë£Œ

- **RAGLight GitHub**: [https://github.com/Bessouat40/RAGLight](https://github.com/Bessouat40/RAGLight)
- **PyPI íŒ¨í‚¤ì§€**: [https://pypi.org/project/raglight/](https://pypi.org/project/raglight/)
- **Ollama**: [https://ollama.ai](https://ollama.ai)
- **ChromaDB**: [https://www.trychroma.com](https://www.trychroma.com)
- **MCP í”„ë¡œí† ì½œ**: "Model Context Protocol" ë¬¸ì„œ ê²€ìƒ‰

RAGLightë¡œ ì¦ê±°ìš´ ê°œë°œ ë˜ì„¸ìš”! ðŸš€

