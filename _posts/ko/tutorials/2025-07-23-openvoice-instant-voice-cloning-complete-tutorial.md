---
title: "OpenVoice ì™„ì „ ê°€ì´ë“œ: MIT ê¸°ìˆ ë¡œ êµ¬í˜„í•˜ëŠ” ì¦‰ì„ ìŒì„± ë³µì œ ë° ë‹¤êµ­ì–´ TTS ì‹œìŠ¤í…œ"
excerpt: "MITì™€ MyShellì´ ê°œë°œí•œ í˜ì‹ ì ì¸ ìŒì„± ë³µì œ ê¸°ìˆ  OpenVoiceë¡œ ì •í™•í•œ í†¤ ì»¬ëŸ¬ ë³µì œ, ìœ ì—°í•œ ìŠ¤íƒ€ì¼ ì œì–´, ì œë¡œìƒ· í¬ë¡œìŠ¤ë§êµ¬ì–¼ ìŒì„± ìƒì„±ì„ êµ¬í˜„í•˜ëŠ” ì™„ì „í•œ ì‹¤ì „ ê°€ì´ë“œ"
seo_title: "OpenVoice ìŒì„± ë³µì œ íŠœí† ë¦¬ì–¼ - MIT TTS ê¸°ìˆ  ì™„ì „ ê°€ì´ë“œ - Thaki Cloud"
seo_description: "OpenVoice V2ë¥¼ ì‚¬ìš©í•œ ì¦‰ì„ ìŒì„± ë³µì œ ë°©ë²•ì„ ì‹¤ì „ ì˜ˆì œì™€ í•¨ê»˜ ìƒì„¸íˆ ì•Œì•„ë³´ì„¸ìš”. ì„¤ì¹˜ë¶€í„° ê³ ê¸‰ í™œìš©ê¹Œì§€ í¬í•¨ëœ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤."
date: 2025-07-23
last_modified_at: 2025-07-23
categories:
  - tutorials
  - dev
tags:
  - OpenVoice
  - ìŒì„±ë³µì œ
  - TTS
  - MITê¸°ìˆ 
  - ìŒì„±í•©ì„±
  - ë‹¤êµ­ì–´TTS
  - ì œë¡œìƒ·í•™ìŠµ
  - MyShell
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/tutorials/openvoice-instant-voice-cloning-complete-tutorial/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 16ë¶„

## ì„œë¡ 

ìŒì„± ê¸°ìˆ ì˜ í˜ì‹ ì´ ê°€ì†í™”ë˜ë©´ì„œ, ê°œì¸ì˜ ëª©ì†Œë¦¬ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³µì œí•˜ê³  ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ ì´ í˜„ì‹¤ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. [OpenVoice](https://github.com/myshell-ai/OpenVoice)ëŠ” MITì™€ MyShellì´ ê³µë™ ê°œë°œí•œ í˜ì‹ ì ì¸ ì˜¤í”ˆì†ŒìŠ¤ ìŒì„± ë³µì œ ì†”ë£¨ì…˜ìœ¼ë¡œ, ë‹¨ ëª‡ ì´ˆì˜ ì°¸ì¡° ìŒì„±ë§Œìœ¼ë¡œë„ ì •í™•í•œ í†¤ ì»¬ëŸ¬ ë³µì œì™€ ë‹¤êµ­ì–´ ìŒì„± ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œëŠ” OpenVoice V2ì˜ í•µì‹¬ ê¸°ëŠ¥ë¶€í„° ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ í™œìš©ë²•ê¹Œì§€, 33.4k GitHub ìŠ¤íƒ€ë¥¼ ë°›ì€ ì´ í˜ì‹ ì ì¸ ê¸°ìˆ ì„ ì™„ì „íˆ ë§ˆìŠ¤í„°í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

## OpenVoice ê¸°ìˆ  ê°œìš”

### í•µì‹¬ í˜ì‹  ê¸°ìˆ 

**OpenVoice V1ì˜ 3ëŒ€ í˜ì‹ **:
1. **ì •í™•í•œ í†¤ ì»¬ëŸ¬ ë³µì œ**: ì°¸ì¡° ìŒì„±ì˜ ê³ ìœ í•œ ìŒìƒ‰ì„ ì •ë°€í•˜ê²Œ ì¬í˜„
2. **ìœ ì—°í•œ ìŒì„± ìŠ¤íƒ€ì¼ ì œì–´**: ê°ì •, ì–µì–‘, ë¦¬ë“¬, ì¼ì‹œì •ì§€ ë“± ì„¸ë°€í•œ ì¡°ì ˆ
3. **ì œë¡œìƒ· í¬ë¡œìŠ¤ë§êµ¬ì–¼**: í›ˆë ¨ ë°ì´í„°ì— ì—†ëŠ” ì–¸ì–´ë„ ì¦‰ì‹œ ì§€ì›

**OpenVoice V2ì˜ ì¶”ê°€ ê°œì„ **:
- **í–¥ìƒëœ ìŒì§ˆ**: ìƒˆë¡œìš´ í›ˆë ¨ ì „ëµìœ¼ë¡œ ë”ìš± ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„±
- **ë„¤ì´í‹°ë¸Œ ë‹¤êµ­ì–´ ì§€ì›**: ì˜ì–´, ìŠ¤í˜ì¸ì–´, í”„ë‘ìŠ¤ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´, í•œêµ­ì–´
- **MIT ë¼ì´ì„ ìŠ¤**: ì™„ì „í•œ ìƒì—…ì  ì´ìš© ê°€ëŠ¥

### ê¸°ìˆ  ì•„í‚¤í…ì²˜

```yaml
openvoice_architecture:
  core_components:
    base_speaker_encoder: "í™”ì ì„ë² ë”© ì¶”ì¶œ"
    tone_color_converter: "ìŒìƒ‰ ë³€í™˜ ì—”ì§„"
    multi_language_synthesizer: "ë‹¤êµ­ì–´ ìŒì„± í•©ì„±"
    style_controller: "ìŒì„± ìŠ¤íƒ€ì¼ ì œì–´"
  
  supported_features:
    voice_cloning: "ë‹¨ì¼ ì°¸ì¡°ë¡œ ì¦‰ì„ ë³µì œ"
    cross_lingual: "ì–¸ì–´ ê°„ ìŒì„± ë³€í™˜"
    emotion_control: "ê°ì • í‘œí˜„ ì¡°ì ˆ"
    speed_control: "ë°œí™” ì†ë„ ì œì–´"
    pitch_control: "ìŒë†’ì´ ì¡°ì ˆ"
  
  output_quality:
    sample_rate: "24kHz"
    bit_depth: "16-bit"
    format: "WAV/MP3"
```

## í™˜ê²½ ì„¤ì • ë° ì„¤ì¹˜

### 1. ê¸°ë³¸ í™˜ê²½ ì¤€ë¹„

```bash
# Python í™˜ê²½ í™•ì¸ (3.8 ì´ìƒ í•„ìš”)
python --version

# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv openvoice_env
source openvoice_env/bin/activate  # Windows: openvoice_env\Scripts\activate

# ê¸°ë³¸ ì˜ì¡´ì„± ì„¤ì¹˜
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install numpy scipy librosa soundfile
pip install gradio jupyter ipython
```

### 2. OpenVoice ì„¤ì¹˜

```bash
# GitHubì—ì„œ í´ë¡ 
git clone https://github.com/myshell-ai/OpenVoice.git
cd OpenVoice

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ê°œë°œ í™˜ê²½ ì„¤ì •
pip install -e .

# ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
mkdir checkpoints
wget https://myshell-public-repo-hosting.s3.amazonaws.com/openvoice/checkpoints_v2.zip
unzip checkpoints_v2.zip -d checkpoints/
```

### 3. GPU ì„¤ì • í™•ì¸

```python
# gpu_check.py
import torch
import torchaudio

def check_environment():
    print("=== OpenVoice í™˜ê²½ ì ê²€ ===")
    
    # PyTorch ë²„ì „ í™•ì¸
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    print(f"TorchAudio ë²„ì „: {torchaudio.__version__}")
    
    # CUDA ì§€ì› í™•ì¸
    if torch.cuda.is_available():
        print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"CUDA ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB")
    else:
        print("CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    # ì˜¤ë””ì˜¤ ì²˜ë¦¬ í™•ì¸
    try:
        import librosa
        import soundfile as sf
        print("ì˜¤ë””ì˜¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ìƒ ì‘ë™")
    except ImportError as e:
        print(f"ì˜¤ë””ì˜¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜¤ë¥˜: {e}")
    
    print("í™˜ê²½ ì ê²€ ì™„ë£Œ!")

if __name__ == "__main__":
    check_environment()
```

## ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. ê°„ë‹¨í•œ ìŒì„± ë³µì œ

```python
# basic_voice_cloning.py
import os
import torch
import librosa
import soundfile as sf
from openvoice import se_extractor
from openvoice.api import BaseSpeakerTTS, ToneColorConverter

def basic_voice_clone():
    """ê¸°ë³¸ ìŒì„± ë³µì œ ì˜ˆì œ"""
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ëª¨ë¸ ë¡œë“œ
    ckpt_base = 'checkpoints/base_speakers/EN'
    ckpt_converter = 'checkpoints/converter'
    
    # TTS ëª¨ë¸ ì´ˆê¸°í™”
    base_speaker_tts = BaseSpeakerTTS(f'{ckpt_base}/config.json', device=device)
    base_speaker_tts.load_ckpt(f'{ckpt_base}/checkpoint.pth')
    
    # í†¤ ì»¬ëŸ¬ ì»¨ë²„í„° ì´ˆê¸°í™”
    tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)
    tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')
    
    # ì°¸ì¡° ìŒì„±ì—ì„œ í™”ì ì„ë² ë”© ì¶”ì¶œ
    reference_speaker = 'resources/example_reference.wav'
    target_se, audio_name = se_extractor.get_se(
        reference_speaker, 
        tone_color_converter, 
        vad=True  # ìŒì„± í™œë™ ê°ì§€ í™œì„±í™”
    )
    
    # ê¸°ë³¸ TTSë¡œ ì¤‘ê°„ ìŒì„± ìƒì„±
    text = "ì•ˆë…•í•˜ì„¸ìš”! OpenVoiceë¡œ ìƒì„±ëœ ìŒì„±ì…ë‹ˆë‹¤."
    src_path = 'temp_output.wav'
    
    base_speaker_tts.tts(
        text, 
        src_path, 
        speaker='default',
        language='KR',  # í•œêµ­ì–´ ì„¤ì •
        speed=1.0
    )
    
    # í†¤ ì»¬ëŸ¬ ë³€í™˜
    output_path = 'cloned_voice_output.wav'
    encode_message = "@MyShell"
    
    tone_color_converter.convert(
        audio_src_path=src_path, 
        src_se=base_speaker_tts.hps.data.spk2id['default'],
        tgt_se=target_se, 
        output_path=output_path,
        message=encode_message
    )
    
    print(f"ìŒì„± ë³µì œ ì™„ë£Œ! ê²°ê³¼: {output_path}")
    
    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    os.remove(src_path)
    
    return output_path

if __name__ == "__main__":
    basic_voice_clone()
```

### 2. ë‹¤êµ­ì–´ ìŒì„± ìƒì„±

```python
# multilingual_tts.py
import torch
from openvoice import se_extractor
from openvoice.api import BaseSpeakerTTS, ToneColorConverter

class MultilingualVoiceCloner:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.setup_models()
    
    def setup_models(self):
        """ë‹¤êµ­ì–´ ëª¨ë¸ ì„¤ì •"""
        
        # ì§€ì› ì–¸ì–´ë³„ ëª¨ë¸ ê²½ë¡œ
        self.language_models = {
            'EN': 'checkpoints/base_speakers/EN',
            'ES': 'checkpoints/base_speakers/ES', 
            'FR': 'checkpoints/base_speakers/FR',
            'ZH': 'checkpoints/base_speakers/ZH',
            'JP': 'checkpoints/base_speakers/JP',
            'KR': 'checkpoints/base_speakers/KR'
        }
        
        # ì»¨ë²„í„° ëª¨ë¸
        ckpt_converter = 'checkpoints/converter'
        self.tone_color_converter = ToneColorConverter(
            f'{ckpt_converter}/config.json', 
            device=self.device
        )
        self.tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')
        
        # ì–¸ì–´ë³„ TTS ëª¨ë¸ ë¡œë“œ
        self.tts_models = {}
        for lang, path in self.language_models.items():
            if os.path.exists(path):
                tts = BaseSpeakerTTS(f'{path}/config.json', device=self.device)
                tts.load_ckpt(f'{path}/checkpoint.pth')
                self.tts_models[lang] = tts
                print(f"{lang} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def clone_voice_multilingual(self, reference_audio, texts_by_language, output_dir="outputs"):
        """ë‹¤êµ­ì–´ ìŒì„± ë³µì œ"""
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ì°¸ì¡° í™”ì ì„ë² ë”© ì¶”ì¶œ
        target_se, _ = se_extractor.get_se(
            reference_audio, 
            self.tone_color_converter, 
            vad=True
        )
        
        results = {}
        
        for language, text in texts_by_language.items():
            if language not in self.tts_models:
                print(f"ì–¸ì–´ {language} ì§€ì›ë˜ì§€ ì•ŠìŒ")
                continue
            
            try:
                # ì¤‘ê°„ ìŒì„± ìƒì„±
                temp_path = f'temp_{language}.wav'
                self.tts_models[language].tts(
                    text,
                    temp_path,
                    speaker='default',
                    language=language,
                    speed=1.0
                )
                
                # í†¤ ì»¬ëŸ¬ ë³€í™˜
                output_path = f'{output_dir}/cloned_{language}.wav'
                self.tone_color_converter.convert(
                    audio_src_path=temp_path,
                    src_se=self.tts_models[language].hps.data.spk2id['default'],
                    tgt_se=target_se,
                    output_path=output_path
                )
                
                results[language] = output_path
                print(f"{language} ìŒì„± ìƒì„± ì™„ë£Œ: {output_path}")
                
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                os.remove(temp_path)
                
            except Exception as e:
                print(f"{language} ìŒì„± ìƒì„± ì‹¤íŒ¨: {e}")
        
        return results

def multilingual_demo():
    """ë‹¤êµ­ì–´ ìŒì„± ë³µì œ ë°ëª¨"""
    
    cloner = MultilingualVoiceCloner()
    
    # ì°¸ì¡° ìŒì„± (ì˜ì–´ í™”ì)
    reference_audio = "resources/reference_speaker.wav"
    
    # ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸
    multilingual_texts = {
        'EN': "Hello, this is a demonstration of OpenVoice multilingual capabilities.",
        'ES': "Hola, esta es una demostraciÃ³n de las capacidades multilingÃ¼es de OpenVoice.",
        'FR': "Bonjour, ceci est une dÃ©monstration des capacitÃ©s multilingues d'OpenVoice.",
        'ZH': "ä½ å¥½ï¼Œè¿™æ˜¯OpenVoiceå¤šè¯­è¨€åŠŸèƒ½çš„æ¼”ç¤ºã€‚",
        'JP': "ã“ã‚“ã«ã¡ã¯ã€ã“ã‚Œã¯OpenVoiceã®å¤šè¨€èªæ©Ÿèƒ½ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚",
        'KR': "ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ OpenVoice ë‹¤êµ­ì–´ ê¸°ëŠ¥ì˜ ì‹œì—°ì…ë‹ˆë‹¤."
    }
    
    # ë‹¤êµ­ì–´ ìŒì„± ìƒì„±
    results = cloner.clone_voice_multilingual(reference_audio, multilingual_texts)
    
    print("\n=== ë‹¤êµ­ì–´ ìŒì„± ìƒì„± ê²°ê³¼ ===")
    for lang, path in results.items():
        print(f"{lang}: {path}")

if __name__ == "__main__":
    multilingual_demo()
```

### 3. ê°ì • ë° ìŠ¤íƒ€ì¼ ì œì–´

```python
# emotion_style_control.py
import torch
import numpy as np
from openvoice import se_extractor
from openvoice.api import BaseSpeakerTTS, ToneColorConverter

class EmotionalVoiceController:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.setup_models()
        
        # ê°ì •ë³„ íŒŒë¼ë¯¸í„° ì„¤ì •
        self.emotion_configs = {
            'neutral': {'speed': 1.0, 'pitch_shift': 0, 'energy': 1.0},
            'happy': {'speed': 1.1, 'pitch_shift': 2, 'energy': 1.2},
            'sad': {'speed': 0.9, 'pitch_shift': -2, 'energy': 0.8},
            'angry': {'speed': 1.2, 'pitch_shift': 1, 'energy': 1.3},
            'calm': {'speed': 0.95, 'pitch_shift': -1, 'energy': 0.9},
            'excited': {'speed': 1.15, 'pitch_shift': 3, 'energy': 1.4}
        }
    
    def setup_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        ckpt_base = 'checkpoints/base_speakers/EN'
        ckpt_converter = 'checkpoints/converter'
        
        self.base_speaker_tts = BaseSpeakerTTS(
            f'{ckpt_base}/config.json', 
            device=self.device
        )
        self.base_speaker_tts.load_ckpt(f'{ckpt_base}/checkpoint.pth')
        
        self.tone_color_converter = ToneColorConverter(
            f'{ckpt_converter}/config.json', 
            device=self.device
        )
        self.tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')
    
    def generate_emotional_voice(self, text, reference_audio, emotion='neutral', output_path=None):
        """ê°ì •ì´ ë‹´ê¸´ ìŒì„± ìƒì„±"""
        
        if emotion not in self.emotion_configs:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ê°ì •: {emotion}")
        
        config = self.emotion_configs[emotion]
        
        # ì°¸ì¡° í™”ì ì„ë² ë”© ì¶”ì¶œ
        target_se, _ = se_extractor.get_se(
            reference_audio, 
            self.tone_color_converter, 
            vad=True
        )
        
        # ê°ì • íŒŒë¼ë¯¸í„° ì ìš©í•˜ì—¬ TTS ìƒì„±
        temp_path = f'temp_emotional_{emotion}.wav'
        self.base_speaker_tts.tts(
            text,
            temp_path,
            speaker='default',
            language='EN',
            speed=config['speed']
        )
        
        # ìŒë†’ì´ ì¡°ì ˆ (í›„ì²˜ë¦¬)
        if config['pitch_shift'] != 0:
            self.apply_pitch_shift(temp_path, config['pitch_shift'])
        
        # í†¤ ì»¬ëŸ¬ ë³€í™˜
        if output_path is None:
            output_path = f'emotional_voice_{emotion}.wav'
        
        self.tone_color_converter.convert(
            audio_src_path=temp_path,
            src_se=self.base_speaker_tts.hps.data.spk2id['default'],
            tgt_se=target_se,
            output_path=output_path
        )
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        os.remove(temp_path)
        
        return output_path
    
    def apply_pitch_shift(self, audio_path, semitones):
        """ìŒë†’ì´ ì¡°ì ˆ"""
        import librosa
        import soundfile as sf
        
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        y, sr = librosa.load(audio_path, sr=None)
        
        # ìŒë†’ì´ ë³€ê²½
        y_shifted = librosa.effects.pitch_shift(y, sr=sr, n_steps=semitones)
        
        # íŒŒì¼ ì €ì¥
        sf.write(audio_path, y_shifted, sr)
    
    def create_emotion_comparison(self, text, reference_audio, output_dir="emotion_comparison"):
        """ê°ì •ë³„ ìŒì„± ë¹„êµ ìƒì„±"""
        
        os.makedirs(output_dir, exist_ok=True)
        results = {}
        
        print("ê°ì •ë³„ ìŒì„± ìƒì„± ì¤‘...")
        for emotion in self.emotion_configs.keys():
            output_path = f"{output_dir}/{emotion}_voice.wav"
            
            try:
                result_path = self.generate_emotional_voice(
                    text, reference_audio, emotion, output_path
                )
                results[emotion] = result_path
                print(f"âœ… {emotion}: {result_path}")
                
            except Exception as e:
                print(f"âŒ {emotion} ìƒì„± ì‹¤íŒ¨: {e}")
        
        return results

def emotion_demo():
    """ê°ì • ì œì–´ ë°ëª¨"""
    
    controller = EmotionalVoiceController()
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì™€ ì°¸ì¡° ìŒì„±
    text = "Welcome to the world of emotional voice synthesis with OpenVoice!"
    reference_audio = "resources/reference_speaker.wav"
    
    # ê°ì •ë³„ ìŒì„± ìƒì„±
    results = controller.create_emotion_comparison(text, reference_audio)
    
    print("\n=== ê°ì •ë³„ ìŒì„± ìƒì„± ì™„ë£Œ ===")
    for emotion, path in results.items():
        config = controller.emotion_configs[emotion]
        print(f"{emotion.upper()}: {path}")
        print(f"  - ì†ë„: {config['speed']}x")
        print(f"  - ìŒë†’ì´: {config['pitch_shift']} ë°˜ìŒ")
        print(f"  - ì—ë„ˆì§€: {config['energy']}x")

if __name__ == "__main__":
    emotion_demo()
```

## ê³ ê¸‰ í™œìš© ì‚¬ë¡€

### 1. ì‹¤ì‹œê°„ ìŒì„± ë³€í™˜ ì‹œìŠ¤í…œ

```python
# real_time_voice_conversion.py
import pyaudio
import wave
import threading
import queue
import numpy as np
from openvoice.api import ToneColorConverter
from openvoice import se_extractor

class RealTimeVoiceConverter:
    def __init__(self, reference_audio_path):
        # ì˜¤ë””ì˜¤ ì„¤ì •
        self.CHUNK = 1024
        self.FORMAT = pyaudio.paInt16
        self.CHANNELS = 1
        self.RATE = 24000
        
        # í ì„¤ì •
        self.audio_queue = queue.Queue()
        self.output_queue = queue.Queue()
        
        # ëª¨ë¸ ì„¤ì •
        self.setup_converter()
        self.load_reference_speaker(reference_audio_path)
        
        # ì˜¤ë””ì˜¤ ì¸í„°í˜ì´ìŠ¤
        self.audio = pyaudio.PyAudio()
        
    def setup_converter(self):
        """ìŒì„± ë³€í™˜ ëª¨ë¸ ì„¤ì •"""
        device = "cuda" if torch.cuda.is_available() else "cpu"
        ckpt_converter = 'checkpoints/converter'
        
        self.tone_color_converter = ToneColorConverter(
            f'{ckpt_converter}/config.json', 
            device=device
        )
        self.tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')
    
    def load_reference_speaker(self, reference_path):
        """ì°¸ì¡° í™”ì ì„ë² ë”© ë¡œë“œ"""
        self.target_se, _ = se_extractor.get_se(
            reference_path, 
            self.tone_color_converter, 
            vad=True
        )
        print(f"ì°¸ì¡° í™”ì ë¡œë“œ ì™„ë£Œ: {reference_path}")
    
    def audio_input_thread(self):
        """ì˜¤ë””ì˜¤ ì…ë ¥ ìŠ¤ë ˆë“œ"""
        stream = self.audio.open(
            format=self.FORMAT,
            channels=self.CHANNELS,
            rate=self.RATE,
            input=True,
            frames_per_buffer=self.CHUNK
        )
        
        print("ğŸ¤ ìŒì„± ì…ë ¥ ì‹œì‘...")
        
        while self.recording:
            try:
                data = stream.read(self.CHUNK, exception_on_overflow=False)
                self.audio_queue.put(data)
            except Exception as e:
                print(f"ì…ë ¥ ì˜¤ë¥˜: {e}")
                break
        
        stream.stop_stream()
        stream.close()
    
    def audio_processing_thread(self):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œ"""
        audio_buffer = []
        buffer_size = self.RATE * 2  # 2ì´ˆ ë²„í¼
        
        while self.recording:
            try:
                # ì˜¤ë””ì˜¤ ë°ì´í„° ìˆ˜ì§‘
                data = self.audio_queue.get(timeout=1)
                audio_array = np.frombuffer(data, dtype=np.int16)
                audio_buffer.extend(audio_array)
                
                # ë²„í¼ê°€ ì¶©ë¶„íˆ ì°¼ì„ ë•Œ ì²˜ë¦¬
                if len(audio_buffer) >= buffer_size:
                    # ìŒì„± ë³€í™˜ ì²˜ë¦¬
                    converted_audio = self.process_audio_chunk(
                        np.array(audio_buffer[:buffer_size])
                    )
                    
                    self.output_queue.put(converted_audio)
                    
                    # ë²„í¼ ìŠ¬ë¼ì´ë”© (50% ì˜¤ë²„ë©)
                    audio_buffer = audio_buffer[buffer_size//2:]
                    
            except queue.Empty:
                continue
            except Exception as e:
                print(f"ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    def process_audio_chunk(self, audio_chunk):
        """ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬"""
        try:
            # ì„ì‹œ íŒŒì¼ ì €ì¥
            temp_input = "temp_input.wav"
            temp_output = "temp_output.wav"
            
            # WAV íŒŒì¼ë¡œ ì €ì¥
            with wave.open(temp_input, 'wb') as wf:
                wf.setnchannels(self.CHANNELS)
                wf.setsampwidth(self.audio.get_sample_size(self.FORMAT))
                wf.setframerate(self.RATE)
                wf.writeframes(audio_chunk.tobytes())
            
            # ìŒì„± ë³€í™˜ (ë¹ ë¥¸ ëª¨ë“œ)
            self.tone_color_converter.convert(
                audio_src_path=temp_input,
                src_se=0,  # ê¸°ë³¸ í™”ì
                tgt_se=self.target_se,
                output_path=temp_output,
                fast_mode=True  # ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¹ ë¥¸ ëª¨ë“œ
            )
            
            # ë³€í™˜ëœ ì˜¤ë””ì˜¤ ë¡œë“œ
            with wave.open(temp_output, 'rb') as wf:
                converted_data = wf.readframes(wf.getnframes())
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.remove(temp_input)
            os.remove(temp_output)
            
            return converted_data
            
        except Exception as e:
            print(f"ë³€í™˜ ì˜¤ë¥˜: {e}")
            return audio_chunk.tobytes()
    
    def audio_output_thread(self):
        """ì˜¤ë””ì˜¤ ì¶œë ¥ ìŠ¤ë ˆë“œ"""
        stream = self.audio.open(
            format=self.FORMAT,
            channels=self.CHANNELS,
            rate=self.RATE,
            output=True,
            frames_per_buffer=self.CHUNK
        )
        
        print("ğŸ”Š ìŒì„± ì¶œë ¥ ì‹œì‘...")
        
        while self.recording:
            try:
                converted_data = self.output_queue.get(timeout=1)
                stream.write(converted_data)
            except queue.Empty:
                continue
            except Exception as e:
                print(f"ì¶œë ¥ ì˜¤ë¥˜: {e}")
        
        stream.stop_stream()
        stream.close()
    
    def start_conversion(self, duration=None):
        """ì‹¤ì‹œê°„ ë³€í™˜ ì‹œì‘"""
        self.recording = True
        
        # ìŠ¤ë ˆë“œ ì‹œì‘
        input_thread = threading.Thread(target=self.audio_input_thread)
        processing_thread = threading.Thread(target=self.audio_processing_thread)
        output_thread = threading.Thread(target=self.audio_output_thread)
        
        input_thread.start()
        processing_thread.start()
        output_thread.start()
        
        print("ğŸš€ ì‹¤ì‹œê°„ ìŒì„± ë³€í™˜ ì‹œì‘!")
        print("Ctrl+Cë¡œ ì¢…ë£Œ...")
        
        try:
            if duration:
                time.sleep(duration)
            else:
                input("Enterë¥¼ ëˆŒëŸ¬ ì¢…ë£Œ...")
        except KeyboardInterrupt:
            pass
        
        # ë³€í™˜ ì¤‘ì§€
        self.recording = False
        
        input_thread.join()
        processing_thread.join()
        output_thread.join()
        
        self.audio.terminate()
        print("ì‹¤ì‹œê°„ ë³€í™˜ ì¢…ë£Œ")

def real_time_demo():
    """ì‹¤ì‹œê°„ ìŒì„± ë³€í™˜ ë°ëª¨"""
    reference_audio = "resources/target_speaker.wav"
    
    converter = RealTimeVoiceConverter(reference_audio)
    converter.start_conversion(duration=30)  # 30ì´ˆ ë™ì•ˆ ì‹¤í–‰

if __name__ == "__main__":
    real_time_demo()
```

### 2. ë°°ì¹˜ ìŒì„± ì²˜ë¦¬ ì‹œìŠ¤í…œ

```python
# batch_voice_processing.py
import os
import json
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import pandas as pd

class BatchVoiceProcessor:
    def __init__(self, max_workers=4):
        self.max_workers = max_workers
        self.setup_logging()
        self.setup_models()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('batch_processing.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # ì–¸ì–´ë³„ ëª¨ë¸ ì„¤ì •
        self.language_models = {
            'EN': 'checkpoints/base_speakers/EN',
            'KR': 'checkpoints/base_speakers/KR',
            'ZH': 'checkpoints/base_speakers/ZH'
        }
        
        # ì»¨ë²„í„° ëª¨ë¸
        ckpt_converter = 'checkpoints/converter'
        self.tone_color_converter = ToneColorConverter(
            f'{ckpt_converter}/config.json', 
            device=device
        )
        self.tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')
        
        # TTS ëª¨ë¸ë“¤
        self.tts_models = {}
        for lang, path in self.language_models.items():
            if os.path.exists(path):
                tts = BaseSpeakerTTS(f'{path}/config.json', device=device)
                tts.load_ckpt(f'{path}/checkpoint.pth')
                self.tts_models[lang] = tts
    
    def process_single_item(self, item):
        """ë‹¨ì¼ ì•„ì´í…œ ì²˜ë¦¬"""
        try:
            # ì‘ì—… ì •ë³´ ì¶”ì¶œ
            text = item['text']
            language = item['language']
            reference_audio = item['reference_audio']
            output_path = item['output_path']
            speaker_name = item.get('speaker_name', 'unknown')
            
            self.logger.info(f"ì²˜ë¦¬ ì‹œì‘: {speaker_name} - {language}")
            
            # ì°¸ì¡° í™”ì ì„ë² ë”© ì¶”ì¶œ
            target_se, _ = se_extractor.get_se(
                reference_audio, 
                self.tone_color_converter, 
                vad=True
            )
            
            # TTS ìƒì„±
            temp_path = f'temp_{os.path.basename(output_path)}'
            self.tts_models[language].tts(
                text,
                temp_path,
                speaker='default',
                language=language,
                speed=item.get('speed', 1.0)
            )
            
            # í†¤ ì»¬ëŸ¬ ë³€í™˜
            self.tone_color_converter.convert(
                audio_src_path=temp_path,
                src_se=self.tts_models[language].hps.data.spk2id['default'],
                tgt_se=target_se,
                output_path=output_path
            )
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.remove(temp_path)
            
            self.logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {output_path}")
            
            return {
                'status': 'success',
                'output_path': output_path,
                'speaker_name': speaker_name,
                'language': language
            }
            
        except Exception as e:
            self.logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {speaker_name} - {str(e)}")
            return {
                'status': 'error',
                'error': str(e),
                'speaker_name': speaker_name,
                'language': language
            }
    
    def process_batch(self, batch_config_path, output_dir="batch_outputs"):
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰"""
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        with open(batch_config_path, 'r', encoding='utf-8') as f:
            batch_config = json.load(f)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ì‘ì—… ëª©ë¡ ì¤€ë¹„
        items = []
        for idx, item in enumerate(batch_config['items']):
            item['output_path'] = os.path.join(
                output_dir, 
                f"{item.get('speaker_name', f'speaker_{idx}')}_{item['language']}.wav"
            )
            items.append(item)
        
        # ë³‘ë ¬ ì²˜ë¦¬
        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ì‘ì—… ì œì¶œ
            future_to_item = {
                executor.submit(self.process_single_item, item): item 
                for item in items
            }
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_item):
                result = future.result()
                results.append(result)
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                completed = len(results)
                total = len(items)
                progress = (completed / total) * 100
                self.logger.info(f"ì§„í–‰ë¥ : {progress:.1f}% ({completed}/{total})")
        
        # ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_report(results, output_dir)
        
        return results
    
    def generate_report(self, results, output_dir):
        """ì²˜ë¦¬ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        # í†µê³„ ê³„ì‚°
        total = len(results)
        successful = len([r for r in results if r['status'] == 'success'])
        failed = total - successful
        
        # ìƒì„¸ ë¦¬í¬íŠ¸
        report = {
            'summary': {
                'total_items': total,
                'successful': successful,
                'failed': failed,
                'success_rate': f"{(successful/total)*100:.1f}%"
            },
            'details': results
        }
        
        # JSON ë¦¬í¬íŠ¸ ì €ì¥
        report_path = os.path.join(output_dir, 'processing_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # CSV ìš”ì•½ ì €ì¥
        df = pd.DataFrame(results)
        csv_path = os.path.join(output_dir, 'processing_summary.csv')
        df.to_csv(csv_path, index=False, encoding='utf-8')
        
        self.logger.info(f"ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_path}")
        
        # ì½˜ì†” ìš”ì•½ ì¶œë ¥
        print(f"\n=== ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ===")
        print(f"ì´ ì‘ì—…: {total}ê°œ")
        print(f"ì„±ê³µ: {successful}ê°œ")
        print(f"ì‹¤íŒ¨: {failed}ê°œ")
        print(f"ì„±ê³µë¥ : {(successful/total)*100:.1f}%")

def create_batch_config():
    """ë°°ì¹˜ ì„¤ì • íŒŒì¼ ìƒì„± ì˜ˆì œ"""
    
    batch_config = {
        "description": "ë‹¤êµ­ì–´ ìŒì„± ë°ì´í„°ì…‹ ìƒì„±",
        "items": [
            {
                "speaker_name": "speaker_001",
                "text": "Welcome to OpenVoice batch processing system.",
                "language": "EN",
                "reference_audio": "references/speaker_001.wav",
                "speed": 1.0
            },
            {
                "speaker_name": "speaker_001", 
                "text": "OpenVoice ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤.",
                "language": "KR",
                "reference_audio": "references/speaker_001.wav",
                "speed": 1.0
            },
            {
                "speaker_name": "speaker_002",
                "text": "This is a demonstration of multilingual voice cloning.",
                "language": "EN", 
                "reference_audio": "references/speaker_002.wav",
                "speed": 1.1
            }
        ]
    }
    
    with open('batch_config.json', 'w', encoding='utf-8') as f:
        json.dump(batch_config, f, indent=2, ensure_ascii=False)
    
    print("ë°°ì¹˜ ì„¤ì • íŒŒì¼ ìƒì„±ë¨: batch_config.json")

def batch_demo():
    """ë°°ì¹˜ ì²˜ë¦¬ ë°ëª¨"""
    
    # ì„¤ì • íŒŒì¼ ìƒì„±
    create_batch_config()
    
    # ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = BatchVoiceProcessor(max_workers=2)
    
    # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    results = processor.process_batch('batch_config.json')
    
    print("ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")

if __name__ == "__main__":
    batch_demo()
```

### 3. ì›¹ API ì„œë¹„ìŠ¤

```python
# web_api_service.py
from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
from pydantic import BaseModel
import uvicorn
import asyncio
import aiofiles
import uuid
import os
from typing import Optional

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="OpenVoice API",
    description="ìŒì„± ë³µì œ ë° ë‹¤êµ­ì–´ TTS API",
    version="2.0.0"
)

# ìš”ì²­ ëª¨ë¸
class VoiceCloneRequest(BaseModel):
    text: str
    language: str = "EN"
    speed: float = 1.0
    emotion: str = "neutral"

class TTSRequest(BaseModel):
    text: str
    language: str = "EN"
    speaker: str = "default"
    speed: float = 1.0

# ì „ì—­ ë³€ìˆ˜
voice_processor = None
active_jobs = {}

@app.on_event("startup")
async def startup_event():
    """ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global voice_processor
    
    print("OpenVoice ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    # ëª¨ë¸ ì´ˆê¸°í™” (ë³„ë„ í´ë˜ìŠ¤ë¡œ êµ¬í˜„)
    voice_processor = VoiceProcessorAPI()
    await voice_processor.initialize()
    
    print("OpenVoice API ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")

class VoiceProcessorAPI:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    async def initialize(self):
        """ë¹„ë™ê¸° ëª¨ë¸ ì´ˆê¸°í™”"""
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, self._load_models)
    
    def _load_models(self):
        """ëª¨ë¸ ë¡œë“œ (ë™ê¸° ì‘ì—…)"""
        # TTS ëª¨ë¸ë“¤
        self.tts_models = {}
        languages = ['EN', 'KR', 'ZH', 'JP']
        
        for lang in languages:
            model_path = f'checkpoints/base_speakers/{lang}'
            if os.path.exists(model_path):
                tts = BaseSpeakerTTS(f'{model_path}/config.json', device=self.device)
                tts.load_ckpt(f'{model_path}/checkpoint.pth')
                self.tts_models[lang] = tts
        
        # ì»¨ë²„í„° ëª¨ë¸
        ckpt_converter = 'checkpoints/converter'
        self.tone_color_converter = ToneColorConverter(
            f'{ckpt_converter}/config.json', 
            device=self.device
        )
        self.tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')
    
    async def clone_voice(self, text: str, reference_audio_path: str, 
                         language: str = "EN", speed: float = 1.0):
        """ìŒì„± ë³µì œ (ë¹„ë™ê¸°)"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None, self._clone_voice_sync, text, reference_audio_path, language, speed
        )
    
    def _clone_voice_sync(self, text: str, reference_audio_path: str, 
                         language: str, speed: float):
        """ìŒì„± ë³µì œ (ë™ê¸° ì‘ì—…)"""
        try:
            # ì°¸ì¡° í™”ì ì„ë² ë”©
            target_se, _ = se_extractor.get_se(
                reference_audio_path, 
                self.tone_color_converter, 
                vad=True
            )
            
            # TTS ìƒì„±
            temp_path = f'temp_{uuid.uuid4()}.wav'
            self.tts_models[language].tts(
                text, temp_path, speaker='default', 
                language=language, speed=speed
            )
            
            # í†¤ ì»¬ëŸ¬ ë³€í™˜
            output_path = f'outputs/{uuid.uuid4()}.wav'
            os.makedirs('outputs', exist_ok=True)
            
            self.tone_color_converter.convert(
                audio_src_path=temp_path,
                src_se=self.tts_models[language].hps.data.spk2id['default'],
                tgt_se=target_se,
                output_path=output_path
            )
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.remove(temp_path)
            
            return output_path
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

# API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.get("/")
async def root():
    """API ì •ë³´"""
    return {
        "service": "OpenVoice API",
        "version": "2.0.0",
        "status": "running",
        "supported_languages": ["EN", "KR", "ZH", "JP"],
        "features": ["voice_cloning", "multilingual_tts", "emotion_control"]
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "models_loaded": len(voice_processor.tts_models) if voice_processor else 0,
        "device": voice_processor.device if voice_processor else "unknown"
    }

@app.post("/voice/clone")
async def clone_voice(
    request: VoiceCloneRequest,
    reference_audio: UploadFile = File(...),
    background_tasks: BackgroundTasks = None
):
    """ìŒì„± ë³µì œ API"""
    
    if not voice_processor:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    if request.language not in voice_processor.tts_models:
        raise HTTPException(status_code=400, detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì–¸ì–´: {request.language}")
    
    # ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥
    reference_path = f"temp_references/{uuid.uuid4()}_{reference_audio.filename}"
    os.makedirs("temp_references", exist_ok=True)
    
    async with aiofiles.open(reference_path, 'wb') as f:
        content = await reference_audio.read()
        await f.write(content)
    
    try:
        # ìŒì„± ë³µì œ ì‹¤í–‰
        output_path = await voice_processor.clone_voice(
            text=request.text,
            reference_audio_path=reference_path,
            language=request.language,
            speed=request.speed
        )
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if background_tasks:
            background_tasks.add_task(cleanup_file, reference_path)
        
        return FileResponse(
            output_path,
            media_type="audio/wav",
            filename=f"cloned_voice_{uuid.uuid4()}.wav"
        )
        
    except Exception as e:
        # ì˜¤ë¥˜ ì‹œ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(reference_path):
            os.remove(reference_path)
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/tts/generate")
async def generate_tts(request: TTSRequest):
    """ê¸°ë³¸ TTS ìƒì„±"""
    
    if not voice_processor:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    if request.language not in voice_processor.tts_models:
        raise HTTPException(status_code=400, detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì–¸ì–´: {request.language}")
    
    try:
        # TTS ìƒì„±
        output_path = f'outputs/{uuid.uuid4()}.wav'
        os.makedirs('outputs', exist_ok=True)
        
        voice_processor.tts_models[request.language].tts(
            request.text,
            output_path,
            speaker=request.speaker,
            language=request.language,
            speed=request.speed
        )
        
        return FileResponse(
            output_path,
            media_type="audio/wav",
            filename=f"tts_output_{uuid.uuid4()}.wav"
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/languages")
async def get_supported_languages():
    """ì§€ì› ì–¸ì–´ ëª©ë¡"""
    if not voice_processor:
        return {"languages": []}
    
    return {
        "languages": list(voice_processor.tts_models.keys()),
        "total": len(voice_processor.tts_models)
    }

async def cleanup_file(file_path: str):
    """íŒŒì¼ ì •ë¦¬ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)"""
    await asyncio.sleep(1)  # ì ì‹œ ëŒ€ê¸°
    if os.path.exists(file_path):
        os.remove(file_path)

# ì„œë²„ ì‹¤í–‰ í•¨ìˆ˜
def start_server(host="0.0.0.0", port=8000):
    """API ì„œë²„ ì‹œì‘"""
    uvicorn.run(
        app, 
        host=host, 
        port=port,
        log_level="info"
    )

if __name__ == "__main__":
    start_server()
```

## í”„ë¡œë•ì…˜ ë°°í¬

### Docker ì»¨í…Œì´ë„ˆí™”

```dockerfile
# Dockerfile
FROM nvidia/cuda:11.8-devel-ubuntu20.04

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    python3.9 \
    python3.9-pip \
    python3.9-dev \
    git \
    wget \
    curl \
    ffmpeg \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

# ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
RUN ln -s /usr/bin/python3.9 /usr/bin/python

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# PyTorch ì„¤ì¹˜ (CUDA ì§€ì›)
RUN pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118

# OpenVoice ì½”ë“œ ë³µì‚¬
COPY . .

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
RUN mkdir -p checkpoints && \
    wget https://myshell-public-repo-hosting.s3.amazonaws.com/openvoice/checkpoints_v2.zip && \
    unzip checkpoints_v2.zip -d checkpoints/ && \
    rm checkpoints_v2.zip

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# ì„œë¹„ìŠ¤ ì‹œì‘
CMD ["python", "web_api_service.py"]
```

### Docker Compose ì„¤ì •

```yaml
# docker-compose.yml
version: '3.8'

services:
  openvoice-api:
    build: .
    container_name: openvoice-service
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ./outputs:/app/outputs
      - ./references:/app/references
      - ./logs:/app/logs
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - OPENVOICE_LOG_LEVEL=INFO
      - OPENVOICE_MAX_WORKERS=4
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  nginx:
    image: nginx:alpine
    container_name: openvoice-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - openvoice-api

  redis:
    image: redis:alpine
    container_name: openvoice-redis
    restart: unless-stopped
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

volumes:
  redis_data:
```

### ì„±ëŠ¥ ìµœì í™”

```python
# performance_optimization.py
import torch
import time
import psutil
import GPUtil
from concurrent.futures import ThreadPoolExecutor

class OpenVoiceOptimizer:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.setup_optimizations()
    
    def setup_optimizations(self):
        """ì„±ëŠ¥ ìµœì í™” ì„¤ì •"""
        
        if torch.cuda.is_available():
            # CUDA ìµœì í™”
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.enabled = True
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            torch.cuda.empty_cache()
            
            # í˜¼í•© ì •ë°€ë„ ì‚¬ìš©
            self.use_amp = True
            self.scaler = torch.cuda.amp.GradScaler()
            
            print(f"CUDA ìµœì í™” í™œì„±í™”: {torch.cuda.get_device_name(0)}")
        else:
            self.use_amp = False
            print("CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    def benchmark_model_performance(self, model, test_texts, iterations=10):
        """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        
        print(f"ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ ë°˜ë³µ)")
        
        times = []
        memory_usage = []
        
        for i in range(iterations):
            start_time = time.time()
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                mem_before = torch.cuda.memory_allocated()
            
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            for text in test_texts:
                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        _ = model.generate_speech(text)
                else:
                    _ = model.generate_speech(text)
            
            # ì‹¤í–‰ ì‹œê°„ ê¸°ë¡
            end_time = time.time()
            execution_time = end_time - start_time
            times.append(execution_time)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                mem_after = torch.cuda.memory_allocated()
                memory_usage.append(mem_after - mem_before)
            
            print(f"ë°˜ë³µ {i+1}: {execution_time:.3f}ì´ˆ")
        
        # í†µê³„ ê³„ì‚°
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        if memory_usage:
            avg_memory = sum(memory_usage) / len(memory_usage) / 1024**2  # MB
        else:
            avg_memory = 0
        
        print(f"\n=== ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ===")
        print(f"í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_time:.3f}ì´ˆ")
        print(f"ìµœì†Œ ì‹¤í–‰ ì‹œê°„: {min_time:.3f}ì´ˆ")
        print(f"ìµœëŒ€ ì‹¤í–‰ ì‹œê°„: {max_time:.3f}ì´ˆ")
        print(f"í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {avg_memory:.1f}MB")
        
        return {
            'avg_time': avg_time,
            'min_time': min_time,
            'max_time': max_time,
            'avg_memory_mb': avg_memory
        }
    
    def optimize_for_batch_processing(self, batch_size=8):
        """ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”"""
        
        optimizations = {
            'torch_settings': {
                'torch.set_num_threads': min(8, psutil.cpu_count()),
                'torch.set_grad_enabled': False,  # ì¶”ë¡  ì „ìš©
            },
            'memory_settings': {
                'max_split_size_mb': 128,
                'memory_fraction': 0.8
            },
            'batch_settings': {
                'optimal_batch_size': batch_size,
                'prefetch_factor': 2
            }
        }
        
        # ì„¤ì • ì ìš©
        torch.set_num_threads(optimizations['torch_settings']['torch.set_num_threads'])
        torch.set_grad_enabled(optimizations['torch_settings']['torch.set_grad_enabled'])
        
        if torch.cuda.is_available():
            torch.cuda.set_per_process_memory_fraction(
                optimizations['memory_settings']['memory_fraction']
            )
        
        return optimizations
    
    def monitor_system_resources(self, duration=60):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""
        
        print(f"ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ ({duration}ì´ˆ)")
        
        start_time = time.time()
        cpu_usage = []
        memory_usage = []
        gpu_usage = []
        
        while time.time() - start_time < duration:
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent(interval=1)
            cpu_usage.append(cpu_percent)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            memory = psutil.virtual_memory()
            memory_usage.append(memory.percent)
            
            # GPU ì‚¬ìš©ë¥ 
            if torch.cuda.is_available():
                try:
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu_usage.append(gpus[0].load * 100)
                except:
                    gpu_usage.append(0)
            
            time.sleep(1)
        
        # í†µê³„ ì¶œë ¥
        print(f"\n=== ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ({duration}ì´ˆ í‰ê· ) ===")
        print(f"CPU: {sum(cpu_usage)/len(cpu_usage):.1f}%")
        print(f"ë©”ëª¨ë¦¬: {sum(memory_usage)/len(memory_usage):.1f}%")
        if gpu_usage:
            print(f"GPU: {sum(gpu_usage)/len(gpu_usage):.1f}%")
        
        return {
            'cpu_avg': sum(cpu_usage)/len(cpu_usage),
            'memory_avg': sum(memory_usage)/len(memory_usage),
            'gpu_avg': sum(gpu_usage)/len(gpu_usage) if gpu_usage else 0
        }

def optimization_demo():
    """ìµœì í™” ë°ëª¨"""
    
    optimizer = OpenVoiceOptimizer()
    
    # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
    batch_optimizations = optimizer.optimize_for_batch_processing(batch_size=4)
    print("ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ì ìš© ì™„ë£Œ")
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    test_texts = [
        "Hello, this is a performance test.",
        "OpenVoice optimization benchmark.",
        "Testing voice synthesis speed."
    ]
    
    # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì‹¤ì œ ëª¨ë¸ì´ ìˆì„ ë•Œ)
    # benchmark_results = optimizer.benchmark_model_performance(model, test_texts)
    
    # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
    resource_stats = optimizer.monitor_system_resources(duration=30)
    
    print("ìµœì í™” ë°ëª¨ ì™„ë£Œ!")

if __name__ == "__main__":
    optimization_demo()
```

## ë¹„ì¦ˆë‹ˆìŠ¤ í™œìš© ì‚¬ë¡€

### 1. ë‹¤êµ­ì–´ ì½˜í…ì¸  ì œì‘

```python
# content_localization.py
class ContentLocalizationPipeline:
    def __init__(self):
        self.setup_openvoice()
        self.supported_languages = {
            'EN': 'English',
            'ES': 'Spanish', 
            'FR': 'French',
            'ZH': 'Chinese',
            'JP': 'Japanese',
            'KR': 'Korean'
        }
    
    def localize_podcast_series(self, original_audio_path, script_translations, host_voice_sample):
        """íŒŸìºìŠ¤íŠ¸ ì‹œë¦¬ì¦ˆ ë‹¤êµ­ì–´ í˜„ì§€í™”"""
        
        results = {}
        
        for lang_code, translated_script in script_translations.items():
            if lang_code not in self.supported_languages:
                continue
            
            print(f"í˜„ì§€í™” ì¤‘: {self.supported_languages[lang_code]}")
            
            # ì±•í„°ë³„ ë¶„í•  ì²˜ë¦¬
            chapters = self.split_script_into_chapters(translated_script)
            chapter_audios = []
            
            for i, chapter in enumerate(chapters):
                chapter_audio = self.generate_chapter_audio(
                    text=chapter,
                    language=lang_code,
                    host_voice=host_voice_sample,
                    chapter_number=i+1
                )
                chapter_audios.append(chapter_audio)
            
            # ì±•í„° ê²°í•©
            final_audio = self.combine_chapters(chapter_audios)
            
            # í›„ì²˜ë¦¬ (ìŒì§ˆ í–¥ìƒ, ë…¸ë©€ë¼ì´ì œì´ì…˜)
            enhanced_audio = self.enhance_audio_quality(final_audio)
            
            output_path = f"localized_podcast_{lang_code}.wav"
            self.save_audio(enhanced_audio, output_path)
            
            results[lang_code] = {
                'language': self.supported_languages[lang_code],
                'output_path': output_path,
                'duration': self.get_audio_duration(output_path),
                'chapters': len(chapters)
            }
        
        return results

# ì‹¤ì œ í™œìš© ì˜ˆì œ
localization = ContentLocalizationPipeline()

# ì›ë³¸ íŒŸìºìŠ¤íŠ¸ì™€ ë²ˆì—­ë³¸
script_translations = {
    'EN': "Welcome to our tech podcast. Today we'll discuss AI innovations...",
    'KR': "ì €í¬ ê¸°ìˆ  íŒŸìºìŠ¤íŠ¸ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤. ì˜¤ëŠ˜ì€ AI í˜ì‹ ì— ëŒ€í•´...",
    'ZH': "æ¬¢è¿æ”¶å¬æˆ‘ä»¬çš„æŠ€æœ¯æ’­å®¢ã€‚ä»Šå¤©æˆ‘ä»¬å°†è®¨è®ºäººå·¥æ™ºèƒ½åˆ›æ–°...",
}

# í˜¸ìŠ¤íŠ¸ ìŒì„± ìƒ˜í”Œ
host_voice = "samples/host_voice.wav"

# ë‹¤êµ­ì–´ í˜„ì§€í™” ì‹¤í–‰
results = localization.localize_podcast_series(
    original_audio_path="original_podcast.wav",
    script_translations=script_translations,
    host_voice_sample=host_voice
)

print("ì½˜í…ì¸  í˜„ì§€í™” ì™„ë£Œ:")
for lang, info in results.items():
    print(f"- {info['language']}: {info['output_path']} ({info['duration']}ì´ˆ)")
```

### 2. êµìœ¡ ì½˜í…ì¸  ê°œì¸í™”

```python
# personalized_education.py
class PersonalizedEducationVoice:
    def __init__(self):
        self.student_profiles = {}
        self.teacher_voices = {}
        self.setup_voice_models()
    
    def create_student_profile(self, student_id, preferences):
        """í•™ìƒë³„ ê°œì¸í™” í”„ë¡œí•„ ìƒì„±"""
        
        profile = {
            'student_id': student_id,
            'preferred_language': preferences.get('language', 'EN'),
            'speaking_speed': preferences.get('speed', 1.0),
            'voice_style': preferences.get('style', 'neutral'),
            'difficulty_level': preferences.get('level', 'intermediate'),
            'learning_disabilities': preferences.get('disabilities', [])
        }
        
        # í•™ìŠµ ì¥ì•  ê³ ë ¤ ì¡°ì •
        if 'dyslexia' in profile['learning_disabilities']:
            profile['speaking_speed'] *= 0.8  # ëŠë¦¬ê²Œ
        
        if 'attention_deficit' in profile['learning_disabilities']:
            profile['voice_style'] = 'engaging'  # ë” ìƒë™ê° ìˆê²Œ
        
        self.student_profiles[student_id] = profile
        return profile
    
    def generate_personalized_lesson(self, lesson_content, student_id, teacher_voice_sample):
        """ê°œì¸í™”ëœ ìˆ˜ì—… ìŒì„± ìƒì„±"""
        
        if student_id not in self.student_profiles:
            raise ValueError(f"í•™ìƒ í”„ë¡œí•„ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {student_id}")
        
        profile = self.student_profiles[student_id]
        
        # ê°œì¸í™” íŒŒë¼ë¯¸í„° ì ìš©
        personalized_audio = self.voice_cloner.clone_voice_with_params(
            text=lesson_content,
            reference_voice=teacher_voice_sample,
            language=profile['preferred_language'],
            speed=profile['speaking_speed'],
            style=profile['voice_style']
        )
        
        # ì ‘ê·¼ì„± í–¥ìƒ ì²˜ë¦¬
        if 'hearing_impaired' in profile['learning_disabilities']:
            personalized_audio = self.enhance_for_hearing_impaired(personalized_audio)
        
        output_path = f"lessons/student_{student_id}_lesson.wav"
        self.save_personalized_lesson(personalized_audio, output_path, profile)
        
        return {
            'audio_path': output_path,
            'student_id': student_id,
            'customizations': profile,
            'duration': self.get_audio_duration(output_path)
        }

# êµìœ¡ ê¸°ê´€ í™œìš© ì‚¬ë¡€
education_system = PersonalizedEducationVoice()

# í•™ìƒë³„ í”„ë¡œí•„ ìƒì„±
students = [
    {
        'id': 'student_001',
        'preferences': {
            'language': 'EN',
            'speed': 0.9,
            'style': 'calm',
            'level': 'beginner',
            'disabilities': ['dyslexia']
        }
    },
    {
        'id': 'student_002', 
        'preferences': {
            'language': 'KR',
            'speed': 1.1,
            'style': 'enthusiastic',
            'level': 'advanced',
            'disabilities': []
        }
    }
]

for student in students:
    education_system.create_student_profile(student['id'], student['preferences'])

# ê°œì¸í™”ëœ ìˆ˜ì—… ìƒì„±
lesson_text = "Today we will learn about photosynthesis in plants..."
teacher_voice = "voices/biology_teacher.wav"

for student in students:
    result = education_system.generate_personalized_lesson(
        lesson_content=lesson_text,
        student_id=student['id'],
        teacher_voice_sample=teacher_voice
    )
    print(f"ê°œì¸í™” ìˆ˜ì—… ìƒì„±: {result['audio_path']}")
```

## ê²°ë¡ 

OpenVoiceëŠ” ìŒì„± ê¸°ìˆ ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•˜ëŠ” í˜ì‹ ì ì¸ ë„êµ¬ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œë¥¼ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì—­ëŸ‰ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### í•µì‹¬ ì„±ê³¼

1. **ì •í™•í•œ ìŒì„± ë³µì œ**: ë‹¨ ëª‡ ì´ˆì˜ ì°¸ì¡° ìŒì„±ìœ¼ë¡œë„ ê³ í’ˆì§ˆ ë³µì œ
2. **ë‹¤êµ­ì–´ ì§€ì›**: 6ê°œ ì–¸ì–´ì˜ ë„¤ì´í‹°ë¸Œ ì§€ì›ê³¼ ì œë¡œìƒ· í™•ì¥
3. **ìœ ì—°í•œ ì œì–´**: ê°ì •, ì†ë„, ìŒë†’ì´ ë“± ì„¸ë°€í•œ ì¡°ì ˆ
4. **í”„ë¡œë•ì…˜ ì¤€ë¹„**: API ì„œë¹„ìŠ¤, ë°°ì¹˜ ì²˜ë¦¬, ì‹¤ì‹œê°„ ë³€í™˜

### ê¸°ìˆ ì  í˜ì‹ 

- **MIT ê¸°ìˆ ë ¥**: ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ì—°êµ¬ ê¸°ê´€ ê¸°ìˆ 
- **ì˜¤í”ˆì†ŒìŠ¤**: ì™„ì „í•œ íˆ¬ëª…ì„±ê³¼ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
- **ìƒì—…ì  ì´ìš©**: MIT ë¼ì´ì„ ìŠ¤ë¡œ ì œì•½ ì—†ëŠ” í™œìš©
- **í™•ì¥ì„±**: í´ë¼ìš°ë“œ ë°°í¬ì™€ ì—”í„°í”„ë¼ì´ì¦ˆ í†µí•©

### ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜

ì´ ê¸°ìˆ ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íšŒë¥¼ ì°½ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **ì½˜í…ì¸  ì œì‘**: ë‹¤êµ­ì–´ íŒŸìºìŠ¤íŠ¸, ì˜¤ë””ì˜¤ë¶, ê´‘ê³ 
- **êµìœ¡ ì„œë¹„ìŠ¤**: ê°œì¸í™”ëœ í•™ìŠµ ì½˜í…ì¸ 
- **ì ‘ê·¼ì„± í–¥ìƒ**: ì‹œê° ì¥ì• ì¸ì„ ìœ„í•œ ìŒì„± ì„œë¹„ìŠ¤
- **ê³ ê° ì„œë¹„ìŠ¤**: ë¸Œëœë“œ ëª©ì†Œë¦¬ë¡œ í†µì¼ëœ AI ìƒë‹´ì›

### ë¯¸ë˜ ì „ë§

```python
future_developments = {
    "real_time_optimization": "ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥ í–¥ìƒ",
    "emotional_intelligence": "ë”ìš± ì •êµí•œ ê°ì • í‘œí˜„",
    "voice_personalization": "ê°œì¸ë³„ ìŒì„± íŠ¹ì„± í•™ìŠµ",
    "multimodal_integration": "ë¹„ë””ì˜¤ì™€ ìŒì„± ë™ê¸°í™”",
    "edge_deployment": "ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ ìµœì í™”"
}
```

OpenVoiceì˜ 33.4k GitHub ìŠ¤íƒ€ëŠ” ë‹¨ìˆœí•œ ì¸ê¸°ê°€ ì•„ë‹Œ, ì‹¤ì œ ê°œë°œìì™€ ê¸°ì—…ë“¤ì´ ì¸ì •í•˜ëŠ” ê¸°ìˆ ì  ìš°ìˆ˜ì„±ì˜ ì¦ê±°ì…ë‹ˆë‹¤. [ê³µì‹ ë ˆí¬ì§€í† ë¦¬](https://github.com/myshell-ai/OpenVoice)ì—ì„œ ìµœì‹  ì—…ë°ì´íŠ¸ë¥¼ í™•ì¸í•˜ê³ , ìŒì„± ê¸°ìˆ ì˜ ë¯¸ë˜ë¥¼ ì„ ë„í•˜ëŠ” í˜ì‹ ì— ë™ì°¸í•´ë³´ì„¸ìš”!

---

**ê´€ë ¨ ë¦¬ì†ŒìŠ¤**:
- [OpenVoice GitHub](https://github.com/myshell-ai/OpenVoice)
- [ì—°êµ¬ ë…¼ë¬¸](https://arxiv.org/abs/2312.01479)
- [ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://research.myshell.ai/open-voice)
- [MyShell í”Œë«í¼](https://myshell.ai) 