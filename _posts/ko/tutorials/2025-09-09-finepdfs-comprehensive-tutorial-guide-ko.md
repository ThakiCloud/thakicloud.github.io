---
title: "FinePDFs: í—ˆê¹…í˜ì´ìŠ¤ í˜ì‹ ì  PDF ë°ì´í„°ì…‹ ì™„ì „ ê°€ì´ë“œ"
excerpt: "470ë§Œ ë¬¸ì„œì˜ FinePDFs ë°ì´í„°ì…‹ ë§ˆìŠ¤í„°í•˜ê¸°. ì¶”ì¶œ, í•„í„°ë§, í›ˆë ¨ê¹Œì§€ í¬ê´„ì ì¸ ì˜ˆì œì™€ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¡œ ì™„ë²½ í•™ìŠµ."
seo_title: "FinePDFs íŠœí† ë¦¬ì–¼: PDF ë°ì´í„°ì…‹ ì²˜ë¦¬ ì™„ì „ ê°€ì´ë“œ - Thaki Cloud"
seo_description: "FinePDFs ë°ì´í„°ì…‹ íš¨ê³¼ì  í™œìš©ë²•ì„ ì‹¤ì œ ì˜ˆì œ, ìµœì í™” íŒ, ì‹¤ë¬´ ì‘ìš©ì‚¬ë¡€ë¡œ í•™ìŠµ. AI í›ˆë ¨ê³¼ ì—°êµ¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ê°€ì´ë“œ."
date: 2025-09-09
categories:
  - tutorials
tags:
  - FinePDFs
  - í—ˆê¹…í˜ì´ìŠ¤
  - PDFì²˜ë¦¬
  - ë°ì´í„°ì…‹
  - ë¨¸ì‹ ëŸ¬ë‹
  - í…ìŠ¤íŠ¸ì¶”ì¶œ
  - ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
lang: ko
permalink: /ko/tutorials/finepdfs-comprehensive-tutorial-guide/
canonical_url: "https://thakicloud.github.io/ko/tutorials/finepdfs-comprehensive-tutorial-guide/"
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 12ë¶„

## ì„œë¡ 

FinePDFsëŠ” ëŒ€ê·œëª¨ ë¬¸ì„œ ì²˜ë¦¬ ë¶„ì•¼ì˜ í˜ì‹ ì  ë°œì „ì„ ë‚˜íƒ€ë‚´ë©°, ì´ 67.9GBì— ë‹¬í•˜ëŠ” 470ë§Œ ê°œì˜ ì¶”ì¶œëœ PDF ë¬¸ì„œë¡œ êµ¬ì„±ëœ ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ í¬ê´„ì ì¸ íŠœí† ë¦¬ì–¼ì€ ì´ í˜ì‹ ì ì¸ ë°ì´í„°ì…‹ ì‘ì—…ì˜ ëª¨ë“  ì¸¡ë©´ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

### FinePDFsì˜ íŠ¹ë³„í•œ ì 

[FinePDFs](https://huggingface.co/datasets/HuggingFaceFW/finepdfs)ëŠ” ê°€ì¥ í¬ê´„ì ì¸ PDF ê¸°ë°˜ ë°ì´í„°ì…‹ ì¤‘ í•˜ë‚˜ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ë³´ì…ë‹ˆë‹¤:

- **ëŒ€ê·œëª¨ ìŠ¤ì¼€ì¼**: 1,808ê°œ ì–¸ì–´ì— ê±¸ì¹œ 470ë§Œ ë¬¸ì„œ
- **ê³ ê¸‰ ì¶”ì¶œ**: ìµœì²¨ë‹¨ docling ë° RolmOCR ê¸°ìˆ  í™œìš©
- **ë„ë©”ì¸ í’ë¶€**: ê³¼í•™, ë²•ë¥ , ê¸°ìˆ  ì½˜í…ì¸ ì˜ ê´‘ë²”ìœ„í•œ ì»¤ë²„ë¦¬ì§€
- **ì˜¤í”ˆ ë¼ì´ì„ ìŠ¤**: ìƒì—…ì  ë° ì—°êµ¬ ìš©ë„ë¥¼ ìœ„í•œ ODC-By ë¼ì´ì„ ìŠ¤
- **í’ˆì§ˆ ì¤‘ì‹¬**: ì§„ì •í•œ ì½˜í…ì¸  ë‹¤ì–‘ì„± ë³´ì¡´ì„ ìœ„í•œ ìµœì†Œ í•„í„°ë§

## ì‚¬ì „ ìš”êµ¬ì‚¬í•­ ë° ì„¤ì •

### í™˜ê²½ ìš”êµ¬ì‚¬í•­

```bash
# Python 3.8+ ê¶Œì¥
python --version

# í•„ìˆ˜ íŒ¨í‚¤ì§€
pip install datasets transformers torch huggingface_hub
pip install pandas numpy matplotlib seaborn
pip install tqdm rich
```

### í•˜ë“œì›¨ì–´ ê¶Œì¥ì‚¬í•­

- **RAM**: ìµœì†Œ 16GB, ê¶Œì¥ 32GB+
- **ì €ì¥ê³µê°„**: ì „ì²´ ë°ì´í„°ì…‹ì„ ìœ„í•œ 100GB+ ì—¬ìœ  ê³µê°„
- **ë„¤íŠ¸ì›Œí¬**: ì´ˆê¸° ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ì•ˆì •ì ì¸ ì¸í„°ë„·
- **GPU**: ì„ íƒì‚¬í•­ì´ì§€ë§Œ ì²˜ë¦¬ë¥¼ ìœ„í•´ ê¶Œì¥

## FinePDFs ì‹œì‘í•˜ê¸°

### ê¸°ë³¸ ë°ì´í„°ì…‹ ë¡œë”©

```python
from datasets import load_dataset
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt

# ë°ì´í„°ì…‹ ë¡œë“œ (ì˜ì–´ ì„œë¸Œì…‹ë¶€í„° ì‹œì‘)
print("FinePDFs ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
dataset = load_dataset("HuggingFaceFW/finepdfs", "eng_Latn")

print(f"ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset['train'])} ë¬¸ì„œ")
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {dataset['train'].column_names}")

# ì²˜ìŒ ëª‡ ê°œ ìƒ˜í”Œ ê²€ì‚¬
for i, sample in enumerate(dataset['train'].take(3)):
    print(f"\n--- ìƒ˜í”Œ {i+1} ---")
    print(f"ì–¸ì–´: {sample['language']}")
    print(f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(sample['text'])} ë¬¸ì")
    print(f"í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {sample['text'][:200]}...")
```

### ì‚¬ìš© ê°€ëŠ¥í•œ ì–¸ì–´ íƒìƒ‰

```python
# ì–¸ì–´ ë©”íƒ€ë°ì´í„° ë¡œë“œ
all_configs = ["eng_Latn", "arb_Arab", "spa_Latn", "fra_Latn", "deu_Latn", 
               "ita_Latn", "por_Latn", "rus_Cyrl", "jpn_Jpan", "kor_Hang"]

language_stats = {}

for config in all_configs[:5]:  # ì²˜ìŒ 5ê°œ ì–¸ì–´ ìƒ˜í”Œ
    try:
        ds = load_dataset("HuggingFaceFW/finepdfs", config, split="train")
        language_stats[config] = len(ds)
        print(f"{config}: {len(ds):,} ë¬¸ì„œ")
    except Exception as e:
        print(f"{config} ë¡œë“œ ì‹¤íŒ¨: {e}")

# ì–¸ì–´ ë¶„í¬ ì‹œê°í™”
plt.figure(figsize=(12, 6))
languages = list(language_stats.keys())
counts = list(language_stats.values())

plt.bar(languages, counts)
plt.title("ì–¸ì–´ë³„ ë¬¸ì„œ ìˆ˜")
plt.xlabel("ì–¸ì–´ ì½”ë“œ")
plt.ylabel("ë¬¸ì„œ ìˆ˜")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("finepdfs_language_distribution.png", dpi=300, bbox_inches='tight')
plt.show()
```

## ê³ ê¸‰ ë°ì´í„°ì…‹ ì‘ì—…

### í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„

```python
import re
from textstat import flesch_reading_ease, flesch_kincaid_grade

def analyze_text_quality(sample_texts, sample_size=1000):
    """ë°ì´í„°ì…‹ ìƒ˜í”Œì˜ í…ìŠ¤íŠ¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¶„ì„"""
    
    metrics = {
        'avg_length': [],
        'word_count': [],
        'sentence_count': [],
        'reading_ease': [],
        'grade_level': [],
        'special_chars_ratio': []
    }
    
    for text in sample_texts[:sample_size]:
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        text_length = len(text)
        word_count = len(text.split())
        sentence_count = len(re.split(r'[.!?]+', text))
        
        # ì½ê¸° ë©”íŠ¸ë¦­ (ì˜¤ë¥˜ ì²˜ë¦¬)
        try:
            reading_ease = flesch_reading_ease(text)
            grade_level = flesch_kincaid_grade(text)
        except:
            reading_ease = grade_level = 0
        
        # íŠ¹ìˆ˜ ë¬¸ì ë¹„ìœ¨
        special_chars = len(re.findall(r'[^\w\s]', text))
        special_ratio = special_chars / max(text_length, 1)
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        metrics['avg_length'].append(text_length)
        metrics['word_count'].append(word_count)
        metrics['sentence_count'].append(sentence_count)
        metrics['reading_ease'].append(reading_ease)
        metrics['grade_level'].append(grade_level)
        metrics['special_chars_ratio'].append(special_ratio)
    
    return metrics

# ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„
print("í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„ ì¤‘...")
sample_texts = [item['text'] for item in dataset['train'].take(1000)]
quality_metrics = analyze_text_quality(sample_texts)

# ê²°ê³¼ í‘œì‹œ
for metric, values in quality_metrics.items():
    avg_value = sum(values) / len(values)
    print(f"{metric}: {avg_value:.2f}")
```

### ë„ë©”ì¸ë³„ í•„í„°ë§

```python
def filter_by_domain(dataset, domain_keywords, min_matches=2):
    """ë„ë©”ì¸ë³„ í‚¤ì›Œë“œë¡œ ë°ì´í„°ì…‹ í•„í„°ë§"""
    
    def contains_domain_keywords(example):
        text_lower = example['text'].lower()
        matches = sum(1 for keyword in domain_keywords if keyword in text_lower)
        return matches >= min_matches
    
    filtered_dataset = dataset.filter(contains_domain_keywords)
    return filtered_dataset

# ì˜ˆì‹œ: ê³¼í•™ ì½˜í…ì¸  í•„í„°ë§
scientific_keywords = [
    'ì—°êµ¬', 'ì¡°ì‚¬', 'ë¶„ì„', 'ì‹¤í—˜', 'ê°€ì„¤',
    'ë°©ë²•ë¡ ', 'ê²°ê³¼', 'ê²°ë¡ ', 'ë™ë£Œ ê²€í† ',
    'ì €ë„', 'ì¶œíŒ', 'ì¸ìš©', 'ì´ˆë¡'
]

print("ê³¼í•™ ì½˜í…ì¸  í•„í„°ë§ ì¤‘...")
scientific_subset = filter_by_domain(dataset['train'], scientific_keywords)
print(f"ê³¼í•™ ë¬¸ì„œ: {len(scientific_subset)} / {len(dataset['train'])}")

# ì˜ˆì‹œ: ë²•ë¥  ì½˜í…ì¸  í•„í„°ë§
legal_keywords = [
    'ë²•ì›', 'íŒì‚¬', 'ë²•', 'ë²•ì ', 'ë²•ë ¹', 'ê·œì •',
    'ê³„ì•½', 'í˜‘ì •', 'ì›ê³ ', 'í”¼ê³ ',
    'ê´€í• ê¶Œ', 'ì†Œì†¡', 'ë³€í˜¸ì‚¬'
]

legal_subset = filter_by_domain(dataset['train'], legal_keywords)
print(f"ë²•ë¥  ë¬¸ì„œ: {len(legal_subset)} / {len(dataset['train'])}")
```

## ì‹¤ìš©ì  ì‘ìš© ì‚¬ë¡€

### 1. ì–¸ì–´ ëª¨ë¸ í›ˆë ¨

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
import torch

def prepare_training_data(dataset, tokenizer, max_length=512):
    """ì–¸ì–´ ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„°ì…‹ ì¤€ë¹„"""
    
    def tokenize_function(examples):
        # í…ìŠ¤íŠ¸ í† í°í™”
        tokenized = tokenizer(
            examples['text'],
            truncation=True,
            padding=True,
            max_length=max_length,
            return_tensors="pt"
        )
        # Causal LMì„ ìœ„í•œ ë¼ë²¨ ì„¤ì •
        tokenized["labels"] = tokenized["input_ids"].clone()
        return tokenized
    
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names
    )
    
    return tokenized_dataset

# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ì´ˆê¸°í™”
model_name = "distilgpt2"  # ë°ëª¨ìš© ê²½ëŸ‰ ëª¨ë¸
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(model_name)

# í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ (ë°ëª¨ìš© ì‘ì€ ì„œë¸Œì…‹ ì‚¬ìš©)
print("í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
train_subset = dataset['train'].select(range(1000))  # ì‘ì€ ìƒ˜í”Œ
tokenized_data = prepare_training_data(train_subset, tokenizer)

# í›ˆë ¨ êµ¬ì„±
training_args = TrainingArguments(
    output_dir="./finepdfs-model",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    warmup_steps=100,
    logging_steps=50,
    save_steps=500,
    evaluation_strategy="no",
    save_total_limit=2,
    load_best_model_at_end=False,
)

# íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data,
    tokenizer=tokenizer,
)

print("í›ˆë ¨ ì‹œì‘... (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
# trainer.train()  # ì‹¤ì œ í›ˆë ¨í•˜ë ¤ë©´ ì£¼ì„ í•´ì œ
print("í›ˆë ¨ êµ¬ì„± ì¤€ë¹„ ì™„ë£Œ!")
```

### 2. ë¬¸ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def create_document_embeddings(texts, model_name="all-MiniLM-L6-v2"):
    """ë¬¸ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ìƒì„±"""
    
    model = SentenceTransformer(model_name)
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´ ë°°ì¹˜ ì²˜ë¦¬
    batch_size = 32
    embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        batch_embeddings = model.encode(batch_texts, show_progress_bar=True)
        embeddings.extend(batch_embeddings)
    
    return np.array(embeddings), model

def find_similar_documents(query, embeddings, texts, model, top_k=5):
    """ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°"""
    
    # ì¿¼ë¦¬ ì¸ì½”ë”©
    query_embedding = model.encode([query])
    
    # ìœ ì‚¬ë„ ê³„ì‚°
    similarities = cosine_similarity(query_embedding, embeddings)[0]
    
    # ìƒìœ„ kê°œ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    results = []
    for idx in top_indices:
        results.append({
            'index': idx,
            'similarity': similarities[idx],
            'text': texts[idx][:200] + "..."
        })
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
print("ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
sample_texts = [item['text'] for item in dataset['train'].take(100)]
embeddings, model = create_document_embeddings(sample_texts)

# ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
query = "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ê³¼ ì¸ê³µì§€ëŠ¥"
similar_docs = find_similar_documents(query, embeddings, sample_texts, model)

print(f"\n'{query}'ì— ëŒ€í•œ ìœ ì‚¬ ë¬¸ì„œ:")
for i, doc in enumerate(similar_docs):
    print(f"\n{i+1}. ìœ ì‚¬ë„: {doc['similarity']:.3f}")
    print(f"í…ìŠ¤íŠ¸: {doc['text']}")
```

### 3. ë‹¤êµ­ì–´ ë¶„ì„

```python
def analyze_multilingual_content(language_configs, sample_size=500):
    """ì—¬ëŸ¬ ì–¸ì–´ì— ê±¸ì¹œ ì½˜í…ì¸  ë¶„ì„"""
    
    results = {}
    
    for lang_config in language_configs:
        try:
            print(f"{lang_config} ë¡œë”© ì¤‘...")
            ds = load_dataset("HuggingFaceFW/finepdfs", lang_config, split="train")
            
            # ë¬¸ì„œ ìƒ˜í”Œë§
            sample = ds.take(min(sample_size, len(ds)))
            texts = [item['text'] for item in sample]
            
            # í†µê³„ ê³„ì‚°
            avg_length = sum(len(text) for text in texts) / len(texts)
            avg_words = sum(len(text.split()) for text in texts) / len(texts)
            
            results[lang_config] = {
                'total_docs': len(ds),
                'avg_length': avg_length,
                'avg_words': avg_words,
                'sample_text': texts[0][:100] + "..." if texts else ""
            }
            
        except Exception as e:
            print(f"{lang_config} ë¡œë”© ì˜¤ë¥˜: {e}")
            results[lang_config] = {'error': str(e)}
    
    return results

# ì—¬ëŸ¬ ì–¸ì–´ ë¶„ì„
multilingual_configs = ["eng_Latn", "spa_Latn", "fra_Latn", "deu_Latn"]
multilingual_analysis = analyze_multilingual_content(multilingual_configs)

# ê²°ê³¼ í‘œì‹œ
for lang, stats in multilingual_analysis.items():
    print(f"\n--- {lang} ---")
    if 'error' in stats:
        print(f"ì˜¤ë¥˜: {stats['error']}")
    else:
        print(f"ì´ ë¬¸ì„œ: {stats['total_docs']:,}")
        print(f"í‰ê·  ê¸¸ì´: {stats['avg_length']:.0f} ë¬¸ì")
        print(f"í‰ê·  ë‹¨ì–´: {stats['avg_words']:.0f}")
        print(f"ìƒ˜í”Œ: {stats['sample_text']}")
```

## ì„±ëŠ¥ ìµœì í™”

### ë©”ëª¨ë¦¬ ê´€ë¦¬

```python
import gc
from datasets import Dataset

def process_large_dataset_in_chunks(dataset, chunk_size=1000, process_func=None):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²­í¬ ë‹¨ìœ„ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
    
    total_size = len(dataset)
    results = []
    
    for i in range(0, total_size, chunk_size):
        print(f"ì²­í¬ {i//chunk_size + 1}/{(total_size-1)//chunk_size + 1} ì²˜ë¦¬ ì¤‘")
        
        # ì²­í¬ ê°€ì ¸ì˜¤ê¸°
        chunk = dataset.select(range(i, min(i + chunk_size, total_size)))
        
        # ì²­í¬ ì²˜ë¦¬
        if process_func:
            chunk_result = process_func(chunk)
            results.append(chunk_result)
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        gc.collect()
    
    return results

def chunk_text_analysis(chunk):
    """ì²­í¬ìš© ì˜ˆì‹œ ì²˜ë¦¬ í•¨ìˆ˜"""
    texts = [item['text'] for item in chunk]
    
    return {
        'chunk_size': len(texts),
        'avg_length': sum(len(text) for text in texts) / len(texts),
        'total_chars': sum(len(text) for text in texts)
    }

# ì˜ˆì‹œ: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²­í¬ ë‹¨ìœ„ ë°ì´í„°ì…‹ ì²˜ë¦¬
print("ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²­í¬ë¡œ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì¤‘...")
chunk_results = process_large_dataset_in_chunks(
    dataset['train'].select(range(5000)),  # ë°ëª¨ìš© ì„œë¸Œì…‹ ì‚¬ìš©
    chunk_size=1000,
    process_func=chunk_text_analysis
)

for i, result in enumerate(chunk_results):
    print(f"ì²­í¬ {i+1}: {result}")
```

### ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ìŠ¤íŠ¸ë¦¬ë°

```python
from datasets import load_dataset

def stream_process_dataset(dataset_name, config, process_func, max_items=None):
    """ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì§€ ì•Šê³  ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
    
    # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ìœ¼ë¡œ ë¡œë“œ
    dataset = load_dataset(dataset_name, config, streaming=True, split="train")
    
    processed_count = 0
    results = []
    
    for item in dataset:
        if max_items and processed_count >= max_items:
            break
        
        result = process_func(item)
        results.append(result)
        processed_count += 1
        
        if processed_count % 100 == 0:
            print(f"{processed_count}ê°œ í•­ëª© ì²˜ë¦¬ë¨...")
    
    return results

def extract_keywords(item):
    """ì˜ˆì‹œ ì²˜ë¦¬ í•¨ìˆ˜"""
    text = item['text'].lower()
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë” ì •êµí•œ ë°©ë²•ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥)
    keywords = []
    for word in ['ai', 'ë¨¸ì‹ ëŸ¬ë‹', 'ì‹ ê²½ë§', 'ì•Œê³ ë¦¬ì¦˜']:
        if word in text:
            keywords.append(word)
    return {'keywords': keywords, 'text_length': len(text)}

# ì˜ˆì‹œ: ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
print("ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì¤‘...")
stream_results = stream_process_dataset(
    "HuggingFaceFW/finepdfs", 
    "eng_Latn",
    extract_keywords,
    max_items=1000
)

print(f"ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ {len(stream_results)}ê°œ í•­ëª© ì²˜ë¦¬ë¨")
```

## ê³ ê¸‰ ì‚¬ìš© ì‚¬ë¡€

### 1. ë„ë©”ì¸ íŠ¹í™” ë§ë­‰ì¹˜ êµ¬ì¶•

```python
def build_domain_corpus(dataset, domain_rules, output_path=None):
    """ë„ë©”ì¸ ê·œì¹™ì„ ê¸°ë°˜ìœ¼ë¡œ ì „ë¬¸ ë§ë­‰ì¹˜ êµ¬ì¶•"""
    
    def evaluate_domain_match(text, rules):
        score = 0
        text_lower = text.lower()
        
        # í•„ìˆ˜ í‚¤ì›Œë“œ (ìµœì†Œ í•˜ë‚˜ëŠ” ìˆì–´ì•¼ í•¨)
        required = rules.get('required', [])
        if required and any(keyword in text_lower for keyword in required):
            score += 2
        elif required:  # í•„ìˆ˜ í‚¤ì›Œë“œê°€ ìˆì§€ë§Œ ì—†ìŒ
            return 0
        
        # ë³´ë„ˆìŠ¤ í‚¤ì›Œë“œ
        bonus = rules.get('bonus', [])
        for keyword in bonus:
            if keyword in text_lower:
                score += 1
        
        # í˜ë„í‹° í‚¤ì›Œë“œ
        penalty = rules.get('penalty', [])
        for keyword in penalty:
            if keyword in text_lower:
                score -= 2
        
        return max(0, score)
    
    # ë„ë©”ì¸ ê·œì¹™ ì •ì˜
    scientific_rules = {
        'required': ['ì—°êµ¬', 'ì¡°ì‚¬', 'ë¶„ì„', 'ì‹¤í—˜'],
        'bonus': ['ë™ë£Œ ê²€í† ', 'ë°©ë²•ë¡ ', 'ê°€ì„¤', 'í†µê³„ì '],
        'penalty': ['ê´‘ê³ ', 'í™ë³´', 'ì§€ê¸ˆ êµ¬ë§¤']
    }
    
    # ë¬¸ì„œ ì ìˆ˜ ë§¤ê¸°ê¸° ë° í•„í„°ë§
    scored_docs = []
    for item in dataset:
        score = evaluate_domain_match(item['text'], scientific_rules)
        if score >= 3:  # ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’
            scored_docs.append({
                'text': item['text'],
                'language': item.get('language', 'unknown'),
                'domain_score': score
            })
    
    print(f"ë„ë©”ì¸ ë§ë­‰ì¹˜ êµ¬ì¶• ì™„ë£Œ: {len(scored_docs)}ê°œ ë¬¸ì„œ")
    
    if output_path:
        # ë§ë­‰ì¹˜ ì €ì¥
        import json
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(scored_docs, f, ensure_ascii=False, indent=2)
        print(f"ë§ë­‰ì¹˜ê°€ {output_path}ì— ì €ì¥ë¨")
    
    return scored_docs

# ê³¼í•™ ë§ë­‰ì¹˜ êµ¬ì¶•
scientific_corpus = build_domain_corpus(
    dataset['train'].take(2000),  # ë°ëª¨ìš© ì„œë¸Œì…‹ ì‚¬ìš©
    {},  # ê·œì¹™ì€ í•¨ìˆ˜ ë‚´ì—ì„œ ì •ì˜ë¨
    "scientific_corpus.json"
)
```

### 2. í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸

```python
def assess_document_quality(text):
    """ë¬¸ì„œì˜ í¬ê´„ì  í’ˆì§ˆ í‰ê°€"""
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    char_count = len(text)
    word_count = len(text.split())
    sentence_count = len(re.split(r'[.!?]+', text))
    
    # í’ˆì§ˆ ì§€í‘œ
    quality_score = 0
    issues = []
    
    # ê¸¸ì´ ê²€ì‚¬
    if word_count < 50:
        issues.append("ë„ˆë¬´ ì§§ìŒ")
        quality_score -= 2
    elif word_count > 100:
        quality_score += 1
    
    # ì¼ê´€ì„± ê²€ì‚¬
    avg_word_length = sum(len(word) for word in text.split()) / max(word_count, 1)
    if avg_word_length > 6:
        issues.append("ì§€ë‚˜ì¹˜ê²Œ ë³µì¡í•œ ì–´íœ˜")
        quality_score -= 1
    
    # ì½˜í…ì¸  ë‹¤ì–‘ì„±
    unique_words = len(set(text.lower().split()))
    diversity_ratio = unique_words / max(word_count, 1)
    if diversity_ratio > 0.7:
        quality_score += 2
    elif diversity_ratio < 0.3:
        issues.append("ë‚®ì€ ì–´íœ˜ ë‹¤ì–‘ì„±")
        quality_score -= 1
    
    # íŠ¹ìˆ˜ ë¬¸ì ë…¸ì´ì¦ˆ
    special_char_ratio = len(re.findall(r'[^\w\s]', text)) / max(char_count, 1)
    if special_char_ratio > 0.3:
        issues.append("ë†’ì€ íŠ¹ìˆ˜ ë¬¸ì ë…¸ì´ì¦ˆ")
        quality_score -= 2
    
    return {
        'quality_score': quality_score,
        'issues': issues,
        'metrics': {
            'char_count': char_count,
            'word_count': word_count,
            'sentence_count': sentence_count,
            'diversity_ratio': diversity_ratio,
            'special_char_ratio': special_char_ratio
        }
    }

# ë°ì´í„°ì…‹ ìƒ˜í”Œì—ì„œ í’ˆì§ˆ í‰ê°€
print("ë¬¸ì„œ í’ˆì§ˆ í‰ê°€ ì¤‘...")
quality_results = []

for item in dataset['train'].take(500):
    quality = assess_document_quality(item['text'])
    quality_results.append(quality)

# í’ˆì§ˆ ë¶„í¬ ë¶„ì„
quality_scores = [result['quality_score'] for result in quality_results]
avg_quality = sum(quality_scores) / len(quality_scores)
high_quality_count = sum(1 for score in quality_scores if score >= 2)

print(f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.2f}")
print(f"ê³ í’ˆì§ˆ ë¬¸ì„œ: {high_quality_count}/{len(quality_results)}")
print(f"í’ˆì§ˆ ë¶„í¬: {Counter(quality_scores)}")
```

## í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### í¬ê´„ì  í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

```python
#!/usr/bin/env python3
"""
FinePDFs ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ë°ì´í„°ì…‹ ë¡œë”©, ì²˜ë¦¬, ë¶„ì„ ê¸°ëŠ¥ ê²€ì¦
"""

import sys
import time
import traceback
from datasets import load_dataset
import pandas as pd
import numpy as np

def test_basic_loading():
    """ê¸°ë³¸ ë°ì´í„°ì…‹ ë¡œë”© ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("ê¸°ë³¸ ë°ì´í„°ì…‹ ë¡œë”© í…ŒìŠ¤íŠ¸ ì¤‘...")
    try:
        dataset = load_dataset("HuggingFaceFW/finepdfs", "eng_Latn", split="train")
        print(f"âœ“ {len(dataset)}ê°œ ë¬¸ì„œë¡œ ë°ì´í„°ì…‹ ë¡œë”© ì„±ê³µ")
        
        # ë°ì´í„° ì ‘ê·¼ í…ŒìŠ¤íŠ¸
        first_item = dataset[0]
        required_fields = ['text', 'language']
        for field in required_fields:
            if field not in first_item:
                raise ValueError(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
        
        print("âœ“ ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âœ— ê¸°ë³¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_processing_functions():
    """ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    print("\nì²˜ë¦¬ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì¤‘...")
    try:
        # ì‘ì€ ìƒ˜í”Œ ë¡œë“œ
        dataset = load_dataset("HuggingFaceFW/finepdfs", "eng_Latn", split="train")
        sample = dataset.take(10)
        
        # í…ìŠ¤íŠ¸ ë¶„ì„ í…ŒìŠ¤íŠ¸
        texts = [item['text'] for item in sample]
        avg_length = sum(len(text) for text in texts) / len(texts)
        
        if avg_length == 0:
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼")
        
        print(f"âœ“ í…ìŠ¤íŠ¸ ë¶„ì„ ì„±ê³µ (í‰ê·  ê¸¸ì´: {avg_length:.0f})")
        
        # í•„í„°ë§ í…ŒìŠ¤íŠ¸
        filtered = [text for text in texts if len(text) > 100]
        print(f"âœ“ í•„í„°ë§ ì„±ê³µ ({len(filtered)}/{len(texts)} ë¬¸ì„œ)")
        
        return True
        
    except Exception as e:
        print(f"âœ— ì²˜ë¦¬ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_memory_efficiency():
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸"""
    print("\në©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
    try:
        # ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        dataset = load_dataset("HuggingFaceFW/finepdfs", "eng_Latn", 
                             streaming=True, split="train")
        
        count = 0
        for item in dataset:
            count += 1
            if count >= 5:  # ì²˜ìŒ 5ê°œ í•­ëª© í…ŒìŠ¤íŠ¸
                break
        
        if count != 5:
            raise ValueError("ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‹¤íŒ¨")
        
        print("âœ“ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì„±ê³µ")
        return True
        
    except Exception as e:
        print(f"âœ— ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def run_all_tests():
    """í¬ê´„ì  í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
    print("=" * 50)
    print("FinePDFs ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸")
    print("=" * 50)
    
    tests = [
        test_basic_loading,
        test_processing_functions,
        test_memory_efficiency
    ]
    
    results = []
    start_time = time.time()
    
    for test in tests:
        try:
            result = test()
            results.append(result)
        except Exception as e:
            print(f"âœ— í…ŒìŠ¤íŠ¸ {test.__name__} í¬ë˜ì‹œ: {e}")
            traceback.print_exc()
            results.append(False)
    
    # ìš”ì•½
    print("\n" + "=" * 50)
    print("í…ŒìŠ¤íŠ¸ ìš”ì•½")
    print("=" * 50)
    
    passed = sum(results)
    total = len(results)
    
    print(f"í†µê³¼í•œ í…ŒìŠ¤íŠ¸: {passed}/{total}")
    print(f"ì„±ê³µë¥ : {passed/total*100:.1f}%")
    print(f"ì´ ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ")
    
    if passed == total:
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        return True
    else:
        print("âš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ìœ„ ì¶œë ¥ì„ í™•ì¸í•˜ì„¸ìš”.")
        return False

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
```

## ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ë° íŒ

### 1. íš¨ìœ¨ì  ë°ì´í„° ë¡œë”©

```python
# ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤: ì ì ˆí•œ ì„œë¸Œì…‹ ì‚¬ìš©
def load_dataset_efficiently(language_code, max_size=None):
    """ìµœì  ì„¤ì •ìœ¼ë¡œ ë°ì´í„°ì…‹ ë¡œë“œ"""
    
    try:
        if max_size and max_size < 10000:
            # ì‘ì€ ë°ì´í„°ì…‹: ì™„ì „ ë¡œë“œ
            dataset = load_dataset("HuggingFaceFW/finepdfs", language_code, 
                                 split=f"train[:{max_size}]")
        else:
            # í° ë°ì´í„°ì…‹: ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©
            dataset = load_dataset("HuggingFaceFW/finepdfs", language_code, 
                                 streaming=True, split="train")
        
        return dataset
        
    except Exception as e:
        print(f"{language_code} ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# ì‚¬ìš© ì˜ˆì‹œ
efficient_dataset = load_dataset_efficiently("eng_Latn", max_size=5000)
```

### 2. ì˜¤ë¥˜ ì²˜ë¦¬ ë° ê°•ê±´ì„±

```python
def robust_text_processing(text, max_retries=3):
    """ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì¬ì‹œë„ ê¸°ëŠ¥ì´ ìˆëŠ” í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
    
    for attempt in range(max_retries):
        try:
            # ì…ë ¥ ê²€ì¦
            if not isinstance(text, str):
                raise TypeError("ì…ë ¥ì€ ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
            
            if len(text) == 0:
                return {"status": "empty", "result": None}
            
            # í…ìŠ¤íŠ¸ ì²˜ë¦¬
            processed = {
                "length": len(text),
                "word_count": len(text.split()),
                "status": "success"
            }
            
            return processed
            
        except Exception as e:
            print(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
            if attempt == max_retries - 1:
                return {"status": "failed", "error": str(e)}
            
            time.sleep(0.1)  # ì¬ì‹œë„ ì „ ì§§ì€ ì§€ì—°

# ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ í¬í•¨í•œ ì‚¬ìš© ì˜ˆì‹œ
def process_dataset_safely(dataset, max_items=1000):
    """í¬ê´„ì  ì˜¤ë¥˜ ì²˜ë¦¬ë¡œ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
    
    successful = 0
    failed = 0
    results = []
    
    for i, item in enumerate(dataset.take(max_items)):
        try:
            result = robust_text_processing(item['text'])
            if result['status'] == 'success':
                successful += 1
                results.append(result)
            else:
                failed += 1
                
        except Exception as e:
            print(f"í•­ëª© {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            failed += 1
    
    print(f"ì²˜ë¦¬ ì™„ë£Œ: {successful}ê°œ ì„±ê³µ, {failed}ê°œ ì‹¤íŒ¨")
    return results
```

## ê²°ë¡ 

FinePDFsëŠ” ë¬¸ì„œ ê¸°ë°˜ ë¨¸ì‹ ëŸ¬ë‹ì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì„ ë‚˜íƒ€ë‚´ë©°, ê³ í’ˆì§ˆì˜ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ì— ëŒ€í•œ ì „ë¡€ ì—†ëŠ” ì ‘ê·¼ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ë‹¤ìŒ ë‚´ìš©ì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤:

### í•µì‹¬ ìš”ì 

1. **ë°ì´í„°ì…‹ ìˆ™ë ¨**: FinePDFsì˜ êµ¬ì¡°ì™€ ê¸°ëŠ¥ ì´í•´
2. **íš¨ìœ¨ì  ì²˜ë¦¬**: ëŒ€ê·œëª¨ ì‘ì—…ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ì¸ì‹ ê¸°ë²•
3. **ì‹¤ìš©ì  ì‘ìš©**: í›ˆë ¨ë¶€í„° ë¶„ì„ê¹Œì§€ì˜ ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€
4. **í’ˆì§ˆ í‰ê°€**: ì½˜í…ì¸  í‰ê°€ ë° í•„í„°ë§ ë°©ë²•
5. **ë‹¤êµ­ì–´ ì§€ì›**: ë°ì´í„°ì…‹ì˜ ì–¸ì–´ì  ë‹¤ì–‘ì„± í™œìš©

### ë‹¤ìŒ ë‹¨ê³„

- **ì‹¤í—˜**: ë‹¤ì–‘í•œ ì–¸ì–´ êµ¬ì„±ê³¼ ë„ë©”ì¸ í•„í„°ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”
- **í™•ì¥**: ì¸í”„ë¼ê°€ í—ˆìš©í•˜ëŠ” ë²”ìœ„ì—ì„œ ë” í° ì„œë¸Œì…‹ì— ê¸°ë²•ì„ ì ìš©í•˜ì„¸ìš”
- **í†µí•©**: FinePDFsë¥¼ ê¸°ì¡´ ML íŒŒì´í”„ë¼ì¸ì— í†µí•©í•˜ì„¸ìš”
- **ê¸°ì—¬**: ê°œì„ ì‚¬í•­ê³¼ ë°œê²¬ì‚¬í•­ì„ ì»¤ë®¤ë‹ˆí‹°ì™€ ê³µìœ í•˜ì„¸ìš”

### ì°¸ê³  ìë£Œ

- [FinePDFs ë°ì´í„°ì…‹ í˜ì´ì§€](https://huggingface.co/datasets/HuggingFaceFW/finepdfs)
- [Hugging Face Datasets ë¬¸ì„œ](https://huggingface.co/docs/datasets/)
- [Doclingì„ í™œìš©í•œ PDF ì²˜ë¦¬](https://github.com/DS4SD/docling)

FinePDFsì™€ í•¨ê»˜ ë¬¸ì„œ ê¸°ë°˜ AIì˜ ë¯¸ë˜ê°€ ì—¬ê¸°ì— ìˆìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ë¶€í„° êµ¬ì¶•ì„ ì‹œì‘í•˜ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì´ ìˆìœ¼ì‹ ê°€ìš”? Hugging Face ì»¤ë®¤ë‹ˆí‹° í¬ëŸ¼ì—ì„œ í† ë¡ ì— ì°¸ì—¬í•˜ê±°ë‚˜ ë°ì´í„°ì…‹ì˜ ì§€ì†ì ì¸ ê°œë°œì— ê¸°ì—¬í•´ë³´ì„¸ìš”.*
