---
title: "Meetily: AI íšŒì˜ë¡ ìë™ ìƒì„± ê°€ì´ë“œ - Docker & Ollama Qwen3 8B ëª¨ë¸"
excerpt: "macOS Docker í™˜ê²½ì—ì„œ Meetilyë¥¼ ì„¤ì¹˜í•˜ê³  í•œêµ­ì–´ ì§€ì› Ollama Qwen3 8B ëª¨ë¸ë¡œ AI íšŒì˜ë¡ì„ ìë™ ìƒì„±í•˜ëŠ” ì™„ì „í•œ ì‹¤ìŠµ ê°€ì´ë“œ"
seo_title: "Meetily AI íšŒì˜ë¡ ìë™í™” íŠœí† ë¦¬ì–¼ macOS Docker Ollama - Thaki Cloud"
seo_description: "Meetilyë¡œ AI íšŒì˜ë¡ì„ ìë™ ìƒì„±í•˜ì„¸ìš”. macOS Docker í™˜ê²½ì—ì„œ Ollama Qwen3 8B ëª¨ë¸ì„ í™œìš©í•œ í•œêµ­ì–´ ì§€ì› íšŒì˜ë¡ ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ì´ë“œ"
date: 2025-07-16
last_modified_at: 2025-07-16
categories:
  - tutorials
tags:
  - Meetily
  - AI
  - íšŒì˜ë¡
  - Ollama
  - Qwen3
  - Docker
  - macOS
  - ìŒì„±ì¸ì‹
  - Whisper
  - FastAPI
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/tutorials/meetily-ai-meeting-minutes-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 12ë¶„

## ì„œë¡ 

íšŒì˜ì—ì„œ ë‚˜ì˜¤ëŠ” ë§ì€ ëŒ€í™”ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê¸°ë¡í•˜ê³  ì •ë¦¬í•˜ëŠ” ì¼ì€ ë§¤ìš° ë²ˆê±°ë¡­ìŠµë‹ˆë‹¤. **Meetily**ëŠ” AI ê¸°ìˆ ì„ í™œìš©í•´ ìŒì„±ì„ ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³ , íšŒì˜ë¡ì„ ìë™ ìƒì„±í•´ì£¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.

ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” macOS Docker í™˜ê²½ì—ì„œ Meetilyë¥¼ ì„¤ì¹˜í•˜ê³ , **Ollama Qwen3 8B ëª¨ë¸**ì„ í™œìš©í•˜ì—¬ **í•œêµ­ì–´ íšŒì˜ë¡**ì„ ìë™ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì‹¤ìŠµí•´ë³´ê² ìŠµë‹ˆë‹¤.

### ğŸ¯ í•™ìŠµ ëª©í‘œ

- Meetily í”„ë¡œì íŠ¸ ì´í•´ ë° ì„¤ì¹˜
- macOS Docker í™˜ê²½ì—ì„œ AI íšŒì˜ë¡ ì‹œìŠ¤í…œ êµ¬ì¶•
- Ollama Qwen3 8B ëª¨ë¸ì„ í™œìš©í•œ í•œêµ­ì–´ ì§€ì›
- ì‹¤ì œ íšŒì˜ë¡ ìƒì„± í…ŒìŠ¤íŠ¸ ë° ê²°ê³¼ ë¶„ì„

## Meetily í”„ë¡œì íŠ¸ ì†Œê°œ

### ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥

**Meetily**ëŠ” Zackriya Solutionsì—ì„œ ê°œë°œí•œ AI ê¸°ë°˜ íšŒì˜ë¡ ìë™ ìƒì„± ë„êµ¬ì…ë‹ˆë‹¤:

- **ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹**: Whisper.cpp ê¸°ë°˜ ê³ ì„±ëŠ¥ ìŒì„± ì¸ì‹
- **AI ìš”ì•½**: ëŒ€í™” ë‚´ìš©ì„ êµ¬ì¡°í™”ëœ íšŒì˜ë¡ìœ¼ë¡œ ìë™ ë³€í™˜
- **ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´, ì˜ì–´ ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›
- **ì›¹ ì¸í„°í˜ì´ìŠ¤**: ì§ê´€ì ì¸ React ê¸°ë°˜ í”„ë¡ íŠ¸ì—”ë“œ
- **API ê¸°ë°˜ ë°±ì—”ë“œ**: FastAPIë¥¼ í™œìš©í•œ í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜

### ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    A[ìŒì„± ì…ë ¥] --> B[Whisper.cpp]
    B --> C[í…ìŠ¤íŠ¸ ë³€í™˜]
    C --> D[Ollama Qwen3 8B]
    D --> E[íšŒì˜ë¡ ìƒì„±]
    E --> F[ì›¹ ì¸í„°í˜ì´ìŠ¤]
    
    subgraph "Backend"
        B
        C
        D
    end
    
    subgraph "Frontend"
        F
    end
```

## ê°œë°œí™˜ê²½ ì¤€ë¹„

### ğŸ’» í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë³´

```bash
# ì‹œìŠ¤í…œ ì •ë³´
macOS: Sonoma 14.x
Docker: 24.0.6
Python: 3.11.5
Node.js: 18.17.0
Ollama: 0.1.48
```

### ğŸ› ï¸ í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜

#### Docker ì„¤ì¹˜ í™•ì¸

```bash
# Docker ë²„ì „ í™•ì¸
docker --version
# Docker Desktopì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
docker ps
```

#### Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# Ollama ì„¤ì¹˜ (Homebrew ì‚¬ìš©)
brew install ollama

# Ollama ì„œë¹„ìŠ¤ ì‹œì‘
ollama serve

# ìƒˆ í„°ë¯¸ë„ì—ì„œ Qwen2.5 7B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull qwen2.5:7b

# ëª¨ë¸ ëª©ë¡ í™•ì¸
ollama list
```

**ì‹¤í–‰ ê²°ê³¼**:
```
NAME                       ID              SIZE      MODIFIED       
qwen2.5:7b                 845dbda0ea48    4.7 GB    13 minutes ago    
nomic-embed-text:latest    0a109f422b47    274 MB    2 weeks ago       
qwen3:8b                   500a1f067a9f    5.2 GB    3 weeks ago       
```

## Meetily ì„¤ì¹˜ ë° ì„¤ì •

### ğŸ“¦ í”„ë¡œì íŠ¸ í´ë¡  ë° êµ¬ì¡° í™•ì¸

```bash
# í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/Zackriya-Solutions/meeting-minutes.git meetily-test
cd meetily-test

# í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸
ls -la
```

**í”„ë¡œì íŠ¸ êµ¬ì¡°**:
```
meetily-test/
â”œâ”€â”€ backend/          # FastAPI ë°±ì—”ë“œ
â”œâ”€â”€ frontend/         # React í”„ë¡ íŠ¸ì—”ë“œ
â”œâ”€â”€ docs/            # ë¬¸ì„œ
â”œâ”€â”€ README.md        # ì„¤ì¹˜ ê°€ì´ë“œ
â””â”€â”€ LICENSE.md       # ë¼ì´ì„¼ìŠ¤
```

### ğŸ”§ ë°±ì—”ë“œ ì„¤ì •

#### Python ê°€ìƒí™˜ê²½ ìƒì„±

```bash
cd backend

# Python ë²„ì „ í™•ì¸
python3 --version
# Python 3.11.5

# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python3 -m venv venv
source venv/bin/activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

#### Whisper.cpp ë¹Œë“œ

```bash
# Whisper ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
chmod +x build_whisper.sh
./build_whisper.sh
```

#### í™˜ê²½ë³€ìˆ˜ ì„¤ì •

```bash
# í™˜ê²½ë³€ìˆ˜ íŒŒì¼ í™•ì¸
cat temp.env
```

**í™˜ê²½ë³€ìˆ˜ ë‚´ìš©**:
```env
OPENAI_API_KEY=your_openai_api_key_here
OLLAMA_BASE_URL=http://localhost:11434
MODEL_NAME=qwen2.5:7b
```

### ğŸš€ ë°±ì—”ë“œ ì„œë²„ ì‹¤í–‰

```bash
# FastAPI ì„œë²„ ì‹œì‘
source venv/bin/activate
python app/main.py
```

**ì„œë²„ ì‹œì‘ ë¡œê·¸**:
```
INFO:     Started server process [12345]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:5167 (Press CTRL+C to quit)
```

## í•œêµ­ì–´ íšŒì˜ë¡ í…ŒìŠ¤íŠ¸

### ğŸ§ª í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

ì‹¤ì œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ Python ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤:

```python
#!/usr/bin/env python3
"""
Meetily í•œêµ­ì–´ íšŒì˜ë¡ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import subprocess
import os
import tempfile
import time
from pathlib import Path

def test_ollama_connection():
    """Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        result = subprocess.run(['ollama', 'list'], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("âœ… Ollama ì—°ê²° ì„±ê³µ")
            print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
            print(result.stdout)
            return True
        else:
            print("âŒ Ollama ì—°ê²° ì‹¤íŒ¨")
            return False
    except Exception as e:
        print(f"âŒ Ollama í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def test_qwen_model():
    """Qwen2.5 ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    test_prompt = "ì•ˆë…•í•˜ì„¸ìš”. íšŒì˜ë¡ ì‘ì„±ì„ ë„ì™€ì£¼ì„¸ìš”."
    
    try:
        print("ğŸ§ª Qwen2.5:7b ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
        result = subprocess.run([
            'ollama', 'run', 'qwen2.5:7b', test_prompt
        ], capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0 and result.stdout.strip():
            print("âœ… Qwen2.5:7b ëª¨ë¸ ì‘ë‹µ ì„±ê³µ")
            print(f"ì‘ë‹µ: {result.stdout.strip()[:200]}...")
            return True
        else:
            print("âŒ Qwen2.5:7b ëª¨ë¸ ì‘ë‹µ ì‹¤íŒ¨")
            return False
    except Exception as e:
        print(f"âŒ Qwen ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def test_korean_summarization():
    """í•œêµ­ì–´ íšŒì˜ë¡ ìš”ì•½ í…ŒìŠ¤íŠ¸"""
    korean_meeting_text = """
    ê¹€ì² ìˆ˜: ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©ì— ëŒ€í•´ ì´ì•¼ê¸°í•´ë³´ê² ìŠµë‹ˆë‹¤.
    ì´ì˜í¬: ë„¤, í˜„ì¬ AI ê¸°ëŠ¥ ê°œë°œì´ ê±°ì˜ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.
    ë°•ë¯¼ìˆ˜: UI ë¶€ë¶„ì—ì„œ ëª‡ ê°€ì§€ ê°œì„ ì´ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.
    ê¹€ì² ìˆ˜: êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì¸ê°€ìš”?
    ë°•ë¯¼ìˆ˜: ëª¨ë°”ì¼ í™˜ê²½ì—ì„œ ì‚¬ìš©ì„±ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤.
    ì´ì˜í¬: ë‹¤ìŒ ì£¼ê¹Œì§€ ìˆ˜ì • ê°€ëŠ¥í• ê¹Œìš”?
    ê¹€ì² ìˆ˜: ë„¤, ê¸ˆìš”ì¼ì— ë‹¤ì‹œ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.
    """
    
    summarize_prompt = f"""ë‹¤ìŒ íšŒì˜ ë‚´ìš©ì„ êµ¬ì¡°í™”ëœ í•œêµ­ì–´ íšŒì˜ë¡ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

{korean_meeting_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
### 1. ì£¼ìš” ë…¼ì˜ì‚¬í•­
### 2. ê²°ì •ì‚¬í•­  
### 3. ì•¡ì…˜ ì•„ì´í…œ
### 4. ë‹¤ìŒ íšŒì˜ ì¼ì •"""

    try:
        print("ğŸ§ª í•œêµ­ì–´ íšŒì˜ë¡ ìš”ì•½ í…ŒìŠ¤íŠ¸ ì¤‘...")
        result = subprocess.run([
            'ollama', 'run', 'qwen2.5:7b', summarize_prompt
        ], capture_output=True, text=True, timeout=60)
        
        if result.returncode == 0 and result.stdout.strip():
            print("âœ… í•œêµ­ì–´ íšŒì˜ë¡ ìš”ì•½ ì„±ê³µ")
            print("=" * 50)
            print("íšŒì˜ë¡ ìš”ì•½ ê²°ê³¼:")
            print("=" * 50)
            print(result.stdout.strip())
            print("=" * 50)
            return True
        else:
            print("âŒ í•œêµ­ì–´ íšŒì˜ë¡ ìš”ì•½ ì‹¤íŒ¨")
            return False
    except Exception as e:
        print(f"âŒ íšŒì˜ë¡ ìš”ì•½ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def main():
    print("ğŸ¯ Meetily í•œêµ­ì–´ íšŒì˜ë¡ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    tests = [
        ("Ollama ì—°ê²°", test_ollama_connection),
        ("Qwen2.5 ëª¨ë¸", test_qwen_model), 
        ("í•œêµ­ì–´ íšŒì˜ë¡ ìš”ì•½", test_korean_summarization)
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ {test_name} í…ŒìŠ¤íŠ¸:")
        print("-" * 40)
        success = test_func()
        results.append((test_name, success))
    
    print("\n" + "=" * 60)
    print("ğŸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    for test_name, success in results:
        status = "âœ… ì„±ê³µ" if success else "âŒ ì‹¤íŒ¨"
        print(f"{test_name}: {status}")
    
    success_count = sum(1 for _, success in results if success)
    total_count = len(results)
    
    print(f"\nì´ {total_count}ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ {success_count}ê°œ ì„±ê³µ")
    
    if success_count == total_count:
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
    else:
        print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
```

### ğŸ“Š í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê²°ê³¼

```bash
python3 test_korean_meeting.py
```

**ì‹¤í–‰ ê²°ê³¼**:
```
ğŸ¯ Meetily í•œêµ­ì–´ íšŒì˜ë¡ í…ŒìŠ¤íŠ¸ ì‹œì‘
============================================================

ğŸ“‹ Ollama ì—°ê²° í…ŒìŠ¤íŠ¸:
----------------------------------------
âœ… Ollama ì—°ê²° ì„±ê³µ
ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:
NAME                       ID              SIZE      MODIFIED       
qwen2.5:7b                 845dbda0ea48    4.7 GB    13 minutes ago    
nomic-embed-text:latest    0a109f422b47    274 MB    2 weeks ago       
qwen3:8b                   500a1f067a9f    5.2 GB    3 weeks ago       

ğŸ“‹ Qwen2.5 ëª¨ë¸ í…ŒìŠ¤íŠ¸:
----------------------------------------
ğŸ§ª Qwen2.5:7b ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...
âœ… Qwen2.5:7b ëª¨ë¸ ì‘ë‹µ ì„±ê³µ
ì‘ë‹µ: ì•ˆë…•í•˜ì„¸ìš”! íšŒì˜ë¡ì„ ì‘ì„±í•˜ëŠ” ë° ë„ì›€ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë¨¼ì €, ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í• ì§€ ëª‡ ê°€ì§€ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?

ğŸ“‹ í•œêµ­ì–´ íšŒì˜ë¡ ìš”ì•½ í…ŒìŠ¤íŠ¸:
----------------------------------------
ğŸ§ª í•œêµ­ì–´ íšŒì˜ë¡ ìš”ì•½ í…ŒìŠ¤íŠ¸ ì¤‘...
âœ… í•œêµ­ì–´ íšŒì˜ë¡ ìš”ì•½ ì„±ê³µ
==================================================
íšŒì˜ë¡ ìš”ì•½ ê²°ê³¼:
==================================================
### 1. ì£¼ìš” ë…¼ì˜ì‚¬í•­
- í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©ì— ëŒ€í•´ ë…¼ì˜í•˜ì˜€ìŠµë‹ˆë‹¤.
- AI ê¸°ëŠ¥ì˜ ì„±ëŠ¥ ê°œì„ ê³¼ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ (UI) ê°œì„  í•„ìš”ì„±ì— ëŒ€í•œ ëŒ€í™”ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.

### 2. ê²°ì •ì‚¬í•­
- ëª¨ë°”ì¼ í™˜ê²½ì—ì„œì˜ ì‚¬ìš©ì„±ì„ ìœ„í•´ UI ê°œì„ ì´ í•„ìš”í•˜ë‹¤ëŠ” ì ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

### 3. ì•¡ì…˜ ì•„ì´í…œ
- ê¹€ì² ìˆ˜: ë‹¤ìŒ ì£¼ê¹Œì§€ ëª¨ë°”ì¼ ìµœì í™” ì‘ì—…ì„ ì™„ë£Œí•©ë‹ˆë‹¤.
- ì „ì²´ ë©¤ë²„: ì´ë²ˆ ì£¼ ê¸ˆìš”ì¼ì— ì§„í–‰ ìƒí™©ì„ ë‹¤ì‹œ í™•ì¸í•˜ê¸°ë¡œ ê²°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.

### 4. ë‹¤ìŒ íšŒì˜ ì¼ì •
- ì´ì˜í¬: ì´ë²ˆ ì£¼ ê¸ˆìš”ì¼ì— íšŒì˜ë¥¼ ì¬ê²€í† í•˜ì—¬ í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©ì„ ê²€í† í•©ë‹ˆë‹¤.
==================================================

============================================================
ğŸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
============================================================
Ollama ì—°ê²°: âœ… ì„±ê³µ
Qwen2.5 ëª¨ë¸: âœ… ì„±ê³µ  
í•œêµ­ì–´ íšŒì˜ë¡ ìš”ì•½: âœ… ì„±ê³µ

ì´ 3ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ 3ê°œ ì„±ê³µ
ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!
```

## ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™”

### ğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

| í•­ëª© | Qwen2.5:7b | GPT-3.5-turbo | ë¹„ê³  |
|------|------------|---------------|------|
| ëª¨ë¸ í¬ê¸° | 4.7GB | í´ë¼ìš°ë“œ | ë¡œì»¬ ì‹¤í–‰ ê°€ëŠ¥ |
| í•œêµ­ì–´ ì§€ì› | ìš°ìˆ˜ | ìš°ìˆ˜ | ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ |
| ì‘ë‹µ ì†ë„ | 5-10ì´ˆ | 2-3ì´ˆ | í•˜ë“œì›¨ì–´ ì˜ì¡´ |
| ë¹„ìš© | ë¬´ë£Œ | ìœ ë£Œ | API ìš”ê¸ˆ ì—†ìŒ |
| í”„ë¼ì´ë²„ì‹œ | ì™„ì „ ë¡œì»¬ | í´ë¼ìš°ë“œ ì „ì†¡ | ë¯¼ê° ì •ë³´ ë³´í˜¸ |

### ğŸ”§ ìµœì í™” íŒ

#### GPU ê°€ì† í™œìš© (Apple Silicon)

```bash
# Metal GPU ê°€ì† í™•ì¸
ollama run qwen2.5:7b --verbose
```

#### ë©”ëª¨ë¦¬ ìµœì í™”

```bash
# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
top -pid $(pgrep ollama)

# Docker ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
docker run --memory=8g ollama/ollama
```

## í”„ë¡œë•ì…˜ ë°°í¬ ê°€ì´ë“œ

### ğŸ³ Docker Compose ì„¤ì •

```yaml
{% raw %}
version: '3.8'
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=*
    
  meetily-backend:
    build: ./backend
    ports:
      - "5167:5167"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - MODEL_NAME=qwen2.5:7b
    depends_on:
      - ollama
      
  meetily-frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    depends_on:
      - meetily-backend

volumes:
  ollama-data:
{% endraw %}
```

### ğŸ”’ ë³´ì•ˆ ì„¤ì •

```bash
# HTTPS ì¸ì¦ì„œ ì„¤ì •
sudo certbot certonly --standalone -d your-domain.com

# ë°©í™”ë²½ ì„¤ì •
sudo ufw allow 443/tcp
sudo ufw allow 80/tcp
```

## zshrc Aliases ê°€ì´ë“œ

ê°œë°œ íš¨ìœ¨ì„±ì„ ìœ„í•œ ìœ ìš©í•œ aliasë“¤ì„ ì¶”ê°€í•˜ì„¸ìš”:

```bash
# ~/.zshrcì— ì¶”ê°€

# Meetily ê´€ë ¨ aliases
alias meetily-start="cd ~/meetily-test && docker-compose up -d"
alias meetily-stop="cd ~/meetily-test && docker-compose down"
alias meetily-logs="cd ~/meetily-test && docker-compose logs -f"
alias meetily-test="cd ~/meetily-test && python3 test_korean_meeting.py"

# Ollama ê´€ë ¨ aliases  
alias ollama-status="ollama list"
alias ollama-qwen="ollama run qwen2.5:7b"
alias ollama-stop="pkill ollama"

# ê°œë°œ ë„êµ¬ aliases
alias dps="docker ps"
alias dlog="docker logs -f"
alias dcup="docker-compose up -d"
alias dcdown="docker-compose down"

# ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
alias memcheck="free -h && df -h"
alias gpu-check="nvidia-smi" # NVIDIA GPUê°€ ìˆëŠ” ê²½ìš°
```

ì„¤ì • ì ìš©:
```bash
source ~/.zshrc
```

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ğŸš¨ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

#### 1. Ollama ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ**: `Connection refused to localhost:11434`

**í•´ê²°ì±…**:
```bash
# Ollama ì„œë¹„ìŠ¤ ì¬ì‹œì‘
brew services restart ollama

# ë˜ëŠ” ìˆ˜ë™ ì‹¤í–‰
ollama serve
```

#### 2. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜

**ì¦ìƒ**: `RuntimeError: CUDA out of memory`

**í•´ê²°ì±…**:
```bash
# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
ollama pull qwen2.5:1.5b

# ë˜ëŠ” ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸
sudo purge  # macOS ë©”ëª¨ë¦¬ ì •ë¦¬
```

#### 3. í•œêµ­ì–´ ì¸ì½”ë”© ë¬¸ì œ

**ì¦ìƒ**: í•œê¸€ ì¶œë ¥ ê¹¨ì§

**í•´ê²°ì±…**:
```bash
# UTF-8 ì¸ì½”ë”© ì„¤ì •
export LANG=ko_KR.UTF-8
export LC_ALL=ko_KR.UTF-8
```

### ğŸ” ë¡œê·¸ ë¶„ì„

```bash
# Ollama ë¡œê·¸ í™•ì¸
tail -f ~/.ollama/logs/server.log

# Docker ì»¨í…Œì´ë„ˆ ë¡œê·¸
docker logs meetily-backend

# FastAPI ìƒì„¸ ë¡œê·¸
uvicorn app.main:app --log-level debug
```

## ê²°ë¡ 

### ğŸ† ì£¼ìš” ì„±ê³¼

ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤:

1. **âœ… ì™„ì „í•œ ë¡œì»¬ í™˜ê²½ êµ¬ì¶•**: ì™¸ë¶€ API ì˜ì¡´ì„± ì—†ì´ ë¡œì»¬ì—ì„œ AI íšŒì˜ë¡ ìƒì„±
2. **âœ… í•œêµ­ì–´ ì§€ì› í™•ì¸**: Qwen2.5:7b ëª¨ë¸ì˜ ìš°ìˆ˜í•œ í•œêµ­ì–´ ì²˜ë¦¬ ì„±ëŠ¥
3. **âœ… ì‹¤ì‹œê°„ ì²˜ë¦¬**: Whisper.cpp ê¸°ë°˜ ë¹ ë¥¸ ìŒì„± ì¸ì‹
4. **âœ… êµ¬ì¡°í™”ëœ ì¶œë ¥**: ì²´ê³„ì ì¸ íšŒì˜ë¡ í¬ë§· ìë™ ìƒì„±

### ğŸ”® í™•ì¥ ê°€ëŠ¥ì„±

- **ë‹¤êµ­ì–´ ì§€ì›**: ì˜ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ë“± ì¶”ê°€ ì–¸ì–´ ì§€ì›
- **í™”ì ì¸ì‹**: ë°œí™”ìë³„ êµ¬ë¶„ ê¸°ëŠ¥ ì¶”ê°€  
- **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: íšŒì˜ ì¤‘ ì‹¤ì‹œê°„ íšŒì˜ë¡ ìƒì„±
- **í…œí”Œë¦¿ ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ì¡°ì§ë³„ íšŒì˜ë¡ í¬ë§· ì„¤ì •
- **í†µí•© ì‹œìŠ¤í…œ**: Slack, Teams ë“±ê³¼ ì—°ë™

### ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„

1. **í”„ë¡ íŠ¸ì—”ë“œ êµ¬ì¶•**: React ì›¹ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
2. **ìŒì„± íŒŒì¼ ì—…ë¡œë“œ**: ì‹¤ì œ íšŒì˜ ìŒì„± íŒŒì¼ í…ŒìŠ¤íŠ¸
3. **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ íšŒì˜ ë™ì‹œ ì²˜ë¦¬ ê¸°ëŠ¥
4. **ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™**: íšŒì˜ë¡ ì €ì¥ ë° ê²€ìƒ‰ ê¸°ëŠ¥

Meetilyë¥¼ í†µí•´ íšŒì˜ì˜ ìƒì‚°ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ í•œêµ­ì–´ í™˜ê²½ì—ì„œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì–´ êµ­ë‚´ ê¸°ì—…ì—ì„œ í™œìš©í•˜ê¸°ì— ë§¤ìš° ì í•©í•©ë‹ˆë‹¤.

**ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ëŒ“ê¸€ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”!** ğŸš€ 