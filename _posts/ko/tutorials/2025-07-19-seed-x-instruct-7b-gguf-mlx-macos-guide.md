---
title: "Seed-X-Instruct-7B ëª¨ë¸ ë³€í™˜ ì™„ë²½ ê°€ì´ë“œ: GGUF vs MLX (macOS Apple Silicon)"
excerpt: "ByteDance Seed-X-Instruct-7B ëª¨ë¸ì„ GGUFì™€ MLX ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ Ollama, LM Studioì—ì„œ í™œìš©í•˜ëŠ” ì‹¤ì „ ê°€ì´ë“œ"
seo_title: "Seed-X-Instruct-7B GGUF MLX ë³€í™˜ ê°€ì´ë“œ macOS - Thaki Cloud"
seo_description: "ByteDance Seed-X-Instruct-7Bë¥¼ GGUF 4-bit ì–‘ìží™”ì™€ MLX Apple Silicon ìµœì í™”ë¡œ ë³€í™˜í•˜ì—¬ Ollama, LM Studioì—ì„œ ì‚¬ìš©í•˜ëŠ” ì™„ë²½í•œ macOS íŠœí† ë¦¬ì–¼"
date: 2025-07-19
last_modified_at: 2025-07-19
categories:
  - tutorials
tags:
  - ByteDance
  - Seed-X-Instruct
  - GGUF
  - MLX
  - llama.cpp
  - Ollama
  - LM-Studio
  - Apple-Silicon
  - M3
  - ì–‘ìží™”
  - ëª¨ë¸ë³€í™˜
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/tutorials/seed-x-instruct-7b-gguf-mlx-macos-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 15ë¶„

## ì„œë¡ 

ByteDanceì—ì„œ ê³µê°œí•œ **Seed-X-Instruct-7B**ëŠ” Mistral ì•„í‚¤í…ì²˜ ê¸°ë°˜ì˜ ìµœì‹  7B íŒŒë¼ë¯¸í„° ì–¸ì–´ëª¨ë¸ìž…ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” macOS Apple Silicon í™˜ê²½ì—ì„œ ì´ ëª¨ë¸ì„ **ë‘ ê°€ì§€ ë°©ë²•**ìœ¼ë¡œ ìµœì í™”í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

### ë³€í™˜ ëª©í‘œ

1. **GGUF + 4-bit ì–‘ìží™”**: llama.cpp ê¸°ë°˜ìœ¼ë¡œ Ollamaì™€ LM Studioì—ì„œ ì‚¬ìš©
2. **MLX + Apple Silicon ìµœì í™”**: Metal ê°€ì†ì„ í™œìš©í•œ ë„¤ì´í‹°ë¸Œ ì„±ëŠ¥

## ì¤€ë¹„ ì‚¬í•­

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

| í•­ëª© | ìµœì†Œ ì‚¬ì–‘ | ê¶Œìž¥ ì‚¬ì–‘ |
|------|-----------|-----------|
| macOS | 13.0+ | 14.0+ |
| Apple Silicon | M1 | M3 Pro/Max |
| RAM | 16GB | 32GB |
| ì €ìž¥ ê³µê°„ | 50GB | 100GB |

### ê°œë°œí™˜ê²½ ì„¤ì •

```bash
# Homebrew ìµœì‹  ì—…ë°ì´íŠ¸
brew update && brew upgrade

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
brew install cmake pkg-config git python@3.11

# Python ê°€ìƒí™˜ê²½ ìƒì„±
python3.11 -m venv venv_seed_x
source venv_seed_x/bin/activate

# Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install --upgrade pip
pip install huggingface_hub transformers torch
```

### Hugging Face í† í° ì„¤ì •

```bash
# Hugging Face CLI ì„¤ì¹˜ ë° ë¡œê·¸ì¸
pip install huggingface_hub[cli]
huggingface-cli login
```

> **ì°¸ê³ **: [Hugging Face í† í° ìƒì„± ê°€ì´ë“œ](https://huggingface.co/settings/tokens)ì—ì„œ Read ê¶Œí•œ í† í°ì„ ë°œê¸‰ë°›ìœ¼ì„¸ìš”.

## ë°©ë²• 1: GGUF ë³€í™˜ (llama.cpp + Ollama)

### 1-1. llama.cpp ë¹Œë“œ

```bash
# llama.cpp ì €ìž¥ì†Œ í´ë¡ 
cd ~/Downloads
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# CMake ë¹Œë“œ (ê¶Œìž¥ ë°©ë²•)
mkdir build
cd build
cmake .. -DGGML_METAL=ON
make -j$(sysctl -n hw.ncpu)

# ë¹Œë“œ ì™„ë£Œ í™•ì¸
ls -la bin/llama-cli bin/llama-quantize
```

> **ì°¸ê³ **: ê¸°ì¡´ Makefile ë°©ì‹ì€ deprecatedë˜ì—ˆìœ¼ë©°, CMakeë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. Metal ê°€ì†ì„ ìœ„í•´ `-DGGML_METAL=ON` ì˜µì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

### 1-2. Seed-X-Instruct-7B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/Models/seed-x-instruct-7b
cd ~/Models

# Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
huggingface-cli download ByteDance-Seed/Seed-X-Instruct-7B \
  --local-dir ./seed-x-instruct-7b \
  --local-dir-use-symlinks False
```

### 1-3. HF â†’ GGUF ë³€í™˜

```bash
# llama.cpp ë¹Œë“œ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd ~/Downloads/llama.cpp

# HF ëª¨ë¸ì„ GGUF FP16ìœ¼ë¡œ ë³€í™˜
python3 convert-hf-to-gguf.py \
  ~/Models/seed-x-instruct-7b \
  --outfile ~/Models/seed-x-instruct-7b-f16.gguf \
  --outtype f16

# ë³€í™˜ ê²°ê³¼ í™•ì¸
ls -lh ~/Models/seed-x-instruct-7b-f16.gguf
```

### 1-4. 4-bit Q4_K_M ì–‘ìží™”

```bash
# CMake ë¹Œë“œ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd ~/Downloads/llama.cpp/build

# Q4_K_M ì–‘ìží™” ì‹¤í–‰
./bin/llama-quantize ~/Models/seed-x-instruct-7b-f16.gguf \
                     ~/Models/seed-x-instruct-7b-q4_k_m.gguf \
                     Q4_K_M

# ì–‘ìží™” ê²°ê³¼ í™•ì¸
ls -lh ~/Models/seed-x-instruct-7b-q4_k_m.gguf
```

### 1-5. llama.cppë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ê¸°ë³¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
./bin/llama-cli -m ~/Models/seed-x-instruct-7b-q4_k_m.gguf \
                -p "ì„¤ëª…í•´ì£¼ì„¸ìš”: ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ëž˜ëŠ”?" \
                -n 200 \
                -ngl 35

# ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰
./bin/llama-cli -m ~/Models/seed-x-instruct-7b-q4_k_m.gguf \
                -i \
                -ngl 35
```

### 1-6. Ollama ì—°ë™

```bash
# Ollama ì„¤ì¹˜ (ì•„ì§ ì„¤ì¹˜í•˜ì§€ ì•Šì€ ê²½ìš°)
brew install ollama

# Modelfile ìƒì„±
cat > ~/Models/Modelfile.seed-x << EOF
FROM ~/Models/seed-x-instruct-7b-q4_k_m.gguf

{% raw %}
TEMPLATE """{{ if .System }}<|system|>
{{ .System }}<|end|>
{{ end }}{{ if .Prompt }}<|user|>
{{ .Prompt }}<|end|>
{{ end }}<|assistant|>
"""
{% endraw %}

PARAMETER stop <|end|>
PARAMETER stop <|user|>
PARAMETER stop <|assistant|>
PARAMETER temperature 0.7
PARAMETER top_p 0.9
EOF

# Ollamaì— ëª¨ë¸ ë“±ë¡
ollama create seed-x-instruct:7b-q4 -f ~/Models/Modelfile.seed-x

# Ollamaë¡œ í…ŒìŠ¤íŠ¸
ollama run seed-x-instruct:7b-q4 "macOSì—ì„œ AI ëª¨ë¸ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì€?"
```

## ë°©ë²• 2: MLX ë³€í™˜ (Apple Silicon ìµœì í™”)

### 2-1. MLX í™˜ê²½ ì„¤ì •

```bash
# ìƒˆë¡œìš´ ê°€ìƒí™˜ê²½ ìƒì„± (MLX ì „ìš©)
python3.11 -m venv venv_mlx
source venv_mlx/bin/activate

# MLX ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install mlx mlx-lm
```

### 2-2. MLX ë³€í™˜ ë° ì–‘ìží™”

```bash
# MLX ë³€í™˜ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/Models/mlx-seed-x-instruct-7b

# HF â†’ MLX ë³€í™˜ + 4-bit ì–‘ìží™”
python -m mlx_lm.convert \
  --hf-path ByteDance-Seed/Seed-X-Instruct-7B \
  --quantize \
  --q-bits 4 \
  --q-group-size 64 \
  --mlx-path ~/Models/mlx-seed-x-instruct-7b

# ë³€í™˜ ê²°ê³¼ í™•ì¸
ls -la ~/Models/mlx-seed-x-instruct-7b/
```

### 2-3. MLX ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```python
# test_mlx_seed_x.py ìƒì„±
cat > ~/Models/test_mlx_seed_x.py << 'EOF'
#!/usr/bin/env python3
import time
from mlx_lm import load, generate

def test_mlx_performance():
    print("ðŸš€ MLX Seed-X-Instruct-7B ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œìž‘")
    
    # ëª¨ë¸ ë¡œë”©
    start_time = time.time()
    model, tokenizer = load("~/Models/mlx-seed-x-instruct-7b")
    load_time = time.time() - start_time
    print(f"ðŸ“¦ ëª¨ë¸ ë¡œë”© ì‹œê°„: {load_time:.2f}ì´ˆ")
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    prompts = [
        "macOSì—ì„œ AI ê°œë°œ í™˜ê²½ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì€?",
        "Apple Silicon M3 ì¹©ì˜ ìž¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "Pythonìœ¼ë¡œ ê°„ë‹¨í•œ ì›¹ í¬ë¡¤ëŸ¬ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì€?"
    ]
    
    for i, prompt in enumerate(prompts, 1):
        print(f"\nðŸŽ¯ í…ŒìŠ¤íŠ¸ {i}: {prompt}")
        
        start_time = time.time()
        response = generate(
            model, 
            tokenizer, 
            prompt=prompt, 
            max_tokens=200,
            temp=0.7
        )
        inference_time = time.time() - start_time
        
        print(f"âš¡ ì¶”ë¡  ì‹œê°„: {inference_time:.2f}ì´ˆ")
        print(f"ðŸ“ ì‘ë‹µ: {response}")
        print("-" * 80)

if __name__ == "__main__":
    test_mlx_performance()
EOF

# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬ ë° í…ŒìŠ¤íŠ¸
chmod +x ~/Models/test_mlx_seed_x.py
python ~/Models/test_mlx_seed_x.py
```

### 2-4. MLX ì±„íŒ… ì¸í„°íŽ˜ì´ìŠ¤

```bash
# MLX ëŒ€í™”í˜• ì±„íŒ… ì‹¤í–‰
python -m mlx_lm.chat --path ~/Models/mlx-seed-x-instruct-7b
```

## LM Studio ì—°ë™

### 3-1. LM Studio ì„¤ì¹˜

```bash
# LM Studio ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜
brew install --cask lm-studio
```

### 3-2. GGUF ëª¨ë¸ ë¡œë“œ

1. **LM Studio ì‹¤í–‰**
2. **Model â†’ Load Model** ì„ íƒ
3. **Local Files** íƒ­ì—ì„œ `~/Models/seed-x-instruct-7b-q4_k_m.gguf` ì„ íƒ
4. **GPU Acceleration** ì„¤ì •:
   - **GPU Layers**: 35 (M3 ê¸°ì¤€)
   - **Context Length**: 4096
   - **Batch Size**: 512

### 3-3. LM Studio API ì„œë²„ ì„¤ì •

```bash
# LM Studio API ì„œë²„ ì‹œìž‘ (í¬íŠ¸ 1234)
# LM Studio â†’ Developer â†’ Start Server

# API í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "seed-x-instruct-7b-q4_k_m",
    "messages": [
      {"role": "user", "content": "macOSì—ì„œ AI ëª¨ë¸ ìµœì í™” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."}
    ],
    "temperature": 0.7,
    "max_tokens": 300
  }'
```

## ì„±ëŠ¥ ë¹„êµ ë° ë²¤ì¹˜ë§ˆí¬

### 4-1. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

```bash
# benchmark_seed_x.sh ìƒì„±
cat > ~/Models/benchmark_seed_x.sh << 'EOF'
#!/bin/bash

echo "ðŸ”¥ Seed-X-Instruct-7B ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"
echo "======================================"

# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
PROMPT="macOSì—ì„œ Python ê°œë°œí™˜ê²½ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."

echo "ðŸ“Š 1. llama.cpp (GGUF Q4_K_M) í…ŒìŠ¤íŠ¸"
time ~/Downloads/llama.cpp/main \
  -m ~/Models/seed-x-instruct-7b-q4_k_m.gguf \
  -p "$PROMPT" \
  -n 200 \
  -ngl 35 \
  --simple-io

echo ""
echo "ðŸ“Š 2. Ollama í…ŒìŠ¤íŠ¸"
time ollama run seed-x-instruct:7b-q4 "$PROMPT"

echo ""
echo "ðŸ“Š 3. MLX í…ŒìŠ¤íŠ¸"
time python ~/Models/test_mlx_seed_x.py

echo "âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ"
EOF

chmod +x ~/Models/benchmark_seed_x.sh
~/Models/benchmark_seed_x.sh
```

### 4-2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

```bash
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
cat > ~/Models/monitor_memory.sh << 'EOF'
#!/bin/bash

echo "ðŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"
echo "========================"

while true; do
  echo "$(date): $(vm_stat | grep "Pages free" | awk '{print $3}' | sed 's/\.//')KB ì—¬ìœ "
  sleep 5
done
EOF

chmod +x ~/Models/monitor_memory.sh
```

## zshrc Aliases ì„¤ì •

### 5-1. íŽ¸ì˜ ê¸°ëŠ¥ ì¶”ê°€

```bash
# ~/.zshrcì— ì¶”ê°€í•  aliasë“¤
cat >> ~/.zshrc << 'EOF'

# ===================
# Seed-X-Instruct-7B Aliases
# ===================

# í™˜ê²½ ë³€ìˆ˜
export SEED_X_MODELS="$HOME/Models"
export LLAMA_CPP_DIR="$HOME/Downloads/llama.cpp"

# ê°€ìƒí™˜ê²½ í™œì„±í™”
alias activate-seed="source $HOME/venv_seed_x/bin/activate"
alias activate-mlx="source $HOME/venv_mlx/bin/activate"

# llama.cpp ê´€ë ¨
alias llama-seed="$LLAMA_CPP_DIR/main -m $SEED_X_MODELS/seed-x-instruct-7b-q4_k_m.gguf"
alias llama-chat="$LLAMA_CPP_DIR/main -m $SEED_X_MODELS/seed-x-instruct-7b-q4_k_m.gguf -i -ngl 35"

# Ollama ê´€ë ¨
alias ollama-seed="ollama run seed-x-instruct:7b-q4"
alias ollama-list="ollama list | grep seed"

# MLX ê´€ë ¨
alias mlx-seed="python -m mlx_lm.chat --path $SEED_X_MODELS/mlx-seed-x-instruct-7b"
alias mlx-test="python $SEED_X_MODELS/test_mlx_seed_x.py"

# ë²¤ì¹˜ë§ˆí¬ ë° ëª¨ë‹ˆí„°ë§
alias benchmark-seed="$SEED_X_MODELS/benchmark_seed_x.sh"
alias monitor-mem="$SEED_X_MODELS/monitor_memory.sh"

# ëª¨ë¸ ë””ë ‰í† ë¦¬ ì´ë™
alias cdmodels="cd $SEED_X_MODELS"

EOF

# zshrc ìž¬ë¡œë“œ
source ~/.zshrc
```

### 5-2. ì‚¬ìš© ì˜ˆì‹œ

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
activate-seed

# llama.cpp ëŒ€í™”í˜• ëª¨ë“œ
llama-chat

# Ollama ì‹¤í–‰
ollama-seed

# MLX ì±„íŒ…
mlx-seed

# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
benchmark-seed
```

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### 6-1. ìžì£¼ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜

| ë¬¸ì œ | ì›ì¸ | í•´ê²°ë°©ë²• |
|------|------|----------|
| `Metal device not found` | Metal ì§€ì› ì•ˆë¨ | `LLAMA_METAL=1`ë¡œ ìž¬ë¹Œë“œ |
| `Out of memory` | RAM ë¶€ì¡± | `-ngl` ê°’ ê°ì†Œ |
| `Model loading failed` | íŒŒì¼ ì†ìƒ | ëª¨ë¸ ìž¬ë‹¤ìš´ë¡œë“œ |
| `Segmentation fault` | í˜¸í™˜ì„± ë¬¸ì œ | llama.cpp ìµœì‹  ë²„ì „ ì‚¬ìš© |

### 6-2. ì„±ëŠ¥ ìµœì í™” íŒ

```bash
# 1. ìµœì ì˜ GPU ë ˆì´ì–´ ìˆ˜ ì°¾ê¸°
for ngl in 20 25 30 35; do
  echo "Testing ngl=$ngl"
  time llama-seed -p "í…ŒìŠ¤íŠ¸" -n 50 -ngl $ngl
done

# 2. ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ì •
llama-seed -c 2048  # ì§§ì€ ëŒ€í™”
llama-seed -c 8192  # ê¸´ ë¬¸ì„œ ì²˜ë¦¬

# 3. ë°°ì¹˜ í¬ê¸° ìµœì í™”
llama-seed -b 256   # ë©”ëª¨ë¦¬ ì ˆì•½
llama-seed -b 1024  # ì„±ëŠ¥ ìš°ì„ 
```

## ê³ ê¸‰ í™œìš©ë²•

### 7-1. Python API í†µí•©

```python
# seed_x_api.py - í†µí•© API í´ëž˜ìŠ¤
import subprocess
import json
from typing import List, Dict
from mlx_lm import load, generate

class SeedXAPI:
    def __init__(self):
        self.mlx_model = None
        self.mlx_tokenizer = None
        self.load_mlx()
    
    def load_mlx(self):
        """MLX ëª¨ë¸ ë¡œë“œ"""
        model_path = "~/Models/mlx-seed-x-instruct-7b"
        self.mlx_model, self.mlx_tokenizer = load(model_path)
    
    def query_llama_cpp(self, prompt: str, max_tokens: int = 200) -> str:
        """llama.cppë¡œ ì¶”ë¡ """
        cmd = [
            "~/Downloads/llama.cpp/main",
            "-m", "~/Models/seed-x-instruct-7b-q4_k_m.gguf",
            "-p", prompt,
            "-n", str(max_tokens),
            "-ngl", "35",
            "--simple-io"
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        return result.stdout
    
    def query_mlx(self, prompt: str, max_tokens: int = 200) -> str:
        """MLXë¡œ ì¶”ë¡ """
        return generate(
            self.mlx_model,
            self.mlx_tokenizer,
            prompt=prompt,
            max_tokens=max_tokens,
            temp=0.7
        )
    
    def query_ollama(self, prompt: str) -> str:
        """Ollamaë¡œ ì¶”ë¡ """
        cmd = ["ollama", "run", "seed-x-instruct:7b-q4", prompt]
        result = subprocess.run(cmd, capture_output=True, text=True)
        return result.stdout

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    api = SeedXAPI()
    prompt = "Python ë”•ì…”ë„ˆë¦¬ì˜ ìž¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    
    print("ðŸ”¹ llama.cpp:", api.query_llama_cpp(prompt))
    print("ðŸ”¹ MLX:", api.query_mlx(prompt))
    print("ðŸ”¹ Ollama:", api.query_ollama(prompt))
```

### 7-2. Gradio ì›¹ ì¸í„°íŽ˜ì´ìŠ¤

```python
# web_interface.py - ì›¹ ê¸°ë°˜ ì¸í„°íŽ˜ì´ìŠ¤
import gradio as gr
from seed_x_api import SeedXAPI

api = SeedXAPI()

def chat_interface(message, history, engine):
    """ì±„íŒ… ì¸í„°íŽ˜ì´ìŠ¤"""
    if engine == "MLX":
        response = api.query_mlx(message)
    elif engine == "llama.cpp":
        response = api.query_llama_cpp(message)
    else:  # Ollama
        response = api.query_ollama(message)
    
    history.append([message, response])
    return history, ""

# Gradio ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„±
with gr.Blocks(title="Seed-X-Instruct-7B ì±„íŒ…") as demo:
    gr.Markdown("# ðŸ¤– Seed-X-Instruct-7B ë©€í‹° ì—”ì§„ ì±„íŒ…")
    
    with gr.Row():
        engine = gr.Radio(
            ["MLX", "llama.cpp", "Ollama"], 
            value="MLX", 
            label="ì¶”ë¡  ì—”ì§„"
        )
    
    chatbot = gr.Chatbot()
    msg = gr.Textbox(label="ë©”ì‹œì§€ ìž…ë ¥")
    
    msg.submit(
        chat_interface, 
        [msg, chatbot, engine], 
        [chatbot, msg]
    )

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
```

## í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë° ì„±ëŠ¥ ë¶„ì„

### 8-1. ì‹¤ì œ í™˜ê²½ í…ŒìŠ¤íŠ¸ ê²°ê³¼

**í…ŒìŠ¤íŠ¸ í™˜ê²½**:
- **ì‹œìŠ¤í…œ**: macOS 15.0 (Apple Silicon)
- **í•˜ë“œì›¨ì–´**: M3 MacBook Pro (36GB RAM)
- **Xcode**: 16.0
- **CMake**: 4.0.2

**í™˜ê²½ êµ¬ì¶• í…ŒìŠ¤íŠ¸ ê²°ê³¼**:
```
ðŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
==================================================
âœ… ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­
âœ… ë””ë ‰í† ë¦¬ ì„¤ì •  
âœ… ì˜ì¡´ì„± í™•ì¸
âœ… Python í™˜ê²½
âœ… Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬
âœ… llama.cpp ë¹Œë“œ (CMake)
âœ… ëª¨ë¸ ì •ë³´ í™•ì¸
âœ… MLX ì„¤ì¹˜
âœ… MLX í…ŒìŠ¤íŠ¸
âœ… í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
âœ… Aliases ê°€ì´ë“œ

ì„±ê³µë¥ : 11/11 (100%)
```

**MLX ì„±ëŠ¥ í…ŒìŠ¤íŠ¸**:
- **MLX ë²„ì „**: 0.26.5
- **MLX-LM ë²„ì „**: 0.26.0
- **ë§¤íŠ¸ë¦­ìŠ¤ ì—°ì‚° (1000Ã—1000)**: 0.197ì´ˆ
- **Metal ê°€ì†**: ì •ìƒ ìž‘ë™

### 8-2. ì˜ˆìƒ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ (M3 MacBook Pro ê¸°ì¤€)

| ë©”íŠ¸ë¦­ | llama.cpp (GGUF) | MLX | Ollama |
|--------|------------------|-----|--------|
| ë¡œë”© ì‹œê°„ | 3.2ì´ˆ | 2.8ì´ˆ | 4.1ì´ˆ |
| ì²« í† í°ê¹Œì§€ | 0.8ì´ˆ | 0.6ì´ˆ | 1.2ì´ˆ |
| í† í°/ì´ˆ | 25.4 | 28.7 | 22.1 |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | 4.2GB | 3.8GB | 4.5GB |
| GPU ì‚¬ìš©ë¥  | 85% | 92% | 80% |

### 8-2. í’ˆì§ˆ í‰ê°€

```bash
# í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
cat > ~/Models/quality_test.sh << 'EOF'
#!/bin/bash

PROMPTS=(
    "Pythonì˜ ìž¥ì  3ê°€ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    "macOSì—ì„œ Docker ì„¤ì¹˜ ë°©ë²•ì€?"
    "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€?"
)

for prompt in "${PROMPTS[@]}"; do
    echo "ðŸŽ¯ í”„ë¡¬í”„íŠ¸: $prompt"
    echo "----------------------------------------"
    
    echo "ðŸ“± MLX ì‘ë‹µ:"
    python -c "
from mlx_lm import load, generate
model, tok = load('~/Models/mlx-seed-x-instruct-7b')
print(generate(model, tok, prompt='$prompt', max_tokens=150))
"
    echo ""
    
    echo "ðŸ–¥ï¸  llama.cpp ì‘ë‹µ:"
    ~/Downloads/llama.cpp/main \
        -m ~/Models/seed-x-instruct-7b-q4_k_m.gguf \
        -p "$prompt" \
        -n 150 \
        --simple-io
    
    echo "========================================"
done
EOF

chmod +x ~/Models/quality_test.sh
```

## ê²°ë¡ 

ì´ íŠœí† ë¦¬ì–¼ì„ í†µí•´ **ByteDance Seed-X-Instruct-7B** ëª¨ë¸ì„ macOS Apple Silicon í™˜ê²½ì—ì„œ ìµœì í™”í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ë°©ë²•ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤:

### ðŸŽ¯ í•µì‹¬ ì„±ê³¼

1. **GGUF ë³€í™˜**: llama.cpp ê¸°ë°˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ 4-bit ì–‘ìží™” ë‹¬ì„±
2. **MLX ìµœì í™”**: Apple Silicon Metal ê°€ì†ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ í™•ë³´
3. **í†µí•© í™˜ê²½**: Ollama, LM Studio ì—°ë™ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ì§€ì›

### ðŸš€ ê¶Œìž¥ ì‚¬ìš©ë²•

- **ê°œë°œ/ì‹¤í—˜ìš©**: MLX (ìµœê³  ì„±ëŠ¥)
- **ì„œë¹„ìŠ¤ ë°°í¬ìš©**: Ollama (ì•ˆì •ì„±)
- **GUI í™˜ê²½**: LM Studio (ì‚¬ìš© íŽ¸ì˜ì„±)

### ðŸ“ˆ ë‹¤ìŒ ë‹¨ê³„

- **íŒŒì¸íŠœë‹**: MLX LoRAë¥¼ í™œìš©í•œ ì»¤ìŠ¤í…€ ëª¨ë¸ í•™ìŠµ
- **ë°°í¬**: FastAPI + MLXë¡œ API ì„œë¹„ìŠ¤ êµ¬ì¶•
- **ëª¨ë‹ˆí„°ë§**: Prometheus + Grafanaë¡œ ì„±ëŠ¥ ì¶”ì 

### ðŸ§ª ì‹¤ì œ í…ŒìŠ¤íŠ¸ í•´ë³´ê¸°

ì´ íŠœí† ë¦¬ì–¼ì˜ ëª¨ë“  ë‹¨ê³„ë¥¼ ìžë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìžˆëŠ” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

```bash
# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰
git clone https://github.com/thakicloud/thakicloud.github.io.git
cd thakicloud.github.io
python3 scripts/test_seed_x_conversion.py
```

**í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆ˜í–‰í•˜ëŠ” ìž‘ì—…**:
- âœ… ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ìžë™ í™•ì¸
- âœ… í•„ìš”í•œ ë„êµ¬ ì„¤ì¹˜ ê²€ì¦
- âœ… Python ê°€ìƒí™˜ê²½ ìžë™ ìƒì„±
- âœ… llama.cpp CMake ë¹Œë“œ ìˆ˜í–‰
- âœ… MLX í™˜ê²½ ì„¤ì • ë° í…ŒìŠ¤íŠ¸
- âœ… íŽ¸ì˜ ìŠ¤í¬ë¦½íŠ¸ ë° aliases ìƒì„±

ì´ì œ ì—¬ëŸ¬ë¶„ë„ ìµœì‹  AI ëª¨ë¸ì„ macOSì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤! ðŸŽ‰ 