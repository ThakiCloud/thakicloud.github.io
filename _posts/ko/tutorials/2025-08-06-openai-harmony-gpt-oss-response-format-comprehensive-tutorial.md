---
title: "OpenAI Harmony: GPT-OSS ì‘ë‹µ í¬ë§·ì˜ ëª¨ë“  ê²ƒ - ì™„ì „ íŠœí† ë¦¬ì–¼"
excerpt: "OpenAIì˜ ì˜¤í”ˆì†ŒìŠ¤ GPT-OSS ëª¨ë¸ì„ ìœ„í•œ Harmony ì‘ë‹µ í¬ë§·ì„ ë§ˆìŠ¤í„°í•˜ê³  ê³ ê¸‰ AI ëŒ€í™” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ë³´ì„¸ìš”."
seo_title: "OpenAI Harmony GPT-OSS ì‘ë‹µ í¬ë§· ì™„ì „ ê°€ì´ë“œ - Thaki Cloud"
seo_description: "Rustì™€ Pythonìœ¼ë¡œ êµ¬í˜„ëœ OpenAI Harmony ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•´ GPT-OSS ëª¨ë¸ì˜ êµ¬ì¡°í™”ëœ ì‘ë‹µê³¼ íˆ´ í˜¸ì¶œì„ ì™„ë²½í•˜ê²Œ ë‹¤ë£¨ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤."
date: 2025-08-06
last_modified_at: 2025-08-06
categories:
  - tutorials
  - llmops
tags:
  - openai-harmony
  - gpt-oss
  - response-format
  - rust
  - python
  - tool-calling
  - chain-of-thought
  - structured-output
  - openai
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/tutorials/openai-harmony-gpt-oss-response-format-comprehensive-tutorial/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 12ë¶„

## ì„œë¡ 

OpenAIê°€ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì‹œë¦¬ì¦ˆ **GPT-OSS**ì™€ í•¨ê»˜ ê³µê°œí•œ **Harmony**ëŠ” AI ëª¨ë¸ì˜ ì‘ë‹µ í¬ë§·ì„ í˜ì‹ ì ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. GitHubì—ì„œ 1,800ê°œ ì´ìƒì˜ ìŠ¤íƒ€ë¥¼ ë°›ì€ ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ë„˜ì–´ì„œ **ì²´ì¸ ì˜¤ë¸Œ ì”½í‚¹(Chain of Thought)**, **íˆ´ í˜¸ì¶œ**, **êµ¬ì¡°í™”ëœ ì¶œë ¥**ì„ í•˜ë‚˜ì˜ í†µí•©ëœ í¬ë§·ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

HarmonyëŠ” Rustë¡œ í•µì‹¬ ë¡œì§ì„ êµ¬í˜„í•˜ê³  Python ë°”ì¸ë”©ì„ ì œê³µí•˜ì—¬ **ê·¹ë„ì˜ ì„±ëŠ¥**ê³¼ **ê°œë°œ í¸ì˜ì„±**ì„ ë™ì‹œì— ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ GPT-OSS ëª¨ë¸ì€ Harmony í¬ë§·ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆê¸° ë•Œë¬¸ì—, ì´ í¬ë§· ì—†ì´ëŠ” ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

## Harmonyì˜ í•µì‹¬ ê°œë…

### 1. í†µí•©ëœ ì‘ë‹µ ì±„ë„ ì‹œìŠ¤í…œ

Harmonyì˜ ê°€ì¥ í˜ì‹ ì ì¸ íŠ¹ì§•ì€ **ë©€í‹° ì±„ë„ ì‘ë‹µ ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤:

```text
ğŸ”„ ì‘ë‹µ ì±„ë„ êµ¬ì¡°:
â”œâ”€â”€ analysis: ë¶„ì„ì  ì‚¬ê³  ê³¼ì •
â”œâ”€â”€ commentary: ì¶”ë¡  ë° ì¤‘ê°„ ê³¼ì •
â””â”€â”€ final: ìµœì¢… ì‚¬ìš©ì ì‘ë‹µ
```

ì´ëŠ” ê¸°ì¡´ AI ëª¨ë¸ì´ ë‹¨ì¼ í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ë˜ ê²ƒê³¼ ë‹¬ë¦¬, **ì‚¬ê³  ê³¼ì •ì„ êµ¬ì¡°í™”**í•˜ì—¬ ë” íˆ¬ëª…í•˜ê³  ì œì–´ ê°€ëŠ¥í•œ AI ì‹œìŠ¤í…œì„ ë§Œë“¤ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

### 2. ê³„ì¸µì  ì§€ì‹œ êµ¬ì¡°

```text
ğŸ—ï¸ ë©”ì‹œì§€ ê³„ì¸µ:
1. system: ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
2. developer: ê°œë°œì ì§€ì‹œì‚¬í•­ ë° ë„êµ¬ ì •ì˜
3. user: ì‚¬ìš©ì ì…ë ¥
4. assistant: AI ì‘ë‹µ
```

### 3. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê¸°ë°˜ íˆ´ ì‹œìŠ¤í…œ

```javascript
// íˆ´ ì •ì˜ ì˜ˆì‹œ
namespace functions {
  type get_weather = (_: {
    location: string,
    format?: "celsius" | "fahrenheit"
  }) => any;
  
  type search_web = (_: {
    query: string,
    limit?: number
  }) => any;
}
```

## í™˜ê²½ ì„¤ì • ë° ì„¤ì¹˜

### 1. Python í™˜ê²½ ì„¤ì •

#### ê¸°ë³¸ ì„¤ì¹˜
```bash
# pipë¥¼ ì´ìš©í•œ ì„¤ì¹˜
pip install openai-harmony

# uvë¥¼ ì´ìš©í•œ ì„¤ì¹˜ (ê¶Œì¥)
uv pip install openai-harmony

# ì„¤ì¹˜ í™•ì¸
python -c "import openai_harmony; print('Harmony ì„¤ì¹˜ ì™„ë£Œ!')"
```

#### ê°œë°œ í™˜ê²½ ì„¤ì •
```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv harmony-env
source harmony-env/bin/activate  # Linux/macOS
# harmony-env\Scripts\activate  # Windows

# ê°œë°œ ì˜ì¡´ì„± ì„¤ì¹˜
pip install openai-harmony pytest mypy ruff
```

### 2. Rust í™˜ê²½ ì„¤ì • (ì„ íƒì‚¬í•­)

Rustì—ì„œ ì§ì ‘ ì‚¬ìš©í•˜ê±°ë‚˜ ê¸°ì—¬í•˜ë ¤ëŠ” ê²½ìš°:

```bash
# Rust íˆ´ì²´ì¸ ì„¤ì¹˜
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source ~/.cargo/env

# í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/openai/harmony.git
cd harmony

# Cargo.tomlì— ì˜ì¡´ì„± ì¶”ê°€
[dependencies]
openai-harmony = { git = "https://github.com/openai/harmony" }
```

### 3. ë¡œì»¬ ê°œë°œ í™˜ê²½ (ê¸°ì—¬ììš©)

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/openai/harmony.git
cd harmony

# Python ê°€ìƒí™˜ê²½ ì„¤ì •
python -m venv .venv
source .venv/bin/activate

# maturin ì„¤ì¹˜ (Rust-Python ë°”ì¸ë”© ë¹Œë“œ ë„êµ¬)
pip install maturin pytest mypy ruff

# ê°œë°œ ëª¨ë“œë¡œ ì»´íŒŒì¼ ë° ì„¤ì¹˜
maturin develop --release

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
cargo test    # Rust í…ŒìŠ¤íŠ¸
pytest        # Python í…ŒìŠ¤íŠ¸
```

## ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. ê°„ë‹¨í•œ ëŒ€í™” ìƒì„±

#### Python ê¸°ë³¸ ì˜ˆì œ
```python
from openai_harmony import (
    load_harmony_encoding,
    HarmonyEncodingName,
    Role,
    Message,
    Conversation,
    DeveloperContent,
    SystemContent,
)

# Harmony ì¸ì½”ë”© ë¡œë“œ
enc = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)

# ëŒ€í™” êµ¬ì„±
convo = Conversation.from_messages([
    Message.from_role_and_content(
        Role.SYSTEM,
        SystemContent.new(),
    ),
    Message.from_role_and_content(
        Role.DEVELOPER,
        DeveloperContent.new().with_instructions("Talk like a pirate!")
    ),
    Message.from_role_and_content(Role.USER, "Arrr, how be you?"),
])

# í† í°ìœ¼ë¡œ ë Œë”ë§
tokens = enc.render_conversation_for_completion(convo, Role.ASSISTANT)
print("ë Œë”ë§ëœ í† í°:", tokens)

# ì‘ë‹µ íŒŒì‹± (ëª¨ë¸ ì‘ë‹µ í›„)
parsed = enc.parse_messages_from_completion_tokens(tokens, role=Role.ASSISTANT)
print("íŒŒì‹±ëœ ë©”ì‹œì§€:", parsed)
```

#### Rust ê¸°ë³¸ ì˜ˆì œ
```rust
use openai_harmony::chat::{Message, Role, Conversation};
use openai_harmony::{HarmonyEncodingName, load_harmony_encoding};

fn main() -> anyhow::Result<()> {
    let enc = load_harmony_encoding(HarmonyEncodingName::HarmonyGptOss)?;
    
    let convo = Conversation::from_messages([
        Message::from_role_and_content(Role::User, "Hello there!")
    ]);
    
    let tokens = enc.render_conversation_for_completion(&convo, Role::Assistant, None)?;
    println!("í† í°: {:?}", tokens);
    
    Ok(())
}
```

### 2. ì‹œìŠ¤í…œ ë° ê°œë°œì ë©”ì‹œì§€ í™œìš©

```python
from openai_harmony import *

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
system_content = SystemContent.new().with_knowledge_cutoff("2024-06")

# ê°œë°œì ì§€ì‹œì‚¬í•­ ë° ë„êµ¬ ì •ì˜
developer_content = DeveloperContent.new().with_instructions("""
# Instructions
Always provide detailed explanations with examples.

# Tools
## functions
namespace functions {
  type get_current_time = () => string;
  type calculate = (_: {
    operation: "add" | "subtract" | "multiply" | "divide",
    a: number,
    b: number
  }) => number;
}
""")

# ëŒ€í™” êµ¬ì„±
conversation = Conversation.from_messages([
    Message.from_role_and_content(Role.SYSTEM, system_content),
    Message.from_role_and_content(Role.DEVELOPER, developer_content),
    Message.from_role_and_content(Role.USER, "What's 15 + 27?"),
])

# ë Œë”ë§
enc = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)
tokens = enc.render_conversation_for_completion(conversation, Role.ASSISTANT)
```

## ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©

### 1. ë©€í‹°ì±„ë„ ì‘ë‹µ ì²˜ë¦¬

Harmonyì˜ í•µì‹¬ ê¸°ëŠ¥ì¸ ë©€í‹°ì±„ë„ ì‘ë‹µì„ í™œìš©í•´ë³´ê² ìŠµë‹ˆë‹¤:

```python
from openai_harmony import *

def create_reasoning_conversation():
    """ì¶”ë¡  ê³¼ì •ì„ í¬í•¨í•œ ëŒ€í™” ìƒì„±"""
    
    system_content = SystemContent.new().with_reasoning("high")
    
    developer_content = DeveloperContent.new().with_instructions("""
# Valid channels: analysis, commentary, final
Channel must be included for every message.

# Instructions
1. Use 'analysis' channel for detailed reasoning
2. Use 'commentary' channel for intermediate thoughts  
3. Use 'final' channel for the user-facing response
    """)
    
    conversation = Conversation.from_messages([
        Message.from_role_and_content(Role.SYSTEM, system_content),
        Message.from_role_and_content(Role.DEVELOPER, developer_content),
        Message.from_role_and_content(
            Role.USER, 
            "Explain quantum computing in simple terms"
        ),
    ])
    
    return conversation

# ì‚¬ìš© ì˜ˆì‹œ
conv = create_reasoning_conversation()
enc = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)
tokens = enc.render_conversation_for_completion(conv, Role.ASSISTANT)

print("=== ë Œë”ë§ëœ í”„ë¡¬í”„íŠ¸ ===")
print(tokens)
```

### 2. ê³ ê¸‰ íˆ´ ì •ì˜ ë° í˜¸ì¶œ

```python
def create_advanced_tool_conversation():
    """ë³µì¡í•œ ë„êµ¬ ì‹œìŠ¤í…œì„ í¬í•¨í•œ ëŒ€í™”"""
    
    tools_definition = """
# Tools

## functions
namespace functions {
  // ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì‹¤í–‰
  type execute_query = (_: {
    query: string,
    database: "users" | "products" | "orders",
    limit?: number
  }) => any;
  
  // íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…
  type file_operation = (_: {
    action: "read" | "write" | "delete",
    path: string,
    content?: string
  }) => any;
  
  // API í˜¸ì¶œ
  type api_request = (_: {
    method: "GET" | "POST" | "PUT" | "DELETE",
    url: string,
    headers?: Record<string, string>,
    body?: any
  }) => any;
}

## data_analysis
namespace data_analysis {
  // ë°ì´í„° ë¶„ì„ ìˆ˜í–‰
  type analyze_dataset = (_: {
    data_source: string,
    analysis_type: "statistical" | "ml" | "visualization",
    parameters?: Record<string, any>
  }) => any;
  
  // ì°¨íŠ¸ ìƒì„±
  type create_chart = (_: {
    data: any[],
    chart_type: "line" | "bar" | "pie" | "scatter",
    title?: string
  }) => any;
}
    """
    
    developer_content = DeveloperContent.new().with_instructions(f"""
# Instructions
You are a data analyst assistant. Use appropriate tools for each task.

{tools_definition}

# Channel Guidelines
- Use 'commentary' channel for tool calls
- Use 'analysis' channel for data interpretation
- Use 'final' channel for user responses
    """)
    
    conversation = Conversation.from_messages([
        Message.from_role_and_content(Role.SYSTEM, SystemContent.new()),
        Message.from_role_and_content(Role.DEVELOPER, developer_content),
        Message.from_role_and_content(
            Role.USER,
            "Analyze the sales data from last quarter and create a visualization"
        ),
    ])
    
    return conversation
```

### 3. êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±

```python
def create_structured_output_conversation():
    """êµ¬ì¡°í™”ëœ JSON ì¶œë ¥ì„ ìœ„í•œ ëŒ€í™”"""
    
    developer_content = DeveloperContent.new().with_instructions("""
# Instructions
Always respond with valid JSON in the following structure:

```json
{
  "analysis": {
    "reasoning_steps": ["step1", "step2", "step3"],
    "confidence": 0.95,
    "assumptions": ["assumption1", "assumption2"]
  },
  "result": {
    "answer": "your answer here",
    "alternatives": ["alt1", "alt2"],
    "recommendation": "your recommendation"
  },
  "metadata": {
    "processing_time": "estimated time",
    "complexity": "low|medium|high",
    "sources_needed": ["source1", "source2"]
  }
}
```

# Validation Rules
- All JSON must be valid and parseable
- Include all required fields
- Use appropriate data types
    """)
    
    conversation = Conversation.from_messages([
        Message.from_role_and_content(Role.SYSTEM, SystemContent.new()),
        Message.from_role_and_content(Role.DEVELOPER, developer_content),
        Message.from_role_and_content(
            Role.USER,
            "What's the best programming language for building a web API?"
        ),
    ])
    
    return conversation
```

## ì‹¤ë¬´ í™œìš© ì‹œë‚˜ë¦¬ì˜¤

### 1. AI ì–´ì‹œìŠ¤í„´íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•

```python
class HarmonyAssistant:
    """Harmony ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸"""
    
    def __init__(self, model_name="gpt-oss"):
        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)
        self.conversation_history = []
        
    def setup_system(self, knowledge_cutoff="2024-06", reasoning_level="medium"):
        """ì‹œìŠ¤í…œ ì„¤ì •"""
        system_content = SystemContent.new().with_knowledge_cutoff(knowledge_cutoff)
        if reasoning_level != "none":
            system_content = system_content.with_reasoning(reasoning_level)
            
        self.system_message = Message.from_role_and_content(Role.SYSTEM, system_content)
        
    def add_tools(self, tools_config):
        """ë„êµ¬ ì„¤ì • ì¶”ê°€"""
        tools_definition = self._generate_tools_definition(tools_config)
        
        developer_content = DeveloperContent.new().with_instructions(f"""
# Assistant Instructions
You are a helpful AI assistant with access to various tools.

# Response Channels
- Use 'analysis' for detailed reasoning
- Use 'commentary' for tool calls and intermediate thoughts
- Use 'final' for user-facing responses

# Available Tools
{tools_definition}

# Guidelines
- Always explain your reasoning in the analysis channel
- Call tools through the commentary channel
- Provide clear, helpful responses in the final channel
        """)
        
        self.developer_message = Message.from_role_and_content(Role.DEVELOPER, developer_content)
        
    def _generate_tools_definition(self, tools_config):
        """ë„êµ¬ ì •ì˜ ìë™ ìƒì„±"""
        definitions = []
        
        {% raw %}for namespace, tools in tools_config.items():
            definitions.append(f"## {namespace}")
            definitions.append(f"namespace {namespace} {{")
            
            for tool_name, tool_spec in tools.items():
                params = tool_spec.get('parameters', {})
                param_str = self._format_parameters(params)
                return_type = tool_spec.get('returns', 'any')
                description = tool_spec.get('description', '')
                
                if description:
                    definitions.append(f"  // {description}")
                definitions.append(f"  type {tool_name} = {param_str} => {return_type};")
                definitions.append("")
            
            definitions.append("}")
            definitions.append(""){% endraw %}
            
        return "\n".join(definitions)
    
    def _format_parameters(self, params):
        """íŒŒë¼ë¯¸í„° íƒ€ì… ì •ì˜ í¬ë§·íŒ…"""
        if not params:
            return "()"
            
        param_items = []
        {% raw %}for name, spec in params.items():
            optional = "?" if spec.get('optional', False) else ""
            param_type = spec.get('type', 'any')
            param_items.append(f"{name}{optional}: {param_type}")
            
        return f"(_: {{{', '.join(param_items)}}})"{% endraw %}
    
    def chat(self, user_message):
        """ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬"""
        # í˜„ì¬ ëŒ€í™”ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.conversation_history.append(
            Message.from_role_and_content(Role.USER, user_message)
        )
        
        # ì „ì²´ ëŒ€í™” êµ¬ì„±
        full_conversation = [
            self.system_message,
            self.developer_message,
        ] + self.conversation_history
        
        conversation = Conversation.from_messages(full_conversation)
        
        # í† í°ìœ¼ë¡œ ë Œë”ë§
        tokens = self.encoding.render_conversation_for_completion(
            conversation, Role.ASSISTANT
        )
        
        return tokens
    
    def process_response(self, model_response_tokens):
        """ëª¨ë¸ ì‘ë‹µ ì²˜ë¦¬"""
        parsed_messages = self.encoding.parse_messages_from_completion_tokens(
            model_response_tokens, role=Role.ASSISTANT
        )
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        for message in parsed_messages:
            self.conversation_history.append(message)
            
        return parsed_messages

# ì‚¬ìš© ì˜ˆì‹œ
assistant = HarmonyAssistant()
assistant.setup_system(reasoning_level="high")

# ë„êµ¬ ì„¤ì •
tools_config = {
    "functions": {
        "get_weather": {
            "description": "í˜„ì¬ ë‚ ì”¨ ì •ë³´ ì¡°íšŒ",
            "parameters": {
                "location": {"type": "string"},
                "format": {"type": '"celsius" | "fahrenheit"', "optional": True}
            },
            "returns": "WeatherInfo"
        },
        "search_web": {
            "description": "ì›¹ ê²€ìƒ‰ ìˆ˜í–‰",
            "parameters": {
                "query": {"type": "string"},
                "limit": {"type": "number", "optional": True}
            },
            "returns": "SearchResult[]"
        }
    }
}

assistant.add_tools(tools_config)

# ëŒ€í™” ì‹œì‘
prompt_tokens = assistant.chat("ì„œìš¸ì˜ ë‚ ì”¨ê°€ ì–´ë•Œ?")
print("ìƒì„±ëœ í”„ë¡¬í”„íŠ¸:")
print(prompt_tokens)
```

### 2. ì½”ë“œ ë¶„ì„ ë° ìƒì„± ì‹œìŠ¤í…œ

```python
class CodeAnalysisSystem:
    """Harmony ê¸°ë°˜ ì½”ë“œ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)
        
    def create_code_analysis_conversation(self, code, analysis_type="full"):
        """ì½”ë“œ ë¶„ì„ì„ ìœ„í•œ ëŒ€í™” ìƒì„±"""
        
        analysis_instructions = {
            "full": "Complete code analysis including structure, performance, and security",
            "security": "Focus on security vulnerabilities and best practices",
            "performance": "Analyze performance bottlenecks and optimization opportunities",
            "structure": "Evaluate code structure, design patterns, and maintainability"
        }
        
        tools_definition = """
# Tools

## code_analysis
namespace code_analysis {
  // ì •ì  ì½”ë“œ ë¶„ì„ ìˆ˜í–‰
  type static_analysis = (_: {
    code: string,
    language: string,
    rules?: string[]
  }) => AnalysisResult;
  
  // ë³µì¡ë„ ê³„ì‚°
  type calculate_complexity = (_: {
    code: string,
    metrics: ("cyclomatic" | "cognitive" | "halstead")[]
  }) => ComplexityMetrics;
  
  // ë³´ì•ˆ ì·¨ì•½ì  ìŠ¤ìº”
  type security_scan = (_: {
    code: string,
    scan_type: "sast" | "dependency" | "secrets"
  }) => SecurityReport;
  
  // ì½”ë“œ ê°œì„  ì œì•ˆ
  type suggest_improvements = (_: {
    code: string,
    focus_areas: string[]
  }) => ImprovementSuggestions;
}

## code_generation
namespace code_generation {
  // ì½”ë“œ ë¦¬íŒ©í† ë§
  type refactor_code = (_: {
    original_code: string,
    refactor_type: "extract_method" | "rename" | "simplify",
    target_language?: string
  }) => RefactoredCode;
  
  // í…ŒìŠ¤íŠ¸ ì½”ë“œ ìƒì„±
  type generate_tests = (_: {
    source_code: string,
    test_framework: string,
    coverage_level: "basic" | "comprehensive"
  }) => TestCode;
}
        """
        
        developer_content = DeveloperContent.new().with_instructions(f"""
# Code Analysis Instructions
{analysis_instructions[analysis_type]}

{tools_definition}

# Analysis Process
1. Use 'analysis' channel for detailed code examination
2. Use 'commentary' channel for tool calls and intermediate analysis
3. Use 'final' channel for summary and recommendations

# Output Format
Provide structured analysis including:
- Code quality assessment (1-10 scale)
- Identified issues with severity levels
- Specific improvement recommendations
- Code examples for suggested changes
        """)
        
        conversation = Conversation.from_messages([
            Message.from_role_and_content(Role.SYSTEM, SystemContent.new()),
            Message.from_role_and_content(Role.DEVELOPER, developer_content),
            Message.from_role_and_content(
                Role.USER,
                f"Please analyze this code:\n\n```\n{code}\n```"
            ),
        ])
        
        return conversation
    
    def analyze_code(self, code, analysis_type="full"):
        """ì½”ë“œ ë¶„ì„ ì‹¤í–‰"""
        conversation = self.create_code_analysis_conversation(code, analysis_type)
        tokens = self.encoding.render_conversation_for_completion(
            conversation, Role.ASSISTANT
        )
        return tokens

# ì‚¬ìš© ì˜ˆì‹œ
code_analyzer = CodeAnalysisSystem()

sample_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)

# ë¹„íš¨ìœ¨ì ì¸ ì¬ê·€ êµ¬í˜„
for i in range(30):
    print(f"fibonacci({i}) = {fibonacci(i)}")
"""

analysis_prompt = code_analyzer.analyze_code(sample_code, "performance")
print("ì½”ë“œ ë¶„ì„ í”„ë¡¬í”„íŠ¸:")
print(analysis_prompt)
```

### 3. ëŒ€í™”í˜• ë¬¸ì„œ ìƒì„± ì‹œìŠ¤í…œ

```python
class DocumentationGenerator:
    """Harmony ê¸°ë°˜ ë¬¸ì„œ ìƒì„± ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)
        
    def create_documentation_conversation(self, project_info, doc_type="api"):
        """ë¬¸ì„œ ìƒì„±ì„ ìœ„í•œ ëŒ€í™” ì„¤ì •"""
        
        doc_templates = {
            "api": {
                "structure": [
                    "Overview", "Authentication", "Endpoints", 
                    "Request/Response Examples", "Error Handling", "SDKs"
                ],
                "format": "OpenAPI 3.0 specification with Markdown documentation"
            },
            "user_guide": {
                "structure": [
                    "Getting Started", "Installation", "Basic Usage",
                    "Advanced Features", "Troubleshooting", "FAQ"
                ],
                "format": "User-friendly Markdown with code examples"
            },
            "technical": {
                "structure": [
                    "Architecture", "System Requirements", "Deployment",
                    "Configuration", "Monitoring", "Maintenance"
                ],
                "format": "Technical documentation with diagrams"
            }
        }
        
        template = doc_templates[doc_type]
        
        tools_definition = """
# Tools

## documentation
namespace documentation {
  // ì½”ë“œë² ì´ìŠ¤ ë¶„ì„
  type analyze_codebase = (_: {
    repository_path: string,
    include_patterns?: string[],
    exclude_patterns?: string[]
  }) => CodebaseAnalysis;
  
  // API ì—”ë“œí¬ì¸íŠ¸ ì¶”ì¶œ
  type extract_api_endpoints = (_: {
    source_files: string[],
    framework?: string
  }) => ApiEndpoint[];
  
  // ì½”ë“œ ì˜ˆì‹œ ìƒì„±
  type generate_code_examples = (_: {
    endpoint: string,
    languages: string[],
    example_type: "request" | "response" | "sdk"
  }) => CodeExample[];
  
  // ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±
  type create_diagram = (_: {
    diagram_type: "architecture" | "sequence" | "flowchart",
    description: string,
    format: "mermaid" | "plantuml"
  }) => DiagramCode;
}

## validation
namespace validation {
  // ë¬¸ì„œ í’ˆì§ˆ ê²€ì¦
  type validate_documentation = (_: {
    content: string,
    doc_type: string,
    check_links?: boolean
  }) => ValidationReport;
  
  // ì˜ˆì‹œ ì½”ë“œ í…ŒìŠ¤íŠ¸
  type test_code_examples = (_: {
    examples: CodeExample[],
    runtime_environment?: string
  }) => TestResults;
}
        """
        
        developer_content = DeveloperContent.new().with_instructions(f"""
# Documentation Generation Instructions

## Document Type: {doc_type.upper()}
Structure: {' â†’ '.join(template['structure'])}
Format: {template['format']}

{tools_definition}

# Generation Process
1. Use 'analysis' channel for project analysis and planning
2. Use 'commentary' channel for tool calls and content generation
3. Use 'final' channel for the completed documentation

# Quality Standards
- Clear, concise writing
- Comprehensive code examples
- Proper formatting and structure
- Accurate technical information
- User-friendly navigation

# Output Requirements
- Complete documentation following the specified structure
- Working code examples in multiple languages where applicable
- Proper markdown formatting with table of contents
- Cross-references and links where appropriate
        """)
        
        conversation = Conversation.from_messages([
            Message.from_role_and_content(Role.SYSTEM, SystemContent.new()),
            Message.from_role_and_content(Role.DEVELOPER, developer_content),
            Message.from_role_and_content(
                Role.USER,
                f"Generate {doc_type} documentation for this project:\n\n{project_info}"
            ),
        ])
        
        return conversation
    
    def generate_documentation(self, project_info, doc_type="api"):
        """ë¬¸ì„œ ìƒì„± ì‹¤í–‰"""
        conversation = self.create_documentation_conversation(project_info, doc_type)
        tokens = self.encoding.render_conversation_for_completion(
            conversation, Role.ASSISTANT
        )
        return tokens

# ì‚¬ìš© ì˜ˆì‹œ
doc_generator = DocumentationGenerator()

project_info = """
Project: E-commerce API
Language: Python (FastAPI)
Features:
- User authentication (JWT)
- Product catalog management
- Shopping cart functionality
- Order processing
- Payment integration (Stripe)
- Admin dashboard

Database: PostgreSQL
Deployment: Docker + Kubernetes
Authentication: OAuth 2.0 + JWT
"""

api_doc_prompt = doc_generator.generate_documentation(project_info, "api")
print("API ë¬¸ì„œ ìƒì„± í”„ë¡¬í”„íŠ¸:")
print(api_doc_prompt[:1000] + "...")
```

## ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§

### 1. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

```python
import time
import psutil
from typing import List, Dict, Any

class HarmonyPerformanceMonitor:
    """Harmony ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)
        self.metrics = []
        
    def benchmark_rendering(self, conversations: List[Conversation], iterations=100):
        """ë Œë”ë§ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        results = []
        
        for i, conversation in enumerate(conversations):
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            
            for _ in range(iterations):
                tokens = self.encoding.render_conversation_for_completion(
                    conversation, Role.ASSISTANT
                )
                
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            
            results.append({
                "conversation_id": i,
                "iterations": iterations,
                "total_time": end_time - start_time,
                "avg_time_per_render": (end_time - start_time) / iterations,
                "memory_usage_mb": end_memory - start_memory,
                "tokens_generated": len(tokens) if isinstance(tokens, str) else len(str(tokens)),
            })
            
        return results
    
    def benchmark_parsing(self, token_sets: List[str], iterations=100):
        """íŒŒì‹± ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        results = []
        
        for i, tokens in enumerate(token_sets):
            start_time = time.time()
            
            for _ in range(iterations):
                parsed = self.encoding.parse_messages_from_completion_tokens(
                    tokens, role=Role.ASSISTANT
                )
                
            end_time = time.time()
            
            results.append({
                "token_set_id": i,
                "iterations": iterations,
                "total_time": end_time - start_time,
                "avg_time_per_parse": (end_time - start_time) / iterations,
                "input_length": len(tokens),
            })
            
        return results
    
    def create_test_conversations(self, complexity_levels=["simple", "medium", "complex"]):
        """í…ŒìŠ¤íŠ¸ìš© ë‹¤ì–‘í•œ ë³µì¡ë„ì˜ ëŒ€í™” ìƒì„±"""
        conversations = []
        
        # ë‹¨ìˆœí•œ ëŒ€í™”
        if "simple" in complexity_levels:
            simple_conv = Conversation.from_messages([
                Message.from_role_and_content(Role.USER, "Hello!")
            ])
            conversations.append(simple_conv)
        
        # ì¤‘ê°„ ë³µì¡ë„ ëŒ€í™” (ì‹œìŠ¤í…œ + ê°œë°œì ë©”ì‹œì§€)
        if "medium" in complexity_levels:
            medium_conv = Conversation.from_messages([
                Message.from_role_and_content(Role.SYSTEM, SystemContent.new()),
                Message.from_role_and_content(
                    Role.DEVELOPER,
                    DeveloperContent.new().with_instructions("Be helpful and detailed.")
                ),
                Message.from_role_and_content(Role.USER, "Explain machine learning"),
            ])
            conversations.append(medium_conv)
        
        # ë³µì¡í•œ ëŒ€í™” (íˆ´ í¬í•¨)
        if "complex" in complexity_levels:
            complex_tools = """
namespace functions {
  type complex_calculation = (_: {
    operations: Array<{
      type: "add" | "subtract" | "multiply" | "divide",
      operands: number[]
    }>,
    precision?: number
  }) => CalculationResult;
  
  type data_analysis = (_: {
    dataset: string,
    analysis_types: ("statistical" | "ml" | "visualization")[],
    parameters: Record<string, any>
  }) => AnalysisReport;
}
            """
            
            complex_conv = Conversation.from_messages([
                Message.from_role_and_content(Role.SYSTEM, SystemContent.new().with_reasoning("high")),
                Message.from_role_and_content(
                    Role.DEVELOPER,
                    DeveloperContent.new().with_instructions(f"Use tools as needed:\n{complex_tools}")
                ),
                Message.from_role_and_content(
                    Role.USER,
                    "Analyze this complex dataset and provide detailed insights with visualizations"
                ),
            ])
            conversations.append(complex_conv)
            
        return conversations
    
    def run_comprehensive_benchmark(self):
        """ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ Harmony ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        # í…ŒìŠ¤íŠ¸ ëŒ€í™” ìƒì„±
        conversations = self.create_test_conversations()
        
        # ë Œë”ë§ ë²¤ì¹˜ë§ˆí¬
        print("ğŸ“Š ë Œë”ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        render_results = self.benchmark_rendering(conversations, iterations=50)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n=== ë Œë”ë§ ì„±ëŠ¥ ê²°ê³¼ ===")
        for result in render_results:
            print(f"ëŒ€í™” {result['conversation_id']}: "
                  f"{result['avg_time_per_render']:.4f}ì´ˆ/ë Œë”ë§, "
                  f"ë©”ëª¨ë¦¬: {result['memory_usage_mb']:.2f}MB")
        
        return {
            "rendering": render_results,
            "timestamp": time.time(),
            "system_info": {
                "cpu_count": psutil.cpu_count(),
                "memory_total": psutil.virtual_memory().total / 1024 / 1024 / 1024,  # GB
            }
        }

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
monitor = HarmonyPerformanceMonitor()
benchmark_results = monitor.run_comprehensive_benchmark()
```

### 2. ë©”ëª¨ë¦¬ ìµœì í™”

```python
import gc
import weakref
from typing import Optional

class OptimizedHarmonyProcessor:
    """ë©”ëª¨ë¦¬ ìµœì í™”ëœ Harmony í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, enable_caching=True, max_cache_size=100):
        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)
        self.enable_caching = enable_caching
        self.max_cache_size = max_cache_size
        self._render_cache = {% raw %}{}{% endraw %} if enable_caching else None
        self._cache_order = [] if enable_caching else None
        
    def render_with_optimization(self, conversation: Conversation, role: Role) -> str:
        """ìµœì í™”ëœ ë Œë”ë§"""
        
        # ìºì‹œ í™•ì¸
        if self.enable_caching:
            cache_key = self._generate_cache_key(conversation, role)
            if cache_key in self._render_cache:
                # ìºì‹œ íˆíŠ¸ - ìºì‹œ ìˆœì„œ ì—…ë°ì´íŠ¸
                self._cache_order.remove(cache_key)
                self._cache_order.append(cache_key)
                return self._render_cache[cache_key]
        
        # ë Œë”ë§ ìˆ˜í–‰
        tokens = self.encoding.render_conversation_for_completion(conversation, role)
        
        # ìºì‹œì— ì €ì¥ (í¬ê¸° ì œí•œ ì ìš©)
        if self.enable_caching:
            self._add_to_cache(cache_key, tokens)
        
        return tokens
    
    def _generate_cache_key(self, conversation: Conversation, role: Role) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ëŒ€í™” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì‹œ ìƒì„±
        content_str = str(conversation) + str(role)
        return str(hash(content_str))
    
    def _add_to_cache(self, key: str, value: str):
        """ìºì‹œì— ì¶”ê°€ (LRU ì •ì±…)"""
        if len(self._render_cache) >= self.max_cache_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = self._cache_order.pop(0)
            del self._render_cache[oldest_key]
        
        self._render_cache[key] = value
        self._cache_order.append(key)
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        if self.enable_caching:
            self._render_cache.clear()
            self._cache_order.clear()
            gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        if not self.enable_caching:
            return {"caching": "disabled"}
        
        return {
            "cache_size": len(self._render_cache),
            "max_cache_size": self.max_cache_size,
            "cache_usage": len(self._render_cache) / self.max_cache_size * 100,
        }

# ì‚¬ìš© ì˜ˆì‹œ
processor = OptimizedHarmonyProcessor(enable_caching=True, max_cache_size=50)

# ëŒ€í™” ìƒì„± ë° ì²˜ë¦¬
conversation = Conversation.from_messages([
    Message.from_role_and_content(Role.USER, "Hello, world!")
])

# ì²« ë²ˆì§¸ ë Œë”ë§ (ìºì‹œ ë¯¸ìŠ¤)
tokens1 = processor.render_with_optimization(conversation, Role.ASSISTANT)

# ë‘ ë²ˆì§¸ ë Œë”ë§ (ìºì‹œ íˆíŠ¸)
tokens2 = processor.render_with_optimization(conversation, Role.ASSISTANT)

print("ìºì‹œ í†µê³„:", processor.get_cache_stats())
print("ê²°ê³¼ ë™ì¼:", tokens1 == tokens2)
```

## ë””ë²„ê¹… ë° ë¬¸ì œ í•´ê²°

### 1. ë””ë²„ê¹… ë„êµ¬

```python
import json
from typing import Any, Dict

class HarmonyDebugger:
    """Harmony ë””ë²„ê¹… ë„êµ¬"""
    
    def __init__(self):
        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)
        
    def validate_conversation(self, conversation: Conversation) -> Dict[str, Any]:
        """ëŒ€í™” êµ¬ì¡° ê²€ì¦"""
        validation_result = {% raw %}{
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "message_count": len(conversation.messages),
            "role_distribution": {},
        }{% endraw %}
        
        # ì—­í• ë³„ ë©”ì‹œì§€ ìˆ˜ ê³„ì‚°
        for message in conversation.messages:
            role = str(message.role)
            validation_result["role_distribution"][role] = \
                validation_result["role_distribution"].get(role, 0) + 1
        
        # ê¸°ë³¸ ê²€ì¦ ê·œì¹™
        has_system = any(msg.role == Role.SYSTEM for msg in conversation.messages)
        has_user = any(msg.role == Role.USER for msg in conversation.messages)
        
        if not has_system:
            validation_result["warnings"].append("ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        if not has_user:
            validation_result["errors"].append("ì‚¬ìš©ì ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            validation_result["is_valid"] = False
        
        # ë©”ì‹œì§€ ìˆœì„œ ê²€ì¦
        previous_role = None
        for i, message in enumerate(conversation.messages):
            if previous_role == message.role and message.role == Role.USER:
                validation_result["warnings"].append(
                    f"ì—°ì†ëœ ì‚¬ìš©ì ë©”ì‹œì§€ ë°œê²¬ (ì¸ë±ìŠ¤: {i})"
                )
            previous_role = message.role
            
        return validation_result
    
    def analyze_tokens(self, tokens: str) -> Dict[str, Any]:
        """í† í° ë¶„ì„"""
        analysis = {% raw %}{
            "total_length": len(tokens),
            "line_count": tokens.count('\n'),
            "special_tokens": {},
            "channel_usage": {},
        }{% endraw %}
        
        # íŠ¹ìˆ˜ í† í° ê³„ì‚°
        special_patterns = [
            ("<|start|>", "start_tokens"),
            ("<|end|>", "end_tokens"), 
            ("<|message|>", "message_tokens"),
            ("system", "system_occurrences"),
            ("developer", "developer_occurrences"),
            ("user", "user_occurrences"),
            ("assistant", "assistant_occurrences"),
        ]
        
        for pattern, key in special_patterns:
            count = tokens.count(pattern)
            if count > 0:
                analysis["special_tokens"][key] = count
        
        # ì±„ë„ ì‚¬ìš© ë¶„ì„
        channels = ["analysis", "commentary", "final"]
        for channel in channels:
            count = tokens.count(f"|{channel}|")
            if count > 0:
                analysis["channel_usage"][channel] = count
                
        return analysis
    
    def debug_render_process(self, conversation: Conversation, role: Role) -> Dict[str, Any]:
        """ë Œë”ë§ ê³¼ì • ë””ë²„ê¹…"""
        debug_info = {
            "input_validation": self.validate_conversation(conversation),
            "render_successful": False,
            "tokens": None,
            "token_analysis": None,
            "error": None,
        }
        
        try:
            # ë Œë”ë§ ì‹œë„
            tokens = self.encoding.render_conversation_for_completion(conversation, role)
            debug_info["render_successful"] = True
            debug_info["tokens"] = tokens
            debug_info["token_analysis"] = self.analyze_tokens(tokens)
            
        except Exception as e:
            debug_info["error"] = {
                "type": type(e).__name__,
                "message": str(e),
            }
            
        return debug_info
    
    def format_debug_report(self, debug_info: Dict[str, Any]) -> str:
        """ë””ë²„ê·¸ ì •ë³´ í¬ë§·íŒ…"""
        report = ["=== Harmony ë””ë²„ê·¸ ë¦¬í¬íŠ¸ ===\n"]
        
        # ì…ë ¥ ê²€ì¦ ê²°ê³¼
        validation = debug_info["input_validation"]
        report.append(f"âœ… ëŒ€í™” ê²€ì¦: {'í†µê³¼' if validation['is_valid'] else 'ì‹¤íŒ¨'}")
        report.append(f"ğŸ“Š ë©”ì‹œì§€ ìˆ˜: {validation['message_count']}")
        report.append(f"ğŸ‘¥ ì—­í•  ë¶„í¬: {validation['role_distribution']}")
        
        if validation["errors"]:
            report.append(f"âŒ ì˜¤ë¥˜: {', '.join(validation['errors'])}")
        if validation["warnings"]:
            report.append(f"âš ï¸ ê²½ê³ : {', '.join(validation['warnings'])}")
        
        report.append("")
        
        # ë Œë”ë§ ê²°ê³¼
        if debug_info["render_successful"]:
            report.append("âœ… ë Œë”ë§: ì„±ê³µ")
            
            analysis = debug_info["token_analysis"]
            report.append(f"ğŸ“ í† í° ê¸¸ì´: {analysis['total_length']}")
            report.append(f"ğŸ“„ ë¼ì¸ ìˆ˜: {analysis['line_count']}")
            
            if analysis["special_tokens"]:
                report.append("ğŸ¯ íŠ¹ìˆ˜ í† í°:")
                for token, count in analysis["special_tokens"].items():
                    report.append(f"  - {token}: {count}")
            
            if analysis["channel_usage"]:
                report.append("ğŸ“º ì±„ë„ ì‚¬ìš©:")
                for channel, count in analysis["channel_usage"].items():
                    report.append(f"  - {channel}: {count}")
                    
        else:
            report.append("âŒ ë Œë”ë§: ì‹¤íŒ¨")
            error = debug_info["error"]
            report.append(f"ğŸ› ì˜¤ë¥˜ ìœ í˜•: {error['type']}")
            report.append(f"ğŸ“ ì˜¤ë¥˜ ë©”ì‹œì§€: {error['message']}")
        
        return "\n".join(report)

# ë””ë²„ê¹… ì‚¬ìš© ì˜ˆì‹œ
debugger = HarmonyDebugger()

# ë¬¸ì œê°€ ìˆëŠ” ëŒ€í™” ìƒì„± (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì—†ìŒ)
problematic_conversation = Conversation.from_messages([
    Message.from_role_and_content(Role.USER, "Hello"),
    Message.from_role_and_content(Role.USER, "Hello again"),  # ì—°ì† ì‚¬ìš©ì ë©”ì‹œì§€
])

# ë””ë²„ê¹… ì‹¤í–‰
debug_result = debugger.debug_render_process(problematic_conversation, Role.ASSISTANT)
debug_report = debugger.format_debug_report(debug_result)

print(debug_report)
```

### 2. ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

```python
class HarmonyTroubleshooter:
    """ì¼ë°˜ì ì¸ Harmony ë¬¸ì œ í•´ê²°ì‚¬"""
    
    @staticmethod
    def fix_common_issues(conversation: Conversation) -> Conversation:
        """ì¼ë°˜ì ì¸ ë¬¸ì œ ìë™ ìˆ˜ì •"""
        messages = list(conversation.messages)
        
        # 1. ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
        has_system = any(msg.role == Role.SYSTEM for msg in messages)
        if not has_system:
            system_msg = Message.from_role_and_content(Role.SYSTEM, SystemContent.new())
            messages.insert(0, system_msg)
        
        # 2. ì—°ì†ëœ ê°™ì€ ì—­í•  ë©”ì‹œì§€ í†µí•©
        consolidated_messages = []
        current_role = None
        current_content = []
        
        for message in messages:
            if message.role == current_role and message.role in [Role.USER, Role.ASSISTANT]:
                # ê°™ì€ ì—­í• ì˜ ì—°ì† ë©”ì‹œì§€ - ë‚´ìš© í†µí•©
                current_content.append(str(message.content))
            else:
                # ë‹¤ë¥¸ ì—­í•  ë˜ëŠ” ì²« ë©”ì‹œì§€
                if current_content:
                    # ì´ì „ ë©”ì‹œì§€ ì €ì¥
                    combined_content = "\n\n".join(current_content)
                    consolidated_messages.append(
                        Message.from_role_and_content(current_role, combined_content)
                    )
                
                # ìƒˆ ë©”ì‹œì§€ ì‹œì‘
                current_role = message.role
                current_content = [str(message.content)]
        
        # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì²˜ë¦¬
        if current_content:
            combined_content = "\n\n".join(current_content)
            consolidated_messages.append(
                Message.from_role_and_content(current_role, combined_content)
            )
        
        return Conversation.from_messages(consolidated_messages)
    
    @staticmethod
    def validate_and_fix(conversation: Conversation) -> tuple[Conversation, list[str]]:
        """ê²€ì¦ ë° ìë™ ìˆ˜ì •"""
        fixes_applied = []
        
        # ìë™ ìˆ˜ì • ì ìš©
        original_count = len(conversation.messages)
        fixed_conversation = HarmonyTroubleshooter.fix_common_issues(conversation)
        
        if len(fixed_conversation.messages) != original_count:
            fixes_applied.append(f"ë©”ì‹œì§€ ìˆ˜ ì¡°ì •: {original_count} â†’ {len(fixed_conversation.messages)}")
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ í™•ì¸
        if not any(msg.role == Role.SYSTEM for msg in conversation.messages):
            if any(msg.role == Role.SYSTEM for msg in fixed_conversation.messages):
                fixes_applied.append("ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€")
        
        return fixed_conversation, fixes_applied

# ë¬¸ì œ í•´ê²° ì‚¬ìš© ì˜ˆì‹œ
problematic_conv = Conversation.from_messages([
    Message.from_role_and_content(Role.USER, "First question"),
    Message.from_role_and_content(Role.USER, "Second question"),
    Message.from_role_and_content(Role.ASSISTANT, "First answer"),
    Message.from_role_and_content(Role.ASSISTANT, "Second answer"),
])

print("=== ì›ë³¸ ëŒ€í™” ===")
for i, msg in enumerate(problematic_conv.messages):
    print(f"{i}: {msg.role} - {str(msg.content)[:50]}...")

fixed_conv, fixes = HarmonyTroubleshooter.validate_and_fix(problematic_conv)

print(f"\n=== ìˆ˜ì •ëœ ëŒ€í™” (ìˆ˜ì •ì‚¬í•­: {fixes}) ===")
for i, msg in enumerate(fixed_conv.messages):
    print(f"{i}: {msg.role} - {str(msg.content)[:50]}...")
```

## ê²°ë¡ 

OpenAI HarmonyëŠ” AI ëª¨ë¸ì˜ ì‘ë‹µ í¬ë§·ì„ **ì²´ê³„ì ì´ê³  êµ¬ì¡°í™”ëœ ë°©ì‹**ìœ¼ë¡œ í˜ì‹ í•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. 1,800ê°œ ì´ìƒì˜ GitHub ìŠ¤íƒ€ê°€ ë³´ì—¬ì£¼ë“¯, ê°œë°œì ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ê·¸ ê°€ì¹˜ë¥¼ ì¸ì •ë°›ê³  ìˆìŠµë‹ˆë‹¤.

### í•µì‹¬ ì¥ì  ìš”ì•½
- **ë©€í‹°ì±„ë„ ì‘ë‹µ**: ì‚¬ê³  ê³¼ì •ì„ ë¶„ë¦¬í•˜ì—¬ íˆ¬ëª…í•œ AI ì‹œìŠ¤í…œ êµ¬ì¶•
- **êµ¬ì¡°í™”ëœ íˆ´ í˜¸ì¶œ**: ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê¸°ë°˜ì˜ ì²´ê³„ì ì¸ í•¨ìˆ˜ ê´€ë¦¬
- **ê³ ì„±ëŠ¥ êµ¬í˜„**: Rust ê¸°ë°˜ í•µì‹¬ ë¡œì§ìœ¼ë¡œ ê·¹ë„ì˜ ì„±ëŠ¥ ìµœì í™”
- **ê°œë°œì ì¹œí™”ì **: Python ë°”ì¸ë”©ìœ¼ë¡œ ì‰¬ìš´ í†µí•©

### í™œìš© ê¶Œì¥ ì‹œë‚˜ë¦¬ì˜¤
- **AI ì–´ì‹œìŠ¤í„´íŠ¸**: ì¶”ë¡  ê³¼ì •ì´ íˆ¬ëª…í•œ ëŒ€í™”í˜• ì‹œìŠ¤í…œ
- **ì½”ë“œ ë¶„ì„**: êµ¬ì¡°í™”ëœ ì½”ë“œ ë¦¬ë·° ë° ê°œì„  ì œì•ˆ
- **ë¬¸ì„œ ìƒì„±**: ì²´ê³„ì ì¸ ê¸°ìˆ  ë¬¸ì„œ ìë™í™”
- **ì—”í„°í”„ë¼ì´ì¦ˆ AI**: í†µì œ ê°€ëŠ¥í•˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ AI ì‹œìŠ¤í…œ

HarmonyëŠ” ë‹¨ìˆœí•œ ì‘ë‹µ í¬ë§·ì„ ë„˜ì–´ì„œ **AI ì‹œìŠ¤í…œì˜ ë‚´ë¶€ ì‘ë™ì„ êµ¬ì¡°í™”í•˜ê³  ì œì–´í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬**ë¥¼ ì œê³µí•©ë‹ˆë‹¤. GPT-OSSì™€ í•¨ê»˜ ì‚¬ìš©ë  ë•Œ ê·¸ ì§„ê°€ë¥¼ ë°œíœ˜í•˜ë©°, ì•ìœ¼ë¡œ ë” ë§ì€ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ë“¤ì´ ì´ í¬ë§·ì„ ì±„íƒí•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

íŠ¹íˆ **Rustì˜ ì„±ëŠ¥**ê³¼ **Pythonì˜ í¸ì˜ì„±**ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ëŠ” ì‹¤ë¬´ í™˜ê²½ì—ì„œ ë†’ì€ ì²˜ë¦¬ëŸ‰ê³¼ ì•ˆì •ì„±ì„ ë³´ì¥í•˜ë©°, ì²´ê³„ì ì¸ AI ì‹œìŠ¤í…œ ê°œë°œì˜ ìƒˆë¡œìš´ í‘œì¤€ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤.

---

**ì°¸ê³  ìë£Œ**
- [OpenAI Harmony GitHub Repository](https://github.com/openai/harmony)
- [GPT-OSS Model Information](https://github.com/openai/harmony#gpt-oss)
- [Harmony Documentation](https://github.com/openai/harmony/tree/main/docs)
- [AGENTS.md - Advanced Usage Guide](https://github.com/openai/harmony/blob/main/AGENTS.md)