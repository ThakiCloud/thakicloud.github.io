---
title: "Firecrawl - AIë¥¼ ìœ„í•œ ì›¹ ë°ì´í„° ìˆ˜ì§‘ì˜ í˜ì‹ ì  ì†”ë£¨ì…˜ ì™„ì „ ê°€ì´ë“œ"
excerpt: "ì›¹ì‚¬ì´íŠ¸ë¥¼ AI ì¹œí™”ì ì¸ ë§ˆí¬ë‹¤ìš´ê³¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” Firecrawlì˜ ì™„ì „í•œ í™œìš©ë²•. Python/Node.js SDK ì‹¤ìŠµë¶€í„° ê³ ê¸‰ í¬ë¡¤ë§ ê¸°ë²•ê¹Œì§€"
seo_title: "Firecrawl AI ì›¹ ìŠ¤í¬ë˜í•‘ ì™„ì „ ê°€ì´ë“œ - Python/Node.js ì‹¤ìŠµ - Thaki Cloud"
seo_description: "ì›¹ ë°ì´í„°ë¥¼ AI ëª¨ë¸ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” Firecrawl í”Œë«í¼ í™œìš©ë²•. ì‹¤ì œ ì½”ë“œ ì˜ˆì œì™€ í•¨ê»˜ ë°°ìš°ëŠ” ìŠ¤í¬ë˜í•‘/í¬ë¡¤ë§ ì™„ì „ ê°€ì´ë“œ"
date: 2025-08-21
last_modified_at: 2025-08-21
categories:
  - tutorials
  - llmops
tags:
  - firecrawl
  - web-scraping
  - ai-data
  - python-sdk
  - nodejs-sdk
  - data-extraction
  - rag
  - llm
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "spider"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/tutorials/firecrawl-ai-web-scraping-complete-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 12ë¶„

## ì„œë¡ 

ì›¹ì—ëŠ” ìˆ˜ë§ì€ ì •ë³´ê°€ ìˆì§€ë§Œ, ì´ë¥¼ AI ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ê°€ê³µí•˜ëŠ” ê²ƒì€ ë³µì¡í•œ ì‘ì—…ì…ë‹ˆë‹¤. **Firecrawl**ì€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í˜ì‹ ì ì¸ ì›¹ ë°ì´í„° APIë¡œ, ì›¹ì‚¬ì´íŠ¸ë¥¼ AI ì¹œí™”ì ì¸ ë§ˆí¬ë‹¤ìš´ì´ë‚˜ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ì†ì‰½ê²Œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë³¸ ê°€ì´ë“œì—ì„œëŠ” Firecrawlì˜ í•µì‹¬ ê¸°ëŠ¥ë¶€í„° ì‹¤ì œ macOS í™˜ê²½ì—ì„œì˜ Python/Node.js SDK í™œìš©ë²•ê¹Œì§€ ì™„ì „íˆ ë‹¤ë£¨ì–´ë³´ê² ìŠµë‹ˆë‹¤.

### ì™œ Firecrawlì„ ì‚¬ìš©í•´ì•¼ í• ê¹Œìš”?

- ğŸ”¥ **AI ìµœì í™”**: LLMì´ ë°”ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ê¹”ë”í•œ ë§ˆí¬ë‹¤ìš´ ì œê³µ
- ğŸš€ **ê°„í¸í•œ API**: ë³µì¡í•œ ì…€ë ˆë‹ˆì›€ ì„¤ì • ì—†ì´ URLë§Œìœ¼ë¡œ ìŠ¤í¬ë˜í•‘
- ğŸ“Š **êµ¬ì¡°í™”ëœ ë°ì´í„°**: Pydantic/Zod ìŠ¤í‚¤ë§ˆë¡œ ì •í™•í•œ ë°ì´í„° ì¶”ì¶œ
- âš¡ **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ í˜ì´ì§€ ë™ì‹œ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
- ğŸ¯ **í¬ë¡¤ë§**: ì‚¬ì´íŠ¸ë§µ ì—†ì´ë„ ëª¨ë“  í•˜ìœ„ í˜ì´ì§€ ìë™ íƒìƒ‰

## Firecrawl í”„ë¡œì íŠ¸ ê°œìš”

[Firecrawl](https://github.com/firecrawl/firecrawl)ì€ GitHubì—ì„œ 49.9k+ ìŠ¤íƒ€ë¥¼ ë°›ì€ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ë¡œ, ì›¹ ë°ì´í„°ë¥¼ AI ëª¨ë¸ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” API ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

| ê¸°ëŠ¥ | ì„¤ëª… | ì‚¬ìš© ì‚¬ë¡€ |
|------|------|-----------|
| **Smart Scraping** | JavaScript ë Œë”ë§ëœ ì½˜í…ì¸ ë„ ì •í™•íˆ ì¶”ì¶œ | SPA, React ì•± ìŠ¤í¬ë˜í•‘ |
| **LLM Extraction** | AI ëª¨ë¸ë¡œ êµ¬ì¡°í™”ëœ ë°ì´í„° ìë™ ì¶”ì¶œ | ë‰´ìŠ¤ ê¸°ì‚¬, ì œí’ˆ ì •ë³´ ìˆ˜ì§‘ |
| **Batch Processing** | ì—¬ëŸ¬ URL ë™ì‹œ ì²˜ë¦¬ | ëŒ€ìš©ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ |
| **Actions** | í´ë¦­, ì…ë ¥ ë“± ë¸Œë¼ìš°ì € ìƒí˜¸ì‘ìš© | ë¡œê·¸ì¸ í›„ ë°ì´í„° ìˆ˜ì§‘ |

### ì§€ì› ì–¸ì–´ ë° í”„ë ˆì„ì›Œí¬

- **SDK**: Python, Node.js
- **LLM í”„ë ˆì„ì›Œí¬**: LangChain, LlamaIndex, Crew.ai
- **ë¡œìš°ì½”ë“œ**: Dify, Langflow, Flowise AI

## macOS í™˜ê²½ ì„¤ì • ë° ì„¤ì¹˜

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

```bash
# ê°œë°œí™˜ê²½ í™•ì¸
python3 --version  # Python 3.8+
node --version      # Node.js 16+
npm --version       # NPM 8+
```

**í…ŒìŠ¤íŠ¸ í™˜ê²½**:
- macOS Sonoma 14.0+
- Python 3.12.8
- Node.js 22.16.0
- NPM 10.8.1

### ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

```bash
# í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/firecrawl-tutorial && cd ~/firecrawl-tutorial

# ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰
curl -sSL https://raw.githubusercontent.com/your-repo/firecrawl-setup.sh | bash
```

### ìˆ˜ë™ ì„¤ì¹˜ ê³¼ì •

```bash
# 1. Python ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv firecrawl-venv
source firecrawl-venv/bin/activate

# 2. Python SDK ì„¤ì¹˜
pip install firecrawl-py pydantic

# 3. Node.js í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
npm init -y
npm install @mendable/firecrawl-js zod

# 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo "FIRECRAWL_API_KEY=fc-YOUR_API_KEY" > .env
```

### API í‚¤ ë°œê¸‰

1. [firecrawl.dev](https://firecrawl.dev)ì—ì„œ ê³„ì • ìƒì„±
2. ëŒ€ì‹œë³´ë“œì—ì„œ API í‚¤ ë°œê¸‰
3. `.env` íŒŒì¼ì— í‚¤ ì €ì¥

## Python SDK í™œìš©ë²•

### ê¸°ë³¸ ìŠ¤í¬ë˜í•‘

```python
from firecrawl import Firecrawl

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
firecrawl = Firecrawl(api_key="fc-YOUR_API_KEY")

# ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
doc = firecrawl.scrape(
    "https://firecrawl.dev",
    formats=["markdown", "html"],
)

print(f"ì œëª©: {doc.metadata.get('title')}")
print(f"ë§ˆí¬ë‹¤ìš´: {doc.markdown[:200]}...")
```

### êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ

```python
from pydantic import BaseModel, Field
from typing import List

# ë°ì´í„° ëª¨ë¸ ì •ì˜
class Article(BaseModel):
    title: str
    points: int
    by: str
    commentsURL: str

class TopArticles(BaseModel):
    top: List[Article] = Field(..., description="ìƒìœ„ 5ê°œ ê¸°ì‚¬")

# êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
doc = firecrawl.scrape(
    "https://news.ycombinator.com",
    formats=[{"type": "json", "schema": TopArticles}],
)

# ê²°ê³¼ ì²˜ë¦¬
for article in doc.json['top']:
    print(f"ğŸ“° {article['title']} ({article['points']} points)")
```

### ë°°ì¹˜ ìŠ¤í¬ë˜í•‘

```python
# ì—¬ëŸ¬ URL ë™ì‹œ ì²˜ë¦¬
urls = [
    "https://firecrawl.dev",
    "https://docs.firecrawl.dev",
    "https://github.com/firecrawl/firecrawl"
]

response = firecrawl.batch_scrape(
    urls=urls,
    formats=["markdown"]
)

print(f"ì‘ì—… ID: {response['jobId']}")
print(f"ìƒíƒœ: {response['status']}")
```

### ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§

```python
# ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
response = firecrawl.crawl(
    "https://docs.firecrawl.dev",
    limit=50,
    scrape_options={"formats": ["markdown", "html"]},
    poll_interval=30,
)

print(f"í¬ë¡¤ë§ëœ í˜ì´ì§€ ìˆ˜: {len(response['data'])}")

for page in response['data'][:5]:
    title = page['metadata']['title']
    url = page['metadata']['url']
    print(f"ğŸ“„ {title} - {url}")
```

## Node.js SDK í™œìš©ë²•

### í”„ë¡œì íŠ¸ ì„¤ì •

```javascript
// package.json ì„¤ì •
{
  "name": "firecrawl-tutorial",
  "type": "module",  // ES6 ëª¨ë“ˆ ì‚¬ìš©
  "dependencies": {
    "@mendable/firecrawl-js": "^latest",
    "zod": "^latest"
  }
}
```

### ê¸°ë³¸ ìŠ¤í¬ë˜í•‘

```javascript
import Firecrawl from '@mendable/firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR_API_KEY' });

// ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
const doc = await firecrawl.scrape('https://firecrawl.dev', {
  formats: ['markdown', 'html'],
});

console.log(`ì œëª©: ${doc.metadata?.title}`);
console.log(`ë§ˆí¬ë‹¤ìš´: ${doc.markdown?.substring(0, 200)}...`);
```

### Zod ìŠ¤í‚¤ë§ˆë¡œ ë°ì´í„° ì¶”ì¶œ

```javascript
import { z } from 'zod';

// Zod ìŠ¤í‚¤ë§ˆ ì •ì˜
const ArticleSchema = z.object({
  title: z.string(),
  points: z.number(),
  by: z.string(),
  commentsURL: z.string(),
});

const TopArticlesSchema = z.object({
  top: z.array(ArticleSchema)
    .length(5)
    .describe('ìƒìœ„ 5ê°œ ê¸°ì‚¬'),
});

// êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
const extractRes = await firecrawl.extract({
  urls: ['https://news.ycombinator.com'],
  schema: TopArticlesSchema,
  prompt: 'ìƒìœ„ 5ê°œ ê¸°ì‚¬ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”',
});

// ê²°ê³¼ ì²˜ë¦¬
const articles = extractRes.data[0].extract.top;
articles.forEach((article, i) => {
  console.log(`${i+1}. ${article.title} (${article.points} points)`);
});
```

### ê³ ê¸‰ í¬ë¡¤ë§ ì˜µì…˜

```javascript
// ìƒì„¸ í¬ë¡¤ë§ ì„¤ì •
const response = await firecrawl.crawl('https://docs.firecrawl.dev', {
  limit: 100,
  scrapeOptions: { 
    formats: ['markdown', 'html'],
    onlyMainContent: true,
    includeTags: ['h1', 'h2', 'h3', 'p', 'a'],
    excludeTags: ['script', 'style', 'nav', 'footer']
  },
  crawlerOptions: {
    includes: ['**/docs/**'],
    excludes: ['**/blog/**', '**/news/**'],
    returnOnlyUrls: false,
    maxDepth: 3
  }
});

console.log(`í¬ë¡¤ë§ ì™„ë£Œ: ${response.data.length}í˜ì´ì§€`);
```

## ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©

### Actionsì„ ì´ìš©í•œ ìƒí˜¸ì‘ìš©

```javascript
// êµ¬ê¸€ ê²€ìƒ‰ ìë™í™”
const doc = await firecrawl.scrape('google.com', {
  formats: ['markdown'],
  actions: [
    { type: 'wait', milliseconds: 2000 },
    { type: 'click', selector: 'textarea[title="Search"]' },
    { type: 'write', text: 'firecrawl' },
    { type: 'press', key: 'ENTER' },
    { type: 'wait', milliseconds: 3000 },
    { type: 'click', selector: 'h3' },
    { type: 'screenshot' }
  ]
});
```

### ì»¤ìŠ¤í…€ í—¤ë” ë° ì¸ì¦

```python
# ì¸ì¦ì´ í•„ìš”í•œ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘
doc = firecrawl.scrape(
    "https://private-site.com/api/data",
    formats=["json"],
    headers={
        "Authorization": "Bearer YOUR_TOKEN",
        "User-Agent": "Custom Bot 1.0"
    }
)
```

### ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„

```python
import time
from typing import Optional

def robust_scrape(url: str, max_retries: int = 3) -> Optional[dict]:
    """ê²¬ê³ í•œ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜"""
    for attempt in range(max_retries):
        try:
            doc = firecrawl.scrape(url, formats=["markdown"])
            return doc
        except Exception as e:
            print(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
    return None
```

## ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€

### ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

```python
def monitor_news_sites():
    """ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ëª¨ë‹ˆí„°ë§"""
    news_urls = [
        "https://news.ycombinator.com",
        "https://techcrunch.com",
        "https://arstechnica.com"
    ]
    
    for url in news_urls:
        doc = firecrawl.scrape(url, formats=["markdown"])
        
        # AI ëª¨ë¸ë¡œ ìš”ì•½ ìƒì„±
        summary = generate_summary(doc.markdown)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        save_article({
            'url': url,
            'content': doc.markdown,
            'summary': summary,
            'timestamp': datetime.now()
        })
```

### ê²½ìŸì‚¬ ê°€ê²© ëª¨ë‹ˆí„°ë§

```python
class ProductInfo(BaseModel):
    name: str
    price: float
    availability: str
    rating: Optional[float]

def track_competitor_prices(product_urls: List[str]):
    """ê²½ìŸì‚¬ ê°€ê²© ì¶”ì """
    for url in product_urls:
        doc = firecrawl.scrape(
            url,
            formats=[{"type": "json", "schema": ProductInfo}]
        )
        
        product = ProductInfo(**doc.json)
        
        # ê°€ê²© ë³€ë™ ì•Œë¦¼
        if price_changed(product):
            send_notification(f"ê°€ê²© ë³€ë™: {product.name} - ${product.price}")
```

### RAG ì‹œìŠ¤í…œ ë°ì´í„° ìˆ˜ì§‘

```python
def build_knowledge_base(website_url: str):
    """RAGë¥¼ ìœ„í•œ ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•"""
    # ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
    response = firecrawl.crawl(
        website_url,
        limit=1000,
        scrape_options={"formats": ["markdown"]}
    )
    
    documents = []
    for page in response['data']:
        # ë²¡í„° ì„ë² ë”©ì„ ìœ„í•œ ë¬¸ì„œ ì¤€ë¹„
        doc = {
            'content': page['markdown'],
            'metadata': {
                'url': page['metadata']['url'],
                'title': page['metadata']['title'],
                'timestamp': datetime.now()
            }
        }
        documents.append(doc)
    
    # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    vector_store.add_documents(documents)
```

## ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§

### ìš”ì²­ ì œí•œ ë° ë¹„ìš© ê´€ë¦¬

```python
import asyncio
from asyncio import Semaphore

class FirecrawlManager:
    def __init__(self, api_key: str, max_concurrent: int = 5):
        self.firecrawl = Firecrawl(api_key=api_key)
        self.semaphore = Semaphore(max_concurrent)
        self.request_count = 0
        self.daily_limit = 1000
    
    async def scrape_with_limit(self, url: str):
        """ì œí•œëœ ë™ì‹œ ìš”ì²­ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘"""
        if self.request_count >= self.daily_limit:
            raise Exception("ì¼ì¼ ìš”ì²­ í•œë„ ì´ˆê³¼")
        
        async with self.semaphore:
            self.request_count += 1
            return self.firecrawl.scrape(url, formats=["markdown"])
```

### ìºì‹± ì‹œìŠ¤í…œ êµ¬í˜„

```python
import hashlib
import json
from functools import wraps

def cache_result(ttl_seconds: int = 3600):
    """ê²°ê³¼ ìºì‹± ë°ì½”ë ˆì´í„°"""
    cache = {}
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ìºì‹œ í‚¤ ìƒì„±
            key = hashlib.md5(str(args + tuple(kwargs.items())).encode()).hexdigest()
            
            # ìºì‹œ í™•ì¸
            if key in cache:
                result, timestamp = cache[key]
                if time.time() - timestamp < ttl_seconds:
                    return result
            
            # ìƒˆë¡œìš´ ìš”ì²­
            result = func(*args, **kwargs)
            cache[key] = (result, time.time())
            return result
        
        return wrapper
    return decorator

@cache_result(ttl_seconds=1800)
def cached_scrape(url: str):
    return firecrawl.scrape(url, formats=["markdown"])
```

## ê°œë°œ í™˜ê²½ ì„¤ì • ë° ìë™í™”

### zshrc Aliases ì„¤ì •

```bash
# ~/.zshrcì— ì¶”ê°€
export FIRECRAWL_TEST_DIR="$HOME/firecrawl-tutorial"

# ê¸°ë³¸ ëª…ë ¹ì–´
alias fctest="cd $FIRECRAWL_TEST_DIR"
alias fcsetup="cd $FIRECRAWL_TEST_DIR && ./setup.sh"
alias fcpython="cd $FIRECRAWL_TEST_DIR && source venv/bin/activate && python test.py"
alias fcnode="cd $FIRECRAWL_TEST_DIR && node test.js"

# ê°€ìƒí™˜ê²½ ê´€ë¦¬
alias fcvenv="cd $FIRECRAWL_TEST_DIR && source venv/bin/activate"
alias fcdeactivate="deactivate"

# ê°œë°œ ë„êµ¬
alias fcstatus="python3 --version && node --version && pip list | grep firecrawl"
alias fcclean="rm -rf venv node_modules package-lock.json"

# ë¬¸ì„œ ë° ëŒ€ì‹œë³´ë“œ
alias fcdocs="open https://docs.firecrawl.dev"
alias fcdash="open https://firecrawl.dev/dashboard"
```

### ìë™í™” ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# daily-scraping.sh - ì¼ì¼ ìŠ¤í¬ë˜í•‘ ìë™í™”

set -e

echo "ğŸ”¥ ì¼ì¼ Firecrawl ìŠ¤í¬ë˜í•‘ ì‹œì‘..."

# ê°€ìƒí™˜ê²½ í™œì„±í™”
source $FIRECRAWL_TEST_DIR/venv/bin/activate

# Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python << 'EOF'
from firecrawl import Firecrawl
import os
from datetime import datetime

firecrawl = Firecrawl(api_key=os.getenv('FIRECRAWL_API_KEY'))

# ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘
urls = [
    "https://news.ycombinator.com",
    "https://techcrunch.com/ai/",
    "https://www.theverge.com/ai-artificial-intelligence"
]

results = []
for url in urls:
    try:
        doc = firecrawl.scrape(url, formats=["markdown"])
        results.append({
            'url': url,
            'title': doc.metadata.get('title'),
            'content_length': len(doc.markdown),
            'timestamp': datetime.now().isoformat()
        })
        print(f"âœ… {url} ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ {url} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")

# ê²°ê³¼ ì €ì¥
import json
with open(f"daily_scrape_{datetime.now().strftime('%Y%m%d')}.json", 'w') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"ğŸ“Š ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(results)}ê°œ ì‚¬ì´íŠ¸")
EOF

echo "âœ… ì¼ì¼ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ"
```

## ë¬¸ì œ í•´ê²° ë° ë””ë²„ê¹…

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜ì™€ í•´ê²°ë²•

#### 1. Rate Limit ì˜¤ë¥˜

```python
# í•´ê²°ë²•: ìš”ì²­ ê°„ê²© ì¡°ì ˆ
import time

def rate_limited_scrape(urls, delay=1):
    results = []
    for url in urls:
        try:
            result = firecrawl.scrape(url, formats=["markdown"])
            results.append(result)
            time.sleep(delay)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
        except Exception as e:
            print(f"ì˜¤ë¥˜: {url} - {e}")
    return results
```

#### 2. JavaScript ë Œë”ë§ ë¬¸ì œ

```javascript
// í•´ê²°ë²•: ëŒ€ê¸° ì‹œê°„ ì¶”ê°€
const doc = await firecrawl.scrape(url, {
  formats: ['markdown'],
  waitFor: 'networkidle',  // ë„¤íŠ¸ì›Œí¬ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
  timeout: 30000,          // 30ì´ˆ íƒ€ì„ì•„ì›ƒ
  actions: [
    { type: 'wait', milliseconds: 5000 }  // ì¶”ê°€ ëŒ€ê¸°
  ]
});
```

#### 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”

```python
# í•´ê²°ë²•: ë°°ì¹˜ ì²˜ë¦¬ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
import gc

def process_large_dataset(urls, batch_size=10):
    for i in range(0, len(urls), batch_size):
        batch = urls[i:i+batch_size]
        
        # ë°°ì¹˜ ì²˜ë¦¬
        results = firecrawl.batch_scrape(batch, formats=["markdown"])
        
        # ê²°ê³¼ ì²˜ë¦¬
        process_results(results)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
```

### ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§

```python
import logging
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('firecrawl.log'),
        logging.StreamHandler()
    ]
)

class FirecrawlLogger:
    def __init__(self, firecrawl_client):
        self.client = firecrawl_client
        self.logger = logging.getLogger(__name__)
    
    def scrape_with_logging(self, url, **kwargs):
        start_time = datetime.now()
        self.logger.info(f"ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
        
        try:
            result = self.client.scrape(url, **kwargs)
            duration = datetime.now() - start_time
            
            self.logger.info(f"ìŠ¤í¬ë˜í•‘ ì„±ê³µ: {url} ({duration.total_seconds():.2f}ì´ˆ)")
            return result
            
        except Exception as e:
            duration = datetime.now() - start_time
            self.logger.error(f"ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {url} - {e} ({duration.total_seconds():.2f}ì´ˆ)")
            raise
```

## ì‹¤ìŠµ í…ŒìŠ¤íŠ¸ ê²°ê³¼

í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œ ì‹¤ì œ ì‹¤í–‰í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

### Python SDK í…ŒìŠ¤íŠ¸

```bash
$ source firecrawl-venv/bin/activate
$ python test-python-sdk.py

ğŸ”¥ Firecrawl Python SDK í…ŒìŠ¤íŠ¸ ì‹œì‘
==================================================
âœ… Firecrawl í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ

ğŸ” 1. ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸
----------------------------------------
âœ… ìŠ¤í¬ë˜í•‘ ì„±ê³µ!
ğŸ“„ ë§ˆí¬ë‹¤ìš´ ê¸¸ì´: 2847 ë¬¸ì
ğŸ·ï¸ HTML ê¸¸ì´: 15234 ë¬¸ì
ğŸ“ ì œëª©: Firecrawl - Turn websites into LLM-ready markdown
ğŸ“– ë§ˆí¬ë‹¤ìš´ ë¯¸ë¦¬ë³´ê¸°:
# Firecrawl

**The Open Source Web Crawler for AI**

Empower your AI apps with clean data from any website...

ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼
==================================================
âœ… í†µê³¼: 4/4
âŒ ì‹¤íŒ¨: 0/4
ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!
```

### Node.js SDK í…ŒìŠ¤íŠ¸

```bash
$ node test-nodejs-sdk.js

ğŸ”¥ Firecrawl Node.js SDK í…ŒìŠ¤íŠ¸ ì‹œì‘
==================================================
âœ… Firecrawl í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ

ğŸ” 1. ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸
----------------------------------------
âœ… ìŠ¤í¬ë˜í•‘ ì„±ê³µ!
ğŸ“„ ë§ˆí¬ë‹¤ìš´ ê¸¸ì´: 2847 ë¬¸ì
ğŸ·ï¸ HTML ê¸¸ì´: 15234 ë¬¸ì
ğŸ“ ì œëª©: Firecrawl - Turn websites into LLM-ready markdown

ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼
==================================================
âœ… í†µê³¼: 4/4
âŒ ì‹¤íŒ¨: 0/4
ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!
```

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

| í…ŒìŠ¤íŠ¸ í•­ëª© | Python SDK | Node.js SDK |
|-------------|------------|-------------|
| ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ | 2.3ì´ˆ | 2.1ì´ˆ |
| êµ¬ì¡°í™”ëœ ì¶”ì¶œ | 4.7ì´ˆ | 4.5ì´ˆ |
| ë°°ì¹˜ ìŠ¤í¬ë˜í•‘ (5ê°œ URL) | 8.2ì´ˆ | 7.9ì´ˆ |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | 45MB | 38MB |

## ê²°ë¡ 

Firecrawlì€ ì›¹ ë°ì´í„°ë¥¼ AI ëª¨ë¸ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. ë³¸ ê°€ì´ë“œë¥¼ í†µí•´ ë‹¤ìŒì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### ì£¼ìš” ì„±ê³¼

- âœ… **ê°„í¸í•œ ì„¤ì¹˜**: ì›í´ë¦­ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ 5ë¶„ ë‚´ í™˜ê²½ êµ¬ì¶•
- âœ… **ì‹¤ì „ í™œìš©**: Python/Node.jsë¡œ ë‹¤ì–‘í•œ ìŠ¤í¬ë˜í•‘ ì‹œë‚˜ë¦¬ì˜¤ êµ¬í˜„
- âœ… **ì„±ëŠ¥ ìµœì í™”**: ìºì‹±, ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
- âœ… **ìë™í™”**: zshrc aliasesì™€ ìŠ¤í¬ë¦½íŠ¸ë¡œ ê°œë°œ ìƒì‚°ì„± í–¥ìƒ

### ë‹¤ìŒ ë‹¨ê³„ ì¶”ì²œ

1. **RAG ì‹œìŠ¤í…œ êµ¬ì¶•**: ìˆ˜ì§‘í•œ ë°ì´í„°ë¡œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ ê°œë°œ
2. **ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**: ì‹¤ì‹œê°„ ìŠ¤í¬ë˜í•‘ í˜„í™© ì‹œê°í™”
3. **ML íŒŒì´í”„ë¼ì¸**: ìˆ˜ì§‘ ë°ì´í„°ë¡œ ìë™ í•™ìŠµ ì‹œìŠ¤í…œ êµ¬ì¶•
4. **ìŠ¤ì¼€ì¼ë§**: Kubernetesë¡œ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ í™˜ê²½ êµ¬ì¶•

### ê´€ë ¨ ë¦¬ì†ŒìŠ¤

- ğŸ“š [Firecrawl ê³µì‹ ë¬¸ì„œ](https://docs.firecrawl.dev)
- ğŸ™ [GitHub ì €ì¥ì†Œ](https://github.com/firecrawl/firecrawl)
- ğŸ’¬ [ì»¤ë®¤ë‹ˆí‹° Discord](https://discord.gg/firecrawl)
- ğŸ¯ [ì‚¬ìš© ì‚¬ë¡€ ëª¨ìŒ](https://firecrawl.dev/examples)

Firecrawlì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ì°¨ì„¸ëŒ€ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•´ë³´ì„¸ìš”. ì›¹ì˜ ë¬´í•œí•œ ì •ë³´ê°€ ì´ì œ ì—¬ëŸ¬ë¶„ì˜ AI ëª¨ë¸ì„ ê¸°ë‹¤ë¦¬ê³  ìˆìŠµë‹ˆë‹¤! ğŸš€
