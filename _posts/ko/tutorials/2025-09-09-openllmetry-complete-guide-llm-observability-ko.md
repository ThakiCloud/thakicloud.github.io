---
title: "OpenLLMetry ì™„ì „ ê°€ì´ë“œ: OpenTelemetryë¥¼ í™œìš©í•œ LLM ê´€ì°° ê°€ëŠ¥ì„±"
excerpt: "OpenLLMetryë¥¼ ì‚¬ìš©í•˜ì—¬ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í¬ê´„ì ì¸ ê´€ì°° ê°€ëŠ¥ì„±ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ì„¸ìš”. ì˜¤í”ˆì†ŒìŠ¤ ì†”ë£¨ì…˜ìœ¼ë¡œ LLM ëª¨ë‹ˆí„°ë§ì„ ì™„ë²½í•˜ê²Œ êµ¬í˜„í•©ë‹ˆë‹¤."
seo_title: "OpenLLMetry íŠœí† ë¦¬ì–¼: LLM ê´€ì°° ê°€ëŠ¥ì„± ë° ëª¨ë‹ˆí„°ë§ ê°€ì´ë“œ - Thaki Cloud"
seo_description: "OpenLLMetryë¥¼ í™œìš©í•œ LLM ê´€ì°° ê°€ëŠ¥ì„± ì™„ì „ íŠœí† ë¦¬ì–¼. ì„¤ì¹˜, êµ¬ì„±, AI ì• í”Œë¦¬ì¼€ì´ì…˜ ëª¨ë‹ˆí„°ë§ì„ ì‹¤ì œ ì˜ˆì œì™€ í•¨ê»˜ í•™ìŠµí•˜ì„¸ìš”."
date: 2025-09-09
categories:
  - tutorials
tags:
  - openllmetry
  - llm-observability
  - opentelemetry
  - ai-monitoring
  - machine-learning
  - python
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
lang: ko
permalink: /ko/tutorials/openllmetry-complete-guide-llm-observability/
canonical_url: "https://thakicloud.github.io/ko/tutorials/openllmetry-complete-guide-llm-observability/"
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 15ë¶„

## ì†Œê°œ

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì ì  ë³µì¡í•´ì§€ê³  í”„ë¡œë•ì…˜ì— ë°°í¬ë˜ë©´ì„œ ëª¨ë‹ˆí„°ë§ê³¼ ê´€ì°° ê°€ëŠ¥ì„±ì´ í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. [OpenLLMetry](https://github.com/traceloop/openllmetry)ëŠ” OpenTelemetry í‘œì¤€ì„ í†µí•´ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ê´€ì°° ê°€ëŠ¥ì„±ì„ ì œê³µí•˜ëŠ” í¬ê´„ì ì¸ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.

OpenLLMetryëŠ” LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„ëœ ì˜¤í”ˆì†ŒìŠ¤ ê´€ì°° ê°€ëŠ¥ì„± í”Œë«í¼ì…ë‹ˆë‹¤. OpenTelemetry ìœ„ì— êµ¬ì¶•ë˜ì–´ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥, ë¹„ìš©, í–‰ë™ì— ëŒ€í•œ ì™„ì „í•œ ê°€ì‹œì„±ì„ ì œê³µí•˜ë©´ì„œ ê¸°ì¡´ ê´€ì°° ê°€ëŠ¥ì„± ì¸í”„ë¼ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

### OpenLLMetryê°€ ì¤‘ìš”í•œ ì´ìœ 

ê¸°ì¡´ ëª¨ë‹ˆí„°ë§ ë„êµ¬ëŠ” LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì ìš©í•  ë•Œ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. OpenLLMetryëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³ ìœ í•œ ê³¼ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤:

- **í† í° ì‚¬ìš©ëŸ‰ ì¶”ì **: ì…ë ¥/ì¶œë ¥ í† í°ê³¼ ê´€ë ¨ ë¹„ìš© ëª¨ë‹ˆí„°ë§
- **ì§€ì—° ì‹œê°„ ë¶„ì„**: ë‹¤ì–‘í•œ ëª¨ë¸ ì œê³µì—…ì²´ ê°„ ì‘ë‹µ ì‹œê°„ ì¶”ì 
- **ì˜¤ë¥˜ ëª¨ë‹ˆí„°ë§**: LLM íŠ¹ìœ ì˜ ì˜¤ë¥˜ì™€ ì‹¤íŒ¨ ìº¡ì²˜ ë° ë¶„ì„
- **ì„±ëŠ¥ ìµœì í™”**: AI ì›Œí¬í”Œë¡œìš°ì˜ ë³‘ëª© ì§€ì  ì‹ë³„
- **ë¹„ìš© ê´€ë¦¬**: ì—¬ëŸ¬ LLM ì œê³µì—…ì²´ì˜ ì§€ì¶œ ëª¨ë‹ˆí„°ë§

## ì‚¬ì „ ìš”êµ¬ì‚¬í•­

OpenLLMetryë¥¼ ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒì´ í•„ìš”í•©ë‹ˆë‹¤:

- **Python 3.8+** ì‹œìŠ¤í…œ ì„¤ì¹˜
- **OpenTelemetry ê°œë…ì— ëŒ€í•œ ê¸°ë³¸ ì´í•´**
- **ëª¨ë‹ˆí„°ë§í•  LLM ì• í”Œë¦¬ì¼€ì´ì…˜** (OpenAI, Anthropic ë“±)
- **ê´€ì°° ê°€ëŠ¥ì„± ë°±ì—”ë“œ** (ê³ ê¸‰ ì„¤ì •ì„ ìœ„í•´ ì„ íƒì‚¬í•­)

## 1ë¶€: OpenLLMetry ì‹œì‘í•˜ê¸°

### ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •

ê°€ì¥ ê°„ë‹¨í•œ ì„¤ì •ë¶€í„° ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤. OpenLLMetryëŠ” ì†ì‰½ê²Œ ì‹œì‘í•  ìˆ˜ ìˆëŠ” í¸ë¦¬í•œ SDKë¥¼ ì œê³µí•©ë‹ˆë‹¤.

#### 1ë‹¨ê³„: OpenLLMetry SDK ì„¤ì¹˜

```bash
# í•µì‹¬ SDK ì„¤ì¹˜
pip install traceloop-sdk

# íŠ¹ì • í†µí•©ì„ ìœ„í•´ ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install openai anthropic  # LLM ì œê³µì—…ì²´ ì˜ˆì‹œ
```

#### 2ë‹¨ê³„: ê¸°ë³¸ ì´ˆê¸°í™”

LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ ì½”ë“œ í•œ ì¤„ì…ë‹ˆë‹¤:

```python
from traceloop.sdk import Traceloop

# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ OpenLLMetry ì´ˆê¸°í™”
Traceloop.init()
```

ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œëŠ” ì¦‰ì‹œ ì¶”ì ì„ í™•ì¸í•˜ê³  ì‹¶ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# ì¦‰ì‹œ ì¶”ì  ê°€ì‹œì„±ì„ ìœ„í•´ ë°°ì¹˜ ì „ì†¡ ë¹„í™œì„±í™”
Traceloop.init(disable_batch=True)
```

#### 3ë‹¨ê³„: ì²« ë²ˆì§¸ ëª¨ë‹ˆí„°ë§ LLM í˜¸ì¶œ

ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ì„ ë³´ì—¬ì£¼ëŠ” ì™„ì „í•œ ì˜ˆì œì…ë‹ˆë‹¤:

```python
import openai
from traceloop.sdk import Traceloop

# OpenLLMetry ì´ˆê¸°í™”
Traceloop.init(disable_batch=True)

# OpenAI í´ë¼ì´ì–¸íŠ¸ êµ¬ì„±
client = openai.OpenAI(api_key="your-api-key")

# ëª¨ë‹ˆí„°ë§ëœ LLM í˜¸ì¶œ
def generate_response(prompt):
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": prompt}
        ],
        max_tokens=100
    )
    return response.choices[0].message.content

# ëª¨ë‹ˆí„°ë§ëœ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    result = generate_response("ì–‘ì ì»´í“¨íŒ…ì„ ê°„ë‹¨í•œ ìš©ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”")
    print(result)
```

ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ OpenLLMetryê°€ ìë™ìœ¼ë¡œ:
- ìš”ì²­ê³¼ ì‘ë‹µì„ ìº¡ì²˜
- í† í° ì‚¬ìš©ëŸ‰ê³¼ ë¹„ìš©ì„ ê¸°ë¡
- ì‘ë‹µ ì§€ì—° ì‹œê°„ì„ ì¸¡ì •
- ë°œìƒí•˜ëŠ” ëª¨ë“  ì˜¤ë¥˜ë¥¼ ì¶”ì 

## 2ë¶€: ê³ ê¸‰ êµ¬ì„±

### ë°ì½”ë ˆì´í„°ë¥¼ í™œìš©í•œ ì‚¬ìš©ì ì •ì˜ ì¶”ì 

OpenLLMetryëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œì§ì˜ ì‚¬ìš©ì ì •ì˜ ì¶”ì ì„ ìœ„í•œ ë°ì½”ë ˆì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

```python
from traceloop.sdk import Traceloop
from traceloop.sdk.decorators import task, workflow

# OpenLLMetry ì´ˆê¸°í™”
Traceloop.init()

@workflow(name="document_analysis_pipeline")
def analyze_document(document_text):
    """ë¬¸ì„œ ë¶„ì„ì„ ìœ„í•œ ë©”ì¸ ì›Œí¬í”Œë¡œìš°"""
    summary = summarize_text(document_text)
    sentiment = analyze_sentiment(document_text)
    keywords = extract_keywords(document_text)
    
    return {
        "summary": summary,
        "sentiment": sentiment,
        "keywords": keywords
    }

@task(name="text_summarization")
def summarize_text(text):
    """ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½"""
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”."},
            {"role": "user", "content": text}
        ],
        max_tokens=150
    )
    return response.choices[0].message.content

@task(name="sentiment_analysis")
def analyze_sentiment(text):
    """í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„"""
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ì´ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”. ê¸ì •ì , ë¶€ì •ì , ë˜ëŠ” ì¤‘ë¦½ì ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."},
            {"role": "user", "content": text}
        ],
        max_tokens=10
    )
    return response.choices[0].message.content

@task(name="keyword_extraction")
def extract_keywords(text):
    """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ìš©ì–´ë¥¼ ì¶”ì¶œ"""
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ì´ í…ìŠ¤íŠ¸ì—ì„œ 5ê°œì˜ í•µì‹¬ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ëª©ë¡ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”."},
            {"role": "user", "content": text}
        ],
        max_tokens=50
    )
    return response.choices[0].message.content
```

### í™˜ê²½ ê¸°ë°˜ êµ¬ì„±

í”„ë¡œë•ì…˜ ë°°í¬ë¥¼ ìœ„í•´ í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•´ OpenLLMetryë¥¼ êµ¬ì„±í•˜ì„¸ìš”:

```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export TRACELOOP_API_KEY="your-traceloop-api-key"
export TRACELOOP_BATCH_EXPORT="true"
export TRACELOOP_TELEMETRY="false"  # í•„ìš”ì‹œ í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™”
```

```python
import os
from traceloop.sdk import Traceloop

# í”„ë¡œë•ì…˜ êµ¬ì„±
Traceloop.init(
    api_key=os.getenv("TRACELOOP_API_KEY"),
    disable_batch=os.getenv("TRACELOOP_BATCH_EXPORT", "true").lower() == "false",
    telemetry_enabled=os.getenv("TRACELOOP_TELEMETRY", "true").lower() == "true"
)
```

## 3ë¶€: ì¸ê¸° LLM í”„ë ˆì„ì›Œí¬ì™€ì˜ í†µí•©

### LangChain í†µí•©

OpenLLMetryëŠ” LangChain ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ ì™„ë²½í•˜ê²Œ í†µí•©ë©ë‹ˆë‹¤:

```python
from langchain.llms import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from traceloop.sdk import Traceloop

# OpenLLMetry ì´ˆê¸°í™”
Traceloop.init()

# LangChain êµ¬ì„± ìš”ì†Œ ìƒì„±
llm = OpenAI(temperature=0.7)
prompt = PromptTemplate(
    input_variables=["topic"],
    template="{topic}ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…ì„ ì‘ì„±í•˜ì„¸ìš”"
)

# ì²´ì¸ ìƒì„± ë° ì‹¤í–‰
chain = LLMChain(llm=llm, prompt=prompt)

# ìë™ìœ¼ë¡œ ì¶”ì ë©ë‹ˆë‹¤
result = chain.run(topic="ë¨¸ì‹ ëŸ¬ë‹")
print(result)
```

### LlamaIndex í†µí•©

LlamaIndex ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ê²½ìš°:

```python
from llama_index import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms import OpenAI
from traceloop.sdk import Traceloop

# OpenLLMetry ì´ˆê¸°í™”
Traceloop.init()

# LlamaIndex êµ¬ì„±
llm = OpenAI(model="gpt-3.5-turbo")

# ë¬¸ì„œ ë¡œë“œ ë° ì¸ë±ìŠ¤ ìƒì„±
documents = SimpleDirectoryReader("data").load_data()
index = VectorStoreIndex.from_documents(documents)

# ì¿¼ë¦¬ ì—”ì§„ ìƒì„±
query_engine = index.as_query_engine(llm=llm)

# ìë™ ì¶”ì ê³¼ í•¨ê»˜ ì¿¼ë¦¬
response = query_engine.query("ì´ ë¬¸ì„œë“¤ì˜ ì£¼ìš” ì£¼ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?")
print(response)
```

## 4ë¶€: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë‹ˆí„°ë§

OpenLLMetryëŠ” ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—…ë„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤:

### Pinecone í†µí•©

```python
import pinecone
from traceloop.sdk import Traceloop

# OpenLLMetry ì´ˆê¸°í™”
Traceloop.init()

# Pinecone ì´ˆê¸°í™”
pinecone.init(
    api_key="your-pinecone-api-key",
    environment="your-environment"
)

# ì¸ë±ìŠ¤ ì°¸ì¡° ìƒì„±
index = pinecone.Index("your-index-name")

# ëª¨ë‹ˆí„°ë§ëœ ë²¡í„° ì‘ì—…
def search_similar_documents(query_vector, top_k=5):
    """ë²¡í„° ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
    results = index.query(
        vector=query_vector,
        top_k=top_k,
        include_metadata=True
    )
    return results

# ëª¨ë‹ˆí„°ë§ëœ ì—…ì„œíŠ¸ ì‘ì—…
def store_document_embedding(doc_id, embedding, metadata):
    """Pineconeì— ë¬¸ì„œ ì„ë² ë”© ì €ì¥"""
    index.upsert([
        (doc_id, embedding, metadata)
    ])
```

### Chroma í†µí•©

```python
import chromadb
from traceloop.sdk import Traceloop

# OpenLLMetry ì´ˆê¸°í™”
Traceloop.init()

# Chroma í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = chromadb.Client()

# ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
collection = client.get_or_create_collection("documents")

# ëª¨ë‹ˆí„°ë§ëœ ì‘ì—…
def add_documents(documents, embeddings, ids, metadatas):
    """Chroma ì»¬ë ‰ì…˜ì— ë¬¸ì„œ ì¶”ê°€"""
    collection.add(
        documents=documents,
        embeddings=embeddings,
        ids=ids,
        metadatas=metadatas
    )

def query_documents(query_text, n_results=5):
    """Chromaì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ì¿¼ë¦¬"""
    results = collection.query(
        query_texts=[query_text],
        n_results=n_results
    )
    return results
```

## 5ë¶€: ê´€ì°° ê°€ëŠ¥ì„± ë°±ì—”ë“œ í†µí•©

### Datadog í†µí•©

ì—”í„°í”„ë¼ì´ì¦ˆ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•´ OpenLLMetryë¥¼ Datadogì— ì—°ê²°:

```python
from opentelemetry import trace
from opentelemetry.exporter.datadog import DatadogExporter, DatadogSpanProcessor
from opentelemetry.sdk.trace import TracerProvider
from traceloop.sdk import Traceloop

# Datadog ë‚´ë³´ë‚´ê¸° êµ¬ì„±
tracer_provider = TracerProvider()
datadog_exporter = DatadogExporter(
    agent_url="http://localhost:8126",  # Datadog Agent URL
    service="llm-application"
)

# Datadog ìŠ¤íŒ¬ í”„ë¡œì„¸ì„œ ì¶”ê°€
tracer_provider.add_span_processor(
    DatadogSpanProcessor(datadog_exporter)
)

# ì¶”ì  ì œê³µì ì„¤ì •
trace.set_tracer_provider(tracer_provider)

# ì‚¬ìš©ì ì •ì˜ ì¶”ì ê¸°ë¡œ OpenLLMetry ì´ˆê¸°í™”
Traceloop.init()
```

### Honeycomb í†µí•©

Honeycomb ê´€ì°° ê°€ëŠ¥ì„±ì„ ìœ„í•´:

```python
import os
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from traceloop.sdk import Traceloop

# Honeycomb ë‚´ë³´ë‚´ê¸° êµ¬ì„±
tracer_provider = TracerProvider()

otlp_exporter = OTLPSpanExporter(
    endpoint="https://api.honeycomb.io/v1/traces",
    headers={
        "x-honeycomb-team": os.getenv("HONEYCOMB_API_KEY"),
        "x-honeycomb-dataset": "llm-traces"
    }
)

# ë°°ì¹˜ ìŠ¤íŒ¬ í”„ë¡œì„¸ì„œ ì¶”ê°€
tracer_provider.add_span_processor(
    BatchSpanProcessor(otlp_exporter)
)

# ì¶”ì  ì œê³µì ì„¤ì •
trace.set_tracer_provider(tracer_provider)

# OpenLLMetry ì´ˆê¸°í™”
Traceloop.init()
```

## 6ë¶€: ì‚¬ìš©ì ì •ì˜ ë©”íŠ¸ë¦­ê³¼ ì†ì„±

### ì‚¬ìš©ì ì •ì˜ ì†ì„± ì¶”ê°€

ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ìœ¼ë¡œ ì¶”ì ì„ í–¥ìƒì‹œí‚¤ì„¸ìš”:

```python
from traceloop.sdk import Traceloop
from traceloop.sdk.decorators import task
from opentelemetry import trace

# OpenLLMetry ì´ˆê¸°í™”
Traceloop.init()

@task(name="customer_support_response")
def handle_customer_query(query, customer_id, priority="normal"):
    """ì‚¬ìš©ì ì •ì˜ ì†ì„±ì„ í¬í•¨í•œ ê³ ê° ì§€ì› ì¿¼ë¦¬ ì²˜ë¦¬"""
    
    # í˜„ì¬ ìŠ¤íŒ¬ ê°€ì ¸ì˜¤ê¸°
    current_span = trace.get_current_span()
    
    # ì‚¬ìš©ì ì •ì˜ ì†ì„± ì¶”ê°€
    current_span.set_attribute("customer.id", customer_id)
    current_span.set_attribute("query.priority", priority)
    current_span.set_attribute("query.length", len(query))
    
    # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ëª¨ë¸ ê²°ì •
    model = "gpt-4" if priority == "high" else "gpt-3.5-turbo"
    current_span.set_attribute("llm.model", model)
    
    # ì‘ë‹µ ìƒì„±
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” ê³ ê° ì§€ì› ìƒë‹´ì›ì…ë‹ˆë‹¤."},
            {"role": "user", "content": query}
        ]
    )
    
    # ì‘ë‹µ ì†ì„± ì¶”ê°€
    response_text = response.choices[0].message.content
    current_span.set_attribute("response.length", len(response_text))
    current_span.set_attribute("response.satisfactory", "unknown")  # ML ëª¨ë¸ë¡œ ê²°ì • ê°€ëŠ¥
    
    return response_text
```

### ì‚¬ìš©ì ì •ì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

ë¹„ì¦ˆë‹ˆìŠ¤ KPIë¥¼ ìœ„í•œ ì‚¬ìš©ì ì •ì˜ ë©”íŠ¸ë¦­ ìƒì„±:

```python
from opentelemetry import metrics
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader
import time

# ë©”íŠ¸ë¦­ êµ¬ì„±
metric_reader = PeriodicExportingMetricReader(
    ConsoleMetricExporter(), 
    export_interval_millis=5000
)
metrics.set_meter_provider(MeterProvider(metric_readers=[metric_reader]))

# ì‚¬ìš©ì ì •ì˜ ì¸¡ì •ê¸° ìƒì„±
meter = metrics.get_meter("llm_application")

# ì‚¬ìš©ì ì •ì˜ ë©”íŠ¸ë¦­ ìƒì„±
request_counter = meter.create_counter(
    "llm_requests_total",
    description="ì´ LLM ìš”ì²­ ìˆ˜"
)

response_time_histogram = meter.create_histogram(
    "llm_response_time",
    description="LLM ì‘ë‹µ ì‹œê°„(ì´ˆ)"
)

token_usage_counter = meter.create_counter(
    "llm_tokens_used",
    description="ì´ ì†Œë¹„ëœ í† í°"
)

def monitored_llm_call(prompt, model="gpt-3.5-turbo"):
    """ì‚¬ìš©ì ì •ì˜ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ LLM í˜¸ì¶œ"""
    start_time = time.time()
    
    try:
        # ìš”ì²­ ì¹´ìš´í„° ì¦ê°€
        request_counter.add(1, {"model": model})
        
        # LLM í˜¸ì¶œ
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        
        # ì‘ë‹µ ì‹œê°„ ê¸°ë¡
        response_time = time.time() - start_time
        response_time_histogram.record(response_time, {"model": model})
        
        # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
        usage = response.usage
        token_usage_counter.add(usage.total_tokens, {
            "model": model,
            "type": "total"
        })
        token_usage_counter.add(usage.prompt_tokens, {
            "model": model,
            "type": "prompt"
        })
        token_usage_counter.add(usage.completion_tokens, {
            "model": model,
            "type": "completion"
        })
        
        return response.choices[0].message.content
        
    except Exception as e:
        request_counter.add(1, {"model": model, "status": "error"})
        raise
```

## 7ë¶€: í”„ë¡œë•ì…˜ ëª¨ë²” ì‚¬ë¡€

### ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µì›ë ¥

í”„ë¡œë•ì…˜ í™˜ê²½ì„ ìœ„í•œ ê²¬ê³ í•œ ì˜¤ë¥˜ ì²˜ë¦¬ êµ¬í˜„:

```python
from traceloop.sdk import Traceloop
from opentelemetry import trace
import logging
import sys

# ë¡œê¹… êµ¬ì„±
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì˜¤ë¥˜ ì²˜ë¦¬ì™€ í•¨ê»˜ OpenLLMetry ì´ˆê¸°í™”
try:
    Traceloop.init(
        disable_batch=False,  # í”„ë¡œë•ì…˜ì—ì„œ ë°°ì¹˜ í™œì„±í™”
        telemetry_enabled=False  # í”„ë¼ì´ë²„ì‹œë¥¼ ìœ„í•´ í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™”
    )
    logger.info("OpenLLMetryê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
except Exception as e:
    logger.error(f"OpenLLMetry ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    # ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤íŒ¨ì‹œí‚¤ì§€ ì•Šê³  ì¶”ì  ì—†ì´ ê³„ì†
    pass

def safe_llm_call(prompt, max_retries=3, backoff_factor=2):
    """ì¬ì‹œë„ ë¡œì§ê³¼ í¬ê´„ì ì¸ ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ í¬í•¨í•œ LLM í˜¸ì¶œ"""
    
    span = trace.get_current_span()
    
    for attempt in range(max_retries):
        try:
            span.set_attribute("retry.attempt", attempt + 1)
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                timeout=30  # í”„ë¡œë•ì…˜ì—ì„œ íƒ€ì„ì•„ì›ƒ ì„¤ì •
            )
            
            span.set_attribute("request.successful", True)
            return response.choices[0].message.content
            
        except openai.RateLimitError as e:
            span.set_attribute("error.type", "rate_limit")
            span.set_attribute("error.message", str(e))
            
            if attempt < max_retries - 1:
                wait_time = backoff_factor ** attempt
                logger.warning(f"ì†ë„ ì œí•œì— ë„ë‹¬, ì¬ì‹œë„ ì „ {wait_time}ì´ˆ ëŒ€ê¸°")
                time.sleep(wait_time)
            else:
                span.set_attribute("request.successful", False)
                raise
                
        except openai.APIError as e:
            span.set_attribute("error.type", "api_error")
            span.set_attribute("error.message", str(e))
            span.set_attribute("request.successful", False)
            
            logger.error(f"OpenAI API ì˜¤ë¥˜: {e}")
            raise
            
        except Exception as e:
            span.set_attribute("error.type", "unknown")
            span.set_attribute("error.message", str(e))
            span.set_attribute("request.successful", False)
            
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            raise
```

### ì„±ëŠ¥ ìµœì í™”

ë†’ì€ ì²˜ë¦¬ëŸ‰ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ OpenLLMetry ìµœì í™”:

```python
from traceloop.sdk import Traceloop
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.trace import TracerProvider
import os

# ê³ ì„±ëŠ¥ êµ¬ì„±
tracer_provider = TracerProvider()

# ë†’ì€ ì²˜ë¦¬ëŸ‰ì„ ìœ„í•œ ë°°ì¹˜ í”„ë¡œì„¸ì„œ êµ¬ì„±
batch_processor = BatchSpanProcessor(
    span_exporter=your_exporter,  # ì„ íƒí•œ ë‚´ë³´ë‚´ê¸°
    max_queue_size=2048,  # í í¬ê¸° ì¦ê°€
    export_timeout_millis=30000,  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
    max_export_batch_size=512,  # ë” í° ë°°ì¹˜ í¬ê¸°
    schedule_delay_millis=500  # ë” ë¹ˆë²ˆí•œ ë‚´ë³´ë‚´ê¸°
)

tracer_provider.add_span_processor(batch_processor)

# ì„±ëŠ¥ ìµœì í™”ì™€ í•¨ê»˜ ì´ˆê¸°í™”
Traceloop.init(
    disable_batch=False,
    resource_attributes={
        "service.name": "llm-application",
        "service.version": "1.0.0",
        "deployment.environment": os.getenv("ENVIRONMENT", "production")
    }
)
```

### ë³´ì•ˆ ë° í”„ë¼ì´ë²„ì‹œ ê³ ë ¤ì‚¬í•­

ë³´ì•ˆ ëª¨ë²” ì‚¬ë¡€ êµ¬í˜„:

```python
from traceloop.sdk import Traceloop
from opentelemetry import trace
import hashlib
import re

# í”„ë¼ì´ë²„ì‹œ ì¤‘ì‹¬ êµ¬ì„±ìœ¼ë¡œ ì´ˆê¸°í™”
Traceloop.init(
    telemetry_enabled=False,  # í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™”
    api_key=os.getenv("TRACELOOP_API_KEY")  # í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
)

def sanitize_prompt(prompt):
    """ì¶”ì  ì „ í”„ë¡¬í”„íŠ¸ì—ì„œ ë¯¼ê°í•œ ì •ë³´ ì œê±°"""
    
    # ì´ë©”ì¼ ì£¼ì†Œ ì œê±°
    prompt = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', prompt)
    
    # ì „í™”ë²ˆí˜¸ ì œê±°
    prompt = re.sub(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', '[PHONE]', prompt)
    
    # ì‹ ìš©ì¹´ë“œ ë²ˆí˜¸ ì œê±°
    prompt = re.sub(r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b', '[CREDIT_CARD]', prompt)
    
    return prompt

def secure_llm_call(prompt, include_prompt_in_trace=False):
    """í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ë¥¼ í¬í•¨í•œ LLM í˜¸ì¶œ"""
    
    span = trace.get_current_span()
    
    # ë‚´ìš©ì„ ë…¸ì¶œí•˜ì§€ ì•Šê³  ì¶”ì ì„ ìœ„í•´ ì›ë³¸ í”„ë¡¬í”„íŠ¸ í•´ì‹œ
    prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()[:16]
    span.set_attribute("prompt.hash", prompt_hash)
    span.set_attribute("prompt.length", len(prompt))
    
    # ì„ íƒì ìœ¼ë¡œ ì •ì œëœ í”„ë¡¬í”„íŠ¸ í¬í•¨
    if include_prompt_in_trace:
        sanitized_prompt = sanitize_prompt(prompt)
        span.set_attribute("prompt.sanitized", sanitized_prompt)
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    
    # í”„ë¼ì´ë²„ì‹œë¥¼ ìœ„í•´ ì‘ë‹µ ë‚´ìš©ì„ ì¶”ì ì— í¬í•¨í•˜ì§€ ì•ŠìŒ
    response_text = response.choices[0].message.content
    span.set_attribute("response.length", len(response_text))
    span.set_attribute("response.hash", hashlib.sha256(response_text.encode()).hexdigest()[:16])
    
    return response_text
```

## 8ë¶€: ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### ì•Œë¦¼ ì„¤ì •

ì¼ë°˜ì ì¸ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ë¬¸ì œì— ëŒ€í•œ ì•Œë¦¼ êµ¬ì„±:

```python
from traceloop.sdk import Traceloop
from opentelemetry import trace
import time

# OpenLLMetry ì´ˆê¸°í™”
Traceloop.init()

class LLMMonitor:
    def __init__(self):
        self.error_count = 0
        self.total_requests = 0
        self.total_cost = 0.0
        self.response_times = []
    
    def track_request(self, success=True, cost=0.0, response_time=0.0):
        """ì•Œë¦¼ì„ ìœ„í•œ ìš”ì²­ ë©”íŠ¸ë¦­ ì¶”ì """
        self.total_requests += 1
        self.total_cost += cost
        self.response_times.append(response_time)
        
        if not success:
            self.error_count += 1
        
        # ì´ë™ í‰ê· ì„ ìœ„í•´ ìµœê·¼ 100ê°œ ì‘ë‹µ ì‹œê°„ë§Œ ìœ ì§€
        if len(self.response_times) > 100:
            self.response_times.pop(0)
        
        # ì•Œë¦¼ ì¡°ê±´ í™•ì¸
        self.check_alerts()
    
    def check_alerts(self):
        """ì•Œë¦¼ ì¡°ê±´ í™•ì¸"""
        
        # ë†’ì€ ì˜¤ë¥˜ìœ¨ ì•Œë¦¼
        if self.total_requests > 10:
            error_rate = self.error_count / self.total_requests
            if error_rate > 0.1:  # 10% ì˜¤ë¥˜ìœ¨
                self.send_alert(f"ë†’ì€ ì˜¤ë¥˜ìœ¨: {error_rate:.2%}")
        
        # ë†’ì€ ì‘ë‹µ ì‹œê°„ ì•Œë¦¼
        if len(self.response_times) > 10:
            avg_response_time = sum(self.response_times[-10:]) / 10
            if avg_response_time > 5.0:  # 5ì´ˆ í‰ê· 
                self.send_alert(f"ë†’ì€ ì‘ë‹µ ì‹œê°„: {avg_response_time:.2f}ì´ˆ")
        
        # ë¹„ìš© ì•Œë¦¼
        if self.total_cost > 100.0:  # $100 ì„ê³„ê°’
            self.send_alert(f"ë†’ì€ ë¹„ìš©: ${self.total_cost:.2f}")
    
    def send_alert(self, message):
        """ì•Œë¦¼ ì „ì†¡ (ì„ í˜¸í•˜ëŠ” ì•Œë¦¼ ë°©ë²• êµ¬í˜„)"""
        print(f"ì•Œë¦¼: {message}")
        # ì—¬ê¸°ì— Slack, ì´ë©”ì¼ ë˜ëŠ” ê¸°íƒ€ ì•Œë¦¼ êµ¬í˜„

# ì „ì—­ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
monitor = LLMMonitor()

def monitored_llm_call_with_alerting(prompt):
    """ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ì„ í¬í•¨í•œ LLM í˜¸ì¶œ"""
    start_time = time.time()
    span = trace.get_current_span()
    
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        response_time = time.time() - start_time
        cost = estimate_cost(response.usage)  # ë¹„ìš© ê³„ì‚° êµ¬í˜„
        
        # ì„±ê³µí•œ ìš”ì²­ ì¶”ì 
        monitor.track_request(success=True, cost=cost, response_time=response_time)
        
        # ìŠ¤íŒ¬ì— ë©”íŠ¸ë¦­ ì¶”ê°€
        span.set_attribute("request.cost", cost)
        span.set_attribute("request.response_time", response_time)
        
        return response.choices[0].message.content
        
    except Exception as e:
        response_time = time.time() - start_time
        
        # ì‹¤íŒ¨í•œ ìš”ì²­ ì¶”ì 
        monitor.track_request(success=False, response_time=response_time)
        
        # ìŠ¤íŒ¬ì— ì˜¤ë¥˜ ì •ë³´ ì¶”ê°€
        span.set_attribute("request.failed", True)
        span.set_attribute("error.message", str(e))
        
        raise

def estimate_cost(usage, model="gpt-3.5-turbo"):
    """í† í° ì‚¬ìš©ëŸ‰ì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ì²­ ë¹„ìš© ì¶”ì •"""
    # ê°„ë‹¨í•œ ë¹„ìš© ê³„ì‚° (í˜„ì¬ ê°€ê²©ìœ¼ë¡œ ì—…ë°ì´íŠ¸)
    pricing = {
        "gpt-3.5-turbo": {"input": 0.001, "output": 0.002}  # 1K í† í°ë‹¹
    }
    
    if model in pricing:
        input_cost = (usage.prompt_tokens / 1000) * pricing[model]["input"]
        output_cost = (usage.completion_tokens / 1000) * pricing[model]["output"]
        return input_cost + output_cost
    
    return 0.0
```

## í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

OpenLLMetry ì„¤ì •ì„ ê²€ì¦í•˜ê¸° ìœ„í•œ í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìƒì„±í•´ë³´ê² ìŠµë‹ˆë‹¤:

```python
#!/usr/bin/env python3
"""
OpenLLMetry í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
OpenLLMetry ì„¤ì¹˜ ë° êµ¬ì„±ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ ì‹¤í–‰í•˜ì„¸ìš”.
"""

import os
import sys
import time
from traceloop.sdk import Traceloop
from traceloop.sdk.decorators import task, workflow

def test_basic_initialization():
    """ê¸°ë³¸ OpenLLMetry ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    print("ê¸°ë³¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        Traceloop.init(disable_batch=True)
        print("âœ… OpenLLMetryê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
        return True
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

@task(name="test_task")
def test_custom_tracing():
    """ì‚¬ìš©ì ì •ì˜ ì¶”ì  ë°ì½”ë ˆì´í„° í…ŒìŠ¤íŠ¸"""
    print("ì‚¬ìš©ì ì •ì˜ ì¶”ì  í…ŒìŠ¤íŠ¸ ì¤‘...")
    time.sleep(0.1)  # ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
    return "ì‘ì—… ì™„ë£Œ"

@workflow(name="test_workflow")
def test_workflow_tracing():
    """ì›Œí¬í”Œë¡œìš° ë ˆë²¨ ì¶”ì  í…ŒìŠ¤íŠ¸"""
    print("ì›Œí¬í”Œë¡œìš° ì¶”ì  í…ŒìŠ¤íŠ¸ ì¤‘...")
    result = test_custom_tracing()
    return f"ì›Œí¬í”Œë¡œìš° ê²°ê³¼: {result}"

def test_environment_configuration():
    """í™˜ê²½ ê¸°ë°˜ êµ¬ì„± í…ŒìŠ¤íŠ¸"""
    print("í™˜ê²½ êµ¬ì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    required_vars = ["TRACELOOP_API_KEY"]
    optional_vars = ["TRACELOOP_BATCH_EXPORT", "TRACELOOP_TELEMETRY"]
    
    for var in required_vars:
        if not os.getenv(var):
            print(f"âš ï¸  ê²½ê³ : {var}ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    for var in optional_vars:
        value = os.getenv(var, "ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        print(f"â„¹ï¸  {var}: {value}")

def run_tests():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ OpenLLMetry í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    print("=" * 40)
    
    tests = [
        test_basic_initialization,
        test_environment_configuration,
        test_workflow_tracing
    ]
    
    results = []
    for test in tests:
        try:
            result = test()
            results.append(result)
            print()
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ {test.__name__} ì‹¤íŒ¨: {e}")
            results.append(False)
            print()
    
    # ìš”ì•½
    passed = sum(1 for r in results if r)
    total = len(results)
    
    print("=" * 40)
    print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {passed}/{total} í†µê³¼")
    
    if passed == total:
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í–ˆìŠµë‹ˆë‹¤! OpenLLMetryë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. êµ¬ì„±ê³¼ ì˜ì¡´ì„±ì„ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    run_tests()
```

## ê²°ë¡ 

OpenLLMetryëŠ” LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê´€ì°° ê°€ëŠ¥ì„±ì„ ìœ„í•œ í¬ê´„ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•˜ë©°, ê¸°ì¡´ OpenTelemetry ì¸í”„ë¼ì™€ì˜ ì™„ë²½í•œ í†µí•©ì„ ì œê³µí•˜ë©´ì„œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ê³ ìœ í•œ ëª¨ë‹ˆí„°ë§ ìš”êµ¬ì‚¬í•­ì„ í•´ê²°í•©ë‹ˆë‹¤.

### ì£¼ìš” ìš”ì 

1. **ê°„ë‹¨í•œ ì„¤ì •**: ëª‡ ì¤„ì˜ ì½”ë“œë¡œ ì‹œì‘ ê°€ëŠ¥
2. **í”„ë ˆì„ì›Œí¬ í†µí•©**: LangChain, LlamaIndex ë“± ì¸ê¸° í”„ë ˆì„ì›Œí¬ì™€ ì™„ë²½ ì—°ë™
3. **í”„ë¡œë•ì…˜ ì¤€ë¹„**: ê²¬ê³ í•œ ì˜¤ë¥˜ ì²˜ë¦¬, ì„±ëŠ¥ ìµœì í™”, ë³´ì•ˆ ê¸°ëŠ¥ í¬í•¨
4. **í™•ì¥ ê°€ëŠ¥**: ì‚¬ìš©ì ì •ì˜ ë©”íŠ¸ë¦­, ì†ì„±, ë°±ì—”ë“œ í†µí•© ì§€ì›
5. **ë¹„ìš© íš¨ìœ¨ì **: ì—¬ëŸ¬ ê´€ì°° ê°€ëŠ¥ì„± ë°±ì—”ë“œë¥¼ ì§€ì›í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤

### ë‹¤ìŒ ë‹¨ê³„

1. **ì‘ê²Œ ì‹œì‘**: ê°œë°œ í™˜ê²½ì—ì„œ ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ë¶€í„° ì‹œì‘
2. **ì‚¬ìš©ì ì •ì˜ ë©”íŠ¸ë¦­ ì¶”ê°€**: ì‚¬ìš© ì‚¬ë¡€ë³„ ë¹„ì¦ˆë‹ˆìŠ¤ ì¶”ì  êµ¬í˜„
3. **í”„ë¡œë•ì…˜ ë°°í¬**: ê²¬ê³ í•œ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì•Œë¦¼ êµ¬ì„±
4. **íŒ€ í†µí•©**: ê¸°ì¡´ ê´€ì°° ê°€ëŠ¥ì„± ì¸í”„ë¼ì— ì—°ê²°
5. **ì§€ì†ì  ê°œì„ **: ì¸ì‚¬ì´íŠ¸ë¥¼ í™œìš©í•´ ì„±ëŠ¥ê³¼ ë¹„ìš© ìµœì í™”

OpenLLMetryëŠ” LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ëª¨ë‹ˆí„°ë§ì„ í›„ì† ì‘ì—…ì´ ì•„ë‹Œ ì¼ê¸‰ ê¸°ëŠ¥ìœ¼ë¡œ ë³€í™”ì‹œì¼œ, ë” ì•ˆì •ì ì´ê³  ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ë©° ë¹„ìš© íš¨ìœ¨ì ì¸ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

ë” ìì„¸í•œ ì •ë³´ëŠ” [OpenLLMetry GitHub ì €ì¥ì†Œ](https://github.com/traceloop/openllmetry)ë¥¼ ë°©ë¬¸í•˜ê±°ë‚˜ [ê³µì‹ ë¬¸ì„œ](https://www.traceloop.com/openllmetry)ë¥¼ í™•ì¸í•˜ì„¸ìš”.

---

*ì¦ê±°ìš´ ëª¨ë‹ˆí„°ë§! ğŸš€*
