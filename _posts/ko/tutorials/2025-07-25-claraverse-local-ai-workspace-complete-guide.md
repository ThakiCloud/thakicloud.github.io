---
title: "ClaraVerse - ì™„ì „í•œ ë¡œì»¬ AI ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë§ˆìŠ¤í„° ê°€ì´ë“œ"
excerpt: "í”„ë¼ì´ë²„ì‹œ ìš°ì„ ì˜ ì™„ì „ ë¡œì»¬ AI ìŠ¤íƒ ClaraVerseë¡œ LLM, ì´ë¯¸ì§€ ìƒì„±, ìë™í™”, ì—ì´ì „íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì—ì„œ êµ¬í˜„í•˜ê¸°"
seo_title: "ClaraVerse ì™„ì „ ê°€ì´ë“œ - ë¡œì»¬ AI ì›Œí¬ìŠ¤í˜ì´ìŠ¤ êµ¬ì¶•í•˜ê¸° - Thaki Cloud"
seo_description: "ClaraVerse ì„¤ì¹˜ë¶€í„° ê³ ê¸‰ ì›Œí¬í”Œë¡œìš°ê¹Œì§€. Ollama, Stable Diffusion, n8n ìë™í™”ë¥¼ í†µí•©í•œ ì™„ì „ ë¡œì»¬ AI í™˜ê²½ êµ¬ì¶•."
date: 2025-07-25
last_modified_at: 2025-07-25
categories:
  - tutorials
  - llmops
tags:
  - claraverse
  - local-ai
  - ollama
  - stable-diffusion
  - privacy-first
  - ai-workspace
  - automation
  - agent-builder
  - electron
  - comfyui
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/tutorials/claraverse-local-ai-workspace-complete-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 18ë¶„

## ì„œë¡ 

[ClaraVerse](https://github.com/badboysm890/ClaraVerse)ëŠ” **ì™„ì „í•œ í”„ë¼ì´ë²„ì‹œë¥¼ ë³´ì¥í•˜ëŠ” ë¡œì»¬ AI ì›Œí¬ìŠ¤í˜ì´ìŠ¤**ì…ë‹ˆë‹¤. API í‚¤ë„, í´ë¼ìš°ë“œ ì˜ì¡´ì„±ë„, ë°ì´í„° ìœ ì¶œ ê±±ì •ë„ ì—†ì´ LLM ì±„íŒ…, ì´ë¯¸ì§€ ìƒì„±, ì›Œí¬í”Œë¡œìš° ìë™í™”, AI ì—ì´ì „íŠ¸ê¹Œì§€ ëª¨ë“  ê²ƒì„ ë¡œì»¬ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” í˜ì‹ ì ì¸ í”Œë«í¼ì…ë‹ˆë‹¤.

### ClaraVerseê°€ í˜ì‹ ì ì¸ ì´ìœ 

- **100% ë¡œì»¬ ì²˜ë¦¬**: ëª¨ë“  AI ëª¨ë¸ì´ ì‚¬ìš©ì ë¨¸ì‹ ì—ì„œ ì‹¤í–‰
- **ì œë¡œ í…”ë ˆë©”íŠ¸ë¦¬**: ë°ì´í„° ìˆ˜ì§‘ì´ë‚˜ ì™¸ë¶€ ì „ì†¡ ì—†ìŒ  
- **ì™„ì „í•œ í†µí•©**: LLM, ì´ë¯¸ì§€ ìƒì„±, ìë™í™”, ì—ì´ì „íŠ¸ê°€ í•˜ë‚˜ì˜ í”Œë«í¼ì—
- **ì˜¤í”ˆì†ŒìŠ¤**: MIT ë¼ì´ì„ ìŠ¤ë¡œ ì™„ì „í•œ íˆ¬ëª…ì„±
- **ìì²´ í˜¸ìŠ¤íŒ…**: ì „ì²´ AI ìŠ¤íƒì„ ì™„ì „íˆ ì†Œìœ 

## ClaraVerse ì•„í‚¤í…ì²˜ ì‹¬í™” ë¶„ì„

### ì‹œìŠ¤í…œ êµ¬ì„±ë„

```
ClaraVerse ì•„í‚¤í…ì²˜:
Frontend Layer: React + Electron + TypeScript
AI Core: Clara Engine + Ollama + Vision Models  
Creative: ComfyUI + Stable Diffusion + Image Processing
Automation: n8n Workflows + Agent Builder + Tools
Developer: SDK + Widget System + Flow Templates
```

### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

**Frontend & UI**:
- React 18 + TypeScript
- Electron for Desktop
- Tailwind CSS + Shadcn/ui
- WebContainer for isolated environments

**AI & ML Backend**:
- Ollama for LLM inference
- ComfyUI for image generation
- Llama.cpp for optimized inference
- ONNX Runtime for cross-platform models

**Automation & Workflow**:
- n8n-inspired workflow engine
- Custom node system
- Agent Builder with tool calling
- WebRTC for real-time communication

## ì„¤ì¹˜ ë° í™˜ê²½ êµ¬ì„±

### 1. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

**ìµœì†Œ ìš”êµ¬ì‚¬í•­**:
- **OS**: Windows 10+, macOS 10.15+, Ubuntu 18.04+
- **RAM**: 8GB (ê¶Œì¥ 16GB+)
- **ì €ì¥ê³µê°„**: 10GB ì—¬ìœ  ê³µê°„
- **GPU**: CUDA/Metal ì§€ì› GPU (ì„ íƒì‚¬í•­, ì„±ëŠ¥ í–¥ìƒ)

**ê¶Œì¥ ì‚¬ì–‘**:
- **RAM**: 32GB+ (ëŒ€í˜• ëª¨ë¸ ì‹¤í–‰ì‹œ)
- **GPU**: NVIDIA RTX 4060+ ë˜ëŠ” Apple M2+
- **ì €ì¥ê³µê°„**: 50GB+ SSD (ëª¨ë¸ ì €ì¥ìš©)
- **CPU**: 8ì½”ì–´ ì´ìƒ

### 2. Ollama ì‚¬ì „ ì„¤ì¹˜

ClaraVerseëŠ” Ollamaë¥¼ LLM ë°±ì—”ë“œë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ë¨¼ì € ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤:

```bash
# macOS (Homebrew)
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# https://ollama.ai/downloadì—ì„œ ì„¤ì¹˜ í”„ë¡œê·¸ë¨ ë‹¤ìš´ë¡œë“œ

# Ollama ì„œë¹„ìŠ¤ ì‹œì‘
ollama serve

# ê¸°ë³¸ ëª¨ë¸ ì„¤ì¹˜ (ë³„ë„ í„°ë¯¸ë„ì—ì„œ)
ollama pull llama3.2:3b    # ë¹ ë¥¸ ì‘ë‹µìš© ê²½ëŸ‰ ëª¨ë¸
ollama pull llama3.2:8b    # ê· í˜•ì¡íŒ ì„±ëŠ¥
ollama pull llama3.2:70b   # ìµœê³  í’ˆì§ˆ (ê³ ì‚¬ì–‘ í•„ìš”)

# ì½”ë”© íŠ¹í™” ëª¨ë¸
ollama pull codellama:7b
ollama pull deepseek-coder:6.7b

# ë‹¤êµ­ì–´ ëª¨ë¸
ollama pull qwen2.5:7b
```

### 3. ClaraVerse ì„¤ì¹˜

#### ë°©ë²• 1: ì‚¬ì „ ë¹Œë“œëœ ë°”ì´ë„ˆë¦¬ (ê¶Œì¥)

```bash
# ìµœì‹  ë¦´ë¦¬ì¦ˆ ë‹¤ìš´ë¡œë“œ
wget https://github.com/badboysm890/ClaraVerse/releases/latest/download/ClaraVerse-darwin-arm64.dmg  # macOS
# ë˜ëŠ”
wget https://github.com/badboysm890/ClaraVerse/releases/latest/download/ClaraVerse-win32-x64.exe     # Windows
# ë˜ëŠ”  
wget https://github.com/badboysm890/ClaraVerse/releases/latest/download/ClaraVerse-linux-x64.AppImage # Linux

# ì„¤ì¹˜ í›„ ì‹¤í–‰
open ClaraVerse.app  # macOS
```

#### ë°©ë²• 2: ì†ŒìŠ¤ì—ì„œ ë¹Œë“œ

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/badboysm890/ClaraVerse.git
cd ClaraVerse

# Node.js ì˜ì¡´ì„± ì„¤ì¹˜
npm install

# ê°œë°œ ì„œë²„ ì‹¤í–‰ (ì›¹ ë²„ì „)
npm run dev

# ë˜ëŠ” Electron ë°ìŠ¤í¬í†± ë²„ì „
npm run electron:dev

# í”„ë¡œë•ì…˜ ë¹Œë“œ
npm run build              # ì›¹ ë¹Œë“œ
npm run electron:build     # ë°ìŠ¤í¬í†± ë¹Œë“œ
```

#### ë°©ë²• 3: Docker ì»¨í…Œì´ë„ˆ (ì‹¤í—˜ì )

```bash
# Docker Composeë¡œ ì‹¤í–‰
git clone https://github.com/badboysm890/ClaraVerse.git
cd ClaraVerse/docker

# í™˜ê²½ ì„¤ì •
cp .env.example .env

# ì»¨í…Œì´ë„ˆ ì‹œì‘
docker-compose up -d

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:3000 ì ‘ì†
```

## ì´ˆê¸° ì„¤ì • ë° êµ¬ì„±

### 1. ì²« ì‹¤í–‰ ì„¤ì •

```bash
# ClaraVerse ì‹¤í–‰ í›„ ì´ˆê¸° ì„¤ì •
# 1. Ollama ì—°ê²° í™•ì¸
curl http://localhost:11434/api/tags

# 2. GPU ê°€ì† í™•ì¸ (NVIDIA)
nvidia-smi

# 3. ë¡œì»¬ ì €ì¥ì†Œ ì„¤ì •
mkdir -p ~/.claraverse/{models,workflows,agents,data}
```

**ì›¹ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •**:
1. ClaraVerse ì‹¤í–‰ â†’ `http://localhost:3000` ì ‘ì†
2. **Settings** â†’ **LLM Configuration**
3. Ollama endpoint: `http://localhost:11434`
4. ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ ë° í…ŒìŠ¤íŠ¸
5. **GPU ê°€ì†** í™œì„±í™” (ê°€ëŠ¥í•œ ê²½ìš°)

### 2. í•µì‹¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# í•„ìˆ˜ ëª¨ë¸ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
cat > install_models.sh << 'EOF'
#!/bin/bash

echo "ğŸ¤– ClaraVerse í•„ìˆ˜ ëª¨ë¸ ì„¤ì¹˜ ì¤‘..."

# LLM ëª¨ë¸
echo "ğŸ“š ì–¸ì–´ ëª¨ë¸ ì„¤ì¹˜..."
ollama pull llama3.2:8b       # ë©”ì¸ ì±„íŒ… ëª¨ë¸
ollama pull codellama:7b      # ì½”ë”© ëª¨ë¸  
ollama pull qwen2.5:7b        # ë‹¤êµ­ì–´ ëª¨ë¸

# ì„ë² ë”© ëª¨ë¸
echo "ğŸ” ì„ë² ë”© ëª¨ë¸ ì„¤ì¹˜..."
ollama pull nomic-embed-text  # í…ìŠ¤íŠ¸ ì„ë² ë”©

# ë¹„ì „ ëª¨ë¸
echo "ğŸ‘ï¸ ë¹„ì „ ëª¨ë¸ ì„¤ì¹˜..."
ollama pull llava:7b          # ì´ë¯¸ì§€ ë¶„ì„

echo "âœ… ëª¨ë“  ëª¨ë¸ ì„¤ì¹˜ ì™„ë£Œ!"
ollama list
EOF

chmod +x install_models.sh
./install_models.sh
```

### 3. Stable Diffusion ì„¤ì •

```bash
# ComfyUI ëª¨ë¸ ë””ë ‰í† ë¦¬ ì„¤ì •
mkdir -p ~/.claraverse/comfyui/{models,input,output}

# Stable Diffusion ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
cd ~/.claraverse/comfyui/models

# SDXL ê¸°ë³¸ ëª¨ë¸
wget -O sdxl_base.safetensors "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors"

# ControlNet ëª¨ë¸
mkdir -p controlnet
cd controlnet
wget -O canny.safetensors "https://huggingface.co/lllyasviel/sd_controlnet_canny/resolve/main/diffusion_pytorch_model.safetensors"

# VAE ëª¨ë¸
cd ../vae
wget -O sdxl_vae.safetensors "https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors"
```

## í•µì‹¬ ê¸°ëŠ¥ í™œìš© ê°€ì´ë“œ

### 1. LLM ì±„íŒ… ì¸í„°í˜ì´ìŠ¤

#### ê¸°ë³¸ ì±„íŒ… ì‚¬ìš©ë²•

```javascript
// ClaraVerse Chat API ì‚¬ìš© ì˜ˆì‹œ
const chat = new ClaraChat({
  model: 'llama3.2:8b',
  temperature: 0.7,
  maxTokens: 2048,
  stream: true
});

// ëŒ€í™” ì‹œì‘
const response = await chat.send({
  message: "Pythonìœ¼ë¡œ ì›¹ ìŠ¤í¬ë˜í•‘ ì½”ë“œë¥¼ ì‘ì„±í•´ì¤˜",
  context: "í”„ë¡œì íŠ¸: ì´ì»¤ë¨¸ìŠ¤ ê°€ê²© ëª¨ë‹ˆí„°ë§",
  tools: ['web_search', 'code_execution']
});

// ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
response.on('token', (token) => {
  console.log(token);
});

response.on('complete', (fullResponse) => {
  console.log('Complete response:', fullResponse);
});
```

#### ê³ ê¸‰ ëŒ€í™” ì„¤ì •

```yaml
# config/chat_profiles.yaml
profiles:
  developer:
    model: codellama:7b
    system_prompt: |
      ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìì…ë‹ˆë‹¤.
      ì½”ë“œëŠ” ëª…í™•í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì‘ì„±í•˜ë©°,
      ë³´ì•ˆê³¼ ì„±ëŠ¥ì„ í•­ìƒ ê³ ë ¤í•©ë‹ˆë‹¤.
    temperature: 0.1
    tools: [code_execution, file_system, git_operations]
    
  creative:
    model: llama3.2:8b  
    system_prompt: |
      ë‹¹ì‹ ì€ ì°½ì˜ì ì¸ ì‘ê°€ì´ì ì•„ì´ë””ì–´ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
      ë…ì°½ì ì´ê³  í¥ë¯¸ë¡œìš´ ì½˜í…ì¸ ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    temperature: 0.9
    tools: [image_generation, web_search]
    
  analyst:
    model: qwen2.5:7b
    system_prompt: |
      ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
      ì •í™•í•œ ë¶„ì„ê³¼ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    temperature: 0.3
    tools: [data_analysis, visualization, web_search]
```

### 2. AI ì—ì´ì „íŠ¸ ë¹Œë”

#### ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ ìƒì„±

```javascript
// Agent Builderë¥¼ í†µí•œ ì—ì´ì „íŠ¸ ì •ì˜
const webScrapingAgent = {
  name: "WebScrapingAgent",
  description: "ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ ì—ì´ì „íŠ¸",
  
  capabilities: [
    "web_scraping",
    "data_parsing", 
    "csv_export",
    "price_monitoring"
  ],
  
  tools: [
    {
      name: "scrape_website",
      description: "ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ",
      parameters: {
        url: { type: "string", required: true },
        selectors: { type: "array", required: true },
        wait_for: { type: "string", required: false }
      }
    },
    {
      name: "parse_product_data",
      description: "ì œí’ˆ ì •ë³´ íŒŒì‹± ë° ì •ë¦¬",
      parameters: {
        raw_html: { type: "string", required: true },
        product_type: { type: "string", required: true }
      }
    }
  ],
  
  workflow: [
    {
      step: "validate_url",
      action: "check_url_accessibility",
      error_handling: "retry_with_delay"
    },
    {
      step: "extract_data", 
      action: "scrape_website",
      params: {
        selectors: ["$product_selectors"]
      }
    },
    {
      step: "process_data",
      action: "parse_product_data", 
      transform: "clean_and_normalize"
    },
    {
      step: "export_results",
      action: "save_to_csv",
      params: {
        filename: "products_${timestamp}.csv"
      }
    }
  ],
  
  scheduling: {
    type: "cron",
    expression: "0 */6 * * *", // 6ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰
    timezone: "Asia/Seoul"
  }
};

// ì—ì´ì „íŠ¸ ë“±ë¡ ë° ì‹¤í–‰
const agent = await ClaraAgent.create(webScrapingAgent);
await agent.deploy();
```

#### ë³µí•© ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°

```yaml
# workflows/content_creation_pipeline.yaml
name: "Content Creation Pipeline"
description: "ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìë™ ìƒì„± íŒŒì´í”„ë¼ì¸"

agents:
  - name: researcher
    role: "ì •ë³´ ìˆ˜ì§‘ ë° ë¦¬ì„œì¹˜"
    model: qwen2.5:7b
    tools: [web_search, pdf_reader, note_taking]
    
  - name: writer  
    role: "ì½˜í…ì¸  ì‘ì„±"
    model: llama3.2:8b
    tools: [text_generation, grammar_check]
    
  - name: designer
    role: "ì´ë¯¸ì§€ ìƒì„±"
    model: stable-diffusion-xl
    tools: [image_generation, image_editing]
    
  - name: editor
    role: "ìµœì¢… í¸ì§‘ ë° ê²€í† "
    model: codellama:7b
    tools: [markdown_formatting, seo_optimization]

workflow:
  1. trigger:
      type: manual
      input: topic, target_audience, word_count
      
  2. research_phase:
      agent: researcher
      tasks:
        - search_web_content: "{% raw %}{{{#123;topic}}#125;}{% endraw %} ê´€ë ¨ ìµœì‹  ì •ë³´ ìˆ˜ì§‘"
        - extract_key_points: "í•µì‹¬ ë‚´ìš© ìš”ì•½"
        - create_outline: "ê¸€ êµ¬ì¡° ì„¤ê³„"
        
  3. writing_phase:
      agent: writer
      inputs: research_results, outline
      tasks:
        - generate_introduction: "ë§¤ë ¥ì ì¸ ì„œë¡  ì‘ì„±"
        - develop_main_content: "ë³¸ë¡  ìƒì„¸ ì‘ì„±"
        - create_conclusion: "íš¨ê³¼ì ì¸ ê²°ë¡  ì‘ì„±"
        
  4. design_phase:
      agent: designer
      inputs: content_draft
      tasks:
        - generate_featured_image: "ëŒ€í‘œ ì´ë¯¸ì§€ ìƒì„±"
        - create_section_graphics: "ì„¹ì…˜ë³„ ë³´ì¡° ì´ë¯¸ì§€"
        - design_infographics: "ì •ë³´ ê·¸ë˜í”½ ì œì‘"
        
  5. editing_phase:
      agent: editor
      inputs: content_draft, images
      tasks:
        - format_markdown: "ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ…"
        - optimize_seo: "SEO ìµœì í™”"
        - final_review: "ìµœì¢… í’ˆì§ˆ ê²€í† "
        
  6. output:
      format: markdown_with_frontmatter
      includes: [content, images, metadata]
      destination: blog_posts/{% raw %}{{{#123;date}}#125;}-{{{#123;slug}}#125;}{% endraw %}.md
```

### 3. ì´ë¯¸ì§€ ìƒì„± ì›Œí¬í”Œë¡œìš°

#### Stable Diffusion ê¸°ë³¸ ì‚¬ìš©ë²•

```javascript
// ComfyUI í†µí•©ì„ í†µí•œ ì´ë¯¸ì§€ ìƒì„±
const imageGenerator = new ClaraImageGen({
  model: 'stable-diffusion-xl',
  backend: 'comfyui'
});

// ê¸°ë³¸ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±
const generateImage = async (prompt) => {
  const workflow = {
    nodes: {
      "1": {
        "class_type": "CheckpointLoaderSimple",
        "inputs": {
          "ckpt_name": "sdxl_base.safetensors"
        }
      },
      "2": {
        "class_type": "CLIPTextEncode", 
        "inputs": {
          "text": prompt,
          "clip": ["1", 1]
        }
      },
      "3": {
        "class_type": "CLIPTextEncode",
        "inputs": {
          "text": "blurry, low quality, distorted",
          "clip": ["1", 1]
        }
      },
      "4": {
        "class_type": "EmptyLatentImage",
        "inputs": {
          "width": 1024,
          "height": 1024, 
          "batch_size": 1
        }
      },
      "5": {
        "class_type": "KSampler",
        "inputs": {
          "seed": Math.floor(Math.random() * 1000000),
          "steps": 20,
          "cfg": 7.5,
          "sampler_name": "euler",
          "scheduler": "normal",
          "denoise": 1.0,
          "model": ["1", 0],
          "positive": ["2", 0],
          "negative": ["3", 0], 
          "latent_image": ["4", 0]
        }
      },
      "6": {
        "class_type": "VAEDecode",
        "inputs": {
          "samples": ["5", 0],
          "vae": ["1", 2]
        }
      },
      "7": {
        "class_type": "SaveImage",
        "inputs": {
          "images": ["6", 0],
          "filename_prefix": "clara_generated"
        }
      }
    }
  };
  
  return await imageGenerator.execute(workflow);
};

// ì‚¬ìš© ì˜ˆì‹œ
const result = await generateImage(
  "A futuristic AI workspace with holographic displays, cyberpunk style, high detail, 4K"
);
console.log('Generated image:', result.images[0]);
```

#### ê³ ê¸‰ ì´ë¯¸ì§€ ì›Œí¬í”Œë¡œìš°

```python
# python/advanced_image_workflows.py
import json
from pathlib import Path

class ClaraImageWorkflows:
    def __init__(self):
        self.workflows_dir = Path("~/.claraverse/workflows/image").expanduser()
        self.workflows_dir.mkdir(parents=True, exist_ok=True)
        
    def create_portrait_workflow(self, style="photorealistic"):
        """ì¸ë¬¼ ì‚¬ì§„ ìƒì„± ì›Œí¬í”Œë¡œìš°"""
        workflow = {
            "name": f"Portrait Generator - {style}",
            "description": "ê³ í’ˆì§ˆ ì¸ë¬¼ ì‚¬ì§„ ìƒì„±",
            
            "parameters": {
                "subject": {"type": "string", "required": True},
                "age": {"type": "integer", "default": 30},
                "gender": {"type": "string", "default": "any"},
                "expression": {"type": "string", "default": "neutral"},
                "lighting": {"type": "string", "default": "professional"},
                "background": {"type": "string", "default": "studio"}
            },
            
            "prompt_template": """
            {style} portrait of {subject}, {age} years old, {gender}, 
            {expression} expression, {lighting} lighting, {background} background,
            high quality, detailed, professional photography, 85mm lens
            """,
            
            "negative_prompt": """
            blurry, low quality, distorted face, multiple heads, 
            bad anatomy, deformed, watermark, text
            """,
            
            "model_settings": {
                "checkpoint": "sdxl_base.safetensors",
                "vae": "sdxl_vae.safetensors", 
                "steps": 30,
                "cfg_scale": 7.0,
                "sampler": "DPM++ 2M Karras",
                "width": 768,
                "height": 1024
            },
            
            "post_processing": [
                {"type": "upscale", "factor": 2},
                {"type": "face_enhance", "strength": 0.5},
                {"type": "color_correction", "auto": True}
            ]
        }
        
        return workflow
    
    def create_product_showcase_workflow(self):
        """ì œí’ˆ ì‡¼ì¼€ì´ìŠ¤ ì´ë¯¸ì§€ ìƒì„±"""
        return {
            "name": "Product Showcase Generator",
            "description": "ì „ë¬¸ì ì¸ ì œí’ˆ ì‚¬ì§„ ìƒì„±",
            
            "stages": [
                {
                    "name": "base_generation",
                    "prompt": "product photography of {product_name}, {product_type}, professional studio lighting, white background, commercial photography, high detail, 4K",
                    "settings": {
                        "steps": 25,
                        "cfg": 6.5,
                        "size": "1024x1024"
                    }
                },
                {
                    "name": "background_replacement", 
                    "type": "inpainting",
                    "mask": "auto_background",
                    "prompt": "{background_style} background, professional, clean"
                },
                {
                    "name": "lighting_enhancement",
                    "type": "controlnet",
                    "control_type": "depth",
                    "prompt": "enhanced lighting, dramatic shadows, professional product photography"
                }
            ],
            
            "variations": [
                {"angle": "front_view", "lighting": "soft_box"},
                {"angle": "side_view", "lighting": "rim_lighting"},
                {"angle": "top_view", "lighting": "overhead"}
            ]
        }
        
    def batch_generate_images(self, workflow, inputs_list):
        """ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„±"""
        results = []
        
        for i, inputs in enumerate(inputs_list):
            try:
                print(f"Processing {i+1}/{len(inputs_list)}: {inputs.get('subject', 'Unknown')}")
                
                # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                result = self.execute_workflow(workflow, inputs)
                results.append({
                    "input": inputs,
                    "output": result,
                    "status": "success"
                })
                
            except Exception as e:
                results.append({
                    "input": inputs,
                    "error": str(e),
                    "status": "failed"
                })
                
        return results

# ì‚¬ìš© ì˜ˆì‹œ
workflows = ClaraImageWorkflows()

# ì¸ë¬¼ ì‚¬ì§„ ë°°ì¹˜ ìƒì„±
portrait_inputs = [
    {"subject": "business woman", "age": 35, "expression": "confident"},
    {"subject": "young developer", "age": 28, "expression": "focused"},
    {"subject": "senior executive", "age": 50, "expression": "authoritative"}
]

portrait_workflow = workflows.create_portrait_workflow("professional")
results = workflows.batch_generate_images(portrait_workflow, portrait_inputs)
```

### 4. n8n ìŠ¤íƒ€ì¼ ì›Œí¬í”Œë¡œìš° ìë™í™”

#### ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° ìƒì„±

```json
{
  "name": "Social Media Content Pipeline",
  "description": "ì†Œì…œë¯¸ë””ì–´ ì½˜í…ì¸  ìë™ ìƒì„± ë° ë°°í¬",
  
  "nodes": [
    {
      "id": "trigger",
      "type": "webhook",
      "name": "Content Request",
      "config": {
        "method": "POST",
        "endpoint": "/create-content",
        "authentication": "api_key"
      }
    },
    
    {
      "id": "content_generator", 
      "type": "llm_chat",
      "name": "Generate Content",
      "config": {
        "model": "llama3.2:8b",
        "prompt": "Create engaging social media content about: {% raw %}{{{#123;$input.topic}}#125;}{% endraw %}\nTone: {% raw %}{{{#123;$input.tone}}#125;}{% endraw %}\nPlatform: {% raw %}{{{#123;$input.platform}}#125;}{% endraw %}\nLength: {% raw %}{{{#123;$input.length}}#125;}{% endraw %}",
        "temperature": 0.8
      },
      "inputs": ["trigger"]
    },
    
    {
      "id": "image_generator",
      "type": "stable_diffusion", 
      "name": "Generate Image",
      "config": {
        "prompt": "{% raw %}{{{#123;$content_generator.output}}#125;}{% endraw %} visual representation, social media style, engaging, colorful",
        "style": "modern",
        "aspect_ratio": "1:1"
      },
      "inputs": ["content_generator"]
    },
    
    {
      "id": "hashtag_generator",
      "type": "llm_chat",
      "name": "Generate Hashtags",
      "config": {
        "model": "qwen2.5:7b",
        "prompt": "Generate relevant hashtags for this content: {% raw %}{{{#123;$content_generator.output}}#125;}{% endraw %}\nPlatform: {% raw %}{{{#123;$input.platform}}#125;}{% endraw %}\nReturn only hashtags, max 10",
        "temperature": 0.3
      },
      "inputs": ["content_generator"]
    },
    
    {
      "id": "content_scheduler",
      "type": "database_insert",
      "name": "Schedule Post",
      "config": {
        "table": "scheduled_posts",
        "data": {
          "content": "{% raw %}{{{#123;$content_generator.output}}#125;}{% endraw %}",
          "image": "{% raw %}{{{#123;$image_generator.output}}#125;}{% endraw %}",
          "hashtags": "{% raw %}{{{#123;$hashtag_generator.output}}#125;}{% endraw %}",
          "platform": "{% raw %}{{{#123;$input.platform}}#125;}{% endraw %}",
          "scheduled_time": "{% raw %}{{{#123;$input.schedule_time}}#125;}{% endraw %}",
          "status": "pending"
        }
      },
      "inputs": ["content_generator", "image_generator", "hashtag_generator"]
    },
    
    {
      "id": "notification",
      "type": "webhook_send",
      "name": "Send Notification", 
      "config": {
        "url": "{% raw %}{{{#123;$input.notification_webhook}}#125;}{% endraw %}",
        "method": "POST",
        "body": {
          "message": "Content created and scheduled successfully",
          "content_id": "{% raw %}{{{#123;$content_scheduler.insert_id}}#125;}{% endraw %}",
          "preview": "{% raw %}{{{#123;$content_generator.output | truncate(100)}}#125;}{% endraw %}"
        }
      },
      "inputs": ["content_scheduler"]
    }
  ],
  
  "triggers": [
    {
      "type": "cron",
      "schedule": "0 9 * * 1-5", // í‰ì¼ ì˜¤ì „ 9ì‹œ
      "data": {
        "topic": "daily_tips",
        "tone": "professional",
        "platform": "linkedin",
        "length": "medium"
      }
    }
  ],
  
  "error_handling": {
    "retry_attempts": 3,
    "retry_delay": "5s",
    "on_failure": "send_error_notification"
  }
}
```

#### ë³µì¡í•œ ë°ì´í„° ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°

```javascript
// workflows/data_processing_pipeline.js
class DataProcessingPipeline {
  constructor() {
    this.nodes = new Map();
    this.connections = new Map();
  }
  
  // ì›¹ ìŠ¤í¬ë˜í•‘ + ë°ì´í„° ë¶„ì„ + ë¦¬í¬íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸
  createAnalyticsPipeline() {
    return {
      "name": "Market Analysis Pipeline",
      "description": "ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘, ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±",
      
      "workflow": [
        {
          "id": "data_sources",
          "type": "parallel_scraping",
          "name": "Multi-source Data Collection",
          "config": {
            "sources": [
              {
                "name": "competitor_pricing",
                "url": "https://competitor1.com/products",
                "selectors": {
                  "price": ".price",
                  "product": ".product-name",
                  "category": ".category"
                },
                "frequency": "daily"
              },
              {
                "name": "market_trends",
                "url": "https://marketdata.com/api/trends",
                "type": "api",
                "auth": "bearer_token",
                "frequency": "hourly"
              },
              {
                "name": "social_sentiment",
                "type": "social_monitoring",
                "keywords": ["product_name", "brand_name"],
                "platforms": ["twitter", "reddit", "instagram"]
              }
            ]
          }
        },
        
        {
          "id": "data_cleaning",
          "type": "data_transformation",
          "name": "Clean and Normalize Data",
          "config": {
            "operations": [
              {"type": "remove_duplicates", "key": "product_id"},
              {"type": "normalize_prices", "currency": "USD"},
              {"type": "standardize_categories"},
              {"type": "sentiment_scoring", "scale": "1-10"}
            ]
          },
          "inputs": ["data_sources"]
        },
        
        {
          "id": "price_analysis",
          "type": "llm_analysis",
          "name": "Price Trend Analysis",
          "config": {
            "model": "qwen2.5:7b",
            "prompt": `
            Analyze the pricing data and provide insights:
            
            Data: {% raw %}{{{#123;$data_cleaning.price_data}}#125;}{% endraw %}
            
            Please provide:
            1. Price trend analysis (increase/decrease/stable)
            2. Competitive positioning
            3. Pricing recommendations
            4. Market opportunities
            
            Format as structured JSON.
            `,
            "output_format": "json"
          },
          "inputs": ["data_cleaning"]
        },
        
        {
          "id": "sentiment_analysis",
          "type": "llm_analysis", 
          "name": "Social Sentiment Analysis",
          "config": {
            "model": "llama3.2:8b",
            "prompt": `
            Analyze social media sentiment data:
            
            Data: {% raw %}{{{#123;$data_cleaning.sentiment_data}}#125;}{% endraw %}
            
            Provide:
            1. Overall sentiment score
            2. Key themes and topics
            3. Emerging issues or opportunities
            4. Competitor sentiment comparison
            
            Format as detailed analysis.
            `
          },
          "inputs": ["data_cleaning"]
        },
        
        {
          "id": "visualization",
          "type": "chart_generation",
          "name": "Create Visualizations",
          "config": {
            "charts": [
              {
                "type": "line_chart",
                "data": "{% raw %}{{{#123;$price_analysis.trends}}#125;}{% endraw %}",
                "title": "Price Trends Over Time",
                "x_axis": "date",
                "y_axis": "price"
              },
              {
                "type": "bar_chart", 
                "data": "{% raw %}{{{#123;$sentiment_analysis.by_platform}}#125;}{% endraw %}",
                "title": "Sentiment by Platform",
                "x_axis": "platform",
                "y_axis": "sentiment_score"
              },
              {
                "type": "scatter_plot",
                "data": "{% raw %}{{{#123;$data_cleaning.competitor_data}}#125;}{% endraw %}",
                "title": "Price vs Quality Matrix",
                "x_axis": "price",
                "y_axis": "quality_score"
              }
            ]
          },
          "inputs": ["price_analysis", "sentiment_analysis"]
        },
        
        {
          "id": "report_generation",
          "type": "document_generation",
          "name": "Generate Executive Report",
          "config": {
            "template": "executive_summary",
            "sections": [
              {
                "title": "Executive Summary",
                "content": "{% raw %}{{{#123;$price_analysis.summary}}#125;}{% endraw %}"
              },
              {
                "title": "Market Trends",
                "content": "{% raw %}{{{#123;$price_analysis.trends}}#125;}{% endraw %}",
                "charts": ["{% raw %}{{{#123;$visualization.price_trends}}#125;}{% endraw %}"]
              },
              {
                "title": "Competitive Analysis", 
                "content": "{% raw %}{{{#123;$price_analysis.competitive_positioning}}#125;}{% endraw %}",
                "charts": ["{% raw %}{{{#123;$visualization.competitor_matrix}}#125;}{% endraw %}"]
              },
              {
                "title": "Social Sentiment",
                "content": "{% raw %}{{{#123;$sentiment_analysis.summary}}#125;}{% endraw %}",
                "charts": ["{% raw %}{{{#123;$visualization.sentiment_chart}}#125;}{% endraw %}"]
              },
              {
                "title": "Recommendations",
                "content": "{% raw %}{{{#123;$price_analysis.recommendations}}#125;}{% endraw %}"
              }
            ],
            "format": "pdf",
            "branding": true
          },
          "inputs": ["price_analysis", "sentiment_analysis", "visualization"]
        },
        
        {
          "id": "distribution",
          "type": "multi_channel_send",
          "name": "Distribute Report",
          "config": {
            "channels": [
              {
                "type": "email",
                "recipients": ["executives@company.com"],
                "subject": "Weekly Market Analysis Report - {% raw %}{{{#123;$date}}#125;}{% endraw %}",
                "body": "Please find attached the latest market analysis report.",
                "attachments": ["{% raw %}{{{#123;$report_generation.pdf}}#125;}{% endraw %}"]
              },
              {
                "type": "slack",
                "channel": "#market-intelligence", 
                "message": "ğŸ“Š New market analysis report available",
                "file": "{% raw %}{{{#123;$report_generation.pdf}}#125;}{% endraw %}"
              },
              {
                "type": "database",
                "table": "reports",
                "data": {
                  "report_date": "{% raw %}{{{#123;$date}}#125;}{% endraw %}",
                  "file_path": "{% raw %}{{{#123;$report_generation.pdf}}#125;}{% endraw %}",
                  "summary": "{% raw %}{{{#123;$price_analysis.summary}}#125;}{% endraw %}",
                  "status": "completed"
                }
              }
            ]
          },
          "inputs": ["report_generation"]
        }
      ],
      
      "scheduling": {
        "type": "cron",
        "expression": "0 6 * * 1", // ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 6ì‹œ
        "timezone": "UTC"
      },
      
      "monitoring": {
        "alerts": [
          {
            "condition": "price_change > 10%",
            "action": "immediate_notification",
            "recipients": ["alerts@company.com"]
          },
          {
            "condition": "sentiment_score < 3",
            "action": "escalate_to_management"
          }
        ]
      }
    };
  }
}
```

## ê³ ê¸‰ í™œìš© ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•

### 1. Clara SDKë¥¼ í™œìš©í•œ ì»¤ìŠ¤í…€ ë„êµ¬ ê°œë°œ

```typescript
// sdk/custom-tools/web-analyzer.ts
import { ClaraTool, ToolDefinition } from '@claraverse/sdk';

interface WebAnalyzerConfig {
  timeout: number;
  userAgent: string;
  followRedirects: boolean;
}

export class WebAnalyzerTool extends ClaraTool {
  name = 'web_analyzer';
  description = 'ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ë° SEO ì ìˆ˜ ê³„ì‚°';
  
  parameters: ToolDefinition = {
    type: 'object',
    properties: {
      url: {
        type: 'string',
        description: 'ë¶„ì„í•  ì›¹ì‚¬ì´íŠ¸ URL'
      },
      analysis_type: {
        type: 'string',
        enum: ['seo', 'performance', 'accessibility', 'all'],
        description: 'ë¶„ì„ ìœ í˜•'
      },
      detailed: {
        type: 'boolean',
        default: false,
        description: 'ìƒì„¸ ë¶„ì„ ì—¬ë¶€'
      }
    },
    required: ['url']
  };
  
  async execute(params: any): Promise<any> {
    const { url, analysis_type = 'all', detailed = false } = params;
    
    try {
      // ì›¹ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘
      const pageData = await this.fetchPageData(url);
      
      // ë¶„ì„ ì‹¤í–‰
      const results: any = {};
      
      if (analysis_type === 'seo' || analysis_type === 'all') {
        results.seo = await this.analyzeSEO(pageData);
      }
      
      if (analysis_type === 'performance' || analysis_type === 'all') {
        results.performance = await this.analyzePerformance(pageData);
      }
      
      if (analysis_type === 'accessibility' || analysis_type === 'all') {
        results.accessibility = await this.analyzeAccessibility(pageData);
      }
      
      // ìƒì„¸ ë¶„ì„ì´ ìš”ì²­ëœ ê²½ìš°
      if (detailed) {
        results.detailed_recommendations = await this.generateRecommendations(results);
      }
      
      return {
        url,
        timestamp: new Date().toISOString(),
        overall_score: this.calculateOverallScore(results),
        analysis: results
      };
      
    } catch (error) {
      throw new Error(`ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤íŒ¨: ${error.message}`);
    }
  }
  
  private async fetchPageData(url: string) {
    // Puppeteer ë˜ëŠ” Playwrightë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ìˆ˜ì§‘
    const response = await fetch(url);
    const html = await response.text();
    
    return {
      html,
      headers: Object.fromEntries(response.headers.entries()),
      status: response.status,
      loadTime: performance.now() // ê°„ë‹¨í•œ ë¡œë“œ ì‹œê°„ ì¸¡ì •
    };
  }
  
  private async analyzeSEO(pageData: any) {
    const { html } = pageData;
    const cheerio = require('cheerio');
    const $ = cheerio.load(html);
    
    return {
      title: {
        exists: $('title').length > 0,
        length: $('title').text().length,
        text: $('title').text(),
        score: this.scoreTitleSEO($('title').text())
      },
      meta_description: {
        exists: $('meta[name="description"]').length > 0,
        length: $('meta[name="description"]').attr('content')?.length || 0,
        text: $('meta[name="description"]').attr('content'),
        score: this.scoreMetaDescription($('meta[name="description"]').attr('content'))
      },
      headings: {
        h1_count: $('h1').length,
        h2_count: $('h2').length,
        h3_count: $('h3').length,
        structure_score: this.scoreHeadingStructure($)
      },
      images: {
        total_count: $('img').length,
        alt_missing: $('img:not([alt])').length,
        alt_score: this.scoreImageAlt($)
      },
      links: {
        internal_count: this.countInternalLinks($),
        external_count: this.countExternalLinks($),
        nofollow_count: $('a[rel*="nofollow"]').length
      }
    };
  }
  
  private async analyzePerformance(pageData: any) {
    // ì„±ëŠ¥ ë¶„ì„ ë¡œì§
    return {
      load_time: pageData.loadTime,
      resource_count: this.countResources(pageData.html),
      page_size: new Blob([pageData.html]).size,
      compression: this.checkCompression(pageData.headers),
      caching: this.checkCaching(pageData.headers)
    };
  }
  
  private async analyzeAccessibility(pageData: any) {
    // ì ‘ê·¼ì„± ë¶„ì„ ë¡œì§
    const cheerio = require('cheerio');
    const $ = cheerio.load(pageData.html);
    
    return {
      alt_text_score: this.scoreImageAlt($),
      heading_structure: this.scoreHeadingStructure($),
      color_contrast: await this.analyzeColorContrast($),
      keyboard_navigation: this.checkKeyboardNavigation($),
      aria_labels: this.checkAriaLabels($)
    };
  }
  
  private async generateRecommendations(results: any): Promise<string[]> {
    const recommendations: string[] = [];
    
    // SEO ê°œì„  ì œì•ˆ
    if (results.seo?.title?.score < 8) {
      recommendations.push('ì œëª© íƒœê·¸ë¥¼ 50-60ì ì‚¬ì´ë¡œ ìµœì í™”í•˜ì„¸ìš”');
    }
    
    if (results.seo?.meta_description?.score < 8) {
      recommendations.push('ë©”íƒ€ ì„¤ëª…ì„ 150-160ì ì‚¬ì´ë¡œ ì‘ì„±í•˜ì„¸ìš”');
    }
    
    // ì„±ëŠ¥ ê°œì„  ì œì•ˆ
    if (results.performance?.load_time > 3000) {
      recommendations.push('í˜ì´ì§€ ë¡œë”© ì‹œê°„ì„ 3ì´ˆ ì´ë‚´ë¡œ ê°œì„ í•˜ì„¸ìš”');
    }
    
    // ì ‘ê·¼ì„± ê°œì„  ì œì•ˆ
    if (results.accessibility?.alt_text_score < 8) {
      recommendations.push('ëª¨ë“  ì´ë¯¸ì§€ì— ì ì ˆí•œ alt í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”');
    }
    
    return recommendations;
  }
}

// ë„êµ¬ ë“±ë¡
export const webAnalyzerTool = new WebAnalyzerTool();
```

### 2. ì»¤ìŠ¤í…€ ìœ„ì ¯ ì‹œìŠ¤í…œ

```typescript
// widgets/dashboard-widgets.ts
import { ClaraWidget, WidgetProps } from '@claraverse/sdk';

// AI ì±„íŒ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìœ„ì ¯
export class AIPerformanceWidget extends ClaraWidget {
  name = 'ai_performance_monitor';
  displayName = 'AI ì„±ëŠ¥ ëª¨ë‹ˆí„°';
  category = 'monitoring';
  
  defaultConfig = {
    refreshInterval: 5000,
    showMetrics: ['response_time', 'token_count', 'model_usage'],
    timeRange: '1h'
  };
  
  async getData(config: any) {
    // Clara ë‚´ë¶€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    const metrics = await this.claraAPI.getMetrics({
      timeRange: config.timeRange,
      metrics: config.showMetrics
    });
    
    return {
      current_model: await this.claraAPI.getCurrentModel(),
      active_sessions: await this.claraAPI.getActiveSessions(),
      performance_metrics: metrics,
      system_resources: await this.getSystemResources()
    };
  }
  
  render(data: any, config: any): React.ReactElement {
    return (
      <div className="ai-performance-widget p-4 bg-gray-900 rounded-lg">
        <h3 className="text-lg font-semibold text-white mb-4">
          AI ì„±ëŠ¥ ëª¨ë‹ˆí„°
        </h3>
        
        <div className="grid grid-cols-2 gap-4">
          <div className="metric-card">
            <div className="text-sm text-gray-400">í˜„ì¬ ëª¨ë¸</div>
            <div className="text-xl text-white">{data.current_model}</div>
          </div>
          
          <div className="metric-card">
            <div className="text-sm text-gray-400">í™œì„± ì„¸ì…˜</div>
            <div className="text-xl text-green-400">{data.active_sessions}</div>
          </div>
          
          <div className="metric-card">
            <div className="text-sm text-gray-400">í‰ê·  ì‘ë‹µì‹œê°„</div>
            <div className="text-xl text-blue-400">
              {data.performance_metrics.avg_response_time}ms
            </div>
          </div>
          
          <div className="metric-card">
            <div className="text-sm text-gray-400">í† í° ì²˜ë¦¬ìœ¨</div>
            <div className="text-xl text-purple-400">
              {data.performance_metrics.tokens_per_second}/s
            </div>
          </div>
        </div>
        
        {/* ì‹¤ì‹œê°„ ì°¨íŠ¸ */}
        <div className="mt-4">
          <ResponsiveChart 
            data={data.performance_metrics.timeline}
            type="line"
            height={200}
          />
        </div>
      </div>
    );
  }
}

// ì›Œí¬í”Œë¡œìš° ìƒíƒœ ìœ„ì ¯
export class WorkflowStatusWidget extends ClaraWidget {
  name = 'workflow_status';
  displayName = 'ì›Œí¬í”Œë¡œìš° ìƒíƒœ';
  category = 'automation';
  
  async getData(config: any) {
    const workflows = await this.claraAPI.getWorkflows();
    const recentRuns = await this.claraAPI.getRecentWorkflowRuns(10);
    
    return {
      total_workflows: workflows.length,
      active_workflows: workflows.filter(w => w.status === 'active').length,
      recent_runs: recentRuns,
      success_rate: this.calculateSuccessRate(recentRuns)
    };
  }
  
  render(data: any, config: any): React.ReactElement {
    return (
      <div className="workflow-status-widget p-4 bg-gray-900 rounded-lg">
        <h3 className="text-lg font-semibold text-white mb-4">
          ì›Œí¬í”Œë¡œìš° ìƒíƒœ
        </h3>
        
        <div className="space-y-3">
          {data.recent_runs.map((run: any, index: number) => (
            <div key={index} className="flex items-center justify-between">
              <div className="flex items-center space-x-2">
                <div className={`w-2 h-2 rounded-full ${
                  run.status === 'success' ? 'bg-green-400' :
                  run.status === 'running' ? 'bg-yellow-400' : 'bg-red-400'
                }`} />
                <span className="text-white">{run.workflow_name}</span>
              </div>
              
              <div className="text-sm text-gray-400">
                {this.formatDuration(run.duration)}
              </div>
            </div>
          ))}
        </div>
        
        <div className="mt-4 pt-4 border-t border-gray-700">
          <div className="text-sm text-gray-400">
            ì„±ê³µë¥ : <span className="text-green-400">{data.success_rate}%</span>
          </div>
        </div>
      </div>
    );
  }
}

// ìœ„ì ¯ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
export const registerCustomWidgets = () => {
  ClaraWidgetRegistry.register([
    new AIPerformanceWidget(),
    new WorkflowStatusWidget()
  ]);
};
```

### 3. ëª¨ë¸ ê´€ë¦¬ ë° ìµœì í™”

```python
# model_management/optimization.py
import torch
import psutil
import json
from pathlib import Path
from typing import Dict, List, Optional

class ClaraModelManager:
    def __init__(self, models_dir: str = "~/.claraverse/models"):
        self.models_dir = Path(models_dir).expanduser()
        self.models_dir.mkdir(parents=True, exist_ok=True)
        self.active_models = {}
        self.model_configs = {}
        
    def optimize_model_loading(self):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ì— ë”°ë¥¸ ëª¨ë¸ ìµœì í™”"""
        system_info = self.get_system_info()
        
        # GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
        if system_info['gpu_memory'] > 12000:  # 12GB+
            recommended_models = ['llama3.2:70b', 'sdxl']
        elif system_info['gpu_memory'] > 8000:  # 8GB+
            recommended_models = ['llama3.2:8b', 'sd1.5']
        else:  # CPU only or low VRAM
            recommended_models = ['llama3.2:3b', 'sd-tiny']
            
        return {
            'recommended_models': recommended_models,
            'optimization_settings': self.get_optimization_settings(system_info),
            'memory_strategy': self.determine_memory_strategy(system_info)
        }
    
    def get_system_info(self) -> Dict:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        info = {
            'cpu_cores': psutil.cpu_count(),
            'ram_total': psutil.virtual_memory().total // (1024**3),  # GB
            'ram_available': psutil.virtual_memory().available // (1024**3),
            'gpu_available': torch.cuda.is_available(),
            'gpu_memory': 0,
            'gpu_count': 0
        }
        
        if torch.cuda.is_available():
            info['gpu_count'] = torch.cuda.device_count()
            info['gpu_memory'] = torch.cuda.get_device_properties(0).total_memory // (1024**2)  # MB
            
        return info
    
    def create_model_config(self, model_name: str, use_case: str = "general"):
        """ì‚¬ìš© ì‚¬ë¡€ë³„ ëª¨ë¸ ì„¤ì • ìƒì„±"""
        system_info = self.get_system_info()
        
        base_config = {
            "model_name": model_name,
            "use_case": use_case,
            "optimization_level": "balanced"
        }
        
        # ì‚¬ìš© ì‚¬ë¡€ë³„ íŠ¹í™” ì„¤ì •
        if use_case == "coding":
            base_config.update({
                "temperature": 0.1,
                "top_p": 0.9,
                "repetition_penalty": 1.1,
                "context_length": 4096,
                "stop_tokens": ["```", "\n\n\n"]
            })
        elif use_case == "creative":
            base_config.update({
                "temperature": 0.8,
                "top_p": 0.95,
                "repetition_penalty": 1.0,
                "context_length": 2048
            })
        elif use_case == "analysis":
            base_config.update({
                "temperature": 0.3,
                "top_p": 0.9,
                "repetition_penalty": 1.05,
                "context_length": 8192
            })
            
        # í•˜ë“œì›¨ì–´ ê¸°ë°˜ ìµœì í™”
        if system_info['gpu_memory'] < 6000:  # 6GB ë¯¸ë§Œ
            base_config.update({
                "gpu_layers": 10,
                "batch_size": 1,
                "thread_count": system_info['cpu_cores'] // 2
            })
        else:
            base_config.update({
                "gpu_layers": 32,
                "batch_size": 4,
                "thread_count": 4
            })
            
        return base_config
    
    def benchmark_models(self, test_prompts: List[str]):
        """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        results = {}
        
        for model_name in self.get_available_models():
            print(f"Benchmarking {model_name}...")
            
            model_results = {
                "response_times": [],
                "memory_usage": [],
                "quality_scores": []
            }
            
            for prompt in test_prompts:
                start_time = time.time()
                memory_before = psutil.virtual_memory().used
                
                # ëª¨ë¸ ì‹¤í–‰
                response = self.run_inference(model_name, prompt)
                
                end_time = time.time()
                memory_after = psutil.virtual_memory().used
                
                model_results["response_times"].append(end_time - start_time)
                model_results["memory_usage"].append(memory_after - memory_before)
                model_results["quality_scores"].append(
                    self.evaluate_response_quality(prompt, response)
                )
            
            # í‰ê·  ê³„ì‚°
            results[model_name] = {
                "avg_response_time": sum(model_results["response_times"]) / len(model_results["response_times"]),
                "avg_memory_usage": sum(model_results["memory_usage"]) / len(model_results["memory_usage"]),
                "avg_quality_score": sum(model_results["quality_scores"]) / len(model_results["quality_scores"]),
                "efficiency_score": self.calculate_efficiency_score(model_results)
            }
            
        return results
    
    def auto_tune_performance(self):
        """ìë™ ì„±ëŠ¥ íŠœë‹"""
        tuning_results = {}
        
        # ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
        optimal_batch_size = self.find_optimal_batch_size()
        tuning_results['batch_size'] = optimal_batch_size
        
        # GPU ë ˆì´ì–´ ìˆ˜ ìµœì í™”
        optimal_gpu_layers = self.find_optimal_gpu_layers()
        tuning_results['gpu_layers'] = optimal_gpu_layers
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì í™”
        optimal_context_length = self.find_optimal_context_length()
        tuning_results['context_length'] = optimal_context_length
        
        # ì„¤ì • ì €ì¥
        self.save_optimized_config(tuning_results)
        
        return tuning_results

# ì‚¬ìš© ì˜ˆì‹œ
model_manager = ClaraModelManager()

# ì‹œìŠ¤í…œ ìµœì í™”
optimization = model_manager.optimize_model_loading()
print("ì¶”ì²œ ëª¨ë¸:", optimization['recommended_models'])

# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
test_prompts = [
    "Pythonìœ¼ë¡œ ì›¹ ìŠ¤í¬ë˜í•‘ ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”",
    "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    "ì°½ì˜ì ì¸ ë§ˆì¼€íŒ… ìº í˜ì¸ ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”"
]

benchmark_results = model_manager.benchmark_models(test_prompts)
print("ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:", json.dumps(benchmark_results, indent=2))

# ìë™ íŠœë‹
tuning_results = model_manager.auto_tune_performance()
print("ìµœì í™”ëœ ì„¤ì •:", tuning_results)
```

## ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§

### 1. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìµœì í™”

```bash
#!/bin/bash
# optimize_clara.sh - ClaraVerse ì„±ëŠ¥ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸

echo "ğŸš€ ClaraVerse ì„±ëŠ¥ ìµœì í™” ì‹œì‘"

# GPU ë©”ëª¨ë¦¬ ìµœì í™”
optimize_gpu() {
    echo "ğŸ“Š GPU ì„¤ì • ìµœì í™” ì¤‘..."
    
    # NVIDIA GPU ì„¤ì •
    if command -v nvidia-smi &> /dev/null; then
        # GPU íŒŒì›Œ ëª¨ë“œ ìµœëŒ€ë¡œ ì„¤ì •
        sudo nvidia-smi -pm 1
        
        # GPU í´ëŸ­ ìµœì í™”
        sudo nvidia-smi -ac 877,1380  # ë©”ëª¨ë¦¬, ê·¸ë˜í”½ í´ëŸ­
        
        echo "âœ… NVIDIA GPU ìµœì í™” ì™„ë£Œ"
    fi
    
    # AMD GPU ì„¤ì • (ROCm)
    if command -v rocm-smi &> /dev/null; then
        # AMD GPU ìµœì í™” ëª…ë ¹ì–´ë“¤
        echo "âœ… AMD GPU ìµœì í™” ì™„ë£Œ"
    fi
}

# ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìµœì í™”
optimize_memory() {
    echo "ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì¤‘..."
    
    # ìŠ¤ì™‘ ìµœì í™”
    echo 10 | sudo tee /proc/sys/vm/swappiness
    
    # íŒŒì¼ ìºì‹œ ìµœì í™”
    echo 3 | sudo tee /proc/sys/vm/drop_caches
    
    # ë©”ëª¨ë¦¬ ì˜¤ë²„ì»¤ë°‹ ì„¤ì •
    echo 1 | sudo tee /proc/sys/vm/overcommit_memory
    
    echo "âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ"
}

# Ollama ìµœì í™”
optimize_ollama() {
    echo "ğŸ§  Ollama ìµœì í™” ì¤‘..."
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    export OLLAMA_NUM_PARALLEL=4
    export OLLAMA_MAX_LOADED_MODELS=2
    export OLLAMA_FLASH_ATTENTION=1
    export OLLAMA_GPU_MEMORY_FRACTION=0.8
    
    # Ollama ì„œë¹„ìŠ¤ ì¬ì‹œì‘
    pkill ollama
    sleep 2
    nohup ollama serve > /tmp/ollama.log 2>&1 &
    
    echo "âœ… Ollama ìµœì í™” ì™„ë£Œ"
}

# Node.js ìµœì í™”
optimize_nodejs() {
    echo "âš¡ Node.js ìµœì í™” ì¤‘..."
    
    # V8 ì—”ì§„ ìµœì í™”
    export NODE_OPTIONS="--max-old-space-size=8192 --max-semi-space-size=512"
    
    # UV_THREADPOOL_SIZE ì¦ê°€
    export UV_THREADPOOL_SIZE=16
    
    echo "âœ… Node.js ìµœì í™” ì™„ë£Œ"
}

# ë©”ì¸ ìµœì í™” ì‹¤í–‰
main() {
    optimize_gpu
    optimize_memory  
    optimize_ollama
    optimize_nodejs
    
    echo "ğŸ‰ ClaraVerse ìµœì í™” ì™„ë£Œ!"
    echo "ğŸ’¡ ê¶Œì¥ì‚¬í•­:"
    echo "  - 16GB+ RAM ì‚¬ìš© ì‹œ ëª¨ë“  ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ ê°€ëŠ¥"
    echo "  - GPU 12GB+ ì‹œ ëŒ€í˜• ëª¨ë¸ ì‚¬ìš© ê¶Œì¥"
    echo "  - SSD ì‚¬ìš©ìœ¼ë¡œ ëª¨ë¸ ë¡œë”© ì†ë„ í–¥ìƒ"
}

main "$@"
```

### 2. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

```typescript
// monitoring/dashboard.tsx
import React, { useState, useEffect } from 'react';
import { Line, Bar, Doughnut } from 'react-chartjs-2';

interface SystemMetrics {
  cpu: number;
  memory: number;
  gpu: number;
  disk: number;
  network: number;
}

interface AIMetrics {
  active_models: string[];
  requests_per_minute: number;
  average_response_time: number;
  token_throughput: number;
  queue_length: number;
}

export const ClaraMonitoringDashboard: React.FC = () => {
  const [systemMetrics, setSystemMetrics] = useState<SystemMetrics | null>(null);
  const [aiMetrics, setAIMetrics] = useState<AIMetrics | null>(null);
  const [alerts, setAlerts] = useState<string[]>([]);
  
  useEffect(() => {
    const interval = setInterval(async () => {
      // ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
      const sysMetrics = await fetch('/api/metrics/system').then(r => r.json());
      setSystemMetrics(sysMetrics);
      
      // AI ë©”íŠ¸ë¦­ ìˆ˜ì§‘
      const aiMetrics = await fetch('/api/metrics/ai').then(r => r.json());
      setAIMetrics(aiMetrics);
      
      // ì•Œë¦¼ í™•ì¸
      checkAlerts(sysMetrics, aiMetrics);
    }, 2000);
    
    return () => clearInterval(interval);
  }, []);
  
  const checkAlerts = (sys: SystemMetrics, ai: AIMetrics) => {
    const newAlerts: string[] = [];
    
    if (sys.memory > 90) newAlerts.push('ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  90% ì´ˆê³¼');
    if (sys.gpu > 95) newAlerts.push('GPU ì‚¬ìš©ë¥  95% ì´ˆê³¼');
    if (ai.average_response_time > 5000) newAlerts.push('ì‘ë‹µì‹œê°„ 5ì´ˆ ì´ˆê³¼');
    if (ai.queue_length > 10) newAlerts.push('ìš”ì²­ ëŒ€ê¸°ì—´ 10ê°œ ì´ˆê³¼');
    
    setAlerts(newAlerts);
  };
  
  return (
    <div className="monitoring-dashboard p-6 bg-gray-900 min-h-screen">
      <h1 className="text-3xl font-bold text-white mb-8">
        ClaraVerse ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
      </h1>
      
      {/* ì•Œë¦¼ ì˜ì—­ */}
      {alerts.length > 0 && (
        <div className="alert-banner bg-red-600 text-white p-4 rounded-lg mb-6">
          <h3 className="font-semibold mb-2">âš ï¸ ê²½ê³ </h3>
          <ul className="list-disc list-inside">
            {alerts.map((alert, index) => (
              <li key={index}>{alert}</li>
            ))}
          </ul>
        </div>
      )}
      
      {/* ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ */}
      <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6 mb-8">
        <MetricCard 
          title="CPU ì‚¬ìš©ë¥ "
          value={`${systemMetrics?.cpu || 0}%`}
          status={getStatus(systemMetrics?.cpu || 0, 80, 90)}
        />
        <MetricCard 
          title="ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ "
          value={`${systemMetrics?.memory || 0}%`}
          status={getStatus(systemMetrics?.memory || 0, 70, 85)}
        />
        <MetricCard 
          title="GPU ì‚¬ìš©ë¥ "
          value={`${systemMetrics?.gpu || 0}%`}
          status={getStatus(systemMetrics?.gpu || 0, 85, 95)}
        />
        <MetricCard 
          title="ë””ìŠ¤í¬ ì‚¬ìš©ë¥ "
          value={`${systemMetrics?.disk || 0}%`}
          status={getStatus(systemMetrics?.disk || 0, 80, 90)}
        />
      </div>
      
      {/* AI ì„±ëŠ¥ ë©”íŠ¸ë¦­ */}
      <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6 mb-8">
        <MetricCard 
          title="í™œì„± ëª¨ë¸"
          value={aiMetrics?.active_models.length || 0}
          subtitle="ê°œ"
        />
        <MetricCard 
          title="ë¶„ë‹¹ ìš”ì²­"
          value={aiMetrics?.requests_per_minute || 0}
          subtitle="req/min"
        />
        <MetricCard 
          title="í‰ê·  ì‘ë‹µì‹œê°„"
          value={`${aiMetrics?.average_response_time || 0}ms`}
          status={getStatus(aiMetrics?.average_response_time || 0, 2000, 5000)}
        />
        <MetricCard 
          title="í† í° ì²˜ë¦¬ìœ¨"
          value={`${aiMetrics?.token_throughput || 0}`}
          subtitle="tokens/s"
        />
      </div>
      
      {/* ì‹¤ì‹œê°„ ì°¨íŠ¸ */}
      <div className="grid grid-cols-1 lg:grid-cols-2 gap-8">
        <div className="chart-container bg-gray-800 p-6 rounded-lg">
          <h3 className="text-lg font-semibold text-white mb-4">
            ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ 
          </h3>
          <Line data={getSystemUsageChartData()} options={chartOptions} />
        </div>
        
        <div className="chart-container bg-gray-800 p-6 rounded-lg">
          <h3 className="text-lg font-semibold text-white mb-4">
            AI ìš”ì²­ ì²˜ë¦¬ëŸ‰
          </h3>
          <Bar data={getRequestThroughputData()} options={chartOptions} />
        </div>
      </div>
    </div>
  );
};

const MetricCard: React.FC<{
  title: string;
  value: string | number;
  subtitle?: string;
  status?: 'good' | 'warning' | 'critical';
}> = ({ title, value, subtitle, status = 'good' }) => {
  const statusColors = {
    good: 'border-green-400 text-green-400',
    warning: 'border-yellow-400 text-yellow-400',
    critical: 'border-red-400 text-red-400'
  };
  
  return (
    <div className={`metric-card bg-gray-800 p-4 rounded-lg border-l-4 ${statusColors[status]}`}>
      <div className="text-sm text-gray-400 mb-1">{title}</div>
      <div className="text-2xl font-bold text-white">
        {value} {subtitle && <span className="text-sm text-gray-400">{subtitle}</span>}
      </div>
    </div>
  );
};

const getStatus = (value: number, warning: number, critical: number): 'good' | 'warning' | 'critical' => {
  if (value >= critical) return 'critical';
  if (value >= warning) return 'warning';
  return 'good';
};
```

### 3. ìë™ ì„±ëŠ¥ íŠœë‹

```python
# auto_tuning/performance_optimizer.py
import json
import time
import psutil
import subprocess
from typing import Dict, List, Tuple
from dataclasses import dataclass

@dataclass
class PerformanceConfig:
    batch_size: int
    context_length: int
    gpu_layers: int
    temperature: float
    top_p: float
    num_threads: int

class AutoPerformanceTuner:
    def __init__(self):
        self.baseline_metrics = None
        self.best_config = None
        self.tuning_history = []
        
    def run_comprehensive_tuning(self) -> Dict:
        """í¬ê´„ì  ì„±ëŠ¥ íŠœë‹ ì‹¤í–‰"""
        
        print("ğŸ”§ ClaraVerse ìë™ ì„±ëŠ¥ íŠœë‹ ì‹œì‘")
        
        # 1. ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì •
        self.baseline_metrics = self.measure_baseline_performance()
        print(f"ğŸ“Š ê¸°ì¤€ì„  ì„±ëŠ¥: {self.baseline_metrics}")
        
        # 2. í•˜ë“œì›¨ì–´ í”„ë¡œíŒŒì¼ë§
        hardware_profile = self.profile_hardware()
        print(f"ğŸ’» í•˜ë“œì›¨ì–´ í”„ë¡œí•„: {hardware_profile}")
        
        # 3. ìµœì  ì„¤ì • íƒìƒ‰
        optimal_configs = self.search_optimal_configurations(hardware_profile)
        
        # 4. A/B í…ŒìŠ¤íŠ¸
        best_config = self.ab_test_configurations(optimal_configs)
        
        # 5. ì„¤ì • ì ìš© ë° ê²€ì¦
        final_metrics = self.apply_and_validate_config(best_config)
        
        tuning_results = {
            'baseline_metrics': self.baseline_metrics,
            'final_metrics': final_metrics,
            'improvement': self.calculate_improvement(self.baseline_metrics, final_metrics),
            'optimal_config': best_config,
            'hardware_profile': hardware_profile
        }
        
        # ê²°ê³¼ ì €ì¥
        self.save_tuning_results(tuning_results)
        
        return tuning_results
    
    def measure_baseline_performance(self) -> Dict:
        """ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì •"""
        test_prompts = [
            "Pythonìœ¼ë¡œ ê°„ë‹¨í•œ ì›¹ ì„œë²„ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
            "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "íš¨ê³¼ì ì¸ ë°ì´í„° ì‹œê°í™”ë¥¼ ìœ„í•œ íŒì„ ì•Œë ¤ì£¼ì„¸ìš”"
        ]
        
        metrics = {
            'response_times': [],
            'memory_usage': [],
            'cpu_usage': [],
            'gpu_usage': [],
            'token_throughput': []
        }
        
        for prompt in test_prompts:
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            start_time = time.time()
            cpu_before = psutil.cpu_percent()
            memory_before = psutil.virtual_memory().percent
            
            # API í˜¸ì¶œ
            response = self.call_clara_api(prompt)
            
            end_time = time.time()
            cpu_after = psutil.cpu_percent()
            memory_after = psutil.virtual_memory().percent
            
            # ë©”íŠ¸ë¦­ ê¸°ë¡
            response_time = end_time - start_time
            metrics['response_times'].append(response_time)
            metrics['cpu_usage'].append(cpu_after - cpu_before)
            metrics['memory_usage'].append(memory_after - memory_before)
            
            # í† í° ì²˜ë¦¬ìœ¨ ê³„ì‚°
            token_count = len(response.split())
            throughput = token_count / response_time
            metrics['token_throughput'].append(throughput)
        
        # í‰ê· ê°’ ê³„ì‚°
        return {
            'avg_response_time': sum(metrics['response_times']) / len(metrics['response_times']),
            'avg_cpu_usage': sum(metrics['cpu_usage']) / len(metrics['cpu_usage']),
            'avg_memory_usage': sum(metrics['memory_usage']) / len(metrics['memory_usage']),
            'avg_token_throughput': sum(metrics['token_throughput']) / len(metrics['token_throughput'])
        }
    
    def profile_hardware(self) -> Dict:
        """í•˜ë“œì›¨ì–´ í”„ë¡œíŒŒì¼ë§"""
        import torch
        
        profile = {
            'cpu_cores': psutil.cpu_count(),
            'cpu_freq': psutil.cpu_freq().max if psutil.cpu_freq() else 0,
            'ram_total': psutil.virtual_memory().total // (1024**3),
            'ram_available': psutil.virtual_memory().available // (1024**3),
            'gpu_available': torch.cuda.is_available(),
            'gpu_count': 0,
            'gpu_memory': 0,
            'gpu_compute_capability': None
        }
        
        if torch.cuda.is_available():
            profile['gpu_count'] = torch.cuda.device_count()
            profile['gpu_memory'] = torch.cuda.get_device_properties(0).total_memory // (1024**2)
            profile['gpu_compute_capability'] = torch.cuda.get_device_capability(0)
        
        return profile
    
    def search_optimal_configurations(self, hardware_profile: Dict) -> List[PerformanceConfig]:
        """ìµœì  ì„¤ì • íƒìƒ‰"""
        
        # í•˜ë“œì›¨ì–´ ê¸°ë°˜ ì„¤ì • í›„ë³´ ìƒì„±
        configs = []
        
        # CPU ê¸°ë°˜ ì„¤ì •
        if hardware_profile['ram_total'] >= 16:
            configs.extend([
                PerformanceConfig(4, 4096, 0, 0.1, 0.9, hardware_profile['cpu_cores']//2),
                PerformanceConfig(8, 4096, 0, 0.1, 0.9, hardware_profile['cpu_cores']//2),
            ])
        else:
            configs.extend([
                PerformanceConfig(1, 2048, 0, 0.1, 0.9, hardware_profile['cpu_cores']//4),
                PerformanceConfig(2, 2048, 0, 0.1, 0.9, hardware_profile['cpu_cores']//4),
            ])
        
        # GPU ê¸°ë°˜ ì„¤ì •
        if hardware_profile['gpu_available'] and hardware_profile['gpu_memory'] > 8000:
            configs.extend([
                PerformanceConfig(8, 4096, 32, 0.1, 0.9, 4),
                PerformanceConfig(16, 4096, 32, 0.1, 0.9, 4),
                PerformanceConfig(8, 8192, 24, 0.1, 0.9, 4),
            ])
        elif hardware_profile['gpu_available']:
            configs.extend([
                PerformanceConfig(4, 2048, 16, 0.1, 0.9, 4),
                PerformanceConfig(2, 4096, 20, 0.1, 0.9, 4),
            ])
        
        return configs
    
    def ab_test_configurations(self, configs: List[PerformanceConfig]) -> PerformanceConfig:
        """A/B í…ŒìŠ¤íŠ¸ë¡œ ìµœì  ì„¤ì • ì„ íƒ"""
        
        best_config = None
        best_score = float('-inf')
        
        test_prompts = [
            "ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ì„ Pythonìœ¼ë¡œ êµ¬í˜„í•´ì£¼ì„¸ìš”",
            "ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë°©ë²•ì„ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "í´ë¼ìš°ë“œ ì•„í‚¤í…ì²˜ ì„¤ê³„ ì›ì¹™ì„ ì•Œë ¤ì£¼ì„¸ìš”"
        ]
        
        for i, config in enumerate(configs):
            print(f"ğŸ§ª ì„¤ì • {i+1}/{len(configs)} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ì„¤ì • ì ìš©
            self.apply_config(config)
            time.sleep(5)  # ì„¤ì • ì ìš© ëŒ€ê¸°
            
            # ì„±ëŠ¥ ì¸¡ì •
            metrics = self.measure_performance_with_config(config, test_prompts)
            
            # ì ìˆ˜ ê³„ì‚° (ì‘ë‹µì‹œê°„, ì²˜ë¦¬ëŸ‰, ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¢…í•©)
            score = self.calculate_performance_score(metrics)
            
            print(f"   ì ìˆ˜: {score:.2f}")
            
            if score > best_score:
                best_score = score
                best_config = config
                
            # ê²°ê³¼ ê¸°ë¡
            self.tuning_history.append({
                'config': config,
                'metrics': metrics,
                'score': score
            })
        
        return best_config
    
    def calculate_performance_score(self, metrics: Dict) -> float:
        """ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)"""
        
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        weights = {
            'response_time': -0.4,    # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            'token_throughput': 0.3,   # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
            'memory_efficiency': 0.2,  # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
            'cpu_efficiency': 0.1      # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
        }
        
        # ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚°
        normalized_scores = {
            'response_time': 1 / (1 + metrics['avg_response_time']),
            'token_throughput': metrics['avg_token_throughput'] / 100,
            'memory_efficiency': 1 / (1 + metrics['avg_memory_usage']),
            'cpu_efficiency': 1 / (1 + metrics['avg_cpu_usage'])
        }
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        score = sum(normalized_scores[key] * weights[key] for key in weights)
        
        return score * 100  # 0-100 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜

# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    tuner = AutoPerformanceTuner()
    results = tuner.run_comprehensive_tuning()
    
    print("\nğŸ¯ ìµœì í™” ê²°ê³¼:")
    print(f"ì‘ë‹µì‹œê°„ ê°œì„ : {results['improvement']['response_time']:.1f}%")
    print(f"ì²˜ë¦¬ëŸ‰ ê°œì„ : {results['improvement']['throughput']:.1f}%")
    print(f"ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {results['improvement']['memory']:.1f}%")
    print(f"\nìµœì  ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸš€")
```

## ë¬¸ì œ í•´ê²° ë° ë””ë²„ê¹…

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

**Ollama ì—°ê²° ì‹¤íŒ¨**:
```bash
# Ollama ìƒíƒœ í™•ì¸
curl http://localhost:11434/api/tags

# Ollama ì„œë¹„ìŠ¤ ì¬ì‹œì‘
pkill ollama
ollama serve

# ë°©í™”ë²½ í™•ì¸
sudo ufw status
sudo ufw allow 11434
```

**GPU ì¸ì‹ ë¬¸ì œ**:
```bash
# NVIDIA GPU í™•ì¸
nvidia-smi
export CUDA_VISIBLE_DEVICES=0

# AMD GPU í™•ì¸ (ROCm)
rocm-smi

# GPU ë©”ëª¨ë¦¬ í™•ì¸
python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"
```

**ë©”ëª¨ë¦¬ ë¶€ì¡±**:
```bash
# ìŠ¤ì™‘ ë©”ëª¨ë¦¬ ì¶”ê°€
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸°
ollama pull llama3.2:3b  # ëŒ€ì‹  í° ëª¨ë¸ ì‚¬ìš©
```

### ê³ ê¸‰ ë””ë²„ê¹…

```javascript
// debugging/clara-debugger.js
class ClaraDebugger {
  static async diagnoseSystem() {
    const diagnosis = {
      system: await this.checkSystemHealth(),
      ollama: await this.checkOllamaHealth(),
      models: await this.checkModelHealth(),
      performance: await this.checkPerformance(),
      storage: await this.checkStorageHealth()
    };
    
    console.table(diagnosis);
    return diagnosis;
  }
  
  static async checkSystemHealth() {
    return {
      cpu_usage: await this.getCPUUsage(),
      memory_usage: await this.getMemoryUsage(),
      disk_usage: await this.getDiskUsage(),
      gpu_status: await this.getGPUStatus()
    };
  }
  
  static async fixCommonIssues() {
    console.log("ğŸ”§ ì¼ë°˜ì ì¸ ë¬¸ì œ ìë™ ìˆ˜ì • ì¤‘...");
    
    // í¬íŠ¸ ì¶©ëŒ í•´ê²°
    await this.killConflictingProcesses();
    
    // ìºì‹œ ì •ë¦¬
    await this.clearCache();
    
    // ì„¤ì • ë³µêµ¬
    await this.restoreDefaultConfig();
    
    console.log("âœ… ìë™ ìˆ˜ì • ì™„ë£Œ");
  }
}

// ì‚¬ìš©ë²•
ClaraDebugger.diagnoseSystem().then(result => {
  if (result.performance.score < 50) {
    ClaraDebugger.fixCommonIssues();
  }
});
```

## ê²°ë¡ 

ClaraVerseëŠ” **ì™„ì „í•œ í”„ë¼ì´ë²„ì‹œì™€ ë¡œì»¬ ì‹¤í–‰ì„ ë³´ì¥í•˜ëŠ” í˜ì‹ ì ì¸ AI ì›Œí¬ìŠ¤í˜ì´ìŠ¤**ì…ë‹ˆë‹¤. ë‹¨ì¼ í”Œë«í¼ì—ì„œ LLM ì±„íŒ…, ì´ë¯¸ì§€ ìƒì„±, ì›Œí¬í”Œë¡œìš° ìë™í™”, AI ì—ì´ì „íŠ¸ê¹Œì§€ ëª¨ë“  ê²ƒì„ ì œê³µí•˜ë©°, ì‚¬ìš©ìì˜ ë°ì´í„°ê°€ ì ˆëŒ€ ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠëŠ” ì™„ì „í•œ ë³´ì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.

### í•µì‹¬ ê°€ì¹˜ ì œì•ˆ

1. **ì™„ì „í•œ í”„ë¼ì´ë²„ì‹œ**: ëª¨ë“  ì²˜ë¦¬ê°€ ë¡œì»¬ì—ì„œ ì´ë£¨ì–´ì§
2. **í†µí•© ì›Œí¬ìŠ¤í˜ì´ìŠ¤**: ë‹¤ì–‘í•œ AI ê¸°ëŠ¥ì„ í•˜ë‚˜ì˜ í”Œë«í¼ì—
3. **ë¬´ì œí•œ í™•ì¥ì„±**: ì˜¤í”ˆì†ŒìŠ¤ë¡œ ì™„ì „í•œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
4. **ë¹„ìš© íš¨ìœ¨ì„±**: API í‚¤ë‚˜ í´ë¼ìš°ë“œ ë¹„ìš© ì—†ìŒ

### ì‹¤ì œ í™œìš© ì‹œë‚˜ë¦¬ì˜¤

- **ê°œë°œì**: ë¡œì»¬ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ + ìë™í™”
- **í¬ë¦¬ì—ì´í„°**: í…ìŠ¤íŠ¸ ìƒì„± + ì´ë¯¸ì§€ ìƒì„± + ì›Œí¬í”Œë¡œìš°
- **ê¸°ì—…**: ì™„ì „í•œ ë°ì´í„° ë³´ì•ˆì´ í•„ìš”í•œ AI ì—…ë¬´
- **ì—°êµ¬ì**: í”„ë¼ì´ë¹— AI ì‹¤í—˜ í™˜ê²½

### ë¯¸ë˜ ì „ë§

ClaraVerseëŠ” **ë¡œì»¬ AIì˜ ë¯¸ë˜**ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. í´ë¼ìš°ë“œ ì˜ì¡´ì„± ì—†ì´ë„ ê°•ë ¥í•œ AI ê¸°ëŠ¥ì„ í™œìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê°œì¸ ì •ë³´ ë³´í˜¸ì™€ ë¹„ìš© ì ˆê°ì„ ë™ì‹œì— ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì§€ê¸ˆ ë°”ë¡œ [ClaraVerse](https://github.com/badboysm890/ClaraVerse)ë¥¼ ì‹œì‘í•˜ì—¬ **ì§„ì •í•œ í”„ë¼ì´ë¹— AI ì›Œí¬ìŠ¤í˜ì´ìŠ¤**ë¥¼ ê²½í—˜í•´ë³´ì„¸ìš”! ğŸš€

---

**ì°¸ê³  ìë£Œ**:
- [ClaraVerse GitHub](https://github.com/badboysm890/ClaraVerse)
- [ClaraVerse ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://claraverse.space)
- [Ollama ê³µì‹ ë¬¸ì„œ](https://ollama.ai/docs)
- [ComfyUI ë¬¸ì„œ](https://github.com/comfyanonymous/ComfyUI) 