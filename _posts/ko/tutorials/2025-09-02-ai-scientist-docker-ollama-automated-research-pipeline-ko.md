---
title: "AI Scientistì™€ Docker & Ollama: ìë™í™”ëœ ì—°êµ¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•í•˜ê¸°"
excerpt: "SakanaAIì˜ AI Scientistë¥¼ OrbStack Docker í™˜ê²½ì—ì„œ êµ¬ë™í•˜ì—¬ ì—°êµ¬ ì›Œí¬í”Œë¡œìš°ë¥¼ í˜ì‹ í•˜ì„¸ìš”. ì´ ì¢…í•© ê°€ì´ë“œëŠ” Ollamaì™€ LM Studio ê°™ì€ ë¡œì»¬ LLMì„ í™œìš©í•˜ì—¬ 24/7 ìë™ ì—°êµ¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ í ê´€ë¦¬ì™€ ì§€ì†ì  ìš´ì˜ ì˜ˆì œì™€ í•¨ê»˜ ì œì‹œí•©ë‹ˆë‹¤."
seo_title: "AI Scientist Docker Ollama ì„¤ì • ê°€ì´ë“œ - ìë™í™”ëœ ì—°êµ¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• - Thaki Cloud"
seo_description: "SakanaAIì˜ AI Scientistë¥¼ OrbStack Docker, Ollama, LM Studioì™€ í•¨ê»˜ ì„¤ì •í•˜ì—¬ ìë™í™”ëœ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì™„ì „í•œ íŠœí† ë¦¬ì–¼ì…ë‹ˆë‹¤. í ê´€ë¦¬, ëª¨ë‹ˆí„°ë§, 24/7 ìš´ì˜ ì˜ˆì œì™€ ì‹¤ìš©ì ì¸ êµ¬í˜„ ê°€ì´ë“œë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
date: 2025-09-02
last_modified_at: 2025-09-02
categories:
  - tutorials
tags:
  - AI-Scientist
  - Docker
  - Ollama
  - LM-Studio
  - ì—°êµ¬-ìë™í™”
  - OrbStack
  - í-ê´€ë¦¬
  - ê³¼í•™-ì—°êµ¬
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/ko/tutorials/ai-scientist-docker-ollama-automated-research-pipeline/"
lang: ko
permalink: /ko/tutorials/ai-scientist-docker-ollama-automated-research-pipeline/
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 18ë¶„

## ì„œë¡ 

ì ë“¤ì–´ ìˆëŠ” ë™ì•ˆì—ë„ ëŠì„ì—†ì´ ë…¼ë¬¸ì„ ìƒì„±í•˜ê³ , ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ë©°, ê³¼í•™ì  ë°œê²¬ì˜ ê²½ê³„ë¥¼ ë„“í˜€ê°€ëŠ” ì§€ì¹  ì¤„ ëª¨ë¥´ëŠ” AI ì—°êµ¬ìê°€ ìˆë‹¤ê³  ìƒìƒí•´ë³´ì„¸ìš”. **SakanaAIì˜ AI Scientist**ì™€ ë¡œì»¬ LLM ì¸í”„ë¼ë¥¼ í™œìš©í•˜ë©´, ì´ëŠ” ë” ì´ìƒ ê³µìƒê³¼í•™ì´ ì•„ë‹Œ ì˜¤ëŠ˜ ë‹¹ì¥ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” í˜„ì‹¤ì…ë‹ˆë‹¤.

ì´ ì¢…í•© ê°€ì´ë“œì—ì„œëŠ” ë‹¤ìŒì„ í™œìš©í•œ ìë™í™”ëœ ì—°êµ¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤:
- **SakanaAIì˜ AI Scientist**: ì™„ì „ ìë™í™”ëœ ê³¼í•™ì  ë°œê²¬ì„ ìœ„í•œ ì„¸ê³„ ìµœì´ˆì˜ ì‹œìŠ¤í…œ
- **OrbStack Docker**: ì›í™œí•œ ë°°í¬ë¥¼ ìœ„í•œ ê²½ëŸ‰í™”ëœ ì»¨í…Œì´ë„ˆí™”
- **Ollama & LM Studio**: ë¹„ìš© íš¨ìœ¨ì ì´ê³  í”„ë¼ì´ë¹—í•œ ì—°êµ¬ë¥¼ ìœ„í•œ ë¡œì»¬ LLM ì¶”ë¡ 
- **í ê´€ë¦¬**: ì§€ëŠ¥ì ì¸ ì‘ì—… ìŠ¤ì¼€ì¤„ë§ì„ í†µí•œ ì§€ì†ì  ìš´ì˜

ì´ íŠœí† ë¦¬ì–¼ì„ ë§ˆì¹˜ë©´, ì§€ì†ì ì¸ ì¸ê°„ì˜ ê°œì… ì—†ì´ë„ ì—¬ëŸ¬ ë„ë©”ì¸ì—ì„œ ê³¼í•™ ë…¼ë¬¸ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ê²¬ê³ í•˜ê³  ìë¦½ì ì¸ ì—°êµ¬ í™˜ê²½ì„ ê°–ì¶”ê²Œ ë©ë‹ˆë‹¤.

## AI Scientist ì´í•´í•˜ê¸°

### AI Scientistì˜ í˜ì‹ ì ì¸ íŠ¹ì§•

[SakanaAIì˜ AI Scientist](https://github.com/SakanaAI/AI-Scientist)ëŠ” ìë™í™”ëœ ì—°êµ¬ì˜ íŒ¨ëŸ¬ë‹¤ì„ ë³€í™”ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì—°êµ¬ìë¥¼ ë•ëŠ” ê¸°ì¡´ì˜ AI ë„êµ¬ì™€ ë‹¬ë¦¬, ì´ ì‹œìŠ¤í…œì€ **ì „ì²´ ì—°êµ¬ í”„ë¡œì íŠ¸ë¥¼ ììœ¨ì ìœ¼ë¡œ ìˆ˜í–‰**í•©ë‹ˆë‹¤:

- **ì¢…ë‹¨ê°„ ìë™í™”**: ì•„ì´ë””ì–´ ìƒì„±ë¶€í„° ë…¼ë¬¸ ì‘ì„± ë° ë™ë£Œ í‰ê°€ê¹Œì§€
- **ë‹¤ì¤‘ í…œí”Œë¦¿ ì§€ì›**: NanoGPT, 2D Diffusion, Grokking ì—°êµ¬ ë„ë©”ì¸
- **ìë™í™”ëœ ì‹¤í—˜**: ì‹¤í—˜ì„ ì„¤ê³„í•˜ê³ , ì‹¤í–‰í•˜ë©°, ë¶„ì„
- **LaTeX ë…¼ë¬¸ ìƒì„±**: ì¶œíŒ ì¤€ë¹„ê°€ ì™„ë£Œëœ í•™ìˆ  ë…¼ë¬¸ ì œì‘
- **ë™ë£Œ í‰ê°€ ì‹œìŠ¤í…œ**: í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•œ ë‚´ì¥ í‰ê°€ ë©”ì»¤ë‹ˆì¦˜

### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”

```mermaid
graph TD
    A["ğŸ¯ ì—°êµ¬ ì•„ì´ë””ì–´<br/>ìƒì„±"] --> B["ğŸ”¬ ì‹¤í—˜<br/>ì„¤ê³„"]
    B --> C["âš™ï¸ ì½”ë“œ<br/>êµ¬í˜„"]
    C --> D["ğŸ§ª ì‹¤í—˜<br/>ì‹¤í–‰"]
    D --> E["ğŸ“Š ê²°ê³¼<br/>ë¶„ì„"]
    E --> F["ğŸ“ ë…¼ë¬¸<br/>ì‘ì„±"]
    F --> G["ğŸ“‹ ë™ë£Œ<br/>í‰ê°€"]
    G --> H["ğŸ“„ ìµœì¢…<br/>ë…¼ë¬¸"]
    
    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#ffebee
    style F fill:#f1f8e9
    style G fill:#fce4ec
    style H fill:#e0f2f1
```

## ì‚¬ì „ ìš”êµ¬ì‚¬í•­ ë° í™˜ê²½ ì„¤ì •

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

```bash
# ìµœì†Œ í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- ë©”ëª¨ë¦¬: 16GB (ëŒ€í˜• ëª¨ë¸ì˜ ê²½ìš° 32GB ê¶Œì¥)
- ì €ì¥ ê³µê°„: 50GB ì—¬ìœ  ê³µê°„
- CPU: 8+ ì½”ì–´ (Apple Silicon ë˜ëŠ” x86_64)
- GPU: ì„ íƒì‚¬í•­ì´ì§€ë§Œ ê¶Œì¥ (NVIDIA RTX 3080+ ë˜ëŠ” Apple M-series)

# ì†Œí”„íŠ¸ì›¨ì–´ ì˜ì¡´ì„±
- macOS 13+ ë˜ëŠ” Linux Ubuntu 20.04+
- OrbStack ë˜ëŠ” Docker Desktop
- Python 3.8+
- Git
```

### OrbStack ì„¤ì¹˜

OrbStackì€ íŠ¹íˆ macOSì—ì„œ Docker Desktopë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

```bash
# Homebrewë¥¼ í†µí•œ OrbStack ì„¤ì¹˜
brew install orbstack

# OrbStack ì„œë¹„ìŠ¤ ì‹œì‘
orbstack start

# ì„¤ì¹˜ í™•ì¸
orbstack --version
```

### Ollama ì„¤ì •

OllamaëŠ” ìš°ìˆ˜í•œ ë¡œì»¬ LLM ì¶”ë¡  ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤:

```bash
# Ollama ì„¤ì¹˜
curl -fsSL https://ollama.ai/install.sh | sh

# Ollama ì„œë¹„ìŠ¤ ì‹œì‘
ollama serve

# ì—°êµ¬ìš© ê¶Œì¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull llama2:70b          # ëŒ€í˜• ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸
ollama pull codellama:34b       # ì½”ë“œ ìƒì„±
ollama pull mistral:7b          # ë¹ ë¥¸ ì¶”ë¡ 
ollama pull deepseek-coder:33b  # ê³ ê¸‰ ì½”ë”©

# ì„¤ì¹˜ í™•ì¸
ollama list
```

### LM Studio ëŒ€ì•ˆ ì„¤ì •

GUI ê¸°ë°˜ ëª¨ë¸ ê´€ë¦¬ë¥¼ ìœ„í•´:

```bash
# https://lmstudio.ai/ì—ì„œ LM Studio ë‹¤ìš´ë¡œë“œ
# API ì„œë²„ ì„¤ì¹˜ ë° êµ¬ì„±
# ê¸°ë³¸ API ì—”ë“œí¬ì¸íŠ¸: http://localhost:1234/v1
```

## AI Scientist ì„¤ì¹˜ ë° êµ¬ì„±

### ì €ì¥ì†Œ ë³µì œ ë° ì„¤ì •

```bash
# AI Scientist ì €ì¥ì†Œ ë³µì œ
git clone https://github.com/SakanaAI/AI-Scientist.git
cd AI-Scientist

# ì„¤ì •ì„ ìœ„í•œ ì „ìš© ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/ai-research-lab
cd ~/ai-research-lab

# AI Scientist íŒŒì¼ ë³µì‚¬
cp -r /path/to/AI-Scientist/* .
```

### Docker í™˜ê²½ êµ¬ì„±

í¬ê´„ì ì¸ Docker ì„¤ì • ìƒì„±:

```dockerfile
# ë¡œì»¬ LLM ì§€ì›ì„ ìœ„í•œ AI Scientist Dockerfile
FROM python:3.9-slim

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    git \
    wget \
    curl \
    build-essential \
    texlive-full \
    pandoc \
    && rm -rf /var/lib/apt/lists/*

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# ìš”êµ¬ì‚¬í•­ ë³µì‚¬ ë° Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# í–¥ìƒëœ ê¸°ëŠ¥ì„ ìœ„í•œ ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN pip install \
    ollama \
    openai \
    anthropic \
    tiktoken \
    matplotlib \
    seaborn \
    jupyter \
    notebook

# AI Scientist ì½”ë“œ ë³µì‚¬
COPY . .

# í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
RUN mkdir -p /app/results /app/logs /app/queue

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ENV PYTHONPATH=/app
ENV OLLAMA_HOST=host.docker.internal:11434
ENV LM_STUDIO_BASE_URL=http://host.docker.internal:1234/v1

# Jupyter ë° ëª¨ë‹ˆí„°ë§ìš© í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8888 8080

# ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
COPY scripts/startup.sh /startup.sh
RUN chmod +x /startup.sh

CMD ["/startup.sh"]
```

### ì™„ì „í•œ ìŠ¤íƒì„ ìœ„í•œ Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  ai-scientist:
    build: .
    container_name: ai-scientist-main
    volumes:
      - ./results:/app/results
      - ./logs:/app/logs
      - ./queue:/app/queue
      - ./templates:/app/templates
    ports:
      - "8888:8888"  # Jupyter
      - "8080:8080"  # ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
    environment:
      - OLLAMA_HOST=host.docker.internal:11434
      - LM_STUDIO_BASE_URL=http://host.docker.internal:1234/v1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    depends_on:
      - redis
    networks:
      - ai-research-net

  redis:
    image: redis:7-alpine
    container_name: ai-scientist-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - ai-research-net

  queue-manager:
    build: .
    container_name: ai-scientist-queue
    command: python scripts/queue_manager.py
    volumes:
      - ./queue:/app/queue
      - ./logs:/app/logs
    depends_on:
      - redis
      - ai-scientist
    networks:
      - ai-research-net

  monitoring:
    build: .
    container_name: ai-scientist-monitor
    command: python scripts/monitoring_dashboard.py
    ports:
      - "8081:8081"
    volumes:
      - ./logs:/app/logs
      - ./results:/app/results
    networks:
      - ai-research-net

volumes:
  redis_data:

networks:
  ai-research-net:
    driver: bridge
```

## ë¡œì»¬ LLM í†µí•©

### Ollama API í†µí•©

Ollamaìš© ì‚¬ìš©ì ì •ì˜ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±:

```python
# scripts/ollama_client.py
import requests
import json
from typing import Dict, List, Optional
import logging

class OllamaClient:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.logger = logging.getLogger(__name__)
    
    def generate(self, 
                model: str,
                prompt: str,
                temperature: float = 0.7,
                max_tokens: int = 4000,
                **kwargs) -> str:
        """Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens,
                    **kwargs
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=300
            )
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "")
            
        except Exception as e:
            self.logger.error(f"Ollama ìƒì„± ì˜¤ë¥˜: {e}")
            raise
    
    def list_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            response.raise_for_status()
            
            models = response.json().get("models", [])
            return [model["name"] for model in models]
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def chat_completion(self,
                       model: str,
                       messages: List[Dict],
                       temperature: float = 0.7,
                       max_tokens: int = 4000) -> str:
        """OpenAI í˜¸í™˜ ì±„íŒ… ì™„ì„±"""
        try:
            # ë©”ì‹œì§€ë¥¼ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
            prompt = self._messages_to_prompt(messages)
            return self.generate(model, prompt, temperature, max_tokens)
            
        except Exception as e:
            self.logger.error(f"ì±„íŒ… ì™„ì„± ì˜¤ë¥˜: {e}")
            raise
    
    def _messages_to_prompt(self, messages: List[Dict]) -> str:
        """OpenAI ë©”ì‹œì§€ í˜•ì‹ì„ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
        prompt_parts = []
        
        for message in messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            if role == "system":
                prompt_parts.append(f"ì‹œìŠ¤í…œ: {content}")
            elif role == "user":
                prompt_parts.append(f"ì‚¬ìš©ì: {content}")
            elif role == "assistant":
                prompt_parts.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {content}")
        
        prompt_parts.append("ì–´ì‹œìŠ¤í„´íŠ¸:")
        return "\n\n".join(prompt_parts)

# í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    client = OllamaClient()
    print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:", client.list_models())
    
    test_response = client.generate(
        model="llama2:7b",
        prompt="ì–‘ì ì»´í“¨íŒ…ì„ ê°„ë‹¨í•œ ìš©ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    )
    print("í…ŒìŠ¤íŠ¸ ì‘ë‹µ:", test_response[:200] + "...")
```

### LM Studio í†µí•©

```python
# scripts/lm_studio_client.py
import openai
from typing import Dict, List
import logging

class LMStudioClient:
    def __init__(self, base_url: str = "http://localhost:1234/v1"):
        self.client = openai.OpenAI(
            base_url=base_url,
            api_key="lm-studio"  # í•„ìˆ˜ì´ì§€ë§Œ ë¬´ì‹œë¨
        )
        self.logger = logging.getLogger(__name__)
    
    def generate(self, 
                model: str,
                prompt: str,
                temperature: float = 0.7,
                max_tokens: int = 4000,
                **kwargs) -> str:
        """LM Studio APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            messages = [{"role": "user", "content": prompt}]
            
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            self.logger.error(f"LM Studio ìƒì„± ì˜¤ë¥˜: {e}")
            raise
    
    def chat_completion(self,
                       model: str,
                       messages: List[Dict],
                       temperature: float = 0.7,
                       max_tokens: int = 4000) -> str:
        """ì§ì ‘ ì±„íŒ… ì™„ì„±"""
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            self.logger.error(f"ì±„íŒ… ì™„ì„± ì˜¤ë¥˜: {e}")
            raise
    
    def list_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        try:
            models = self.client.models.list()
            return [model.id for model in models.data]
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []

# í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    client = LMStudioClient()
    print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:", client.list_models())
    
    test_response = client.generate(
        model="local-model",
        prompt="ê¸°ê³„ í•™ìŠµì„ ê°„ë‹¨í•œ ìš©ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    )
    print("í…ŒìŠ¤íŠ¸ ì‘ë‹µ:", test_response[:200] + "...")
```

## í ê´€ë¦¬ ì‹œìŠ¤í…œ

### Redis ê¸°ë°˜ í êµ¬í˜„

```python
# scripts/queue_manager.py
import redis
import json
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum

class TaskStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class ResearchTask:
    id: str
    template: str
    model: str
    num_ideas: int
    priority: int = 1
    created_at: datetime = None
    started_at: datetime = None
    completed_at: datetime = None
    status: TaskStatus = TaskStatus.PENDING
    progress: int = 0
    error_message: str = ""
    results_path: str = ""
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

class QueueManager:
    def __init__(self, redis_host: str = "localhost", redis_port: int = 6379):
        self.redis_client = redis.Redis(
            host=redis_host, 
            port=redis_port, 
            decode_responses=True
        )
        self.logger = logging.getLogger(__name__)
        
        # í í‚¤
        self.pending_queue = "ai_scientist:pending"
        self.running_queue = "ai_scientist:running"
        self.completed_queue = "ai_scientist:completed"
        self.failed_queue = "ai_scientist:failed"
        self.task_data = "ai_scientist:tasks"
    
    def add_task(self, task: ResearchTask) -> str:
        """ìƒˆë¡œìš´ ì—°êµ¬ ì‘ì—…ì„ íì— ì¶”ê°€"""
        try:
            # ì‘ì—… ë°ì´í„° ì €ì¥
            task_json = json.dumps(asdict(task), default=str)
            self.redis_client.hset(self.task_data, task.id, task_json)
            
            # ìš°ì„ ìˆœìœ„ë¡œ ëŒ€ê¸° íì— ì¶”ê°€
            self.redis_client.zadd(
                self.pending_queue, 
                {task.id: task.priority}
            )
            
            self.logger.info(f"ì‘ì—… {task.id} íì— ì¶”ê°€ë¨")
            return task.id
            
        except Exception as e:
            self.logger.error(f"ì‘ì—… ì¶”ê°€ ì˜¤ë¥˜: {e}")
            raise
    
    def get_next_task(self) -> Optional[ResearchTask]:
        """ë‹¤ìŒ ìµœê³  ìš°ì„ ìˆœìœ„ ì‘ì—… ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ìµœê³  ìš°ì„ ìˆœìœ„ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
            task_ids = self.redis_client.zrevrange(
                self.pending_queue, 0, 0
            )
            
            if not task_ids:
                return None
            
            task_id = task_ids[0]
            
            # ì‹¤í–‰ íë¡œ ì´ë™
            self.redis_client.zrem(self.pending_queue, task_id)
            self.redis_client.sadd(self.running_queue, task_id)
            
            # ì‘ì—… ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            task_data = self.redis_client.hget(self.task_data, task_id)
            if not task_data:
                return None
            
            task_dict = json.loads(task_data)
            task = ResearchTask(**task_dict)
            task.status = TaskStatus.RUNNING
            task.started_at = datetime.now()
            
            # ì‘ì—… ì—…ë°ì´íŠ¸
            self.update_task(task)
            
            return task
            
        except Exception as e:
            self.logger.error(f"ë‹¤ìŒ ì‘ì—… ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return None
    
    def update_task(self, task: ResearchTask):
        """ì‘ì—… ìƒíƒœ ë° ë°ì´í„° ì—…ë°ì´íŠ¸"""
        try:
            task_json = json.dumps(asdict(task), default=str)
            self.redis_client.hset(self.task_data, task.id, task_json)
            
        except Exception as e:
            self.logger.error(f"ì‘ì—… ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def complete_task(self, task_id: str, results_path: str = ""):
        """ì‘ì—…ì„ ì™„ë£Œë¡œ í‘œì‹œ"""
        try:
            task = self.get_task(task_id)
            if not task:
                return
            
            # ì™„ë£Œ íë¡œ ì´ë™
            self.redis_client.srem(self.running_queue, task_id)
            self.redis_client.sadd(self.completed_queue, task_id)
            
            # ì‘ì—… ì—…ë°ì´íŠ¸
            task.status = TaskStatus.COMPLETED
            task.completed_at = datetime.now()
            task.progress = 100
            task.results_path = results_path
            
            self.update_task(task)
            self.logger.info(f"ì‘ì—… {task_id} ì™„ë£Œë¨")
            
        except Exception as e:
            self.logger.error(f"ì‘ì—… ì™„ë£Œ ì˜¤ë¥˜: {e}")
    
    def fail_task(self, task_id: str, error_message: str = ""):
        """ì‘ì—…ì„ ì‹¤íŒ¨ë¡œ í‘œì‹œ"""
        try:
            task = self.get_task(task_id)
            if not task:
                return
            
            # ì‹¤íŒ¨ íë¡œ ì´ë™
            self.redis_client.srem(self.running_queue, task_id)
            self.redis_client.sadd(self.failed_queue, task_id)
            
            # ì‘ì—… ì—…ë°ì´íŠ¸
            task.status = TaskStatus.FAILED
            task.completed_at = datetime.now()
            task.error_message = error_message
            
            self.update_task(task)
            self.logger.error(f"ì‘ì—… {task_id} ì‹¤íŒ¨: {error_message}")
            
        except Exception as e:
            self.logger.error(f"ì‘ì—… ì‹¤íŒ¨ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    def get_task(self, task_id: str) -> Optional[ResearchTask]:
        """IDë¡œ ì‘ì—… ê°€ì ¸ì˜¤ê¸°"""
        try:
            task_data = self.redis_client.hget(self.task_data, task_id)
            if not task_data:
                return None
            
            task_dict = json.loads(task_data)
            return ResearchTask(**task_dict)
            
        except Exception as e:
            self.logger.error(f"ì‘ì—… ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return None
    
    def get_queue_stats(self) -> Dict[str, int]:
        """í í†µê³„ ê°€ì ¸ì˜¤ê¸°"""
        try:
            return {
                "pending": self.redis_client.zcard(self.pending_queue),
                "running": self.redis_client.scard(self.running_queue),
                "completed": self.redis_client.scard(self.completed_queue),
                "failed": self.redis_client.scard(self.failed_queue)
            }
            
        except Exception as e:
            self.logger.error(f"í í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {}
    
    def list_tasks(self, status: TaskStatus = None) -> List[ResearchTask]:
        """ìƒíƒœë³„ ì‘ì—… ëª©ë¡"""
        try:
            if status == TaskStatus.PENDING:
                task_ids = self.redis_client.zrevrange(self.pending_queue, 0, -1)
            elif status == TaskStatus.RUNNING:
                task_ids = list(self.redis_client.smembers(self.running_queue))
            elif status == TaskStatus.COMPLETED:
                task_ids = list(self.redis_client.smembers(self.completed_queue))
            elif status == TaskStatus.FAILED:
                task_ids = list(self.redis_client.smembers(self.failed_queue))
            else:
                # ëª¨ë“  ì‘ì—… ê°€ì ¸ì˜¤ê¸°
                task_ids = list(self.redis_client.hkeys(self.task_data))
            
            tasks = []
            for task_id in task_ids:
                task = self.get_task(task_id)
                if task:
                    tasks.append(task)
            
            return tasks
            
        except Exception as e:
            self.logger.error(f"ì‘ì—… ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []

# ì›Œì»¤ í”„ë¡œì„¸ìŠ¤
class ResearchWorker:
    def __init__(self, queue_manager: QueueManager):
        self.queue_manager = queue_manager
        self.logger = logging.getLogger(__name__)
        self.running = False
    
    def start(self):
        """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì‹œì‘"""
        self.running = True
        self.logger.info("ì—°êµ¬ ì›Œì»¤ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        while self.running:
            try:
                task = self.queue_manager.get_next_task()
                
                if task:
                    self.logger.info(f"ì‘ì—… ì²˜ë¦¬ ì¤‘: {task.id}")
                    self.process_task(task)
                else:
                    # ì‚¬ìš© ê°€ëŠ¥í•œ ì‘ì—…ì´ ì—†ìŒ, ëŒ€ê¸°
                    time.sleep(10)
                    
            except KeyboardInterrupt:
                self.logger.info("ì›Œì»¤ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
                break
            except Exception as e:
                self.logger.error(f"ì›Œì»¤ ì˜¤ë¥˜: {e}")
                time.sleep(30)
    
    def process_task(self, task: ResearchTask):
        """ì—°êµ¬ ì‘ì—… ì²˜ë¦¬"""
        try:
            # AI Scientist ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
            import subprocess
            import os
            
            # ëª…ë ¹ ì¤€ë¹„
            cmd = [
                "python", "launch_scientist.py",
                "--model", task.model,
                "--experiment", task.template,
                "--num-ideas", str(task.num_ideas),
                "--out-dir", f"results/{task.id}"
            ]
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            task.progress = 10
            self.queue_manager.update_task(task)
            
            # AI Scientist ì‹¤í–‰
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode == 0:
                # ì„±ê³µ
                results_path = f"results/{task.id}"
                self.queue_manager.complete_task(task.id, results_path)
                self.logger.info(f"ì‘ì—… {task.id}ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
            else:
                # ì‹¤íŒ¨
                error_msg = result.stderr or "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                self.queue_manager.fail_task(task.id, error_msg)
                self.logger.error(f"ì‘ì—… {task.id} ì‹¤íŒ¨: {error_msg}")
                
        except subprocess.TimeoutExpired:
            self.queue_manager.fail_task(task.id, "ì‘ì—… íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            self.queue_manager.fail_task(task.id, str(e))
    
    def stop(self):
        """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì¤‘ì§€"""
        self.running = False
        self.logger.info("ì—°êµ¬ ì›Œì»¤ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤")

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # í ë§¤ë‹ˆì € ì´ˆê¸°í™”
    queue_manager = QueueManager()
    
    # ì›Œì»¤ ìƒì„± ë° ì‹œì‘
    worker = ResearchWorker(queue_manager)
    
    try:
        worker.start()
    except KeyboardInterrupt:
        worker.stop()
```

## ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬

### ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ

```python
# scripts/monitoring_dashboard.py
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import time
from datetime import datetime, timedelta
from queue_manager import QueueManager, TaskStatus

st.set_page_config(
    page_title="AI Scientist ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ§‘â€ğŸ”¬",
    layout="wide"
)

class MonitoringDashboard:
    def __init__(self):
        self.queue_manager = QueueManager()
    
    def render_header(self):
        """ëŒ€ì‹œë³´ë“œ í—¤ë” ë Œë”ë§"""
        st.title("ğŸ§‘â€ğŸ”¬ AI Scientist ì—°êµ¬ ëŒ€ì‹œë³´ë“œ")
        st.markdown("ìë™í™”ëœ ì—°êµ¬ íŒŒì´í”„ë¼ì¸ì˜ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§")
        
        # ìƒˆë¡œ ê³ ì¹¨ ë²„íŠ¼
        if st.button("ğŸ”„ ìƒˆë¡œ ê³ ì¹¨", key="refresh"):
            st.rerun()
    
    def render_queue_stats(self):
        """í í†µê³„ ë Œë”ë§"""
        stats = self.queue_manager.get_queue_stats()
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("â³ ëŒ€ê¸° ì¤‘", stats.get("pending", 0))
        
        with col2:
            st.metric("ğŸ”„ ì‹¤í–‰ ì¤‘", stats.get("running", 0))
        
        with col3:
            st.metric("âœ… ì™„ë£Œ", stats.get("completed", 0))
        
        with col4:
            st.metric("âŒ ì‹¤íŒ¨", stats.get("failed", 0))
    
    def render_task_timeline(self):
        """ì‘ì—… íƒ€ì„ë¼ì¸ ì°¨íŠ¸ ë Œë”ë§"""
        st.subheader("ğŸ“Š ì‘ì—… íƒ€ì„ë¼ì¸")
        
        # ëª¨ë“  ì‘ì—… ê°€ì ¸ì˜¤ê¸°
        all_tasks = self.queue_manager.list_tasks()
        
        if not all_tasks:
            st.info("ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # íƒ€ì„ë¼ì¸ìš© ë°ì´í„° ì¤€ë¹„
        timeline_data = []
        for task in all_tasks:
            timeline_data.append({
                "ì‘ì—… ID": task.id[:8],
                "í…œí”Œë¦¿": task.template,
                "ëª¨ë¸": task.model,
                "ìƒíƒœ": task.status.value,
                "ìƒì„±ë¨": task.created_at,
                "ì‹œì‘ë¨": task.started_at,
                "ì™„ë£Œë¨": task.completed_at,
                "ì†Œìš” ì‹œê°„": self._calculate_duration(task)
            })
        
        df = pd.DataFrame(timeline_data)
        
        # ìƒíƒœ ë¶„í¬ íŒŒì´ ì°¨íŠ¸
        col1, col2 = st.columns(2)
        
        with col1:
            status_counts = df["ìƒíƒœ"].value_counts()
            fig_pie = px.pie(
                values=status_counts.values,
                names=status_counts.index,
                title="ì‘ì—… ìƒíƒœ ë¶„í¬"
            )
            st.plotly_chart(fig_pie, use_container_width=True)
        
        with col2:
            # ì†Œìš” ì‹œê°„ íˆìŠ¤í† ê·¸ë¨
            completed_tasks = df[df["ìƒíƒœ"] == "completed"]
            if not completed_tasks.empty:
                fig_hist = px.histogram(
                    completed_tasks,
                    x="ì†Œìš” ì‹œê°„",
                    title="ì‘ì—… ì†Œìš” ì‹œê°„ ë¶„í¬ (ë¶„)",
                    nbins=20
                )
                st.plotly_chart(fig_hist, use_container_width=True)
            else:
                st.info("ì•„ì§ ì™„ë£Œëœ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤")
    
    def render_task_list(self):
        """ìƒì„¸ ì‘ì—… ëª©ë¡ ë Œë”ë§"""
        st.subheader("ğŸ“‹ ì‘ì—… ì„¸ë¶€ ì •ë³´")
        
        # ìƒíƒœ í•„í„°
        status_filter = st.selectbox(
            "ìƒíƒœë³„ í•„í„°",
            ["ì „ì²´", "pending", "running", "completed", "failed"]
        )
        
        # í•„í„°ë§ëœ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
        if status_filter == "ì „ì²´":
            tasks = self.queue_manager.list_tasks()
        else:
            tasks = self.queue_manager.list_tasks(TaskStatus(status_filter))
        
        if not tasks:
            st.info(f"{status_filter} ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # ì‘ì—… í…Œì´ë¸” ìƒì„±
        task_data = []
        for task in tasks:
            task_data.append({
                "ID": task.id[:8] + "...",
                "í…œí”Œë¦¿": task.template,
                "ëª¨ë¸": task.model,
                "ì•„ì´ë””ì–´": task.num_ideas,
                "ìƒíƒœ": task.status.value.title(),
                "ì§„í–‰ë¥ ": f"{task.progress}%",
                "ìƒì„±ë¨": task.created_at.strftime("%Y-%m-%d %H:%M") if task.created_at else "N/A",
                "ì†Œìš” ì‹œê°„": self._calculate_duration(task),
                "ì˜¤ë¥˜": task.error_message[:50] + "..." if len(task.error_message) > 50 else task.error_message
            })
        
        df = pd.DataFrame(task_data)
        st.dataframe(df, use_container_width=True)
    
    def render_resource_usage(self):
        """ìì› ì‚¬ìš©ëŸ‰ ë©”íŠ¸ë¦­ ë Œë”ë§"""
        st.subheader("ğŸ’» ìì› ì‚¬ìš©ëŸ‰")
        
        # ì‹¤ì œ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ì— ì—°ê²°í•´ì•¼ í•¨
        # ë°ëª¨ ëª©ì ìœ¼ë¡œ í”Œë ˆì´ìŠ¤í™€ë” ë°ì´í„° í‘œì‹œ
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # CPU ì‚¬ìš©ëŸ‰ ì‹œë®¬ë ˆì´ì…˜
            cpu_usage = 65  # ì‹¤ì œ ëª¨ë‹ˆí„°ë§ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
            fig_cpu = go.Figure(go.Indicator(
                mode="gauge+number",
                value=cpu_usage,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "CPU ì‚¬ìš©ëŸ‰ %"},
                gauge={'axis': {'range': [None, 100]},
                       'bar': {'color': "darkblue"},
                       'steps': [
                           {'range': [0, 50], 'color': "lightgray"},
                           {'range': [50, 80], 'color': "yellow"},
                           {'range': [80, 100], 'color': "red"}
                       ]}
            ))
            fig_cpu.update_layout(height=250)
            st.plotly_chart(fig_cpu, use_container_width=True)
        
        with col2:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹œë®¬ë ˆì´ì…˜
            mem_usage = 78
            fig_mem = go.Figure(go.Indicator(
                mode="gauge+number",
                value=mem_usage,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ %"},
                gauge={'axis': {'range': [None, 100]},
                       'bar': {'color': "darkgreen"},
                       'steps': [
                           {'range': [0, 50], 'color': "lightgray"},
                           {'range': [50, 80], 'color': "yellow"},
                           {'range': [80, 100], 'color': "red"}
                       ]}
            ))
            fig_mem.update_layout(height=250)
            st.plotly_chart(fig_mem, use_container_width=True)
        
        with col3:
            # GPU ì‚¬ìš©ëŸ‰ ì‹œë®¬ë ˆì´ì…˜ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            gpu_usage = 45
            fig_gpu = go.Figure(go.Indicator(
                mode="gauge+number",
                value=gpu_usage,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "GPU ì‚¬ìš©ëŸ‰ %"},
                gauge={'axis': {'range': [None, 100]},
                       'bar': {'color': "darkred"},
                       'steps': [
                           {'range': [0, 50], 'color': "lightgray"},
                           {'range': [50, 80], 'color': "yellow"},
                           {'range': [80, 100], 'color': "red"}
                       ]}
            ))
            fig_gpu.update_layout(height=250)
            st.plotly_chart(fig_gpu, use_container_width=True)
    
    def render_logs(self):
        """ìµœê·¼ ë¡œê·¸ ë Œë”ë§"""
        st.subheader("ğŸ“œ ìµœê·¼ ë¡œê·¸")
        
        # ì‹¤ì œ ë¡œê·¸ íŒŒì¼ì—ì„œ ì½ì–´ì™€ì•¼ í•¨
        # ë°ëª¨ ëª©ì ìœ¼ë¡œ í”Œë ˆì´ìŠ¤í™€ë” í‘œì‹œ
        log_entries = [
            "2025-09-02 14:30:15 - INFO - ì‘ì—… 12345678 ì²˜ë¦¬ ì‹œì‘",
            "2025-09-02 14:28:42 - INFO - Ollama ëª¨ë¸ llama2:70b ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë¨",
            "2025-09-02 14:25:10 - INFO - ìƒˆ ì‘ì—…ì´ íì— ì¶”ê°€ë¨: nanoGPT_lite",
            "2025-09-02 14:22:33 - INFO - ì‘ì—… 87654321 ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë¨",
            "2025-09-02 14:20:15 - ERROR - ì‘ì—… 11111111 ì‹¤íŒ¨: ì—°ê²° íƒ€ì„ì•„ì›ƒ"
        ]
        
        for entry in log_entries:
            level = "INFO" if "INFO" in entry else "ERROR" if "ERROR" in entry else "WARNING"
            if level == "INFO":
                st.info(entry)
            elif level == "ERROR":
                st.error(entry)
            else:
                st.warning(entry)
    
    def _calculate_duration(self, task) -> str:
        """ì‘ì—… ì†Œìš” ì‹œê°„ ê³„ì‚°"""
        if task.completed_at and task.started_at:
            duration = task.completed_at - task.started_at
            return f"{duration.total_seconds() / 60:.1f} ë¶„"
        elif task.started_at:
            duration = datetime.now() - task.started_at
            return f"{duration.total_seconds() / 60:.1f} ë¶„ (ì§„í–‰ ì¤‘)"
        else:
            return "ì‹œì‘ë˜ì§€ ì•ŠìŒ"
    
    def run(self):
        """ëŒ€ì‹œë³´ë“œ ì‹¤í–‰"""
        self.render_header()
        
        # 30ì´ˆë§ˆë‹¤ ìë™ ìƒˆë¡œ ê³ ì¹¨
        if "last_refresh" not in st.session_state:
            st.session_state.last_refresh = time.time()
        
        if time.time() - st.session_state.last_refresh > 30:
            st.session_state.last_refresh = time.time()
            st.rerun()
        
        # ë©”ì¸ ì½˜í…ì¸ 
        self.render_queue_stats()
        st.divider()
        
        self.render_task_timeline()
        st.divider()
        
        self.render_task_list()
        st.divider()
        
        self.render_resource_usage()
        st.divider()
        
        self.render_logs()

# ëŒ€ì‹œë³´ë“œ ì‹¤í–‰
if __name__ == "__main__":
    dashboard = MonitoringDashboard()
    dashboard.run()
```

## ì‹¤ìš©ì  êµ¬í˜„ ì˜ˆì œ

### ì˜ˆì œ 1: 24/7 ë‹¤ì¤‘ í…œí”Œë¦¿ ì—°êµ¬

```bash
#!/bin/bash
# scripts/deploy_ai_scientist_24_7.sh

echo "ğŸš€ AI Scientist 24/7 ì—°êµ¬ íŒŒì´í”„ë¼ì¸ ë°°í¬ ì¤‘"

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export OLLAMA_HOST="localhost:11434"
export LM_STUDIO_BASE_URL="http://localhost:1234/v1"

# OrbStack ì‹œì‘
echo "OrbStack ì‹œì‘ ì¤‘..."
orbstack start

# Ollama ì‹œì‘
echo "Ollama ì‹œì‘ ì¤‘..."
ollama serve &
OLLAMA_PID=$!

# Ollamaê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
echo "Ollama ì¤€ë¹„ ëŒ€ê¸° ì¤‘..."
sleep 10

# í•„ìš”í•œ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
echo "ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ í™•ì¸ ì¤‘..."
ollama pull llama2:70b &
ollama pull codellama:34b &
ollama pull mistral:7b &
ollama pull deepseek-coder:33b &
wait

# Docker Compose ìŠ¤íƒ ì‹œì‘
echo "AI Scientist ìŠ¤íƒ ì‹œì‘ ì¤‘..."
cd ~/ai-research-lab
docker-compose up -d

# ì„œë¹„ìŠ¤ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
echo "ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ëŒ€ê¸° ì¤‘..."
sleep 30

# ì´ˆê¸° ì—°êµ¬ ë°°ì¹˜ ì œì¶œ
echo "ì´ˆê¸° ì—°êµ¬ ì‘ì—… ì œì¶œ ì¤‘..."
python scripts/task_submitter.py --batch

# ì§€ëŠ¥í˜• ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
echo "ì§€ëŠ¥í˜• ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì¤‘..."
python scripts/intelligent_scheduler.py &
SCHEDULER_PID=$!

# ìì› ëª¨ë‹ˆí„°ë§ ì‹œì‘
echo "ìì› ëª¨ë‹ˆí„°ë§ ì‹œì‘ ì¤‘..."
python scripts/resource_monitor.py &
MONITOR_PID=$!

echo "âœ… AI Scientist 24/7 íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ë°°í¬ë˜ì—ˆìŠµë‹ˆë‹¤!"
echo ""
echo "ğŸŒ ì ‘ê·¼ í¬ì¸íŠ¸:"
echo "  - ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ: http://localhost:8081"
echo "  - Jupyter ë…¸íŠ¸ë¶: http://localhost:8888"
echo "  - í í†µê³„: ë¡œê·¸ ë˜ëŠ” ëŒ€ì‹œë³´ë“œ ì°¸ì¡°"
echo ""
echo "ğŸ“Š ëª¨ë‹ˆí„°ë§ ë°©ë²•:"
echo "  docker-compose logs -f"
echo "  tail -f logs/scheduler.log"
echo "  tail -f logs/resource_monitor.log"
echo ""
echo "ğŸ›‘ ì¤‘ì§€ ë°©ë²•:"
echo "  docker-compose down"
echo "  kill $OLLAMA_PID $SCHEDULER_PID $MONITOR_PID"

# ì •ë¦¬ë¥¼ ìœ„í•œ PID ì €ì¥
echo "$OLLAMA_PID $SCHEDULER_PID $MONITOR_PID" > .ai_scientist_pids

echo "íŒŒì´í”„ë¼ì¸ì´ 24/7 ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì¤‘ì§€í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”."

# ì¸í„°ëŸ½íŠ¸ ëŒ€ê¸°
trap 'echo "AI Scientist íŒŒì´í”„ë¼ì¸ ì¤‘ì§€ ì¤‘..."; docker-compose down; kill $OLLAMA_PID $SCHEDULER_PID $MONITOR_PID; exit' INT
while true; do sleep 1; done
```

### ì˜ˆì œ 2: ì£¼ë§ ì—°êµ¬ ë§ˆë¼í†¤

```python
# scripts/weekend_research_marathon.py
"""
ì£¼ë§ ì—°êµ¬ ë§ˆë¼í†¤
ì£¼ë§ ë™ì•ˆ í¬ê´„ì ì¸ ì—°êµ¬ ì‹¤í—˜ì„ ìë™ìœ¼ë¡œ ì‹¤í–‰
"""

import schedule
import time
import logging
from datetime import datetime, timedelta
from task_submitter import TaskSubmitter
from queue_manager import QueueManager

class WeekendMarathon:
    def __init__(self):
        self.submitter = TaskSubmitter()
        self.queue_manager = QueueManager()
        self.logger = logging.getLogger(__name__)
        
        # ë§ˆë¼í†¤ êµ¬ì„±
        self.marathon_templates = [
            "nanoGPT_lite",
            "2d_diffusion", 
            "grokking"
        ]
        
        self.marathon_models = [
            "llama2:70b",      # ëŒ€í˜• ì»¨í…ìŠ¤íŠ¸
            "codellama:34b",   # ì½”ë“œ ì „ë¬¸ê°€
            "mistral:7b",      # ë¹ ë¥¸ ì¶”ë¡ 
            "deepseek-coder:33b"  # ê³ ê¸‰ ì½”ë”©
        ]
        
        # ì—°êµ¬ ê°•ë„ ë ˆë²¨
        self.research_intensity = {
            "light": {"ideas_per_task": 2, "parallel_tasks": 2},
            "medium": {"ideas_per_task": 3, "parallel_tasks": 3},
            "heavy": {"ideas_per_task": 5, "parallel_tasks": 4},
            "extreme": {"ideas_per_task": 8, "parallel_tasks": 6}
        }
    
    def friday_evening_prep(self):
        """ê¸ˆìš”ì¼ ì €ë… ì¤€ë¹„"""
        self.logger.info("ğŸ¯ ê¸ˆìš”ì¼ ì €ë…: ì£¼ë§ ë§ˆë¼í†¤ ì¤€ë¹„")
        
        # ì´ˆê¸° ë¬´ê±°ìš´ ë°°ì¹˜ ì œì¶œ
        task_ids = self.submitter.submit_research_batch(
            templates=self.marathon_templates,
            models=self.marathon_models[:2],  # 2ê°œ ëª¨ë¸ë¡œ ì‹œì‘
            ideas_per_task=5,
            priority=3
        )
        
        self.logger.info(f"ğŸ“ ê¸ˆìš”ì¼ ì¤€ë¹„ ì‘ì—… {len(task_ids)}ê°œ ì œì¶œë¨")
    
    def saturday_morning_boost(self):
        """í† ìš”ì¼ ì•„ì¹¨ ì—°êµ¬ ë¶€ìŠ¤íŠ¸"""
        self.logger.info("ğŸŒ… í† ìš”ì¼ ì•„ì¹¨: ì—°êµ¬ íŒŒì´í”„ë¼ì¸ ë¶€ìŠ¤íŒ…")
        
        # ë¬´ê±°ìš´ ê°•ë„ ì—°êµ¬
        intensity = self.research_intensity["heavy"]
        
        task_ids = self.submitter.submit_research_batch(
            templates=self.marathon_templates,
            models=self.marathon_models,  # ëª¨ë“  ëª¨ë¸
            ideas_per_task=intensity["ideas_per_task"],
            priority=3
        )
        
        self.logger.info(f"ğŸš€ í† ìš”ì¼ ë¶€ìŠ¤íŠ¸: {len(task_ids)}ê°œ ì‘ì—… ì œì¶œë¨")
    
    def saturday_evening_deep_dive(self):
        """í† ìš”ì¼ ì €ë… ì‹¬í™” ì—°êµ¬"""
        self.logger.info("ğŸŒ™ í† ìš”ì¼ ì €ë…: ì‹¬í™” ì—°êµ¬ ë‹¤ì´ë¹™")
        
        # ê°•ë ¥í•œ ëª¨ë¸ë¡œ ê°€ì¥ ë³µì¡í•œ í…œí”Œë¦¿ì— ì§‘ì¤‘
        task_ids = self.submitter.submit_research_batch(
            templates=["grokking"],  # ê°€ì¥ ë³µì¡í•¨
            models=["llama2:70b", "deepseek-coder:33b"],
            ideas_per_task=8,  # ë§ì€ ì•„ì´ë””ì–´
            priority=4  # ìµœê³  ìš°ì„ ìˆœìœ„
        )
        
        self.logger.info(f"ğŸ§  ì‹¬í™” ë‹¤ì´ë¹™: {len(task_ids)}ê°œ ë³µì¡í•œ ì‘ì—…")
    
    def sunday_comprehensive_sweep(self):
        """ì¼ìš”ì¼ í¬ê´„ì  ì—°êµ¬ ìŠ¤ìœ•"""
        self.logger.info("ğŸ”„ ì¼ìš”ì¼: í¬ê´„ì  ì—°êµ¬ ìŠ¤ìœ•")
        
        # ëª¨ë“  í…œí”Œë¦¿ì—ì„œ ê·¹í•œ ê°•ë„
        intensity = self.research_intensity["extreme"]
        
        task_ids = self.submitter.submit_research_batch(
            templates=self.marathon_templates,
            models=self.marathon_models,
            ideas_per_task=intensity["ideas_per_task"],
            priority=3
        )
        
        self.logger.info(f"ğŸŒŠ ì¼ìš”ì¼ ìŠ¤ìœ•: {len(task_ids)}ê°œ ì‘ì—…")
    
    def sunday_evening_wind_down(self):
        """ì¼ìš”ì¼ ì €ë… ë§ˆë¬´ë¦¬"""
        self.logger.info("ğŸŒ… ì¼ìš”ì¼ ì €ë…: ë§ˆë¼í†¤ ë§ˆë¬´ë¦¬")
        
        # ì›”ìš”ì¼ ì¤€ë¹„ë¥¼ ìœ„í•œ ê°€ë²¼ìš´ ê°•ë„
        intensity = self.research_intensity["light"]
        
        task_ids = self.submitter.submit_research_batch(
            templates=self.marathon_templates[:1],  # í•œ í…œí”Œë¦¿ë§Œ
            models=self.marathon_models[:2],        # ë¹ ë¥¸ ëª¨ë¸ë“¤
            ideas_per_task=intensity["ideas_per_task"],
            priority=2
        )
        
        self.logger.info(f"ğŸŒ™ ë§ˆë¬´ë¦¬: {len(task_ids)}ê°œ ìµœì¢… ì‘ì—…")
        
        # ì£¼ë§ ë³´ê³ ì„œ ìƒì„±
        self.generate_weekend_report()
    
    def generate_weekend_report(self):
        """í¬ê´„ì ì¸ ì£¼ë§ ì—°êµ¬ ë³´ê³ ì„œ ìƒì„±"""
        try:
            # ì£¼ë§ ì‘ì—… (ê¸ˆìš”ì¼ ì˜¤í›„ 6ì‹œ ~ ì¼ìš”ì¼ ì˜¤í›„ 11ì‹œ)
            friday_start = datetime.now().replace(
                hour=18, minute=0, second=0, microsecond=0
            ) - timedelta(days=2)  # ì§€ë‚œ ê¸ˆìš”ì¼
            
            sunday_end = datetime.now().replace(
                hour=23, minute=0, second=0, microsecond=0
            )
            
            # ì‘ì—… í†µê³„ ê°€ì ¸ì˜¤ê¸°
            all_tasks = self.queue_manager.list_tasks()
            weekend_tasks = [
                task for task in all_tasks
                if task.created_at and friday_start <= task.created_at <= sunday_end
            ]
            
            # ë³´ê³ ì„œ ìƒì„±
            report = {
                "ì£¼ë§_ê¸°ê°„": f"{friday_start.strftime('%Y-%m-%d %H:%M')} ~ {sunday_end.strftime('%Y-%m-%d %H:%M')}",
                "ì´_ì‘ì—…": len(weekend_tasks),
                "ì™„ë£Œëœ_ì‘ì—…": len([t for t in weekend_tasks if t.status.value == "completed"]),
                "ì‹¤íŒ¨í•œ_ì‘ì—…": len([t for t in weekend_tasks if t.status.value == "failed"]),
                "ì´_ìƒì„±ëœ_ì•„ì´ë””ì–´": sum(t.num_ideas for t in weekend_tasks),
                "ì‚¬ìš©ëœ_í…œí”Œë¦¿": list(set(t.template for t in weekend_tasks)),
                "ì‚¬ìš©ëœ_ëª¨ë¸": list(set(t.model for t in weekend_tasks))
            }
            
            # ë³´ê³ ì„œ ë¡œê·¸
            self.logger.info("ğŸ“Š ì£¼ë§ ì—°êµ¬ ë§ˆë¼í†¤ ë³´ê³ ì„œ:")
            for key, value in report.items():
                self.logger.info(f"  {key}: {value}")
            
            # íŒŒì¼ì— ë³´ê³ ì„œ ì €ì¥
            with open(f"reports/weekend_marathon_{datetime.now().strftime('%Y%m%d')}.json", "w") as f:
                import json
                json.dump(report, f, indent=2, default=str, ensure_ascii=False)
                
        except Exception as e:
            self.logger.error(f"ì£¼ë§ ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
    
    def setup_marathon_schedule(self):
        """ì£¼ë§ ë§ˆë¼í†¤ ìŠ¤ì¼€ì¤„ ì„¤ì •"""
        # ê¸ˆìš”ì¼ ì €ë… ì¤€ë¹„
        schedule.every().friday.at("18:00").do(self.friday_evening_prep)
        
        # í† ìš”ì¼ ìŠ¤ì¼€ì¤„
        schedule.every().saturday.at("08:00").do(self.saturday_morning_boost)
        schedule.every().saturday.at("20:00").do(self.saturday_evening_deep_dive)
        
        # ì¼ìš”ì¼ ìŠ¤ì¼€ì¤„
        schedule.every().sunday.at("10:00").do(self.sunday_comprehensive_sweep)
        schedule.every().sunday.at("22:00").do(self.sunday_evening_wind_down)
        
        self.logger.info("ğŸ—“ï¸ ì£¼ë§ ë§ˆë¼í†¤ ìŠ¤ì¼€ì¤„ êµ¬ì„±ë¨")
    
    def run_marathon(self):
        """ì£¼ë§ ë§ˆë¼í†¤ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
        self.setup_marathon_schedule()
        
        self.logger.info("ğŸƒâ€â™‚ï¸ ì£¼ë§ ì—°êµ¬ ë§ˆë¼í†¤ ì‹œì‘ë¨")
        self.logger.info("ìŠ¤ì¼€ì¤„:")
        self.logger.info("  ê¸ˆìš”ì¼ 18:00 - ì¤€ë¹„")
        self.logger.info("  í† ìš”ì¼ 08:00 - ì•„ì¹¨ ë¶€ìŠ¤íŠ¸")
        self.logger.info("  í† ìš”ì¼ 20:00 - ì‹¬í™” ë‹¤ì´ë¹™")
        self.logger.info("  ì¼ìš”ì¼ 10:00 - í¬ê´„ì  ìŠ¤ìœ•")
        self.logger.info("  ì¼ìš”ì¼ 22:00 - ë§ˆë¬´ë¦¬")
        
        while True:
            try:
                schedule.run_pending()
                time.sleep(60)
                
            except KeyboardInterrupt:
                self.logger.info("ì‚¬ìš©ìì— ì˜í•´ ì£¼ë§ ë§ˆë¼í†¤ ì¤‘ì§€ë¨")
                break
            except Exception as e:
                self.logger.error(f"ë§ˆë¼í†¤ ì˜¤ë¥˜: {e}")
                time.sleep(300)

# ë…ë¦½ì ì¸ ì£¼ë§ í‚¥ì˜¤í”„
def manual_weekend_kickoff():
    """ìˆ˜ë™ìœ¼ë¡œ ì£¼ë§ ì—°êµ¬ ë§ˆë¼í†¤ ì‹œì‘"""
    marathon = WeekendMarathon()
    
    print("ğŸ ì£¼ë§ ì—°êµ¬ ë§ˆë¼í†¤ ì§€ê¸ˆ ì‹œì‘!")
    
    # ëª¨ë“  ë§ˆë¼í†¤ ë‹¨ê³„ë¥¼ ì¦‰ì‹œ ì‹¤í–‰
    marathon.friday_evening_prep()
    time.sleep(60)
    
    marathon.saturday_morning_boost()
    time.sleep(60)
    
    marathon.saturday_evening_deep_dive()
    time.sleep(60)
    
    marathon.sunday_comprehensive_sweep()
    
    print("âœ… ì£¼ë§ ë§ˆë¼í†¤ ì‘ì—… ì œì¶œ ì™„ë£Œ!")
    print("ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§: http://localhost:8081")

if __name__ == "__main__":
    import sys
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/weekend_marathon.log'),
            logging.StreamHandler()
        ]
    )
    
    if len(sys.argv) > 1 and sys.argv[1] == "--kickoff":
        manual_weekend_kickoff()
    else:
        marathon = WeekendMarathon()
        try:
            marathon.run_marathon()
        except KeyboardInterrupt:
            print("ì£¼ë§ ë§ˆë¼í†¤ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ë¨")
```

### ì˜ˆì œ 3: ì ì‘í˜• ì—°êµ¬ í

```python
# scripts/adaptive_research_queue.py
"""
ì ì‘í˜• ì—°êµ¬ í
ì‹œìŠ¤í…œ ì„±ëŠ¥, í•˜ë£¨ ì¤‘ ì‹œê°„, ì—°êµ¬ ë„ë©”ì¸ ì„ í˜¸ë„ì— ë”°ë¼
ì—°êµ¬ ì‘ì—…ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ê´€ë¦¬
"""

import time
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple
from queue_manager import QueueManager, ResearchTask, TaskStatus
from task_submitter import TaskSubmitter

class AdaptiveResearchQueue:
    def __init__(self):
        self.queue_manager = QueueManager()
        self.submitter = TaskSubmitter()
        self.logger = logging.getLogger(__name__)
        
        # ì—°êµ¬ ë„ë©”ì¸ êµ¬ì„±
        self.domain_configs = {
            "nanoGPT_lite": {
                "ë³µì¡ë„": "ì¤‘ê°„",
                "ìì›_ì§‘ì•½ë„": "ë†’ìŒ",
                "í‰ê· _ì†Œìš”ì‹œê°„": 120,  # ë¶„
                "ì„±ê³µë¥ ": 0.85,
                "ì„ í˜¸_ëª¨ë¸": ["codellama:34b", "deepseek-coder:33b"]
            },
            "2d_diffusion": {
                "ë³µì¡ë„": "ë†’ìŒ", 
                "ìì›_ì§‘ì•½ë„": "ë§¤ìš°_ë†’ìŒ",
                "í‰ê· _ì†Œìš”ì‹œê°„": 180,
                "ì„±ê³µë¥ ": 0.75,
                "ì„ í˜¸_ëª¨ë¸": ["llama2:70b", "mistral:7b"]
            },
            "grokking": {
                "ë³µì¡ë„": "ë§¤ìš°_ë†’ìŒ",
                "ìì›_ì§‘ì•½ë„": "ì¤‘ê°„",
                "í‰ê· _ì†Œìš”ì‹œê°„": 200,
                "ì„±ê³µë¥ ": 0.70,
                "ì„ í˜¸_ëª¨ë¸": ["llama2:70b", "deepseek-coder:33b"]
            }
        }
        
        # ì„±ëŠ¥ ì„ê³„ê°’
        self.performance_thresholds = {
            "cpu_high": 80,
            "memory_high": 85,
            "cpu_critical": 95,
            "memory_critical": 95
        }
        
        # ì ì‘í˜• ë§¤ê°œë³€ìˆ˜
        self.adaptation_history = []
        self.performance_history = []
    
    def analyze_system_performance(self) -> Dict:
        """í˜„ì¬ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë° ìš©ëŸ‰ ë¶„ì„"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ì„ ê°€ì ¸ì™€ì•¼ í•¨
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ë°ì´í„° ì‚¬ìš©
            import random
            
            cpu_usage = random.uniform(30, 90)
            memory_usage = random.uniform(40, 85)
            
            # ìš©ëŸ‰ ë ˆë²¨ ê²°ì •
            if (cpu_usage > self.performance_thresholds["cpu_critical"] or 
                memory_usage > self.performance_thresholds["memory_critical"]):
                capacity_level = "critical"
                recommended_concurrency = 1
            elif (cpu_usage > self.performance_thresholds["cpu_high"] or 
                  memory_usage > self.performance_thresholds["memory_high"]):
                capacity_level = "high"
                recommended_concurrency = 2
            elif cpu_usage > 50 or memory_usage > 50:
                capacity_level = "medium"
                recommended_concurrency = 3
            else:
                capacity_level = "low"
                recommended_concurrency = 4
            
            analysis = {
                "cpu_usage": cpu_usage,
                "memory_usage": memory_usage,
                "capacity_level": capacity_level,
                "recommended_concurrency": recommended_concurrency,
                "timestamp": datetime.now().isoformat()
            }
            
            # íˆìŠ¤í† ë¦¬ì— ì €ì¥
            self.performance_history.append(analysis)
            
            # ìµœê·¼ 24ì‹œê°„ë§Œ ìœ ì§€
            cutoff = datetime.now() - timedelta(hours=24)
            self.performance_history = [
                p for p in self.performance_history
                if datetime.fromisoformat(p["timestamp"]) > cutoff
            ]
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "cpu_usage": 50,
                "memory_usage": 50,
                "capacity_level": "medium",
                "recommended_concurrency": 2
            }
    
    def get_optimal_research_mix(self, performance: Dict) -> List[Dict]:
        """ì„±ëŠ¥ì— ë”°ë¥¸ ìµœì  ì—°êµ¬ ì‘ì—… ë¯¹ìŠ¤ ê²°ì •"""
        capacity_level = performance["capacity_level"]
        current_hour = datetime.now().hour
        
        # ì‹œê°„ ê¸°ë°˜ ì„ í˜¸ë„
        if 22 <= current_hour or current_hour <= 6:
            # ì•¼ê°„ ì‹œê°„: ë³µì¡í•˜ê³  ì¥ì‹œê°„ ì‹¤í–‰ë˜ëŠ” ì‘ì—… ì„ í˜¸
            time_preference = "complex"
        elif 9 <= current_hour <= 17:
            # ì—…ë¬´ ì‹œê°„: ë¹ ë¥´ê³  ê°€ë²¼ìš´ ì‘ì—… ì„ í˜¸
            time_preference = "light"
        else:
            # ì €ë…: ê· í˜• ì¡íŒ ì ‘ê·¼
            time_preference = "balanced"
        
        # ìš©ëŸ‰ ê¸°ë°˜ ì¡°ì •
        if capacity_level == "critical":
            # ê°€ë²¼ìš´ ì‘ì—…ë§Œ
            return self._get_light_task_mix()
        elif capacity_level == "high":
            # ì¤‘ê°„ ë³µì¡ë„ ì‘ì—…
            return self._get_medium_task_mix(time_preference)
        elif capacity_level == "medium":
            # ê· í˜• ì¡íŒ ë¯¹ìŠ¤
            return self._get_balanced_task_mix(time_preference)
        else:
            # ë‚®ì€ ì‚¬ìš©ëŸ‰: ë¬´ê±°ìš´ ì‘ì—… ì‹¤í–‰ ê°€ëŠ¥
            return self._get_heavy_task_mix(time_preference)
    
    def _get_light_task_mix(self) -> List[Dict]:
        """ê°€ë²¼ìš´ ê³„ì‚° ë¶€í•˜ ì‘ì—… ë¯¹ìŠ¤"""
        return [
            {
                "template": "nanoGPT_lite",
                "model": "mistral:7b",
                "num_ideas": 2,
                "priority": 1
            }
        ]
    
    def _get_medium_task_mix(self, time_preference: str) -> List[Dict]:
        """ì¤‘ê°„ ê³„ì‚° ë¶€í•˜ ì‘ì—… ë¯¹ìŠ¤"""
        if time_preference == "light":
            return [
                {
                    "template": "nanoGPT_lite",
                    "model": "mistral:7b",
                    "num_ideas": 3,
                    "priority": 2
                },
                {
                    "template": "2d_diffusion",
                    "model": "mistral:7b",
                    "num_ideas": 2,
                    "priority": 1
                }
            ]
        else:
            return [
                {
                    "template": "nanoGPT_lite",
                    "model": "codellama:34b",
                    "num_ideas": 3,
                    "priority": 2
                },
                {
                    "template": "grokking",
                    "model": "llama2:70b",
                    "num_ideas": 2,
                    "priority": 2
                }
            ]
    
    def _get_balanced_task_mix(self, time_preference: str) -> List[Dict]:
        """ê· í˜• ì¡íŒ ê³„ì‚° ë¶€í•˜ ì‘ì—… ë¯¹ìŠ¤"""
        if time_preference == "complex":
            return [
                {
                    "template": "grokking",
                    "model": "llama2:70b",
                    "num_ideas": 4,
                    "priority": 3
                },
                {
                    "template": "2d_diffusion",
                    "model": "deepseek-coder:33b",
                    "num_ideas": 3,
                    "priority": 2
                },
                {
                    "template": "nanoGPT_lite",
                    "model": "codellama:34b",
                    "num_ideas": 3,
                    "priority": 2
                }
            ]
        else:
            return [
                {
                    "template": "nanoGPT_lite",
                    "model": "codellama:34b",
                    "num_ideas": 3,
                    "priority": 2
                },
                {
                    "template": "2d_diffusion",
                    "model": "mistral:7b",
                    "num_ideas": 3,
                    "priority": 2
                }
            ]
    
    def _get_heavy_task_mix(self, time_preference: str) -> List[Dict]:
        """ë¬´ê±°ìš´ ê³„ì‚° ë¶€í•˜ ì‘ì—… ë¯¹ìŠ¤"""
        return [
            {
                "template": "grokking",
                "model": "llama2:70b",
                "num_ideas": 6,
                "priority": 3
            },
            {
                "template": "2d_diffusion",
                "model": "deepseek-coder:33b",
                "num_ideas": 5,
                "priority": 3
            },
            {
                "template": "nanoGPT_lite",
                "model": "codellama:34b",
                "num_ideas": 4,
                "priority": 2
            },
            {
                "template": "grokking",
                "model": "deepseek-coder:33b",
                "num_ideas": 4,
                "priority": 2
            }
        ]
    
    def should_add_tasks(self, performance: Dict) -> bool:
        """ìƒˆ ì‘ì—…ì„ ì¶”ê°€í•´ì•¼ í•˜ëŠ”ì§€ ê²°ì •"""
        stats = self.queue_manager.get_queue_stats()
        
        running_tasks = stats.get("running", 0)
        pending_tasks = stats.get("pending", 0)
        
        # ì‹œìŠ¤í…œì´ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ìœ¼ë©´ ì‘ì—… ì¶”ê°€ ì•ˆ í•¨
        if performance["capacity_level"] == "critical":
            return False
        
        # ê¶Œì¥ ë™ì‹œì„±ì— ë”°ë¼ ì‘ì—… ì¶”ê°€
        recommended_concurrency = performance["recommended_concurrency"]
        total_active = running_tasks + pending_tasks
        
        return total_active < recommended_concurrency
    
    def adaptive_task_submission(self):
        """í˜„ì¬ ì¡°ê±´ì— ë”°ë¼ ì ì‘ì ìœ¼ë¡œ ì‘ì—… ì œì¶œ"""
        try:
            # ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„
            performance = self.analyze_system_performance()
            
            self.logger.info(
                f"ì‹œìŠ¤í…œ ë¶„ì„: CPU {performance['cpu_usage']:.1f}%, "
                f"ë©”ëª¨ë¦¬ {performance['memory_usage']:.1f}%, "
                f"ìš©ëŸ‰: {performance['capacity_level']}"
            )
            
            # ì‘ì—…ì„ ì¶”ê°€í•´ì•¼ í•˜ëŠ”ì§€ í™•ì¸
            if not self.should_add_tasks(performance):
                self.logger.info("ì‹œìŠ¤í…œì´ ë°”ì˜ê±°ë‚˜ íê°€ ê°€ë“ ì°¸, ì‘ì—… ì¶”ê°€ ê±´ë„ˆëœ€")
                return
            
            # ìµœì  ì‘ì—… ë¯¹ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            task_mix = self.get_optimal_research_mix(performance)
            
            # ì‘ì—… ì œì¶œ
            submitted_tasks = []
            for task_config in task_mix:
                task_id = self._submit_single_task(task_config)
                if task_id:
                    submitted_tasks.append(task_id)
            
            # ì ì‘ ê¸°ë¡
            adaptation_record = {
                "timestamp": datetime.now().isoformat(),
                "performance": performance,
                "task_mix": task_mix,
                "submitted_tasks": len(submitted_tasks)
            }
            
            self.adaptation_history.append(adaptation_record)
            
            self.logger.info(
                f"ì ì‘í˜• ì œì¶œ: {performance['capacity_level']} ìš©ëŸ‰ ì‹œìŠ¤í…œì„ ìœ„í•œ "
                f"{len(submitted_tasks)}ê°œ ì‘ì—…"
            )
            
        except Exception as e:
            self.logger.error(f"ì ì‘í˜• ì‘ì—… ì œì¶œ ì˜¤ë¥˜: {e}")
    
    def _submit_single_task(self, task_config: Dict) -> str:
        """êµ¬ì„±ì— ë”°ë¼ ë‹¨ì¼ ì‘ì—… ì œì¶œ"""
        try:
            import uuid
            
            task_id = str(uuid.uuid4())
            task = ResearchTask(
                id=task_id,
                template=task_config["template"],
                model=task_config["model"],
                num_ideas=task_config["num_ideas"],
                priority=task_config["priority"]
            )
            
            self.queue_manager.add_task(task)
            return task_id
            
        except Exception as e:
            self.logger.error(f"ì‘ì—… ì œì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def run_adaptive_queue(self, check_interval: int = 300):
        """ì ì‘í˜• í ê´€ë¦¬ ì‹œìŠ¤í…œ ì‹¤í–‰"""
        self.logger.info(f"ğŸ§  ì ì‘í˜• ì—°êµ¬ í ì‹œì‘ ({check_interval}ì´ˆë§ˆë‹¤ í™•ì¸)")
        
        while True:
            try:
                # ì ì‘í˜• ì‘ì—… ì œì¶œ ìˆ˜í–‰
                self.adaptive_task_submission()
                
                # ì£¼ê¸°ì  ë³´ê³ ì„œ ìƒì„±
                if len(self.adaptation_history) % 12 == 0:  # 12ë²ˆì˜ ì ì‘ë§ˆë‹¤
                    report = self.generate_adaptation_report()
                    self.logger.info(f"ğŸ“Š ì ì‘ ë³´ê³ ì„œ: {report}")
                
                # ë‹¤ìŒ í™•ì¸ê¹Œì§€ ëŒ€ê¸°
                time.sleep(check_interval)
                
            except KeyboardInterrupt:
                self.logger.info("ì‚¬ìš©ìì— ì˜í•´ ì ì‘í˜• í ì¤‘ì§€ë¨")
                break
            except Exception as e:
                self.logger.error(f"ì ì‘í˜• í ì˜¤ë¥˜: {e}")
                time.sleep(60)
    
    def generate_adaptation_report(self) -> Dict:
        """ì ì‘ ì„±ëŠ¥ì— ëŒ€í•œ ë³´ê³ ì„œ ìƒì„±"""
        try:
            if not self.adaptation_history:
                return {"error": "ì ì‘ íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤"}
            
            # ì ì‘ í†µê³„ ê³„ì‚°
            recent_adaptations = self.adaptation_history[-24:]  # ìµœê·¼ 24ë²ˆì˜ ì ì‘
            
            capacity_distribution = {}
            task_distribution = {}
            
            for adaptation in recent_adaptations:
                capacity = adaptation["performance"]["capacity_level"]
                capacity_distribution[capacity] = capacity_distribution.get(capacity, 0) + 1
                
                for task in adaptation["task_mix"]:
                    template = task["template"]
                    task_distribution[template] = task_distribution.get(template, 0) + 1
            
            # ì„±ëŠ¥ íŠ¸ë Œë“œ
            avg_cpu = sum(a["performance"]["cpu_usage"] for a in recent_adaptations) / len(recent_adaptations)
            avg_memory = sum(a["performance"]["memory_usage"] for a in recent_adaptations) / len(recent_adaptations)
            
            report = {
                "ë¶„ì„_ê¸°ê°„": f"ìµœê·¼ {len(recent_adaptations)}ë²ˆì˜ ì ì‘",
                "í‰ê· _ì„±ëŠ¥": {
                    "cpu_ì‚¬ìš©ëŸ‰": round(avg_cpu, 1),
                    "ë©”ëª¨ë¦¬_ì‚¬ìš©ëŸ‰": round(avg_memory, 1)
                },
                "ìš©ëŸ‰_ë¶„í¬": capacity_distribution,
                "ì‘ì—…_í…œí”Œë¦¿_ë¶„í¬": task_distribution,
                "ì´_ì œì¶œëœ_ì‘ì—…": sum(a["submitted_tasks"] for a in recent_adaptations),
                "ì ì‘_ë¹ˆë„": len(recent_adaptations)
            }
            
            return report
            
        except Exception as e:
            self.logger.error(f"ì ì‘ ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return {"error": str(e)}

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/adaptive_queue.log'),
            logging.StreamHandler()
        ]
    )
    
    adaptive_queue = AdaptiveResearchQueue()
    
    try:
        adaptive_queue.run_adaptive_queue(check_interval=300)  # 5ë¶„ë§ˆë‹¤ í™•ì¸
    except KeyboardInterrupt:
        print("ì ì‘í˜• ì—°êµ¬ íê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤")
```

## ë¬¸ì œ í•´ê²° ë° ìµœì í™”

### ì¼ë°˜ì ì¸ ë¬¸ì œ ë° í•´ê²°ì±…

#### ë¬¸ì œ 1: Ollama ì—°ê²° ì‹¤íŒ¨

```bash
# Ollama ì—°ê²°ì„± ë””ë²„ê·¸
curl http://localhost:11434/api/tags

# Ollama ë¡œê·¸ í™•ì¸
journalctl -u ollama --follow

# Ollama ì„œë¹„ìŠ¤ ì¬ì‹œì‘
sudo systemctl restart ollama

# ëŒ€ì•ˆ: ìˆ˜ë™ ì¬ì‹œì‘
pkill ollama
ollama serve
```

#### ë¬¸ì œ 2: Docker ë©”ëª¨ë¦¬ ë¬¸ì œ

```yaml
# docker-compose.override.yml
version: '3.8'

services:
  ai-scientist:
    deploy:
      resources:
        limits:
          memory: 16G
          cpus: '8'
        reservations:
          memory: 8G
          cpus: '4'
    environment:
      - MALLOC_ARENA_MAX=2
      - PYTHONHASHSEED=0
```

#### ë¬¸ì œ 3: í ì •ì²´ ì‘ì—…

```python
# scripts/queue_recovery.py
from queue_manager import QueueManager, TaskStatus
from datetime import datetime, timedelta

def recover_stuck_tasks():
    """ë„ˆë¬´ ì˜¤ë˜ ì‹¤í–‰ëœ ì‘ì—… ë³µêµ¬"""
    queue_manager = QueueManager()
    
    # 4ì‹œê°„ ì´ìƒ ì‹¤í–‰ëœ ì‘ì—… ì°¾ê¸°
    cutoff_time = datetime.now() - timedelta(hours=4)
    running_tasks = queue_manager.list_tasks(TaskStatus.RUNNING)
    
    stuck_tasks = [
        task for task in running_tasks
        if task.started_at and task.started_at < cutoff_time
    ]
    
    for task in stuck_tasks:
        print(f"ì •ì²´ëœ ì‘ì—… ë³µêµ¬ ì¤‘: {task.id}")
        queue_manager.fail_task(
            task.id, 
            "ì‘ì—… íƒ€ì„ì•„ì›ƒ - ìë™ ë³µêµ¬ë¨"
        )
    
    print(f"{len(stuck_tasks)}ê°œì˜ ì •ì²´ëœ ì‘ì—…ì„ ë³µêµ¬í–ˆìŠµë‹ˆë‹¤")

if __name__ == "__main__":
    recover_stuck_tasks()
```

### ì„±ëŠ¥ ìµœì í™” íŒ

1. **ëª¨ë¸ ì„ íƒ ì „ëµ**:
   ```python
   # ì´ˆê¸° ì‹¤í—˜ì—ëŠ” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
   quick_models = ["mistral:7b", "llama2:13b"]
   
   # ìµœì¢… ê²€ì¦ì—ëŠ” í° ëª¨ë¸ ì‚¬ìš©
   powerful_models = ["llama2:70b", "deepseek-coder:33b"]
   ```

2. **ìì› ê´€ë¦¬**:
   ```bash
   # ì‹œìŠ¤í…œ ìš©ëŸ‰ì— ë”°ë¼ ë™ì‹œ ì‘ì—… ì œí•œ
   export MAX_CONCURRENT_TASKS=2
   
   # ì„ì‹œ íŒŒì¼ìš© ë¹ ë¥¸ ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©
   export TMPDIR=/tmp/ai-scientist-fast
   mkdir -p $TMPDIR
   ```

3. **ë„¤íŠ¸ì›Œí¬ ìµœì í™”**:
   ```bash
   # ë¹ ë¥¸ í•´ìƒë„ë¥¼ ìœ„í•œ ë¡œì»¬ DNS ì‚¬ìš©
   echo "127.0.0.1 host.docker.internal" >> /etc/hosts
   
   # Docker ë„¤íŠ¸ì›Œí‚¹ ìµœì í™”
   docker network create --driver bridge ai-research-net
   ```

## ê²°ë¡ 

ì´ ì¢…í•© ê°€ì´ë“œëŠ” SakanaAIì˜ AI Scientistë¥¼ ì‚¬ìš©í•˜ì—¬ ì •êµí•˜ê³  ìë™í™”ëœ ì—°êµ¬ íŒŒì´í”„ë¼ì¸ì„ ì„¤ì •í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ê²ƒì„ ì œê³µí•©ë‹ˆë‹¤. ë¡œì»¬ LLM, ì§€ëŠ¥ì  í ê´€ë¦¬, ì ì‘í˜• ìì› í• ë‹¹ì˜ ì¡°í•©ì€ 24ì‹œê°„ ì—°ì†ì ìœ¼ë¡œ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ì‹œìŠ¤í…œì„ ë§Œë“­ë‹ˆë‹¤.

### ë‹¬ì„±ëœ ì£¼ìš” ì´ì 

1. **24/7 ì—°êµ¬ ìš´ì˜**: ì¸ê°„ì˜ ê°œì… ì—†ì´ ì§€ì†ì ì¸ ê³¼í•™ì  ë°œê²¬
2. **ë¹„ìš© íš¨ìœ¨ì„±**: ë¡œì»¬ LLM ì¶”ë¡ ìœ¼ë¡œ ëŒ€ê·œëª¨ ì—°êµ¬ì—ì„œ API ë¹„ìš© ì œê±°
3. **ì ì‘í˜• ì§€ëŠ¥**: ì‹œìŠ¤í…œì´ ì„±ëŠ¥ ì¡°ê±´ì— ìë™ìœ¼ë¡œ ì¡°ì •
4. **í¬ê´„ì  ëª¨ë‹ˆí„°ë§**: ì—°êµ¬ ì§„í–‰ ìƒí™©ê³¼ ì‹œìŠ¤í…œ ìƒíƒœì— ëŒ€í•œ ì‹¤ì‹œê°„ ê°€ì‹œì„±
5. **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: ìƒˆë¡œìš´ ì—°êµ¬ ë„ë©”ì¸ì„ ìˆ˜ìš©í•˜ë„ë¡ ì‰½ê²Œ í™•ì¥

### ë‹¤ìŒ ë‹¨ê³„

1. **í…œí”Œë¦¿ í™•ì¥**: íŠ¹ì • ë„ë©”ì¸ì„ ìœ„í•œ ì‚¬ìš©ì ì •ì˜ ì—°êµ¬ í…œí”Œë¦¿ ìƒì„±
2. **ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •**: ì—°êµ¬ ë°ì´í„°ì—ì„œ ë¡œì»¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ë” ë‚˜ì€ ê²°ê³¼ ì–»ê¸°
3. **í†µí•© ê°•í™”**: ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ë° ì—°êµ¬ APIì™€ ì—°ê²°
4. **í˜‘ì—… ê¸°ëŠ¥**: ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì› ë° ì—°êµ¬ íŒ€ í˜‘ì—… ì¶”ê°€
5. **ì¶œíŒ íŒŒì´í”„ë¼ì¸**: ë…¼ë¬¸ ì œì¶œ ë° ê²€í†  í”„ë¡œì„¸ìŠ¤ ìë™í™”

ìë™í™”ëœ ê³¼í•™ ì—°êµ¬ì˜ ë¯¸ë˜ê°€ ì—¬ê¸°ì— ìˆìœ¼ë©°, ì´ ì„¤ì •ìœ¼ë¡œ ì—¬ëŸ¬ë¶„ì€ ì´ ê¸°ìˆ  í˜ì‹ ì˜ ìµœì „ì„ ì— ì„œê²Œ ë©ë‹ˆë‹¤. ì˜¤ëŠ˜ë¶€í„° ì—°êµ¬ ë§ˆë¼í†¤ì„ ì‹œì‘í•˜ê³  ì ë“¤ì–´ ìˆëŠ” ë™ì•ˆ AIê°€ ì¸ê°„ ì§€ì‹ì˜ ê²½ê³„ë¥¼ ë„“í˜€ê°€ë„ë¡ í•˜ì„¸ìš”! ğŸ§‘â€ğŸ”¬ğŸš€
