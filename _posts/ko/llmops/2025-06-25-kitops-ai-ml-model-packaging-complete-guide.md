---
title: "KitOps ì™„ì „ ê°€ì´ë“œ: AI/ML ëª¨ë¸ íŒ¨í‚¤ì§•ê³¼ ë²„ì „ ê´€ë¦¬ì˜ ìƒˆë¡œìš´ í‘œì¤€"
excerpt: "OCI í‘œì¤€ ê¸°ë°˜ KitOpsë¡œ AI/ML ëª¨ë¸, ë°ì´í„°ì…‹, ì½”ë“œë¥¼ í†µí•© íŒ¨í‚¤ì§•í•˜ê³  ë²„ì „ ê´€ë¦¬í•˜ëŠ” ì‹¤ì „ ê°€ì´ë“œ"
date: 2025-06-25
categories: 
  - llmops
  - tutorials
tags: 
  - kitops
  - modelkit
  - ai-ml-packaging
  - oci-artifacts
  - model-versioning
  - mlops-tools
  - kubernetes-deployment
author_profile: true
toc: true
toc_label: "KitOps ì™„ì „ ê°€ì´ë“œ"
---

## ê°œìš”

[KitOps](https://github.com/kitops-ml/kitops)ëŠ” AI/ML í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ í˜ì‹ ì ì¸ íŒ¨í‚¤ì§•, ë²„ì „ ê´€ë¦¬, ê³µìœ  ì‹œìŠ¤í…œì…ë‹ˆë‹¤. OCI(Open Container Initiative) í‘œì¤€ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ê¸°ì¡´ì˜ AI/ML, ê°œë°œ, DevOps ë„êµ¬ë“¤ê³¼ ì™„ë²½í•˜ê²Œ í˜¸í™˜ë˜ë©°, ì—”í„°í”„ë¼ì´ì¦ˆ ì»¨í…Œì´ë„ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” KitOpsì˜ í•µì‹¬ ê°œë…ë¶€í„° ì‹¤ì œ í”„ë¡œë•ì…˜ ë°°í¬ê¹Œì§€ ì™„ì „í•œ ì›Œí¬í”Œë¡œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.

## KitOpsì˜ í•µì‹¬ ê°€ì¹˜

### ğŸ **í†µí•© íŒ¨í‚¤ì§• (Unified Packaging)**
ModelKit íŒ¨í‚¤ì§€ëŠ” ëª¨ë¸, ë°ì´í„°ì…‹, ì„¤ì •, ì½”ë“œë¥¼ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤. í”„ë¡œì íŠ¸ì— í•„ìš”í•œ ë§Œí¼ ì¶”ê°€í•˜ê±°ë‚˜ ì œì™¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ­ **ë²„ì „ ê´€ë¦¬ (Versioning)**
ê° ModelKitì€ íƒœê·¸ê°€ ì§€ì •ë˜ì–´ ì–´ë–¤ ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ì´ í•¨ê»˜ ì‘ë™í•˜ëŠ”ì§€ ëª¨ë“  íŒ€ì›ì´ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ”’ **ë³€ì¡° ë°©ì§€ (Tamper-proofing)**
ê° ModelKit íŒ¨í‚¤ì§€ëŠ” ìì²´ì™€ í¬í•¨ëœ ëª¨ë“  ì•„í‹°íŒ©íŠ¸ì— ëŒ€í•œ SHA ë‹¤ì´ì œìŠ¤íŠ¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

### ğŸ¤© **ì„ íƒì  ì–¸íŒ¨í‚¹ (Selective Unpacking)**
`kit unpack --filter` ëª…ë ¹ìœ¼ë¡œ ModelKitì—ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì–¸íŒ¨í‚¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### macOS ì„¤ì¹˜

```bash
# Homebrewë¥¼ í†µí•œ ì„¤ì¹˜
brew tap kitops-ml/kitops
brew install kitops

# ë˜ëŠ” ì§ì ‘ ë‹¤ìš´ë¡œë“œ
curl -L https://github.com/kitops-ml/kitops/releases/latest/download/kitops-darwin-amd64.tar.gz | tar -xz
sudo mv kit /usr/local/bin/
```

### Linux ì„¤ì¹˜

```bash
# ìµœì‹  ë¦´ë¦¬ìŠ¤ ë‹¤ìš´ë¡œë“œ
curl -L https://github.com/kitops-ml/kitops/releases/latest/download/kitops-linux-amd64.tar.gz | tar -xz
sudo mv kit /usr/local/bin/

# ì„¤ì¹˜ í™•ì¸
kit version
```

### Docker í™˜ê²½ì—ì„œ ì‚¬ìš©

```dockerfile
FROM alpine:latest
RUN apk add --no-cache curl tar
RUN curl -L https://github.com/kitops-ml/kitops/releases/latest/download/kitops-linux-amd64.tar.gz | tar -xz
RUN mv kit /usr/local/bin/
```

## Kitfile ì‘ì„±ë²•

### ê¸°ë³¸ Kitfile êµ¬ì¡°

```yaml
# Kitfile
manifestVersion: v1alpha1
package:
  name: my-llm-model
  version: 1.0.0
  description: "Fine-tuned LLM for customer service"
  authors: ["Data Science Team"]

model:
  - name: customer-service-llm
    path: ./models/pytorch_model.bin
    description: "PyTorch fine-tuned model"
    framework: pytorch
    version: "2.1.0"

datasets:
  - name: training-data
    path: ./data/train.jsonl
    description: "Customer service training dataset"
  - name: validation-data
    path: ./data/val.jsonl
    description: "Validation dataset"

code:
  - name: inference-script
    path: ./src/inference.py
    description: "Model inference script"
  - name: preprocessing
    path: ./src/preprocess.py
    description: "Data preprocessing utilities"

docs:
  - name: model-card
    path: ./docs/model_card.md
    description: "Model documentation and metrics"
  - name: api-docs
    path: ./docs/api.md
    description: "API usage documentation"

configs:
  - name: training-config
    path: ./config/training.yaml
    description: "Training hyperparameters"
  - name: inference-config
    path: ./config/inference.json
    description: "Inference configuration"
```

### ê³ ê¸‰ Kitfile ì˜ˆì‹œ

```yaml
# Advanced Kitfile with model parts and metadata
manifestVersion: v1alpha1
package:
  name: multimodal-assistant
  version: 2.1.0
  description: "Multi-modal AI assistant with vision and text capabilities"
  authors: ["AI Research Team"]
  license: "MIT"
  repository: "https://github.com/company/multimodal-assistant"

# Base model reference
modelParts:
  - name: base-llm
    path: registry.company.com/models/llama-2-7b:latest
    description: "Base LLaMA 2 7B model"

model:
  - name: fine-tuned-multimodal
    path: ./models/adapter_model.safetensors
    description: "LoRA adapter for multimodal tasks"
    framework: transformers
    version: "4.35.0"
    size: "2.1GB"
    format: "safetensors"

datasets:
  - name: multimodal-training
    path: ./data/multimodal_train.parquet
    description: "Image-text paired training data"
    size: "15GB"
    samples: 500000
  - name: evaluation-suite
    path: ./data/eval/
    description: "Comprehensive evaluation datasets"

code:
  - name: training-pipeline
    path: ./src/train.py
    description: "Multi-modal training pipeline"
  - name: inference-api
    path: ./src/api/
    description: "FastAPI inference server"
  - name: evaluation-scripts
    path: ./src/eval/
    description: "Model evaluation utilities"

# Custom metadata
metadata:
  performance:
    bleu_score: 0.85
    rouge_l: 0.78
    accuracy: 0.92
  training:
    epochs: 10
    learning_rate: 2e-5
    batch_size: 16
    gpu_hours: 240
  deployment:
    min_memory: "8GB"
    recommended_gpu: "RTX 4090"
    inference_time: "150ms"
```

## ModelKit ìƒì„± ë° ê´€ë¦¬

### 1. ModelKit íŒ¨í‚¹

```bash
# ê¸°ë³¸ íŒ¨í‚¹
kit pack . -t my-registry.com/my-model:1.0.0

# íŠ¹ì • ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ì§ì ‘ í‘¸ì‹œ
kit pack . -t my-registry.com/my-model:1.0.0 --push

# ì••ì¶• ì˜µì…˜ ì§€ì •
kit pack . -t my-model:1.0.0 --compression gzip

# ì„œëª…ê³¼ í•¨ê»˜ íŒ¨í‚¹
kit pack . -t my-model:1.0.0 --sign --key ./private.key
```

### 2. ModelKit ì–¸íŒ¨í‚¹

```bash
# ì „ì²´ ModelKit ì–¸íŒ¨í‚¹
kit unpack my-registry.com/my-model:1.0.0 -d ./unpacked

# ëª¨ë¸ë§Œ ì–¸íŒ¨í‚¹
kit unpack my-registry.com/my-model:1.0.0 --model -d ./model-only

# ë°ì´í„°ì…‹ê³¼ ì½”ë“œë§Œ ì–¸íŒ¨í‚¹
kit unpack my-registry.com/my-model:1.0.0 --datasets --code -d ./data-code

# íŠ¹ì • ì•„í‹°íŒ©íŠ¸ë§Œ ì–¸íŒ¨í‚¹
kit unpack my-registry.com/my-model:1.0.0 --filter "*.py,*.yaml" -d ./filtered
```

### 3. ModelKit ì •ë³´ ì¡°íšŒ

```bash
# ModelKit ìƒì„¸ ì •ë³´
kit info my-registry.com/my-model:1.0.0

# ë¡œì»¬ ModelKit ëª©ë¡
kit list

# ì›ê²© ë ˆì§€ìŠ¤íŠ¸ë¦¬ì˜ ModelKit ëª©ë¡
kit list my-registry.com/my-models

# ModelKit íˆìŠ¤í† ë¦¬
kit history my-registry.com/my-model
```

## ì‹¤ì „ ì›Œí¬í”Œë¡œ ì˜ˆì‹œ

### ì‹œë‚˜ë¦¬ì˜¤ 1: LLM Fine-tuning í”„ë¡œì íŠ¸

#### 1ë‹¨ê³„: í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ì •

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir llm-finetuning-project
cd llm-finetuning-project

# ë””ë ‰í† ë¦¬ êµ¬ì¡°
mkdir -p {models,data,src,config,docs}

# ê¸°ë³¸ íŒŒì¼ êµ¬ì¡°
tree .
```

```
llm-finetuning-project/
â”œâ”€â”€ Kitfile
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ tokenizer.json
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”œâ”€â”€ val.jsonl
â”‚   â””â”€â”€ test.jsonl
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ inference.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ training.yaml
â”‚   â””â”€â”€ inference.json
â””â”€â”€ docs/
    â”œâ”€â”€ model_card.md
    â””â”€â”€ README.md
```

#### 2ë‹¨ê³„: Kitfile ì‘ì„±

```yaml
manifestVersion: v1alpha1
package:
  name: customer-support-llm
  version: 1.0.0
  description: "Fine-tuned LLM for customer support automation"
  authors: ["AI Team <ai@company.com>"]

model:
  - name: finetuned-model
    path: ./models/
    description: "Fine-tuned customer support model"
    framework: "transformers"
    baseModel: "microsoft/DialoGPT-medium"

datasets:
  - name: support-conversations
    path: ./data/train.jsonl
    description: "Customer support conversation dataset"
    samples: 50000
  - name: validation-set
    path: ./data/val.jsonl
    description: "Validation dataset"

code:
  - name: training-script
    path: ./src/train.py
    description: "Model fine-tuning script"
  - name: inference-engine
    path: ./src/inference.py
    description: "Inference API server"

configs:
  - name: training-params
    path: ./config/training.yaml
    description: "Training hyperparameters"

docs:
  - name: documentation
    path: ./docs/
    description: "Model documentation and usage guide"

metadata:
  training:
    epochs: 5
    learning_rate: 5e-5
    batch_size: 8
    validation_loss: 0.45
  performance:
    perplexity: 12.3
    bleu_score: 0.72
  hardware:
    gpu_type: "RTX 4090"
    training_time: "6 hours"
```

#### 3ë‹¨ê³„: ModelKit ìƒì„± ë° ë²„ì „ ê´€ë¦¬

```bash
# ê°œë°œ ë²„ì „ ìƒì„±
kit pack . -t company-registry.com/ai/customer-support-llm:dev

# í…ŒìŠ¤íŠ¸ í›„ ìŠ¤í…Œì´ì§• ë²„ì „
kit pack . -t company-registry.com/ai/customer-support-llm:1.0.0-staging

# í”„ë¡œë•ì…˜ ë¦´ë¦¬ìŠ¤
kit pack . -t company-registry.com/ai/customer-support-llm:1.0.0 --push

# íƒœê·¸ ì¶”ê°€
kit tag company-registry.com/ai/customer-support-llm:1.0.0 latest
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

#### RAG ì‹œìŠ¤í…œ Kitfile

```yaml
manifestVersion: v1alpha1
package:
  name: rag-knowledge-assistant
  version: 2.0.0
  description: "RAG-based knowledge assistant with vector search"
  authors: ["Knowledge Team"]

modelParts:
  - name: embedding-model
    path: registry.com/models/sentence-transformers:latest
    description: "Sentence transformer for embeddings"
  - name: llm-base
    path: registry.com/models/llama-2-13b-chat:latest
    description: "Base LLM for generation"

model:
  - name: fine-tuned-retriever
    path: ./models/retriever/
    description: "Fine-tuned retrieval model"
  - name: generation-adapter
    path: ./models/generation/adapter_model.safetensors
    description: "LoRA adapter for domain-specific generation"

datasets:
  - name: knowledge-base
    path: ./data/knowledge_corpus.jsonl
    description: "Domain knowledge corpus"
    documents: 100000
  - name: qa-pairs
    path: ./data/qa_training.jsonl
    description: "Question-answer training pairs"
  - name: vector-index
    path: ./data/embeddings/
    description: "Pre-computed vector embeddings"

code:
  - name: rag-pipeline
    path: ./src/rag/
    description: "Complete RAG pipeline implementation"
  - name: vector-search
    path: ./src/search/
    description: "Vector similarity search engine"
  - name: api-server
    path: ./src/api/
    description: "FastAPI server with RAG endpoints"

configs:
  - name: rag-config
    path: ./config/rag_config.yaml
    description: "RAG pipeline configuration"
  - name: search-config
    path: ./config/search_params.json
    description: "Vector search parameters"

metadata:
  performance:
    retrieval_recall_at_5: 0.89
    generation_bleu: 0.81
    end_to_end_latency: "250ms"
  infrastructure:
    vector_db: "Pinecone"
    embedding_dim: 768
    index_size: "2GB"
```

## Kubernetes ë°°í¬

### 1. ModelKitì—ì„œ Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±

```bash
# Kubernetes ë°°í¬ ì„¤ì • ìƒì„±
kit deploy create kubernetes \
  --modelkit company-registry.com/ai/customer-support-llm:1.0.0 \
  --output ./k8s-deployment.yaml

# KServe ë°°í¬ ì„¤ì • ìƒì„±
kit deploy create kserve \
  --modelkit company-registry.com/ai/customer-support-llm:1.0.0 \
  --output ./kserve-deployment.yaml
```

### 2. ìƒì„±ëœ Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: customer-support-llm
  labels:
    app: customer-support-llm
    version: "1.0.0"
spec:
  replicas: 3
  selector:
    matchLabels:
      app: customer-support-llm
  template:
    metadata:
      labels:
        app: customer-support-llm
    spec:
      initContainers:
      - name: model-loader
        image: kitops/kit:latest
        command: ["kit", "unpack", "--model", "--code"]
        args: ["company-registry.com/ai/customer-support-llm:1.0.0"]
        volumeMounts:
        - name: model-storage
          mountPath: /models
      containers:
      - name: inference-server
        image: python:3.9-slim
        command: ["python", "/app/inference.py"]
        ports:
        - containerPort: 8000
        volumeMounts:
        - name: model-storage
          mountPath: /models
        env:
        - name: MODEL_PATH
          value: "/models"
        resources:
          requests:
            memory: "4Gi"
            cpu: "1000m"
          limits:
            memory: "8Gi"
            cpu: "2000m"
      volumes:
      - name: model-storage
        emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: customer-support-llm-service
spec:
  selector:
    app: customer-support-llm
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

### 3. KServe ë°°í¬

```yaml
# kserve-deployment.yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: customer-support-llm
spec:
  predictor:
    model:
      modelFormat:
        name: pytorch
      storageUri: "kit://company-registry.com/ai/customer-support-llm:1.0.0"
      resources:
        requests:
          memory: 4Gi
          cpu: "2"
        limits:
          memory: 8Gi
          cpu: "4"
      env:
      - name: STORAGE_URI
        value: "kit://company-registry.com/ai/customer-support-llm:1.0.0"
```

### 4. ë°°í¬ ì‹¤í–‰

```bash
# Kubernetes í´ëŸ¬ìŠ¤í„°ì— ë°°í¬
kubectl apply -f k8s-deployment.yaml

# KServe ë°°í¬ (KServeê°€ ì„¤ì¹˜ëœ í´ëŸ¬ìŠ¤í„°)
kubectl apply -f kserve-deployment.yaml

# ë°°í¬ ìƒíƒœ í™•ì¸
kubectl get pods -l app=customer-support-llm
kubectl get svc customer-support-llm-service

# ë¡œê·¸ í™•ì¸
kubectl logs -l app=customer-support-llm -f
```

## ì»¨í…Œì´ë„ˆ ë°°í¬

### 1. ModelKitì—ì„œ ì»¨í…Œì´ë„ˆ ìƒì„±

```bash
# ê¸°ë³¸ ì»¨í…Œì´ë„ˆ ìƒì„±
kit deploy create docker \
  --modelkit company-registry.com/ai/customer-support-llm:1.0.0 \
  --output ./Dockerfile

# ì»¤ìŠ¤í…€ ë² ì´ìŠ¤ ì´ë¯¸ì§€ ì‚¬ìš©
kit deploy create docker \
  --modelkit company-registry.com/ai/customer-support-llm:1.0.0 \
  --base-image python:3.9-slim \
  --output ./Dockerfile.custom
```

### 2. ìƒì„±ëœ Dockerfile

```dockerfile
# Dockerfile
FROM python:3.9-slim

# KitOps CLI ì„¤ì¹˜
RUN apt-get update && apt-get install -y curl && \
    curl -L https://github.com/kitops-ml/kitops/releases/latest/download/kitops-linux-amd64.tar.gz | tar -xz && \
    mv kit /usr/local/bin/ && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# ModelKit ì–¸íŒ¨í‚¹
RUN kit unpack company-registry.com/ai/customer-support-llm:1.0.0 --code --model --configs -d /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì¶”ê°€ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN pip install torch transformers flask gunicorn

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
CMD ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "inference:app"]
```

### 3. ì»¨í…Œì´ë„ˆ ë¹Œë“œ ë° ì‹¤í–‰

```bash
# ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t customer-support-llm:1.0.0 .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  --name customer-support-api \
  -p 8000:8000 \
  --memory=8g \
  --cpus=2 \
  customer-support-llm:1.0.0

# ë¡œê·¸ í™•ì¸
docker logs -f customer-support-api

# í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/health
```

## ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©

### 1. ëª¨ë¸ ì„œëª… ë° ê²€ì¦

```bash
# GPG í‚¤ ìƒì„±
gpg --full-generate-key

# ëª¨ë¸ ì„œëª…í•˜ì—¬ íŒ¨í‚¹
kit pack . -t signed-model:1.0.0 --sign --key ~/.gnupg/secring.gpg

# ì„œëª… ê²€ì¦
kit verify signed-model:1.0.0 --key ~/.gnupg/pubring.gpg
```

### 2. ë©€í‹° ì•„í‚¤í…ì²˜ ì§€ì›

```bash
# ARM64 ì•„í‚¤í…ì²˜ìš© ModelKit
kit pack . -t my-model:1.0.0-arm64 --platform linux/arm64

# AMD64 ì•„í‚¤í…ì²˜ìš© ModelKit
kit pack . -t my-model:1.0.0-amd64 --platform linux/amd64

# ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±
kit manifest create my-model:1.0.0 \
  my-model:1.0.0-arm64 \
  my-model:1.0.0-amd64
```

### 3. ê°œë°œ ëª¨ë“œ í™œìš©

```bash
# ë¡œì»¬ ê°œë°œ ì„œë²„ ì‹œì‘
kit dev start --modelkit my-model:latest --port 8080

# ëª¨ë¸ê³¼ ìƒí˜¸ì‘ìš©
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Hello, how can you help me?"}],
    "max_tokens": 100
  }'

# ê°œë°œ ì„œë²„ ì¤‘ì§€
kit dev stop
```

## CI/CD í†µí•©

### GitHub Actions ì›Œí¬í”Œë¡œ

```yaml
# .github/workflows/model-deployment.yml
name: Model Deployment Pipeline

on:
  push:
    branches: [main]
    paths: ['models/**', 'Kitfile']

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Install KitOps
      run: |
        curl -L https://github.com/kitops-ml/kitops/releases/latest/download/kitops-linux-amd64.tar.gz | tar -xz
        sudo mv kit /usr/local/bin/
    
    - name: Login to registry
      run: |
        echo ${{ secrets.REGISTRY_PASSWORD }} | kit auth login ${{ secrets.REGISTRY_URL }} -u ${{ secrets.REGISTRY_USERNAME }} --password-stdin
    
    - name: Build ModelKit
      run: |
        VERSION=$(git rev-parse --short HEAD)
        kit pack . -t ${{ secrets.REGISTRY_URL }}/models/customer-support-llm:$VERSION
        kit pack . -t ${{ secrets.REGISTRY_URL }}/models/customer-support-llm:latest
    
    - name: Run tests
      run: |
        # ModelKit í…ŒìŠ¤íŠ¸
        kit unpack ${{ secrets.REGISTRY_URL }}/models/customer-support-llm:latest --code -d ./test-env
        cd test-env && python -m pytest tests/
    
    - name: Push ModelKit
      run: |
        VERSION=$(git rev-parse --short HEAD)
        kit push ${{ secrets.REGISTRY_URL }}/models/customer-support-llm:$VERSION
        kit push ${{ secrets.REGISTRY_URL }}/models/customer-support-llm:latest
    
    - name: Deploy to staging
      if: github.ref == 'refs/heads/main'
      run: |
        kit deploy create kubernetes \
          --modelkit ${{ secrets.REGISTRY_URL }}/models/customer-support-llm:latest \
          --output k8s-staging.yaml
        kubectl apply -f k8s-staging.yaml --namespace=staging
    
    - name: Run integration tests
      run: |
        # ìŠ¤í…Œì´ì§• í™˜ê²½ í…ŒìŠ¤íŠ¸
        sleep 60  # ë°°í¬ ëŒ€ê¸°
        curl -f http://staging-api.company.com/health
    
    - name: Deploy to production
      if: github.ref == 'refs/heads/main'
      run: |
        kubectl apply -f k8s-staging.yaml --namespace=production
```

### Jenkins Pipeline

```groovy
// Jenkinsfile
pipeline {
    agent any
    
    environment {
        REGISTRY_URL = credentials('registry-url')
        REGISTRY_CREDS = credentials('registry-credentials')
        KUBECONFIG = credentials('kubeconfig')
    }
    
    stages {
        stage('Setup') {
            steps {
                script {
                    // KitOps ì„¤ì¹˜
                    sh '''
                        curl -L https://github.com/kitops-ml/kitops/releases/latest/download/kitops-linux-amd64.tar.gz | tar -xz
                        sudo mv kit /usr/local/bin/
                    '''
                }
            }
        }
        
        stage('Build ModelKit') {
            steps {
                script {
                    def version = env.BUILD_NUMBER
                    sh """
                        kit auth login ${REGISTRY_URL} -u ${REGISTRY_CREDS_USR} -p ${REGISTRY_CREDS_PSW}
                        kit pack . -t ${REGISTRY_URL}/models/customer-support-llm:${version}
                        kit pack . -t ${REGISTRY_URL}/models/customer-support-llm:latest
                    """
                }
            }
        }
        
        stage('Test') {
            steps {
                sh '''
                    kit unpack ${REGISTRY_URL}/models/customer-support-llm:latest --code -d ./test-env
                    cd test-env && python -m pytest tests/ --junitxml=test-results.xml
                '''
                junit 'test-env/test-results.xml'
            }
        }
        
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                sh '''
                    kit push ${REGISTRY_URL}/models/customer-support-llm:${BUILD_NUMBER}
                    kit push ${REGISTRY_URL}/models/customer-support-llm:latest
                    
                    kit deploy create kubernetes \\
                        --modelkit ${REGISTRY_URL}/models/customer-support-llm:latest \\
                        --output k8s-deployment.yaml
                    
                    kubectl apply -f k8s-deployment.yaml
                '''
            }
        }
    }
    
    post {
        always {
            cleanWs()
        }
        success {
            slackSend channel: '#ai-deployments', 
                     color: 'good', 
                     message: "âœ… Model deployment successful: ${env.JOB_NAME} - ${env.BUILD_NUMBER}"
        }
        failure {
            slackSend channel: '#ai-deployments', 
                     color: 'danger', 
                     message: "âŒ Model deployment failed: ${env.JOB_NAME} - ${env.BUILD_NUMBER}"
        }
    }
}
```

## ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„±

### 1. ModelKit ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
# monitoring/metrics_collector.py
import time
import psutil
import GPUtil
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# ë©”íŠ¸ë¦­ ì •ì˜
MODEL_REQUESTS = Counter('model_requests_total', 'Total model requests', ['model_name', 'version'])
MODEL_LATENCY = Histogram('model_inference_duration_seconds', 'Model inference latency')
MODEL_MEMORY = Gauge('model_memory_usage_bytes', 'Model memory usage')
GPU_UTILIZATION = Gauge('gpu_utilization_percent', 'GPU utilization')

class ModelMonitor:
    def __init__(self, model_name, model_version):
        self.model_name = model_name
        self.model_version = model_version
        
    def record_request(self):
        MODEL_REQUESTS.labels(
            model_name=self.model_name, 
            version=self.model_version
        ).inc()
    
    def record_latency(self, duration):
        MODEL_LATENCY.observe(duration)
    
    def update_system_metrics(self):
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory = psutil.virtual_memory()
        MODEL_MEMORY.set(memory.used)
        
        # GPU ì‚¬ìš©ë¥ 
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                GPU_UTILIZATION.set(gpus[0].load * 100)
        except:
            pass

# ì‚¬ìš© ì˜ˆì‹œ
monitor = ModelMonitor("customer-support-llm", "1.0.0")

def inference_with_monitoring(input_text):
    start_time = time.time()
    monitor.record_request()
    
    try:
        # ì‹¤ì œ ì¶”ë¡  ë¡œì§
        result = model.generate(input_text)
        
        # ë©”íŠ¸ë¦­ ê¸°ë¡
        duration = time.time() - start_time
        monitor.record_latency(duration)
        monitor.update_system_metrics()
        
        return result
    except Exception as e:
        # ì—ëŸ¬ ë©”íŠ¸ë¦­ ê¸°ë¡
        ERROR_COUNTER.labels(error_type=type(e).__name__).inc()
        raise

# Prometheus ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘
start_http_server(8001)
```

### 2. Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì •

```json
{
  "dashboard": {
    "title": "KitOps Model Performance",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(model_requests_total[5m])",
            "legendFormat": "{{model_name}} v{{version}}"
          }
        ]
      },
      {
        "title": "Inference Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, model_inference_duration_seconds_bucket)",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, model_inference_duration_seconds_bucket)",
            "legendFormat": "50th percentile"
          }
        ]
      },
      {
        "title": "Memory Usage",
        "type": "singlestat",
        "targets": [
          {
            "expr": "model_memory_usage_bytes / 1024 / 1024 / 1024",
            "legendFormat": "Memory (GB)"
          }
        ]
      },
      {
        "title": "GPU Utilization",
        "type": "singlestat",
        "targets": [
          {
            "expr": "gpu_utilization_percent",
            "legendFormat": "GPU %"
          }
        ]
      }
    ]
  }
}
```

## ë³´ì•ˆ ë° ê±°ë²„ë„ŒìŠ¤

### 1. ëª¨ë¸ ë³´ì•ˆ ì •ì±…

```yaml
# security-policy.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kitops-security-policy
data:
  policy.yaml: |
    security:
      signing:
        required: true
        trusted_keys:
          - "ai-team@company.com"
          - "mlops-team@company.com"
      
      scanning:
        enabled: true
        vulnerability_threshold: "HIGH"
        scan_on_push: true
      
      access_control:
        registry_auth: true
        rbac_enabled: true
        allowed_registries:
          - "company-registry.com"
          - "staging-registry.com"
      
      audit:
        log_all_operations: true
        retention_days: 90
        compliance_mode: "SOC2"
```

### 2. ëª¨ë¸ ê±°ë²„ë„ŒìŠ¤ ì›Œí¬í”Œë¡œ

```python
# governance/model_approval.py
import yaml
from dataclasses import dataclass
from typing import List, Dict, Any
from enum import Enum

class ApprovalStatus(Enum):
    PENDING = "pending"
    APPROVED = "approved"
    REJECTED = "rejected"

@dataclass
class ModelApproval:
    model_name: str
    version: str
    requester: str
    approvers: List[str]
    status: ApprovalStatus
    performance_metrics: Dict[str, float]
    security_scan_results: Dict[str, Any]
    bias_evaluation: Dict[str, Any]

class ModelGovernance:
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
    
    def submit_for_approval(self, modelkit_ref: str) -> str:
        """ëª¨ë¸ ìŠ¹ì¸ ìš”ì²­ ì œì¶œ"""
        # ModelKit ì •ë³´ ì¶”ì¶œ
        info = self.extract_modelkit_info(modelkit_ref)
        
        # ìë™ ê²€ì¦ ì‹¤í–‰
        validation_results = self.run_automated_validation(modelkit_ref)
        
        # ìŠ¹ì¸ ìš”ì²­ ìƒì„±
        approval = ModelApproval(
            model_name=info['name'],
            version=info['version'],
            requester=info['author'],
            approvers=self.get_required_approvers(info),
            status=ApprovalStatus.PENDING,
            performance_metrics=validation_results['performance'],
            security_scan_results=validation_results['security'],
            bias_evaluation=validation_results['bias']
        )
        
        # ìŠ¹ì¸ ì›Œí¬í”Œë¡œ ì‹œì‘
        return self.initiate_approval_workflow(approval)
    
    def run_automated_validation(self, modelkit_ref: str) -> Dict[str, Any]:
        """ìë™í™”ëœ ëª¨ë¸ ê²€ì¦"""
        results = {}
        
        # ì„±ëŠ¥ ê²€ì¦
        results['performance'] = self.validate_performance(modelkit_ref)
        
        # ë³´ì•ˆ ìŠ¤ìº”
        results['security'] = self.security_scan(modelkit_ref)
        
        # í¸í–¥ì„± í‰ê°€
        results['bias'] = self.bias_evaluation(modelkit_ref)
        
        # ê·œì • ì¤€ìˆ˜ í™•ì¸
        results['compliance'] = self.compliance_check(modelkit_ref)
        
        return results
    
    def validate_performance(self, modelkit_ref: str) -> Dict[str, float]:
        """ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦"""
        # ModelKit ì–¸íŒ¨í‚¹ ë° ëª¨ë¸ ë¡œë“œ
        # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€
        # ì„±ëŠ¥ ì„ê³„ê°’ í™•ì¸
        return {
            "accuracy": 0.95,
            "latency_p95": 150.0,
            "memory_usage": 4.2
        }
    
    def security_scan(self, modelkit_ref: str) -> Dict[str, Any]:
        """ë³´ì•ˆ ì·¨ì•½ì  ìŠ¤ìº”"""
        return {
            "vulnerabilities": [],
            "signature_valid": True,
            "dependencies_secure": True
        }
    
    def bias_evaluation(self, modelkit_ref: str) -> Dict[str, Any]:
        """í¸í–¥ì„± í‰ê°€"""
        return {
            "demographic_parity": 0.85,
            "equalized_odds": 0.88,
            "fairness_score": 0.92
        }
```

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

### 1. ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°ì±…

**ë¬¸ì œ: ModelKit íŒ¨í‚¹ ì‹¤íŒ¨**
```bash
# í•´ê²°ì±… 1: ê¶Œí•œ í™•ì¸
ls -la Kitfile
chmod 644 Kitfile

# í•´ê²°ì±… 2: ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
df -h

# í•´ê²°ì±… 3: ìƒì„¸ ë¡œê·¸ í™•ì¸
kit pack . -t my-model:1.0.0 --verbose
```

**ë¬¸ì œ: ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì¸ì¦ ì‹¤íŒ¨**
```bash
# í•´ê²°ì±…: ì¸ì¦ ì •ë³´ ì¬ì„¤ì •
kit auth logout registry.company.com
kit auth login registry.company.com -u username

# í† í° ê¸°ë°˜ ì¸ì¦
echo $REGISTRY_TOKEN | kit auth login registry.company.com -u username --password-stdin
```

**ë¬¸ì œ: Kubernetes ë°°í¬ ì‹¤íŒ¨**
```bash
# í•´ê²°ì±… 1: ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl describe pod -l app=customer-support-llm

# í•´ê²°ì±… 2: ì´ë¯¸ì§€ í’€ ì‹œí¬ë¦¿ ì„¤ì •
kubectl create secret docker-registry regcred \
  --docker-server=registry.company.com \
  --docker-username=username \
  --docker-password=password

# í•´ê²°ì±… 3: ë…¸ë“œ ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl top nodes
kubectl describe node
```

### 2. ë””ë²„ê¹… ë„êµ¬

```bash
# ModelKit ë‚´ìš© ê²€ì‚¬
kit inspect my-model:1.0.0

# ë ˆì´ì–´ ë¶„ì„
kit layers my-model:1.0.0

# ì˜ì¡´ì„± íŠ¸ë¦¬
kit deps my-model:1.0.0 --tree

# ë¬´ê²°ì„± ê²€ì¦
kit verify my-model:1.0.0 --checksum
```

## ê²°ë¡ 

KitOpsëŠ” AI/ML í”„ë¡œì íŠ¸ì˜ ë³µì¡í•œ ì•„í‹°íŒ©íŠ¸ ê´€ë¦¬ ë¬¸ì œë¥¼ OCI í‘œì¤€ ê¸°ë°˜ì˜ í†µí•© ì†”ë£¨ì…˜ìœ¼ë¡œ í•´ê²°í•©ë‹ˆë‹¤. ModelKitì„ í†µí•´ ëª¨ë¸, ë°ì´í„°, ì½”ë“œ, ì„¤ì •ì„ í•˜ë‚˜ì˜ ë¶ˆë³€ íŒ¨í‚¤ì§€ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, ì„ íƒì  ì–¸íŒ¨í‚¹ìœ¼ë¡œ í•„ìš”í•œ ë¶€ë¶„ë§Œ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ¯ **í•µì‹¬ ì¥ì **

1. **í‘œì¤€í™”**: OCI í˜¸í™˜ìœ¼ë¡œ ê¸°ì¡´ ì¸í”„ë¼ì™€ ì™„ë²½ í†µí•©
2. **ë³´ì•ˆ**: ì„œëª…, ê²€ì¦, ë³€ì¡° ë°©ì§€ ê¸°ëŠ¥ ì œê³µ
3. **íš¨ìœ¨ì„±**: ì„ íƒì  ì–¸íŒ¨í‚¹ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ì ˆì•½
4. **í™•ì¥ì„±**: Kubernetes, Docker ë“± ë‹¤ì–‘í•œ ë°°í¬ í™˜ê²½ ì§€ì›
5. **ê±°ë²„ë„ŒìŠ¤**: ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ëª¨ë¸ ê´€ë¦¬ ë° ê°ì‚¬ ê¸°ëŠ¥

### ğŸš€ **ë‹¤ìŒ ë‹¨ê³„**

- [KitOps ê³µì‹ ë¬¸ì„œ](https://kitops.ml) ì°¸ì¡°
- [GitHub ë¦¬í¬ì§€í† ë¦¬](https://github.com/kitops-ml/kitops)ì—ì„œ ìµœì‹  ì—…ë°ì´íŠ¸ í™•ì¸
- [Discord ì»¤ë®¤ë‹ˆí‹°](https://discord.gg/kitops) ì°¸ì—¬
- ì‹¤ì œ í”„ë¡œì íŠ¸ì— KitOps ë„ì… ê²€í† 

KitOpsë¥¼ í†µí•´ AI/ML í”„ë¡œì íŠ¸ì˜ ì•„í‹°íŒ©íŠ¸ ê´€ë¦¬ë¥¼ í˜ì‹ í•˜ê³ , íŒ€ ê°„ í˜‘ì—…ì„ ê°œì„ í•˜ë©°, í”„ë¡œë•ì…˜ ë°°í¬ë¥¼ ì•ˆì „í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. 