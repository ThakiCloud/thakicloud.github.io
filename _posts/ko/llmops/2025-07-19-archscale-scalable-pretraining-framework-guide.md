---
title: "Microsoft ArchScale: í™•ì¥ ê°€ëŠ¥í•œ LLM ì‚¬ì „ í›ˆë ¨ í”„ë ˆì„ì›Œí¬ ì™„ë²½ ê°€ì´ë“œ"
excerpt: "ArchScaleì„ í™œìš©í•œ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì—°êµ¬ì™€ ëŒ€ê·œëª¨ ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•. Î¼P++ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ë¶€í„° 128K ì»¨í…ìŠ¤íŠ¸ í›ˆë ¨ê¹Œì§€"
seo_title: "ArchScale LLM ì‚¬ì „ í›ˆë ¨ ê°€ì´ë“œ - Microsoft í”„ë ˆì„ì›Œí¬ - Thaki Cloud"
seo_description: "Microsoft ArchScaleì„ í™œìš©í•œ ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ ì‚¬ì „ í›ˆë ¨ ì™„ë²½ ê°€ì´ë“œ. Î¼P++ ìŠ¤ì¼€ì¼ë§, ë¡± ì»¨í…ìŠ¤íŠ¸ í›ˆë ¨, ë¶„ì‚° ì²˜ë¦¬, ì„±ëŠ¥ í‰ê°€ê¹Œì§€ ì‹¤ë¬´ ì¤‘ì‹¬ ì„¤ëª…"
date: 2025-07-19
last_modified_at: 2025-07-19
categories:
  - llmops
tags:
  - ArchScale
  - Microsoft
  - LLM
  - ì‚¬ì „í›ˆë ¨
  - ìŠ¤ì¼€ì¼ë§
  - Î¼P++
  - ë¶„ì‚°í›ˆë ¨
  - ë¡±ì»¨í…ìŠ¤íŠ¸
  - PyTorch
  - torchrun
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/llmops/archscale-scalable-pretraining-framework-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 25ë¶„

## ì„œë¡ 

**Microsoft ArchScale**ì€ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì—°êµ¬ë¥¼ ìœ„í•œ ë‹¨ìˆœí•˜ë©´ì„œë„ í™•ì¥ ê°€ëŠ¥í•œ ì‚¬ì „ í›ˆë ¨ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. 110Më¶€í„° 3.3B íŒŒë¼ë¯¸í„°ê¹Œì§€ì˜ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í›ˆë ¨í•  ìˆ˜ ìˆìœ¼ë©°, **Î¼P++** ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì„ í†µí•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì˜ ë¹„ìš©ì„ ëŒ€í­ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ArchScaleì˜ í•µì‹¬ íŠ¹ì§•

- ğŸ¯ **ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ ì§€ì›**: Phi4-mini-Flash, SambaY, Transformer++, Mamba ë“±
- ğŸ“Š **Î¼P++ ìŠ¤ì¼€ì¼ë§**: ì‘ì€ ëª¨ë¸ì—ì„œ íŠœë‹í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ëŒ€ê·œëª¨ ëª¨ë¸ì— ìë™ ì „ì´
- ğŸš€ **íš¨ìœ¨ì ì¸ í›ˆë ¨**: Flash Attention, RoPE, fused kernel ë“± ìµœì í™” ê¸°ìˆ 
- ğŸ“ **ë¡± ì»¨í…ìŠ¤íŠ¸**: ìµœëŒ€ 128K ì‹œí€€ìŠ¤ ê¸¸ì´ê¹Œì§€ ê°€ë³€ ê¸¸ì´ í›ˆë ¨
- ğŸ” **í¬ê´„ì  í‰ê°€**: RULER, Phonebook, ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ë“±

## í™˜ê²½ ì„¤ì • ë° ì„¤ì¹˜

### 1. Docker í™˜ê²½ êµ¬ì„±

ArchScaleì€ Dockerë¥¼ í†µí•œ í™˜ê²½ ì„¤ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤:

```bash
# ArchScale ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/microsoft/ArchScale.git
cd ArchScale

# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t archscale:latest .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (GPU ì§€ì›)
docker run --gpus all -it --rm \
    -v $(pwd):/workspace \
    -v /path/to/data:/data \
    archscale:latest bash
```

### 2. ì§ì ‘ ì„¤ì¹˜

Dockerë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°:

```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install lightning datasets transformers wandb

# ArchScale í´ë¡  ë° ì„¤ì¹˜
git clone https://github.com/microsoft/ArchScale.git
cd ArchScale
pip install -e .
```

### 3. ë°ì´í„° ì¤€ë¹„

#### SlimPajama ë°ì´í„°ì…‹ í† í°í™”

```bash
# Samba ì½”ë“œë² ì´ìŠ¤ ì°¸ì¡°í•˜ì—¬ ë°ì´í„° í† í°í™”
# ì˜ˆì‹œ: SlimPajama ì „ì²˜ë¦¬
python scripts/prepare_data.py \
    --input_dir /path/to/slimpajama \
    --output_dir /path/to/tokenized_data \
    --tokenizer microsoft/Phi-4-mini-flash-reasoning \
    --context_length 8192
```

## í•µì‹¬ ê°œë…: Î¼P++ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™

### Î¼P++ë€?

**Î¼P++ (Maximal Update Parameterization)**ëŠ” ëª¨ë¸ í¬ê¸°ê°€ ë³€í•´ë„ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìë™ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

```python
# BaseHyperparameters ì„¤ì • ì˜ˆì‹œ
from lit_gpt.config import BaseHyperparameters

# d16 (depth=16) ê¸°ì¤€ í•˜ì´í¼íŒŒë¼ë¯¸í„°
base_hps = BaseHyperparameters(
    eta0=5e-4,          # í•™ìŠµë¥ 
    b0=8388608,         # ë°°ì¹˜ í¬ê¸° (í† í° ìˆ˜)
    warmup_tokens0=25_165_824_000,  # ì›Œë°ì—… í† í° ìˆ˜
    # ê¸°íƒ€ ìµœì í™” ê´€ë ¨ íŒŒë¼ë¯¸í„°ë“¤
)

# ì‹¤ì œ d8 ë˜ëŠ” d24 ëª¨ë¸ í›ˆë ¨ ì‹œ ìë™ ìŠ¤ì¼€ì¼ë§ë¨
```

### ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì˜ ì¥ì 

1. **ë¹„ìš© ì ˆê°**: ì‘ì€ ëª¨ë¸ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í›„ ëŒ€ê·œëª¨ ëª¨ë¸ì— ì ìš©
2. **ì¼ê´€ì„±**: ëª¨ë¸ í¬ê¸°ì™€ ê´€ê³„ì—†ì´ ì•ˆì •ì ì¸ í›ˆë ¨
3. **íš¨ìœ¨ì„±**: ì‹¤í—˜ íšŒìˆ˜ ëŒ€í­ ê°ì†Œ

## ì‹¤ì „ í›ˆë ¨ ê°€ì´ë“œ

### 1. Phi4-mini-Flash ëª¨ë¸ ì‚¬ì „ í›ˆë ¨

5T í† í°ì˜ ê³ í’ˆì§ˆ ë°ì´í„°ë¡œ Phi4-mini-Flash ëª¨ë¸ì„ í›ˆë ¨:

```bash
#!/bin/bash
# phi4_pretrain.sh

export LIGHTNING_ARTIFACTS_DIR='/path/to/output_dir'
export MASTER_ADDR='master_node_ip'
export MASTER_PORT='29500'

# 1K GPU (128 ë…¸ë“œ Ã— 8 GPU)ë¡œ ë¶„ì‚° í›ˆë ¨
torchrun --nnodes=128 --nproc_per_node=8 \
    --rdzv_backend=c10d \
    --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
    pretrain.py \
    --train_data_dir /path/to/phi4/data \
    --base_hps.eta0=5e-4 \
    --base_hps.b0=8388608 \
    --base_hps.warmup_tokens0=25_165_824_000 \
    --ctx_len 8192 \
    --max_tokens 5e12 \
    --resume="auto" \
    --train_model phi4miniflash \
    --depth 32 \
    --train_name scaling
```

### 2. SambaY ì•„í‚¤í…ì²˜ í›ˆë ¨ (ê¶Œì¥)

ë” ê¹”ë”í•œ ì•„í‚¤í…ì²˜ì¸ SambaY ëª¨ë¸ í›ˆë ¨:

```bash
#!/bin/bash
# sambay_pretrain.sh

torchrun --nnodes=128 --nproc_per_node=8 \
    --rdzv_backend=c10d \
    --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
    pretrain.py \
    --train_data_dir /path/to/data \
    --train_model sambayda \
    --depth 24 \
    --vocab_size 200064 \
    --train_name scaling_mup_tie \
    --ctx_len 8192 \
    --max_tokens 1e12
```

### 3. FLOPs ìŠ¤ì¼€ì¼ë§ ì‹¤í—˜

110Më¶€í„° 3.3B íŒŒë¼ë¯¸í„°ê¹Œì§€ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ëª¨ë¸ í›ˆë ¨:

```bash
#!/bin/bash
# flops_scaling.sh

export MASTER_ADDR='localhost'
export MASTER_PORT='29500'

# 8 GPUë¡œ ë‹¤ì–‘í•œ depth ì‹¤í—˜
for depth in 8 12 16 20 24; do
    echo "Training model with depth=${depth}"
    
    torchrun --nnodes=1 --nproc_per_node=8 \
        --rdzv_backend=c10d \
        --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
        pretrain.py \
        --train_data_dir /path/to/slim_pajama/data \
        --val_data_dir /path/to/slim_pajama/data \
        --train_model sambay \
        --depth ${depth} \
        --train_name scaling_mup \
        --max_tokens 1e11
done

# ìŠ¤ì¼€ì¼ë§ ê³¡ì„  ìƒì„±
python plot_flops_scaling.py \
    --log_dir /path/to/output_dir \
    --output_file flops_scaling_results.png
```

### 4. ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì‹¤í—˜

1B íŒŒë¼ë¯¸í„° ëª¨ë¸ë¡œ 100B~600B í† í° ìŠ¤ì¼€ì¼ë§:

```bash
#!/bin/bash
# data_scaling.sh

# 64 GPU (8 ë…¸ë“œ Ã— 8 GPU)ë¡œ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
for tokens in 1e11 2e11 3e11 4e11 5e11 6e11; do
    echo "Training with ${tokens} tokens"
    
    torchrun --nnodes=8 --nproc_per_node=8 \
        --rdzv_backend=c10d \
        --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
        pretrain.py \
        --train_data_dir /path/to/slim_pajama/data \
        --val_data_dir /path/to/slim_pajama/data \
        --train_model transformer \
        --depth 16 \
        --max_tokens ${tokens} \
        --train_name scaling_mup_tie
done

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ê³¡ì„  ë¶„ì„
python plot_data_scaling.py \
    --log_dir /path/to/output_dir \
    --output_file data_scaling_results.png
```

## í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì „ëµ

### 1. Î¼P++ë¥¼ í™œìš©í•œ íš¨ìœ¨ì  íŠœë‹

```bash
#!/bin/bash
# hyperparameter_sweep.sh

# d16, 1B ëª¨ë¸ ê¸°ì¤€ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
# ì‹¤ì œë¡œëŠ” d8 ëª¨ë¸ì—ì„œ ë¹ ë¥´ê²Œ ì‹¤í—˜
for lr in 4e-4 1e-4 1e-3; do
    for batch_size in 4194304 8388608 16777216; do
        echo "Testing lr=${lr}, batch_size=${batch_size}"
        
        torchrun --nnodes=1 --nproc_per_node=8 \
            --rdzv_backend=c10d \
            --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
            pretrain.py \
            --train_data_dir /path/to/slim_pajama/data \
            --val_data_dir /path/to/slim_pajama/data \
            --train_model transformer \
            --depth 8 \
            --base_hps.eta0=${lr} \
            --base_hps.b0=${batch_size} \
            --train_name "hp_sweep_lr${lr}_bs${batch_size}" \
            --max_tokens 1e10
    done
done
```

### 2. ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„ì„

```python
# analyze_hyperparameters.py
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

def analyze_sweep_results(log_dir):
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìŠ¤ìœ„í”„ ê²°ê³¼ ë¶„ì„"""
    results = []
    
    for log_file in Path(log_dir).glob("*/metrics.csv"):
        # ê° ì‹¤í—˜ì˜ ìµœì¢… ì†ì‹¤ê°’ ì¶”ì¶œ
        df = pd.read_csv(log_file)
        final_loss = df['val_loss'].min()
        
        # ì‹¤í—˜ëª…ì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        exp_name = log_file.parent.name
        lr = float(exp_name.split('lr')[1].split('_')[0])
        bs = int(exp_name.split('bs')[1])
        
        results.append({
            'learning_rate': lr,
            'batch_size': bs,
            'final_loss': final_loss
        })
    
    results_df = pd.DataFrame(results)
    
    # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ê¸°
    best_idx = results_df['final_loss'].idxmin()
    best_config = results_df.iloc[best_idx]
    
    print(f"ìµœì  ì„¤ì •:")
    print(f"í•™ìŠµë¥ : {best_config['learning_rate']}")
    print(f"ë°°ì¹˜ í¬ê¸°: {best_config['batch_size']}")
    print(f"ìµœì¢… ì†ì‹¤: {best_config['final_loss']:.4f}")
    
    return results_df, best_config

if __name__ == "__main__":
    results_df, best_config = analyze_sweep_results("/path/to/sweep_logs")
```

## ë¡± ì»¨í…ìŠ¤íŠ¸ í›ˆë ¨

### 1. ProLong-64K ë°ì´í„°ë¡œ 32K ì»¨í…ìŠ¤íŠ¸ í›ˆë ¨

```bash
#!/bin/bash
# long_context_training.sh

# ProLong-64K ë°ì´í„° ì „ì²˜ë¦¬ (ì‚¬ì „ í† í°í™”)
python scripts/preprocess_prolong.py \
    --input_dir /path/to/prolong_raw \
    --output_dir /path/to/prolong_tokenized \
    --max_length 65536 \
    --shuffle

# 32K ì‹œí€€ìŠ¤ ê¸¸ì´ë¡œ í›ˆë ¨
torchrun --nnodes=1 --nproc_per_node=8 \
    --rdzv_backend=c10d \
    --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
    pretrain.py \
    --train_data_dir /path/to/prolong_tokenized \
    --val_data_dir /path/to/prolong_tokenized \
    --train_model transformer \
    --depth 16 \
    --ctx_len 32768 \
    --max_tokens 4e10 \
    --train_name scaling_mup_rbase_varlen
```

### 2. 128K ì»¨í…ìŠ¤íŠ¸ í›ˆë ¨ (ìµœëŒ€ ê·œëª¨)

```bash
#!/bin/bash
# ultra_long_context.sh

# ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œë¡œ 128K ì»¨í…ìŠ¤íŠ¸ í›ˆë ¨
torchrun --nnodes=4 --nproc_per_node=8 \
    --rdzv_backend=c10d \
    --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
    pretrain.py \
    --train_data_dir /path/to/prolong_tokenized \
    --train_model transformer \
    --depth 20 \
    --ctx_len 131072 \
    --fsdp_save_mem=true \
    --train_name ultra_long_context \
    --max_tokens 2e10
```

### 3. ê°€ë³€ ê¸¸ì´ í›ˆë ¨ ì„¤ì •

```python
# variable_length_config.py
from lit_gpt.config import Config

def setup_variable_length_training():
    """ê°€ë³€ ê¸¸ì´ í›ˆë ¨ì„ ìœ„í•œ ì„¤ì •"""
    config = Config(
        # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
        n_layer=16,
        n_head=16,
        n_embd=2048,
        
        # ê°€ë³€ ê¸¸ì´ ì„¤ì •
        variable_length=True,
        pack_sequences=True,
        eos_token_id=2,  # EOS í† í°ìœ¼ë¡œ ë¬¸ì„œ ë¶„ë¦¬
        
        # RoPE ì„¤ì • (ê¸´ ì»¨í…ìŠ¤íŠ¸ìš©)
        rope_base=10000,  # ê¸°ë³¸ê°’
        rope_scaling=None,  # ë˜ëŠ” {"type": "linear", "factor": 2.0}
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        use_gradient_checkpointing=True,
        attention_dropout=0.0,
        residual_dropout=0.1
    )
    
    return config
```

## Mamba ì•„í‚¤í…ì²˜ í›ˆë ¨

### 1. Mamba-1 ì„¤ì¹˜

```bash
# ê°€ë³€ ê¸¸ì´ Mamba ì„¤ì¹˜
git clone https://github.com/zigzagcai/varlen_mamba.git --branch feat/add-cu_seqlens
cd varlen_mamba
pip install --no-build-isolation -e .
```

### 2. Mamba ëª¨ë¸ í›ˆë ¨

```bash
#!/bin/bash
# mamba_training.sh

torchrun --nnodes=2 --nproc_per_node=8 \
    --rdzv_backend=c10d \
    --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
    pretrain.py \
    --train_data_dir /path/to/data \
    --train_model mamba \
    --depth 16 \
    --ctx_len 16384 \
    --train_name mamba_varlen \
    --max_tokens 1e11
```

## í¬ê´„ì  í‰ê°€ ì‹œìŠ¤í…œ

### 1. í‘œì¤€ NLP ë²¤ì¹˜ë§ˆí¬

```bash
#!/bin/bash
# standard_evaluation.sh

python eval.py \
    --model ArchScale \
    --model_args pretrained=/path/to/checkpoint.pth,config="sambay_d16" \
    --tasks wikitext,lambada_openai,arc_easy,arc_challenge,winogrande,hellaswag,piqa,social_iqa \
    --device cuda \
    --batch_size 16 \
    --trust_remote_code \
    --output_path /path/to/eval_results.json
```

### 2. ë¡± ì»¨í…ìŠ¤íŠ¸ í‰ê°€

#### RULER ë²¤ì¹˜ë§ˆí¬

```bash
#!/bin/bash
# ruler_evaluation.sh

# Needle-in-a-Haystack í…ŒìŠ¤íŠ¸
python eval.py \
    --model ArchScale \
    --model_args pretrained=/path/to/checkpoint.pth,config="sambay_d16",max_length=32768,tokenizer=Orkhan/llama-2-7b-absa \
    --metadata='{"max_seq_lengths":[32768]}' \
    --tasks niah_single_1,niah_single_2,niah_single_3 \
    --device cuda \
    --batch_size 4 \
    --trust_remote_code
```

#### Phonebook í‰ê°€

```bash
#!/bin/bash
# phonebook_evaluation.sh

python eval_phonebook.py \
    --checkpoint_path /path/to/checkpoint.pth \
    --config "sambay_d16" \
    --min_eval_len 1850 \
    --max_eval_len 32000 \
    --output_dir /path/to/phonebook_results \
    --eval_batch_size 4 \
    --num_samples 1000
```

### 3. ì¶”ë¡  ëŠ¥ë ¥ í‰ê°€

```bash
#!/bin/bash
# reasoning_evaluation.sh

# ìˆ˜í•™/ê³¼í•™ ì¶”ë¡  í‰ê°€ (vLLM ë°±ì—”ë“œ í•„ìš”)
./eval_reason.sh microsoft/Phi-4-mini-flash-reasoning aime24 /path/to/output_dir

# GSM8K ìˆ˜í•™ ë¬¸ì œ í•´ê²°
./eval_reason.sh microsoft/Phi-4-mini-flash-reasoning gsm8k /path/to/output_dir

# MATH ë°ì´í„°ì…‹ í‰ê°€
./eval_reason.sh microsoft/Phi-4-mini-flash-reasoning math /path/to/output_dir
```

### 4. í‰ê°€ ê²°ê³¼ ë¶„ì„

```python
# evaluation_analysis.py
import json
import pandas as pd
import matplotlib.pyplot as plt

def analyze_evaluation_results(results_dir):
    """í‰ê°€ ê²°ê³¼ ì¢…í•© ë¶„ì„"""
    
    # í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¡œë“œ
    with open(f"{results_dir}/standard_eval.json", 'r') as f:
        standard_results = json.load(f)
    
    # ë¡± ì»¨í…ìŠ¤íŠ¸ ê²°ê³¼ ë¡œë“œ
    with open(f"{results_dir}/phonebook_results.json", 'r') as f:
        phonebook_results = json.load(f)
    
    # ì¶”ë¡  ê²°ê³¼ ë¡œë“œ
    with open(f"{results_dir}/reasoning_results.json", 'r') as f:
        reasoning_results = json.load(f)
    
    # ê²°ê³¼ ì •ë¦¬
    summary = {
        "Standard NLP": {
            "HellaSwag": standard_results.get("hellaswag", {}).get("acc_norm", 0),
            "PIQA": standard_results.get("piqa", {}).get("acc_norm", 0),
            "WinoGrande": standard_results.get("winogrande", {}).get("acc", 0),
            "ARC-E": standard_results.get("arc_easy", {}).get("acc_norm", 0),
            "ARC-C": standard_results.get("arc_challenge", {}).get("acc_norm", 0)
        },
        "Long Context": {
            "Phonebook (16K)": phonebook_results.get("16384", {}).get("accuracy", 0),
            "Phonebook (32K)": phonebook_results.get("32768", {}).get("accuracy", 0),
        },
        "Reasoning": {
            "GSM8K": reasoning_results.get("gsm8k", {}).get("accuracy", 0),
            "AIME": reasoning_results.get("aime24", {}).get("accuracy", 0),
            "MATH": reasoning_results.get("math", {}).get("accuracy", 0)
        }
    }
    
    # ê²°ê³¼ ì‹œê°í™”
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    for idx, (category, scores) in enumerate(summary.items()):
        tasks = list(scores.keys())
        values = list(scores.values())
        
        axes[idx].bar(tasks, values)
        axes[idx].set_title(f"{category} Performance")
        axes[idx].set_ylabel("Accuracy")
        axes[idx].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig(f"{results_dir}/evaluation_summary.png", dpi=300, bbox_inches='tight')
    
    return summary

# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    summary = analyze_evaluation_results("/path/to/eval_results")
    print("í‰ê°€ ê²°ê³¼ ìš”ì•½:")
    for category, scores in summary.items():
        print(f"\n{category}:")
        for task, score in scores.items():
            print(f"  {task}: {score:.3f}")
```

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 1. Weights & Biases í†µí•©

```python
# wandb_integration.py
import wandb
from datetime import datetime

def setup_wandb_logging(config):
    """W&B ë¡œê¹… ì„¤ì •"""
    
    # í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
    wandb.init(
        project="archscale-pretraining",
        name=f"experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        config={
            "model_architecture": config.train_model,
            "depth": config.depth,
            "context_length": config.ctx_len,
            "max_tokens": config.max_tokens,
            "learning_rate": config.base_hps.eta0,
            "batch_size": config.base_hps.b0,
            "scaling_method": "muP++" if "mup" in config.train_name else "standard"
        },
        tags=[config.train_model, f"depth_{config.depth}", config.train_name]
    )
    
    return wandb

def log_training_metrics(step, metrics):
    """í›ˆë ¨ ë©”íŠ¸ë¦­ ë¡œê¹…"""
    wandb.log({
        "step": step,
        "train/loss": metrics["train_loss"],
        "train/perplexity": metrics["train_ppl"],
        "val/loss": metrics["val_loss"],
        "val/perplexity": metrics["val_ppl"],
        "lr": metrics["learning_rate"],
        "gpu_memory": metrics["gpu_memory_gb"],
        "tokens_per_second": metrics["tokens_per_sec"]
    })

def log_evaluation_results(eval_results):
    """í‰ê°€ ê²°ê³¼ ë¡œê¹…"""
    for task, result in eval_results.items():
        wandb.log({
            f"eval/{task}": result["accuracy"],
            f"eval/{task}_samples": result["num_samples"]
        })
```

### 2. ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§

```python
# system_monitor.py
import psutil
import GPUtil
import torch
import time
from threading import Thread

class SystemMonitor:
    def __init__(self, log_interval=60):
        self.log_interval = log_interval
        self.monitoring = False
        
    def start_monitoring(self):
        """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitoring = True
        monitor_thread = Thread(target=self._monitor_loop)
        monitor_thread.daemon = True
        monitor_thread.start()
        
    def stop_monitoring(self):
        """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring = False
        
    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring:
            metrics = self._collect_metrics()
            self._log_metrics(metrics)
            time.sleep(self.log_interval)
    
    def _collect_metrics(self):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        # CPU ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        # GPU ë©”íŠ¸ë¦­
        gpus = GPUtil.getGPUs()
        gpu_metrics = []
        for gpu in gpus:
            gpu_metrics.append({
                "id": gpu.id,
                "utilization": gpu.load * 100,
                "memory_used": gpu.memoryUsed,
                "memory_total": gpu.memoryTotal,
                "temperature": gpu.temperature
            })
        
        # PyTorch ë©”ëª¨ë¦¬ (CUDA)
        torch_memory = {}
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                torch_memory[f"cuda_{i}"] = {
                    "allocated": torch.cuda.memory_allocated(i) / 1024**3,  # GB
                    "reserved": torch.cuda.memory_reserved(i) / 1024**3,    # GB
                    "max_allocated": torch.cuda.max_memory_allocated(i) / 1024**3
                }
        
        return {
            "timestamp": time.time(),
            "cpu_percent": cpu_percent,
            "memory_percent": memory.percent,
            "memory_available_gb": memory.available / 1024**3,
            "gpu_metrics": gpu_metrics,
            "torch_memory": torch_memory
        }
    
    def _log_metrics(self, metrics):
        """ë©”íŠ¸ë¦­ ë¡œê¹…"""
        print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] System Metrics:")
        print(f"  CPU: {metrics['cpu_percent']:.1f}%")
        print(f"  Memory: {metrics['memory_percent']:.1f}%")
        
        for gpu in metrics['gpu_metrics']:
            print(f"  GPU {gpu['id']}: {gpu['utilization']:.1f}% | "
                  f"{gpu['memory_used']:.1f}GB/{gpu['memory_total']:.1f}GB | "
                  f"{gpu['temperature']}Â°C")
        
        # W&Bì—ë„ ë¡œê¹…
        if wandb.run is not None:
            wandb.log({
                "system/cpu_percent": metrics['cpu_percent'],
                "system/memory_percent": metrics['memory_percent'],
                **{f"system/gpu_{gpu['id']}_util": gpu['utilization'] 
                   for gpu in metrics['gpu_metrics']},
                **{f"system/gpu_{gpu['id']}_memory": gpu['memory_used'] 
                   for gpu in metrics['gpu_metrics']}
            })
```

## í”„ë¡œë•ì…˜ ë°°í¬ ì „ëµ

### 1. ëª¨ë¸ ë³€í™˜ ë° ìµœì í™”

```python
# model_optimization.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from lit_gpt.model import GPT
from lit_gpt.config import Config

def convert_archscale_to_hf(checkpoint_path, config_name, output_dir):
    """ArchScale ëª¨ë¸ì„ HuggingFace í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    
    # ArchScale ëª¨ë¸ ë¡œë“œ
    config = Config.from_name(config_name)
    model = GPT(config)
    
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
    model.load_state_dict(checkpoint["model"])
    
    # HuggingFace í¬ë§·ìœ¼ë¡œ ë³€í™˜
    # (ì‹¤ì œ êµ¬í˜„ì€ ì•„í‚¤í…ì²˜ë³„ë¡œ ë‹¤ë¦„)
    hf_model = convert_to_hf_format(model, config)
    
    # ì €ì¥
    hf_model.save_pretrained(output_dir)
    
    return hf_model

def quantize_model(model_path, output_path, quantization_type="int8"):
    """ëª¨ë¸ ì–‘ìí™”"""
    import torch.quantization as quant
    
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    if quantization_type == "int8":
        model = quant.quantize_dynamic(
            model, {torch.nn.Linear}, dtype=torch.qint8
        )
    elif quantization_type == "int4":
        # BitsAndBytes ì‚¬ìš©
        from transformers import BitsAndBytesConfig
        
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=quantization_config,
            device_map="auto"
        )
    
    model.save_pretrained(output_path)
    return model
```

### 2. ì„œë¹™ ìµœì í™”

```python
# serving_optimization.py
import vllm
from vllm import LLM, SamplingParams
import ray

class ArchScaleInferenceServer:
    def __init__(self, model_path, gpu_memory_utilization=0.9):
        """ArchScale ëª¨ë¸ ì„œë¹™ ì„œë²„"""
        
        # vLLM ì—”ì§„ ì´ˆê¸°í™”
        self.llm = LLM(
            model=model_path,
            gpu_memory_utilization=gpu_memory_utilization,
            max_model_len=32768,  # ë¡± ì»¨í…ìŠ¤íŠ¸ ì§€ì›
            enable_prefix_caching=True,  # í”„ë¦¬í”½ìŠ¤ ìºì‹±ìœ¼ë¡œ ì†ë„ í–¥ìƒ
            enforce_eager=False,  # CUDA ê·¸ë˜í”„ ìµœì í™”
        )
        
        # ê¸°ë³¸ ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
        self.default_sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=1024,
            repetition_penalty=1.1
        )
    
    def generate(self, prompts, sampling_params=None):
        """ë°°ì¹˜ ìƒì„±"""
        if sampling_params is None:
            sampling_params = self.default_sampling_params
            
        outputs = self.llm.generate(prompts, sampling_params)
        
        results = []
        for output in outputs:
            results.append({
                "prompt": output.prompt,
                "generated_text": output.outputs[0].text,
                "finish_reason": output.outputs[0].finish_reason,
                "tokens": len(output.outputs[0].token_ids)
            })
        
        return results
    
    async def generate_stream(self, prompt, sampling_params=None):
        """ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""
        if sampling_params is None:
            sampling_params = self.default_sampling_params
            
        # vLLM ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„
        for output in self.llm.generate_stream([prompt], sampling_params):
            yield output.outputs[0].text

# Ray Serveë¥¼ í™œìš©í•œ í™•ì¥ ê°€ëŠ¥í•œ ë°°í¬
@ray.serve.deployment(num_replicas=2)
class ArchScaleService:
    def __init__(self, model_path):
        self.inference_server = ArchScaleInferenceServer(model_path)
    
    async def __call__(self, request):
        prompts = request["prompts"]
        sampling_params = SamplingParams(**request.get("sampling_params", {}))
        
        results = self.inference_server.generate(prompts, sampling_params)
        return {"results": results}
```

### 3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

```python
# performance_benchmark.py
import time
import asyncio
import aiohttp
import statistics
from concurrent.futures import ThreadPoolExecutor

class PerformanceBenchmark:
    def __init__(self, server_url="http://localhost:8000"):
        self.server_url = server_url
        
    async def single_request_latency(self, prompt, num_requests=100):
        """ë‹¨ì¼ ìš”ì²­ ì§€ì—°ì‹œê°„ ì¸¡ì •"""
        latencies = []
        
        async with aiohttp.ClientSession() as session:
            for _ in range(num_requests):
                start_time = time.time()
                
                async with session.post(
                    f"{self.server_url}/generate",
                    json={"prompts": [prompt]}
                ) as response:
                    await response.json()
                
                latency = time.time() - start_time
                latencies.append(latency)
        
        return {
            "mean_latency": statistics.mean(latencies),
            "median_latency": statistics.median(latencies),
            "p95_latency": sorted(latencies)[int(0.95 * len(latencies))],
            "p99_latency": sorted(latencies)[int(0.99 * len(latencies))]
        }
    
    async def throughput_test(self, prompt, concurrent_requests=50, duration_seconds=60):
        """ì²˜ë¦¬ëŸ‰ ì¸¡ì •"""
        completed_requests = 0
        start_time = time.time()
        
        async def worker():
            nonlocal completed_requests
            async with aiohttp.ClientSession() as session:
                while time.time() - start_time < duration_seconds:
                    try:
                        async with session.post(
                            f"{self.server_url}/generate",
                            json={"prompts": [prompt]}
                        ) as response:
                            await response.json()
                            completed_requests += 1
                    except Exception as e:
                        print(f"Request failed: {e}")
        
        # ë™ì‹œ ìš”ì²­ ì‹¤í–‰
        tasks = [worker() for _ in range(concurrent_requests)]
        await asyncio.gather(*tasks)
        
        actual_duration = time.time() - start_time
        throughput = completed_requests / actual_duration
        
        return {
            "total_requests": completed_requests,
            "duration_seconds": actual_duration,
            "requests_per_second": throughput
        }
    
    def token_generation_speed(self, prompts, output_lengths):
        """í† í° ìƒì„± ì†ë„ ì¸¡ì •"""
        total_tokens = 0
        total_time = 0
        
        for prompt, expected_length in zip(prompts, output_lengths):
            start_time = time.time()
            
            # ì‹¤ì œ ìš”ì²­ (ë™ê¸°ì‹)
            import requests
            response = requests.post(
                f"{self.server_url}/generate",
                json={
                    "prompts": [prompt],
                    "sampling_params": {"max_tokens": expected_length}
                }
            )
            
            generation_time = time.time() - start_time
            actual_tokens = len(response.json()["results"][0]["generated_text"].split())
            
            total_tokens += actual_tokens
            total_time += generation_time
        
        return {
            "total_tokens": total_tokens,
            "total_time": total_time,
            "tokens_per_second": total_tokens / total_time
        }

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì˜ˆì‹œ
async def run_benchmark():
    benchmark = PerformanceBenchmark()
    
    test_prompt = "Explain the concept of machine learning in simple terms:"
    
    # ì§€ì—°ì‹œê°„ í…ŒìŠ¤íŠ¸
    latency_results = await benchmark.single_request_latency(test_prompt)
    print("Latency Results:", latency_results)
    
    # ì²˜ë¦¬ëŸ‰ í…ŒìŠ¤íŠ¸
    throughput_results = await benchmark.throughput_test(test_prompt)
    print("Throughput Results:", throughput_results)
    
    # í† í° ìƒì„± ì†ë„ í…ŒìŠ¤íŠ¸
    speed_results = benchmark.token_generation_speed([test_prompt], [100])
    print("Generation Speed:", speed_results)

if __name__ == "__main__":
    asyncio.run(run_benchmark())
```

## ì‹¤ì „ ìš´ì˜ ê°€ì´ë“œ

### 1. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

```python
# checkpoint_manager.py
import torch
import shutil
from pathlib import Path
import json
from datetime import datetime

class CheckpointManager:
    def __init__(self, base_dir, max_checkpoints=5):
        self.base_dir = Path(base_dir)
        self.max_checkpoints = max_checkpoints
        self.metadata_file = self.base_dir / "checkpoint_metadata.json"
        
    def save_checkpoint(self, model, optimizer, scheduler, step, metrics):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_dir = self.base_dir / f"checkpoint_{step}"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ìƒíƒœ ì €ì¥
        torch.save({
            "model": model.state_dict(),
            "optimizer": optimizer.state_dict(),
            "scheduler": scheduler.state_dict(),
            "step": step,
            "metrics": metrics
        }, checkpoint_dir / "model.pth")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "step": step,
            "timestamp": datetime.now().isoformat(),
            "metrics": metrics,
            "path": str(checkpoint_dir)
        }
        
        self._update_metadata(metadata)
        self._cleanup_old_checkpoints()
        
        return checkpoint_dir
    
    def load_latest_checkpoint(self):
        """ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if not self.metadata_file.exists():
            return None
            
        with open(self.metadata_file, 'r') as f:
            all_metadata = json.load(f)
        
        if not all_metadata:
            return None
            
        # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        latest = max(all_metadata, key=lambda x: x["step"])
        checkpoint_path = Path(latest["path"]) / "model.pth"
        
        if checkpoint_path.exists():
            return torch.load(checkpoint_path), latest
        
        return None
    
    def _update_metadata(self, new_metadata):
        """ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        all_metadata = []
        
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r') as f:
                all_metadata = json.load(f)
        
        all_metadata.append(new_metadata)
        
        with open(self.metadata_file, 'w') as f:
            json.dump(all_metadata, f, indent=2)
    
    def _cleanup_old_checkpoints(self):
        """ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
        with open(self.metadata_file, 'r') as f:
            all_metadata = json.load(f)
        
        if len(all_metadata) <= self.max_checkpoints:
            return
        
        # ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
        sorted_metadata = sorted(all_metadata, key=lambda x: x["step"])
        to_remove = sorted_metadata[:-self.max_checkpoints]
        
        for metadata in to_remove:
            checkpoint_path = Path(metadata["path"])
            if checkpoint_path.exists():
                shutil.rmtree(checkpoint_path)
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        remaining_metadata = sorted_metadata[-self.max_checkpoints:]
        with open(self.metadata_file, 'w') as f:
            json.dump(remaining_metadata, f, indent=2)
```

### 2. ìë™í™”ëœ ì‹¤í—˜ ê´€ë¦¬

```python
# experiment_manager.py
import yaml
import subprocess
import time
from pathlib import Path
import wandb

class ExperimentManager:
    def __init__(self, config_file):
        with open(config_file, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.experiments = self.config["experiments"]
        self.base_command = self.config["base_command"]
        
    def run_experiment_suite(self):
        """ì‹¤í—˜ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
        results = {}
        
        for exp_name, exp_config in self.experiments.items():
            print(f"Starting experiment: {exp_name}")
            
            # ì‹¤í—˜ ì‹¤í–‰
            result = self._run_single_experiment(exp_name, exp_config)
            results[exp_name] = result
            
            # ê²°ê³¼ ë¡œê¹…
            self._log_experiment_result(exp_name, result)
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        self._generate_summary_report(results)
        
        return results
    
    def _run_single_experiment(self, exp_name, exp_config):
        """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
        
        # ëª…ë ¹ì–´ ìƒì„±
        command = self._build_command(exp_config)
        
        # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        
        try:
            # ì‹¤í—˜ ì‹¤í–‰
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=exp_config.get("timeout", 3600*24)  # 24ì‹œê°„ ê¸°ë³¸
            )
            
            duration = time.time() - start_time
            
            if result.returncode == 0:
                # ì„±ê³µì ì¸ ì‹¤í—˜
                metrics = self._extract_metrics(result.stdout)
                return {
                    "status": "success",
                    "duration": duration,
                    "metrics": metrics,
                    "stdout": result.stdout[-1000:],  # ë§ˆì§€ë§‰ 1000ìë§Œ
                    "stderr": result.stderr
                }
            else:
                # ì‹¤íŒ¨í•œ ì‹¤í—˜
                return {
                    "status": "failed",
                    "duration": duration,
                    "error": result.stderr,
                    "returncode": result.returncode
                }
                
        except subprocess.TimeoutExpired:
            return {
                "status": "timeout",
                "duration": time.time() - start_time,
                "error": "Experiment timed out"
            }
        except Exception as e:
            return {
                "status": "error",
                "duration": time.time() - start_time,
                "error": str(e)
            }
    
    def _build_command(self, exp_config):
        """ì‹¤í—˜ ëª…ë ¹ì–´ ìƒì„±"""
        command_parts = [self.base_command]
        
        for key, value in exp_config.items():
            if key in ["timeout", "description"]:
                continue
                
            if isinstance(value, bool):
                if value:
                    command_parts.append(f"--{key}")
            else:
                command_parts.append(f"--{key} {value}")
        
        return " ".join(command_parts)
    
    def _extract_metrics(self, stdout):
        """ì¶œë ¥ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        metrics = {}
        
        for line in stdout.split('\n'):
            if 'final_loss:' in line:
                try:
                    metrics['final_loss'] = float(line.split('final_loss:')[1].strip())
                except:
                    pass
            elif 'best_val_loss:' in line:
                try:
                    metrics['best_val_loss'] = float(line.split('best_val_loss:')[1].strip())
                except:
                    pass
        
        return metrics

# ì‹¤í—˜ ì„¤ì • ì˜ˆì‹œ (experiments.yaml)
experiments_yaml = """
base_command: "torchrun --nnodes=1 --nproc_per_node=8 pretrain.py"

experiments:
  small_lr_sweep_1:
    train_data_dir: "/path/to/data"
    train_model: "transformer"
    depth: 8
    base_hps.eta0: 1e-4
    max_tokens: 1e10
    train_name: "lr_sweep_1e4"
    timeout: 7200
    description: "Learning rate 1e-4 test"
    
  small_lr_sweep_2:
    train_data_dir: "/path/to/data"
    train_model: "transformer"
    depth: 8
    base_hps.eta0: 5e-4
    max_tokens: 1e10
    train_name: "lr_sweep_5e4"
    timeout: 7200
    description: "Learning rate 5e-4 test"
    
  small_lr_sweep_3:
    train_data_dir: "/path/to/data"
    train_model: "transformer"
    depth: 8
    base_hps.eta0: 1e-3
    max_tokens: 1e10
    train_name: "lr_sweep_1e3"
    timeout: 7200
    description: "Learning rate 1e-3 test"
"""
```

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### 1. ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ë° í•´ê²°ë°©ë²•

```python
# troubleshooting.py

class ArchScaleTroubleshooter:
    @staticmethod
    def diagnose_oom_error():
        """OOM ì˜¤ë¥˜ ì§„ë‹¨ ë° í•´ê²°ì±…"""
        print("ğŸ” Out of Memory (OOM) ì˜¤ë¥˜ ì§„ë‹¨:")
        print("1. GPU ë©”ëª¨ë¦¬ í™•ì¸:")
        print("   nvidia-smi")
        print("\n2. ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°:")
        print("   --base_hps.b0=4194304  # ê¸°ë³¸ê°’ì˜ ì ˆë°˜")
        print("\n3. Gradient Checkpointing í™œì„±í™”:")
        print("   --gradient_checkpointing=true")
        print("\n4. FSDP ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ:")
        print("   --fsdp_save_mem=true")
        print("\n5. ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¤„ì´ê¸°:")
        print("   --ctx_len=4096  # ê¸°ë³¸ 8192ì—ì„œ ì ˆë°˜ìœ¼ë¡œ")
    
    @staticmethod
    def diagnose_slow_training():
        """í›ˆë ¨ ì†ë„ ì €í•˜ ì§„ë‹¨"""
        print("ğŸ” í›ˆë ¨ ì†ë„ ì €í•˜ ì§„ë‹¨:")
        print("1. ë°ì´í„° ë¡œë”© ë³‘ëª© í™•ì¸:")
        print("   --num_workers=8  # ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜ ì¦ê°€")
        print("\n2. Mixed Precision í™œì„±í™”:")
        print("   --precision=bf16")
        print("\n3. Compiled ëª¨ë“œ í™œìš©:")
        print("   --compile=true")
        print("\n4. Flash Attention í™•ì¸:")
        print("   pip install flash-attn")
        print("\n5. ë°ì´í„°ì…‹ ì‚¬ì „ í† í°í™”:")
        print("   ì‚¬ì „ì— í† í°í™”ëœ ë°ì´í„° ì‚¬ìš©")
    
    @staticmethod
    def diagnose_distributed_issues():
        """ë¶„ì‚° í›ˆë ¨ ë¬¸ì œ ì§„ë‹¨"""
        print("ğŸ” ë¶„ì‚° í›ˆë ¨ ë¬¸ì œ ì§„ë‹¨:")
        print("1. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸:")
        print("   ping $MASTER_ADDR")
        print("\n2. í¬íŠ¸ ì‚¬ìš© ê°€ëŠ¥ í™•ì¸:")
        print("   netstat -an | grep $MASTER_PORT")
        print("\n3. NCCL í™˜ê²½ë³€ìˆ˜ ì„¤ì •:")
        print("   export NCCL_DEBUG=INFO")
        print("   export NCCL_IB_DISABLE=1  # InfiniBand ë¬¸ì œ ì‹œ")
        print("\n4. ë°©í™”ë²½ ì„¤ì • í™•ì¸:")
        print("   í¬íŠ¸ 29500-29510 ë²”ìœ„ ì—´ê¸°")
        print("\n5. ë…¸ë“œ ê°„ ì‹œê°„ ë™ê¸°í™”:")
        print("   ntpdate -s time.nist.gov")
```

### 2. ì„±ëŠ¥ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
#!/bin/bash
# performance_optimization_checklist.sh

echo "ğŸš€ ArchScale ì„±ëŠ¥ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸"

echo "1. í•˜ë“œì›¨ì–´ ì„¤ì • í™•ì¸:"
echo "   - GPU ë©”ëª¨ë¦¬: 24GB+ ê¶Œì¥"
echo "   - GPU ì—°ê²°: NVLink ë˜ëŠ” PCIe 4.0+"
echo "   - ìŠ¤í† ë¦¬ì§€: NVMe SSD ê¶Œì¥"
echo "   - ë„¤íŠ¸ì›Œí¬: 10Gbps+ (ë¶„ì‚° í›ˆë ¨ ì‹œ)"

echo -e "\n2. ì†Œí”„íŠ¸ì›¨ì–´ ìµœì í™”:"
echo "   - PyTorch 2.0+ ì‚¬ìš©"
echo "   - CUDA 12.1+ ê¶Œì¥"
echo "   - Flash Attention 2.0 ì„¤ì¹˜"
echo "   - xFormers ì„¤ì¹˜ ê³ ë ¤"

echo -e "\n3. í›ˆë ¨ ì„¤ì • ìµœì í™”:"
echo "   - Mixed Precision (bf16) ì‚¬ìš©"
echo "   - Gradient Accumulation ì¡°ì •"
echo "   - DataLoader num_workers íŠœë‹"
echo "   - Prefetch factor ì¡°ì •"

echo -e "\n4. ë©”ëª¨ë¦¬ ìµœì í™”:"
echo "   - FSDP (Fully Sharded Data Parallel) ì‚¬ìš©"
echo "   - Gradient Checkpointing í™œì„±í™”"
echo "   - Model Sharding ê³ ë ¤"
echo "   - CPU Offloading (í•„ìš” ì‹œ)"

echo -e "\n5. ë°ì´í„° íŒŒì´í”„ë¼ì¸:"
echo "   - ì‚¬ì „ í† í°í™”ëœ ë°ì´í„° ì‚¬ìš©"
echo "   - ë°ì´í„° ìºì‹± í™œì„±í™”"
echo "   - ë³‘ë ¬ ë°ì´í„° ë¡œë”©"
echo "   - ì••ì¶•ëœ ë°ì´í„° í¬ë§· ì‚¬ìš©"
```

## ê²°ë¡ 

**Microsoft ArchScale**ì€ ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ ì‚¬ì „ í›ˆë ¨ì„ ìœ„í•œ ê°•ë ¥í•˜ê³  ìœ ì—°í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œ ë‹¤ë£¬ ë‚´ìš©ë“¤ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì„±ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### ğŸ¯ í•µì‹¬ ì„±ê³¼

1. **ë¹„ìš© íš¨ìœ¨ì ì¸ ì—°êµ¬**: Î¼P++ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„ìš© 90% ì ˆê°
2. **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: 110Më¶€í„° ìˆ˜ì‹­ì–µ íŒŒë¼ë¯¸í„°ê¹Œì§€ ì¼ê´€ëœ í›ˆë ¨ íŒŒì´í”„ë¼ì¸
3. **ë¡± ì»¨í…ìŠ¤íŠ¸ ì§€ì›**: 128K í† í°ê¹Œì§€ íš¨ìœ¨ì ì¸ í›ˆë ¨
4. **í¬ê´„ì ì¸ í‰ê°€**: RULER, Phonebook, ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ë¥¼ í†µí•œ ë‹¤ë©´ì  ì„±ëŠ¥ í‰ê°€

### ğŸš€ ì‹¤ë¬´ ì ìš© í¬ì¸íŠ¸

- **ìŠ¤íƒ€íŠ¸ì—…**: ì œí•œëœ ìì›ìœ¼ë¡œ íš¨ê³¼ì ì¸ ëª¨ë¸ ì—°êµ¬
- **ì—°êµ¬ê¸°ê´€**: ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ ì‹¤í—˜ê³¼ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ ì—°êµ¬
- **ê¸°ì—…**: ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ì˜ íš¨ìœ¨ì ì¸ ì‚¬ì „ í›ˆë ¨
- **ê°œë°œì**: ìµœì‹  LLM ê¸°ìˆ ì„ í™œìš©í•œ í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œ êµ¬ì¶•

### ğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„

1. **ì†Œê·œëª¨ ì‹¤í—˜**: 8 GPUë¡œ ì‹œì‘í•˜ì—¬ Î¼P++ ìŠ¤ì¼€ì¼ë§ ê²€ì¦
2. **ì•„í‚¤í…ì²˜ íƒìƒ‰**: SambaY, Mamba ë“± ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ ë¹„êµ
3. **í”„ë¡œë•ì…˜ ìµœì í™”**: vLLM, quantizationì„ í†µí•œ ì„œë¹™ ìµœì í™”
4. **ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬**: [ArchScale GitHub](https://github.com/microsoft/ArchScale)ì— ê°œì„ ì‚¬í•­ ê¸°ì—¬

ArchScaleì„ í†µí•´ ì—¬ëŸ¬ë¶„ë§Œì˜ ì°¨ì„¸ëŒ€ ì–¸ì–´ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê°œë°œí•´ë³´ì„¸ìš”! ğŸš€

---

**ì°¸ê³  ìë£Œ**:
- [Microsoft ArchScale GitHub](https://github.com/microsoft/ArchScale)
- [Î¼P++ ë…¼ë¬¸](https://arxiv.org/abs/2507.06607)
- [Flash Attention ë…¼ë¬¸](https://arxiv.org/abs/2205.14135)
- [RULER ë²¤ì¹˜ë§ˆí¬](https://arxiv.org/abs/2404.06654) 