---
title: "Axolotl: í†µí•© LLM íŒŒì¸íŠœë‹ í”„ë ˆì„ì›Œí¬ ì™„ì „ ê°€ì´ë“œ"
excerpt: "ë‹¤ì–‘í•œ AI ëª¨ë¸ì˜ í¬ìŠ¤íŠ¸ íŠ¸ë ˆì´ë‹ì„ ê°„ì†Œí™”í•˜ëŠ” Axolotl í”„ë ˆì„ì›Œí¬ì˜ ê¸°ëŠ¥, ì„¤ì¹˜, ì‚¬ìš©ë²• ë° ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ"
date: 2025-06-17
categories:
  - llmops
tags:
  - Axolotl
  - Fine-tuning
  - LoRA
  - QLoRA
  - DPO
  - Multi-GPU Training
  - LLM Training
  - Post-training
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
---

## ê°œìš”

Axolotlì€ ë‹¤ì–‘í•œ AI ëª¨ë¸ì˜ í¬ìŠ¤íŠ¸ íŠ¸ë ˆì´ë‹ì„ ê°„ì†Œí™”í•˜ë„ë¡ ì„¤ê³„ëœ í†µí•© í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. LLaMA, Mistral, Mixtral, Pythia ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì§€ì›í•˜ë©°, HuggingFace transformersì˜ causal language modelê³¼ í˜¸í™˜ë©ë‹ˆë‹¤.

**GitHub í†µê³„**:

- â­ **9.6k stars**
- ğŸ´ **1k forks**
- ğŸ‘¥ **198+ contributors**
- ğŸ“¦ **16 releases**

---

## ì£¼ìš” ê¸°ëŠ¥

### ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›

**ì§€ì› ëª¨ë¸**:

- LLaMA (Llama 4 í¬í•¨)
- Mistral (Magistral í¬í•¨)
- Mixtral
- Pythia
- HuggingFace transformers causal language models ì „ì²´

### í›ˆë ¨ ë°©ë²•ë¡ 

**íŒŒì¸íŠœë‹ ê¸°ë²•**:

- **Full Fine-tuning**: ì „ì²´ ëª¨ë¸ íŒŒë¼ë¯¸í„° í›ˆë ¨
- **LoRA**: Low-Rank Adaptation
- **QLoRA**: Quantized LoRA
- **GPTQ**: Gradient-based Post-Training Quantization
- **QAT**: Quantization Aware Training

**ê³ ê¸‰ í›ˆë ¨ ê¸°ë²•**:

- **Preference Tuning**: DPO, IPO, KTO, ORPO
- **Reinforcement Learning**: GRPO (Group Relative Policy Optimization)
- **Multimodal Training**: ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ íŒŒì¸íŠœë‹
- **Reward Modelling**: RM (Reward Modelling) / PRM (Process Reward Modelling)

### ì„±ëŠ¥ ìµœì í™”

**ë©”ëª¨ë¦¬ ë° ì†ë„ ìµœì í™”**:

- **Multipacking**: íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
- **Flash Attention**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜
- **Xformers**: ìµœì í™”ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ì—°ì‚°
- **Flex Attention**: ìœ ì—°í•œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
- **Liger Kernel**: ê³ ì„±ëŠ¥ ì»¤ë„
- **Cut Cross Entropy**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì†ì‹¤ ê³„ì‚°
- **Sequence Parallelism (SP)**: ì‹œí€€ìŠ¤ ë³‘ë ¬í™”

**ë¶„ì‚° í›ˆë ¨**:

- **Multi-GPU Training**: FSDP1, FSDP2, DeepSpeed
- **Multi-node Training**: Torchrun, Ray
- **LoRA Optimizations**: ë‹¨ì¼/ë‹¤ì¤‘ GPU í™˜ê²½ì—ì„œ LoRA ìµœì í™”

---

## ìµœì‹  ì—…ë°ì´íŠ¸ (2025ë…„)

### 2025ë…„ 6ì›”

**Magistral ì§€ì› ì¶”ê°€** - mistral-common tokenizerë¥¼ ì‚¬ìš©í•˜ëŠ” Magistral ëª¨ë¸ í›ˆë ¨ ì§€ì›

### 2025ë…„ 5ì›”

**Quantization Aware Training (QAT)** - ì–‘ìí™” ì¸ì‹ í›ˆë ¨ ì§€ì› ì¶”ê°€

### 2025ë…„ 4ì›”

**Llama 4 ì§€ì›** - Axolotlì˜ linearized ë²„ì „ìœ¼ë¡œ Llama 4 ëª¨ë¸ í›ˆë ¨ ì§€ì›

### 2025ë…„ 3ì›”

**Sequence Parallelism (SP)** - ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¥ì„ ìœ„í•œ ì‹œí€€ìŠ¤ ë³‘ë ¬í™” êµ¬í˜„
**Multimodal Fine-tuning (Beta)** - ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ íŒŒì¸íŠœë‹ ì§€ì›

### 2025ë…„ 2ì›”

**LoRA ìµœì í™”** - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ ë° í›ˆë ¨ ì†ë„ í–¥ìƒ
**GRPO ì§€ì›** - Group Relative Policy Optimization ì¶”ê°€

### 2025ë…„ 1ì›”

**Reward Modelling** - RM/PRM íŒŒì¸íŠœë‹ ì§€ì› ì¶”ê°€

---

## ì„¤ì¹˜ ë° ì„¤ì •

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

**í•˜ë“œì›¨ì–´**:

- NVIDIA GPU (Ampere ì´ìƒ, `bf16` ë° Flash Attentionìš©)
- AMD GPU ì§€ì›
- Python 3.11
- PyTorch â‰¥2.5.1

### ì„¤ì¹˜ ë°©ë²•

```bash
# ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip3 install -U packaging==23.2 setuptools==75.8.0 wheel ninja

# Axolotl ì„¤ì¹˜ (Flash Attention, DeepSpeed í¬í•¨)
pip3 install --no-build-isolation axolotl[flash-attn,deepspeed]

# ì˜ˆì œ ì„¤ì • íŒŒì¼ ë‹¤ìš´ë¡œë“œ
axolotl fetch examples
axolotl fetch deepspeed_configs  # ì„ íƒì‚¬í•­
```

### Docker ì„¤ì¹˜

```bash
# Docker ì´ë¯¸ì§€ ì‚¬ìš©
docker pull axolotlai/axolotl:main-latest

# Docker Compose ì‹¤í–‰
docker-compose up -d
```

---

## ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

### ì²« ë²ˆì§¸ íŒŒì¸íŠœë‹

```bash
# ì˜ˆì œ ì„¤ì • íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
axolotl fetch examples

# ë˜ëŠ” ì‚¬ìš©ì ì§€ì • ê²½ë¡œ ì§€ì •
axolotl fetch examples --dest path/to/folder

# LoRAë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ í›ˆë ¨
axolotl train examples/llama-3/lora-1b.yml
```

### ê¸°ë³¸ ì„¤ì • íŒŒì¼ ì˜ˆì‹œ

```yaml
# lora-config.yml
base_model: meta-llama/Llama-2-7b-hf
model_type: LlamaForCausalLM

# LoRA ì„¤ì •
adapter: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1
lora_target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj

# ë°ì´í„°ì…‹ ì„¤ì •
datasets:
  - path: alpaca_data.json
    type: alpaca

# í›ˆë ¨ ì„¤ì •
sequence_len: 2048
sample_packing: true
pad_to_sequence_len: true

# í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°
learning_rate: 0.0002
train_on_inputs: false
group_by_length: false
bf16: auto
fp16: false
tf32: false

# ë°°ì¹˜ í¬ê¸° ë° ê·¸ë˜ë””ì–¸íŠ¸ ì„¤ì •
micro_batch_size: 2
gradient_accumulation_steps: 1
num_epochs: 3
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
cosine_eta_min: 0.0

# ì €ì¥ ì„¤ì •
output_dir: ./lora-out
save_strategy: epoch
save_steps: 1000

# ë¡œê¹…
logging_steps: 1
```

---

## ê³ ê¸‰ í›ˆë ¨ ê¸°ë²•

### LoRA íŒŒì¸íŠœë‹

```yaml
# LoRA ìµœì í™” ì„¤ì •
adapter: lora
lora_r: 64
lora_alpha: 128
lora_dropout: 0.05
lora_target_linear: true  # ëª¨ë“  linear layer ëŒ€ìƒ

# LoRA ìµœì í™” í™œì„±í™”
lora_optimizations: true
```

### QLoRA ì„¤ì •

```yaml
# QLoRA ì„¤ì •
adapter: qlora
load_in_4bit: true
load_in_8bit: false

# BitsAndBytes ì„¤ì •
bnb_4bit_compute_dtype: bfloat16
bnb_4bit_use_double_quant: true
bnb_4bit_quant_type: nf4
```

### DPO (Direct Preference Optimization)

```yaml
# DPO ì„¤ì •
rl: dpo
dpo_beta: 0.1

# ì„ í˜¸ë„ ë°ì´í„°ì…‹
datasets:
  - path: Anthropic/hh-rlhf
    type: chatml.intel
    field_human: chosen
    field_assistant: rejected
```

### GRPO (Group Relative Policy Optimization)

```yaml
# GRPO ì„¤ì •
rl: grpo
grpo_beta: 0.01
grpo_reference_free: true

# ë³´ìƒ ëª¨ë¸ ì„¤ì •
reward_model: OpenAssistant/reward-model-deberta-v3-large-v2
```

---

## ë©€í‹°ëª¨ë‹¬ íŒŒì¸íŠœë‹

### ë¹„ì „-ì–¸ì–´ ëª¨ë¸ ì„¤ì •

```yaml
# ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì„¤ì •
model_type: LlavaForCausalLM
base_model: llava-hf/llava-1.5-7b-hf

# ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì •
chat_template: llava
image_token: "<image>"
image_square_pad: true

# ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹
datasets:
  - path: liuhaotian/LLaVA-Instruct-150K
    type: llava_instruct
```

---

## ë¶„ì‚° í›ˆë ¨ ì„¤ì •

### Multi-GPU í›ˆë ¨ (FSDP)

```yaml
# FSDP ì„¤ì •
fsdp:
  - full_shard
  - auto_wrap
fsdp_config:
  fsdp_limit_all_gathers: true
  fsdp_sync_module_states: true
  fsdp_offload_params: false
  fsdp_use_orig_params: false
  fsdp_cpu_ram_efficient_loading: true
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_sharding_strategy: FULL_SHARD
```

### DeepSpeed ì„¤ì •

```yaml
# DeepSpeed ì„¤ì •
deepspeed: deepspeed_configs/zero2.json
```

```json
// deepspeed_configs/zero2.json
{
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "offload_param": {
      "device": "cpu",
      "pin_memory": true
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8,
    "contiguous_gradients": true
  },
  "fp16": {
    "enabled": "auto",
    "loss_scale": 0,
    "loss_scale_window": 1000,
    "initial_scale_power": 16,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": "auto",
      "betas": "auto",
      "eps": "auto",
      "weight_decay": "auto"
    }
  },
  "scheduler": {
    "type": "WarmupLR",
    "params": {
      "warmup_min_lr": "auto",
      "warmup_max_lr": "auto",
      "warmup_num_steps": "auto"
    }
  }
}
```

### Multi-node í›ˆë ¨

```bash
# Torchrunì„ ì‚¬ìš©í•œ multi-node í›ˆë ¨
torchrun --nnodes=2 --nproc_per_node=8 --rdzv_id=100 --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR:29400 \
  -m axolotl.cli.train config.yml

# Rayë¥¼ ì‚¬ìš©í•œ ë¶„ì‚° í›ˆë ¨
ray start --head
axolotl train config.yml --ray
```

---

## ë°ì´í„°ì…‹ ì²˜ë¦¬

### ì§€ì› ë°ì´í„° ì†ŒìŠ¤

**ë¡œì»¬ ë° í´ë¼ìš°ë“œ**:

- ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ
- HuggingFace Datasets
- AWS S3
- Azure Blob Storage
- Google Cloud Storage
- Oracle Cloud Infrastructure (OCI)

### ë°ì´í„°ì…‹ í˜•ì‹

```yaml
# Alpaca í˜•ì‹
datasets:
  - path: alpaca_data.json
    type: alpaca

# ShareGPT í˜•ì‹
datasets:
  - path: ShareGPT_V3_unfiltered_cleaned_split.json
    type: sharegpt
    conversation_template: chatml

# ì‚¬ìš©ì ì •ì˜ í˜•ì‹
datasets:
  - path: custom_data.jsonl
    type: completion
    field: text
```

### ë°ì´í„° ì „ì²˜ë¦¬

```python
# ì‚¬ìš©ì ì •ì˜ ë°ì´í„° ì „ì²˜ë¦¬
def custom_preprocess(example):
    """ì‚¬ìš©ì ì •ì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    instruction = example['instruction']
    input_text = example['input']
    output = example['output']
    
    if input_text:
        prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n"
    else:
        prompt = f"### Instruction:\n{instruction}\n\n### Response:\n"
    
    return {
        'text': prompt + output + "<|endoftext|>"
    }

# ì„¤ì • íŒŒì¼ì—ì„œ ì‚¬ìš©
datasets:
  - path: data.json
    type: completion
    preprocess_function: custom_preprocess
```

---

## ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

### ë©”ëª¨ë¦¬ ìµœì í™”

```yaml
# Flash Attention í™œì„±í™”
flash_attention: true

# Gradient Checkpointing
gradient_checkpointing: true

# Mixed Precision
bf16: auto
fp16: false
tf32: true

# Multipacking
sample_packing: true
pad_to_sequence_len: true
```

### ì†ë„ ìµœì í™”

```yaml
# Liger Kernel ì‚¬ìš©
use_liger_kernel: true

# Cut Cross Entropy
use_cut_cross_entropy: true

# Optimized Scheduler
lr_scheduler: cosine_with_restarts
cosine_restarts: 2

# DataLoader ìµœì í™”
dataloader_num_workers: 4
dataloader_pin_memory: true
```

### Sequence Parallelism

```yaml
# SP ì„¤ì •
sequence_parallel: true
sp_size: 2  # SP ê·¸ë£¹ í¬ê¸°

# ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬
sequence_len: 32768
max_packed_sequence_len: 32768
```

---

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### Weights & Biases í†µí•©

```yaml
# W&B ì„¤ì •
wandb_project: axolotl-finetuning
wandb_entity: your-team
wandb_watch: gradients
wandb_name: llama-7b-lora
wandb_log_model: checkpoint

# ë¡œê¹… ì„¤ì •
logging_steps: 10
eval_steps: 500
save_steps: 1000
```

### TensorBoard ì‚¬ìš©

```yaml
# TensorBoard ë¡œê¹…
use_tensorboard: true
tensorboard_log_dir: ./logs

# ë©”íŠ¸ë¦­ ë¡œê¹…
log_metrics: true
log_predictions: true
```

### ì»¤ìŠ¤í…€ ì½œë°±

```python
# ì‚¬ìš©ì ì •ì˜ ì½œë°±
from transformers import TrainerCallback

class CustomCallback(TrainerCallback):
    def on_log(self, args, state, control, model=None, logs=None, **kwargs):
        if logs:
            print(f"Step {state.global_step}: Loss = {logs.get('train_loss', 'N/A')}")
    
    def on_save(self, args, state, control, **kwargs):
        print(f"Model saved at step {state.global_step}")

# ì„¤ì •ì—ì„œ ì‚¬ìš©
callbacks:
  - CustomCallback
```

---

## í‰ê°€ ë° ì¶”ë¡ 

### ëª¨ë¸ í‰ê°€

```yaml
# í‰ê°€ ì„¤ì •
eval_sample_packing: false
eval_table_size: 5
eval_max_new_tokens: 128
do_eval: true
eval_steps: 500

# í‰ê°€ ë°ì´í„°ì…‹
eval_dataset:
  - path: eval_data.json
    type: alpaca
```

### ì¶”ë¡  ì‹¤í–‰

```python
# í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì¶”ë¡ 
from axolotl.utils.models import load_model, load_tokenizer

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model, tokenizer = load_model(
    base_model="meta-llama/Llama-2-7b-hf",
    model_type="LlamaForCausalLM",
    tokenizer_type="LlamaTokenizer",
    cfg=cfg,
    adapter="lora",
    inference=True
)

# ì¶”ë¡  ì‹¤í–‰
def generate_response(prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(prompt):]

# ì‚¬ìš© ì˜ˆì‹œ
prompt = "### Instruction:\nExplain quantum computing\n\n### Response:\n"
response = generate_response(prompt)
print(response)
```

---

## ì‹¤ë¬´ ì ìš© ì‚¬ë¡€

### ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ ê°œë°œ

```yaml
# ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸
base_model: microsoft/BioGPT-Large
datasets:
  - path: medical_qa_dataset.json
    type: alpaca
  - path: medical_literature.jsonl
    type: completion

# ë„ë©”ì¸ íŠ¹í™” í† í¬ë‚˜ì´ì €
special_tokens:
  - "<medical_term>"
  - "<diagnosis>"
  - "<treatment>"
```

### ë‹¤êµ­ì–´ ëª¨ë¸ íŒŒì¸íŠœë‹

```yaml
# ë‹¤êµ­ì–´ ì„¤ì •
base_model: facebook/xglm-7.5B
datasets:
  - path: multilingual_data.json
    type: sharegpt
    
# ì–¸ì–´ë³„ ê°€ì¤‘ì¹˜
dataset_weights:
  - 0.4  # ì˜ì–´
  - 0.3  # í•œêµ­ì–´
  - 0.2  # ì¼ë³¸ì–´
  - 0.1  # ì¤‘êµ­ì–´
```

### ì½”ë“œ ìƒì„± ëª¨ë¸

```yaml
# ì½”ë“œ ìƒì„± íŠ¹í™”
base_model: Salesforce/codegen-16B-multi
datasets:
  - path: code_alpaca.json
    type: alpaca
  - path: github_code.jsonl
    type: completion

# ì½”ë“œ íŠ¹í™” ì„¤ì •
special_tokens:
  - "<code>"
  - "</code>"
  - "<comment>"
  - "</comment>"
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

**ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜**:

```yaml
# ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
micro_batch_size: 1
gradient_accumulation_steps: 8
gradient_checkpointing: true
load_in_8bit: true
```

**í›ˆë ¨ ì†ë„ ê°œì„ **:

```yaml
# ì†ë„ ìµœì í™”
sample_packing: true
flash_attention: true
use_liger_kernel: true
dataloader_num_workers: 8
```

**ìˆ˜ë ´ ë¬¸ì œ**:

```yaml
# í•™ìŠµë¥  ì¡°ì •
learning_rate: 1e-5
lr_scheduler: linear
warmup_steps: 100
weight_decay: 0.01
```

### ë””ë²„ê¹… ë„êµ¬

```bash
# ìƒì„¸ ë¡œê¹… í™œì„±í™”
export AXOLOTL_DEBUG=1
axolotl train config.yml --debug

# í”„ë¡œíŒŒì¼ë§
axolotl train config.yml --profile

# ì„¤ì • ê²€ì¦
axolotl validate config.yml
```

---

## ì»¤ë®¤ë‹ˆí‹° ë° ì§€ì›

### ê³µì‹ ë¦¬ì†ŒìŠ¤

- **ê³µì‹ ë¬¸ì„œ**: [docs.axolotl.ai](https://docs.axolotl.ai)
- **GitHub**: [axolotl-ai-cloud/axolotl](https://github.com/axolotl-ai-cloud/axolotl)
- **Discord**: ì»¤ë®¤ë‹ˆí‹° ì§€ì› ì±„ë„
- **ì˜ˆì œ ë””ë ‰í† ë¦¬**: ë‹¤ì–‘í•œ ì‚¬ìš© ì‚¬ë¡€ ì˜ˆì œ

### ì „ë¬¸ ì§€ì›

**ê¸°ì—… ì§€ì›**: wing@axolotl.aië¡œ ë¬¸ì˜

### ìŠ¤í°ì„œ

**Modal**: í´ë¼ìš°ë“œ ê¸°ë°˜ Gen AI ëª¨ë¸ ë°°í¬ ë° ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ íŒŒì¸íŠœë‹ í”Œë«í¼

---

## ê²°ë¡ 

Axolotlì€ í˜„ëŒ€ì ì¸ LLM íŒŒì¸íŠœë‹ì˜ ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ëŠ” í¬ê´„ì ì¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì£¼ìš” ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

**ê¸°ìˆ ì  ìš°ìˆ˜ì„±**:

- ìµœì‹  íŒŒì¸íŠœë‹ ê¸°ë²• ì§€ì› (LoRA, QLoRA, DPO, GRPO ë“±)
- ê³ ì„±ëŠ¥ ìµœì í™” (Flash Attention, Sequence Parallelism ë“±)
- í™•ì¥ ê°€ëŠ¥í•œ ë¶„ì‚° í›ˆë ¨ (Multi-GPU, Multi-node)

**ì‚¬ìš© í¸ì˜ì„±**:

- ë‹¨ì¼ YAML ì„¤ì • íŒŒì¼ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬
- í’ë¶€í•œ ì˜ˆì œì™€ ë¬¸ì„œ
- í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹° ì§€ì›

**ì‹¤ë¬´ ì ìš©ì„±**:

- ë‹¤ì–‘í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì§€ì›
- í´ë¼ìš°ë“œ ë° ì˜¨í”„ë ˆë¯¸ìŠ¤ í™˜ê²½ í˜¸í™˜
- ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ ê°œë°œ ì§€ì›

Axolotlì„ í†µí•´ ì—°êµ¬ìì™€ ì—”ì§€ë‹ˆì–´ëŠ” ë³µì¡í•œ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì— ì–½ë§¤ì´ì§€ ì•Šê³  ëª¨ë¸ ê°œì„ ê³¼ í˜ì‹ ì— ì§‘ì¤‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§€ì†ì ì¸ ì—…ë°ì´íŠ¸ì™€ ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬ë¥¼ í†µí•´ LLM íŒŒì¸íŠœë‹ì˜ í‘œì¤€ ë„êµ¬ë¡œ ìë¦¬ì¡ê³  ìˆìŠµë‹ˆë‹¤.

ë” ìì„¸í•œ ì •ë³´ëŠ” [Axolotl GitHub ì €ì¥ì†Œ](https://github.com/axolotl-ai-cloud/axolotl)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
