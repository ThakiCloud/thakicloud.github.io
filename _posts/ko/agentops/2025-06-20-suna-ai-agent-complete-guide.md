---
title: "Suna AI Agent ì™„ë²½ ê°€ì´ë“œ: ì˜¤í”ˆì†ŒìŠ¤ ë²”ìš© AI ì—ì´ì „íŠ¸ë¡œ ì‹¤ì œ ì—…ë¬´ ìë™í™”í•˜ê¸°"
excerpt: "Kortix AIì˜ Sunaë¥¼ í™œìš©í•´ ë¸Œë¼ìš°ì € ìë™í™”, ë°ì´í„° ë¶„ì„, íŒŒì¼ ê´€ë¦¬ ë“± ì‹¤ì œ ì—…ë¬´ë¥¼ ìë™í™”í•˜ëŠ” ë°©ë²•ê³¼ ì…€í”„ í˜¸ìŠ¤íŒ… ì„¤ì •ì„ ì™„ë²½í•˜ê²Œ ë§ˆìŠ¤í„°í•˜ëŠ” ì¢…í•© ê°€ì´ë“œì…ë‹ˆë‹¤."
date: 2025-06-20
categories:
  - agentops
tags:
  - suna
  - kortix-ai
  - open-source-agent
  - browser-automation
  - workflow-automation
  - supabase
  - daytona
  - ai-assistant
  - self-hosting
author_profile: true
toc: true
toc_label: Suna Agent Guide
---

## ê°œìš”

[Suna](https://github.com/kortix-ai/suna)ëŠ” Kortix AIì—ì„œ ê°œë°œí•œ **ì˜¤í”ˆì†ŒìŠ¤ ë²”ìš© AI ì—ì´ì „íŠ¸**ë¡œ, ì‹¤ì œ ì—…ë¬´ë¥¼ ëŒ€ì‹  ìˆ˜í–‰í•˜ëŠ” ë””ì§€í„¸ ë™ë°˜ìì…ë‹ˆë‹¤. **14.9k ìŠ¤íƒ€**ì™€ **2.2k í¬í¬**ë¥¼ ê¸°ë¡í•˜ë©° ì˜¤í”ˆì†ŒìŠ¤ AI ì—ì´ì „íŠ¸ ìƒíƒœê³„ì—ì„œ ì£¼ëª©ë°›ê³  ìˆëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ìì—°ì–´ ëŒ€í™”ë¥¼ í†µí•´ ë³µì¡í•œ ì—°êµ¬, ë°ì´í„° ë¶„ì„, ì¼ìƒì ì¸ ê³¼ì œë¥¼ í•´ê²°í•˜ë©°, ê°•ë ¥í•œ ë¸Œë¼ìš°ì € ìë™í™”ì™€ ì§ê´€ì ì¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ ê²°í•©í•˜ì—¬ ì‹¤ì§ˆì ì¸ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### ğŸ¯ Sunaì˜ í•µì‹¬ íŠ¹ì§•

- **ë¸Œë¼ìš°ì € ìë™í™”**: ì›¹ íƒìƒ‰ ë° ë°ì´í„° ì¶”ì¶œ
- **íŒŒì¼ ê´€ë¦¬**: ë¬¸ì„œ ìƒì„± ë° í¸ì§‘
- **ì›¹ í¬ë¡¤ë§**: í™•ì¥ëœ ê²€ìƒ‰ ê¸°ëŠ¥
- **ì‹œìŠ¤í…œ ì‘ì—…**: ëª…ë ¹ì¤„ ì‹¤í–‰
- **ì›¹ì‚¬ì´íŠ¸ ë°°í¬**: ìë™í™”ëœ ë°°í¬ í”„ë¡œì„¸ìŠ¤
- **API í†µí•©**: ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ì™€ì˜ ì—°ë™

## 1. ì•„í‚¤í…ì²˜ ì´í•´

### 1.1 ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ

SunaëŠ” 4ê°œì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "Frontend Layer"
        F[Next.js/React UI]
    end
    
    subgraph "Backend Layer"
        B[Python/FastAPI Server]
    end
    
    subgraph "Execution Layer"
        A[Agent Docker Container]
    end
    
    subgraph "Data Layer"
        D[Supabase Database]
    end
    
    F --> B
    B --> A
    B --> D
    A --> D
```

### 1.2 ì»´í¬ë„ŒíŠ¸ ìƒì„¸ ë¶„ì„

| ì»´í¬ë„ŒíŠ¸ | ê¸°ìˆ  ìŠ¤íƒ | ì£¼ìš” ì—­í•  |
| --- | --- | --- |
| **Backend API** | Python/FastAPI | REST ì—”ë“œí¬ì¸íŠ¸, ìŠ¤ë ˆë“œ ê´€ë¦¬, LLM í†µí•© |
| **Frontend** | Next.js/React | ë°˜ì‘í˜• UI, ì±„íŒ… ì¸í„°í˜ì´ìŠ¤, ëŒ€ì‹œë³´ë“œ |
| **Agent Docker** | Docker | ê²©ë¦¬ëœ ì‹¤í–‰ í™˜ê²½, ë³´ì•ˆ ê¸°ëŠ¥ |
| **Supabase DB** | PostgreSQL | ë°ì´í„° ì§€ì†ì„±, ì¸ì¦, ì‹¤ì‹œê°„ êµ¬ë… |

## 2. ì„¤ì¹˜ ë° ì„¤ì •

### 2.1 ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

```bash
# í•„ìˆ˜ ì†Œí”„íŠ¸ì›¨ì–´
- Docker & Docker Compose
- Python 3.9+
- Node.js 18+
- Git

# ìµœì†Œ í•˜ë“œì›¨ì–´ ìŠ¤í™
- CPU: 2 cores
- RAM: 4GB
- Storage: 10GB
```

### 2.2 Quick Start ì„¤ì¹˜

```bash
# 1. ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/kortix-ai/suna.git
cd suna

# 2. ì„¤ì • ë§ˆë²•ì‚¬ ì‹¤í–‰
python setup.py

# 3. ì»¨í…Œì´ë„ˆ ì‹œì‘
python start.py
```

### 2.3 ì„¤ì • ë§ˆë²•ì‚¬ ê°€ì´ë“œ

ì„¤ì • ë§ˆë²•ì‚¬ëŠ” ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìë™í™”í•©ë‹ˆë‹¤:

**1ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**

```bash
# .env íŒŒì¼ ìƒì„±
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_anon_key
REDIS_URL=redis://localhost:6379
ANTHROPIC_API_KEY=your_anthropic_key
OPENAI_API_KEY=your_openai_key
```

**2ë‹¨ê³„: Supabase í”„ë¡œì íŠ¸ ì„¤ì •**

```sql
-- ì‚¬ìš©ì í…Œì´ë¸”
CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  email TEXT UNIQUE NOT NULL,
  created_at TIMESTAMP DEFAULT NOW()
);

-- ëŒ€í™” í…Œì´ë¸”  
CREATE TABLE conversations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id),
  title TEXT,
  created_at TIMESTAMP DEFAULT NOW()
);

-- ë©”ì‹œì§€ í…Œì´ë¸”
CREATE TABLE messages (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  conversation_id UUID REFERENCES conversations(id),
  content TEXT,
  role TEXT CHECK (role IN ('user', 'assistant')),
  created_at TIMESTAMP DEFAULT NOW()
);
```

**3ë‹¨ê³„: Docker ì»¨í…Œì´ë„ˆ êµ¬ì„±**

```yaml
# docker-compose.yaml
version: '3.8'
services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - REDIS_URL=${REDIS_URL}
    volumes:
      - ./backend:/app
  
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_BACKEND_URL=http://localhost:8000
  
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
```

## 3. í•µì‹¬ ê¸°ëŠ¥ í™œìš©

### 3.1 ë¸Œë¼ìš°ì € ìë™í™”

SunaëŠ” Playwrightë¥¼ í™œìš©í•œ ê°•ë ¥í•œ ë¸Œë¼ìš°ì € ìë™í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

```python
# backend/agents/browser_agent.py
from playwright.async_api import async_playwright

class BrowserAgent:
    def __init__(self):
        self.playwright = None
        self.browser = None
        self.page = None
    
    async def start_browser(self, headless=True):
        """ë¸Œë¼ìš°ì € ì‹œì‘"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=headless,
            args=['--no-sandbox', '--disable-dev-shm-usage']
        )
        self.page = await self.browser.new_page()
        
        # User-Agent ì„¤ì •
        await self.page.set_user_agent(
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
    
    async def navigate_and_extract(self, url, selectors):
        """í˜ì´ì§€ íƒìƒ‰ ë° ë°ì´í„° ì¶”ì¶œ"""
        await self.page.goto(url, wait_until='networkidle')
        
        results = {}
        for key, selector in selectors.items():
            try:
                element = await self.page.wait_for_selector(selector, timeout=5000)
                results[key] = await element.inner_text()
            except:
                results[key] = None
        
        return results
    
    async def take_screenshot(self, path):
        """ìŠ¤í¬ë¦°ìƒ· ì´¬ì˜"""
        await self.page.screenshot(path=path, full_page=True)
```

### 3.2 íŒŒì¼ ê´€ë¦¬ ì‹œìŠ¤í…œ

```python
# backend/agents/file_agent.py
import os
import shutil
from pathlib import Path

class FileAgent:
    def __init__(self, workspace_dir="/tmp/suna_workspace"):
        self.workspace_dir = Path(workspace_dir)
        self.workspace_dir.mkdir(exist_ok=True)
    
    def create_document(self, filename, content, doc_type="txt"):
        """ë¬¸ì„œ ìƒì„±"""
        file_path = self.workspace_dir / f"{filename}.{doc_type}"
        
        if doc_type == "md":
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
        elif doc_type == "pdf":
            # PDF ìƒì„± ë¡œì§
            self._create_pdf(file_path, content)
        
        return str(file_path)
    
    def read_file(self, file_path):
        """íŒŒì¼ ì½ê¸°"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            return f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}"
    
    def organize_files(self, pattern="*"):
        """íŒŒì¼ ì •ë¦¬"""
        files = list(self.workspace_dir.glob(pattern))
        
        # íŒŒì¼ íƒ€ì…ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        for file in files:
            file_type = file.suffix[1:] if file.suffix else "unknown"
            type_dir = self.workspace_dir / file_type
            type_dir.mkdir(exist_ok=True)
            
            # íŒŒì¼ ì´ë™
            shutil.move(str(file), str(type_dir / file.name))
```

### 3.3 API í†µí•©

```python
# backend/integrations/api_manager.py
import httpx
from typing import Dict, Any

class APIManager:
    def __init__(self):
        self.session = httpx.AsyncClient()
        self.api_keys = {
            'tavily': os.getenv('TAVILY_API_KEY'),
            'firecrawl': os.getenv('FIRECRAWL_API_KEY'),
            'rapidapi': os.getenv('RAPIDAPI_KEY')
        }
    
    async def search_web(self, query: str, max_results: int = 10):
        """Tavily ì›¹ ê²€ìƒ‰"""
        url = "https://api.tavily.com/search"
        payload = {
            "api_key": self.api_keys['tavily'],
            "query": query,
            "max_results": max_results,
            "search_depth": "advanced"
        }
        
        response = await self.session.post(url, json=payload)
        return response.json()
    
    async def crawl_website(self, url: str):
        """Firecrawl ì›¹ í¬ë¡¤ë§"""
        headers = {
            'Authorization': f'Bearer {self.api_keys["firecrawl"]}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            "url": url,
            "formats": ["markdown", "html"],
            "extract": {
                "schema": {
                    "type": "object", 
                    "properties": {
                        "title": {"type": "string"},
                        "content": {"type": "string"},
                        "links": {"type": "array"}
                    }
                }
            }
        }
        
        response = await self.session.post(
            "https://api.firecrawl.dev/v1/scrape",
            headers=headers,
            json=payload
        )
        return response.json()
```

## 4. ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€

### 4.1 ê²½ìŸì‚¬ ë¶„ì„ ìë™í™”

```python
# examples/competitor_analysis.py
async def analyze_competitors(industry, location, agent):
    """ê²½ìŸì‚¬ ë¶„ì„ ì›Œí¬í”Œë¡œìš°"""
    
    # 1. ì‹œì¥ ì¡°ì‚¬
    search_query = f"{industry} companies {location} market leaders"
    search_results = await agent.api_manager.search_web(search_query, max_results=20)
    
    # 2. ê° íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ ë¶„ì„
    competitors = []
    for result in search_results['results'][:10]:
        company_data = await agent.api_manager.crawl_website(result['url'])
        
        # 3. ë¸Œë¼ìš°ì € ìë™í™”ë¡œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
        selectors = {
            'about': 'section[class*="about"], .about-section',
            'services': '.services, .products',
            'contact': '.contact, footer'
        }
        
        browser_data = await agent.browser.navigate_and_extract(
            result['url'], selectors
        )
        
        competitors.append({
            'name': company_data.get('title', 'Unknown'),
            'url': result['url'],
            'description': company_data.get('content', '')[:500],
            'strengths': browser_data.get('services', 'N/A'),
            'contact_info': browser_data.get('contact', 'N/A')
        })
    
    # 4. PDF ë³´ê³ ì„œ ìƒì„±
    report_content = generate_competitor_report(competitors, industry, location)
    report_path = agent.file_agent.create_document(
        f"competitor_analysis_{industry}_{location}", 
        report_content, 
        "pdf"
    )
    
    return report_path, competitors

def generate_competitor_report(competitors, industry, location):
    """ê²½ìŸì‚¬ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
    report = f"""
# {industry} ì—…ê³„ ê²½ìŸì‚¬ ë¶„ì„ ë³´ê³ ì„œ
**ì§€ì—­**: {location}
**ë¶„ì„ ì¼ì**: {datetime.now().strftime('%Y-%m-%d')}

## ì£¼ìš” ê²½ìŸì‚¬ í˜„í™©

"""
    
    for i, comp in enumerate(competitors, 1):
        report += f"""
### {i}. {comp['name']}

**ì›¹ì‚¬ì´íŠ¸**: {comp['url']}

**ê°œìš”**: {comp['description']}

**ì£¼ìš” ì„œë¹„ìŠ¤**: {comp['strengths']}

**ì—°ë½ì²˜**: {comp['contact_info']}

---
"""
    
    # ì‹œì¥ ë¶„ì„ ìš”ì•½
    report += f"""
## ì‹œì¥ ë¶„ì„ ìš”ì•½

**ì´ ë¶„ì„ ê¸°ì—… ìˆ˜**: {len(competitors)}ê°œ

**ì£¼ìš” íŠ¸ë Œë“œ**:
- ë””ì§€í„¸ ì „í™˜ ê°€ì†í™”
- ê³ ê° ê²½í—˜ ì¤‘ì‹¬ì˜ ì„œë¹„ìŠ¤ ì œê³µ
- ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì • ê°•í™”

**ê¸°íšŒ ìš”ì†Œ**:
- í‹ˆìƒˆ ì‹œì¥ ì§„ì… ê¸°íšŒ
- í˜ì‹ ì  ê¸°ìˆ  ë„ì…ì„ í†µí•œ ì°¨ë³„í™”
- íŒŒíŠ¸ë„ˆì‹­ì„ í†µí•œ ì‹œì¥ í™•ëŒ€
"""
    
    return report
```

### 4.2 LinkedIn ì¸ì¬ ê²€ìƒ‰

```python
# examples/linkedin_recruitment.py
async def find_candidates(position, location, requirements, agent):
    """LinkedIn ì¸ì¬ ê²€ìƒ‰ ìë™í™”"""
    
    # LinkedIn ê²€ìƒ‰ URL êµ¬ì„±
    search_url = f"https://www.linkedin.com/search/people/?keywords={position}&location={location}"
    
    await agent.browser.start_browser(headless=False)  # LinkedInì€ ë¡œê·¸ì¸ í•„ìš”
    await agent.browser.page.goto(search_url)
    
    # ë¡œê·¸ì¸ í”„ë¡¬í”„íŠ¸ (ì‹¤ì œ êµ¬í˜„ì‹œ OAuth ë˜ëŠ” API ì‚¬ìš© ê¶Œì¥)
    print("LinkedInì— ë¡œê·¸ì¸í•˜ê³  Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
    input()
    
    # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
    candidates = []
    
    # í”„ë¡œí•„ ì„ íƒì
    profile_selectors = {
        'name': '.entity-result__title-text a span[aria-hidden="true"]',
        'title': '.entity-result__primary-subtitle',
        'location': '.entity-result__secondary-subtitle',
        'experience': '.entity-result__summary'
    }
    
    # í˜ì´ì§€ë³„ ë°ì´í„° ìˆ˜ì§‘
    for page in range(3):  # 3í˜ì´ì§€ê¹Œì§€ ê²€ìƒ‰
        await agent.browser.page.wait_for_selector('.entity-result__item')
        
        profiles = await agent.browser.page.query_selector_all('.entity-result__item')
        
        for profile in profiles[:5]:  # í˜ì´ì§€ë‹¹ 5ëª…ì”©
            candidate_data = {}
            
            for key, selector in profile_selectors.items():
                try:
                    element = await profile.query_selector(selector)
                    if element:
                        candidate_data[key] = await element.inner_text()
                    else:
                        candidate_data[key] = "N/A"
                except:
                    candidate_data[key] = "N/A"
            
            # ìš”êµ¬ì‚¬í•­ í•„í„°ë§
            if meets_requirements(candidate_data, requirements):
                candidates.append(candidate_data)
        
        # ë‹¤ìŒ í˜ì´ì§€ë¡œ
        try:
            next_button = await agent.browser.page.query_selector('[aria-label="Next"]')
            if next_button:
                await next_button.click()
                await agent.browser.page.wait_for_load_state('networkidle')
        except:
            break
    
    # ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
    report = generate_candidate_report(candidates, position, location)
    report_path = agent.file_agent.create_document(
        f"candidates_{position}_{location}", report, "md"
    )
    
    return candidates, report_path

def meets_requirements(candidate, requirements):
    """í›„ë³´ìê°€ ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ëŠ”ì§€ í™•ì¸"""
    title = candidate.get('title', '').lower()
    experience = candidate.get('experience', '').lower()
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ì‚¬ìš©)
    for req in requirements:
        if req.lower() in title or req.lower() in experience:
            return True
    return False
```

### 4.3 SEO ë¶„ì„ ìë™í™”

```python
# examples/seo_analysis.py
async def analyze_seo(website_url, agent):
    """SEO ë¶„ì„ ìë™í™”"""
    
    # 1. ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§
    site_data = await agent.api_manager.crawl_website(website_url)
    
    # 2. ë¸Œë¼ìš°ì €ë¡œ ì¶”ê°€ SEO ìš”ì†Œ ë¶„ì„
    await agent.browser.start_browser()
    await agent.browser.page.goto(website_url)
    
    seo_elements = await extract_seo_elements(agent.browser.page)
    
    # 3. í˜ì´ì§€ ì†ë„ ë¶„ì„
    performance_metrics = await analyze_page_speed(agent.browser.page)
    
    # 4. í‚¤ì›Œë“œ ë¶„ì„
    content = site_data.get('content', '')
    keyword_analysis = analyze_keywords(content)
    
    # 5. ê²½ìŸì‚¬ í‚¤ì›Œë“œ ë¶„ì„
    competitor_keywords = await analyze_competitor_keywords(website_url, agent)
    
    # 6. SEO ë³´ê³ ì„œ ìƒì„±
    seo_report = {
        'url': website_url,
        'seo_elements': seo_elements,
        'performance': performance_metrics,
        'keywords': keyword_analysis,
        'competitors': competitor_keywords,
        'recommendations': generate_seo_recommendations(seo_elements, keyword_analysis)
    }
    
    # 7. ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
    report_content = generate_seo_report(seo_report)
    report_path = agent.file_agent.create_document(
        f"seo_analysis_{website_url.replace('https://', '').replace('/', '_')}", 
        report_content, 
        "md"
    )
    
    return seo_report, report_path

async def extract_seo_elements(page):
    """SEO ìš”ì†Œ ì¶”ì¶œ"""
    seo_data = {}
    
    # ë©”íƒ€ íƒœê·¸
    try:
        title = await page.title()
        seo_data['title'] = title
        seo_data['title_length'] = len(title)
    except:
        seo_data['title'] = None
    
    # ë©”íƒ€ ë””ìŠ¤í¬ë¦½ì…˜
    try:
        meta_desc = await page.get_attribute('meta[name="description"]', 'content')
        seo_data['meta_description'] = meta_desc
        seo_data['meta_desc_length'] = len(meta_desc) if meta_desc else 0
    except:
        seo_data['meta_description'] = None
    
    # í—¤ë”© íƒœê·¸
    headings = {}
    for i in range(1, 7):
        try:
            h_tags = await page.query_selector_all(f'h{i}')
            headings[f'h{i}'] = [await tag.inner_text() for tag in h_tags]
        except:
            headings[f'h{i}'] = []
    
    seo_data['headings'] = headings
    
    # ì´ë¯¸ì§€ alt í…ìŠ¤íŠ¸
    try:
        images = await page.query_selector_all('img')
        images_without_alt = 0
        total_images = len(images)
        
        for img in images:
            alt = await img.get_attribute('alt')
            if not alt:
                images_without_alt += 1
        
        seo_data['images'] = {
            'total': total_images,
            'without_alt': images_without_alt,
            'alt_coverage': (total_images - images_without_alt) / total_images * 100 if total_images > 0 else 0
        }
    except:
        seo_data['images'] = {'total': 0, 'without_alt': 0, 'alt_coverage': 0}
    
    return seo_data

def analyze_keywords(content):
    """í‚¤ì›Œë“œ ë¶„ì„"""
    import re
    from collections import Counter
    
    # í…ìŠ¤íŠ¸ ì •ì œ
    text = re.sub(r'<[^>]+>', '', content.lower())
    words = re.findall(r'\b[a-zA-Zê°€-í£]{3,}\b', text)
    
    # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ ì˜ˆì‹œ)
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
    words = [word for word in words if word not in stop_words]
    
    # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
    keyword_freq = Counter(words)
    
    return {
        'top_keywords': keyword_freq.most_common(20),
        'total_words': len(words),
        'unique_words': len(set(words))
    }
```

## 5. ê³ ê¸‰ ì„¤ì • ë° ìµœì í™”

### 5.1 ì„±ëŠ¥ ìµœì í™”

```python
# backend/core/optimization.py
import asyncio
from functools import wraps
import redis
import json

class PerformanceOptimizer:
    def __init__(self):
        self.redis_client = redis.Redis.from_url(os.getenv('REDIS_URL'))
        self.cache_ttl = 3600  # 1ì‹œê°„
    
    def cache_result(self, cache_key_prefix):
        """ê²°ê³¼ ìºì‹± ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # ìºì‹œ í‚¤ ìƒì„±
                cache_key = f"{cache_key_prefix}:{hash(str(args) + str(kwargs))}"
                
                # ìºì‹œì—ì„œ í™•ì¸
                cached_result = self.redis_client.get(cache_key)
                if cached_result:
                    return json.loads(cached_result)
                
                # ì‹¤í–‰ ë° ìºì‹±
                result = await func(*args, **kwargs)
                self.redis_client.setex(
                    cache_key, 
                    self.cache_ttl, 
                    json.dumps(result, default=str)
                )
                
                return result
            return wrapper
        return decorator
    
    async def batch_process(self, tasks, batch_size=5):
        """ë°°ì¹˜ ì²˜ë¦¬"""
        results = []
        for i in range(0, len(tasks), batch_size):
            batch = tasks[i:i+batch_size]
            batch_results = await asyncio.gather(*batch, return_exceptions=True)
            results.extend(batch_results)
        return results
```

### 5.2 ë³´ì•ˆ ê°•í™”

```python
# backend/core/security.py
import hashlib
import secrets
from cryptography.fernet import Fernet

class SecurityManager:
    def __init__(self):
        self.encryption_key = Fernet.generate_key()
        self.cipher_suite = Fernet(self.encryption_key)
    
    def encrypt_sensitive_data(self, data):
        """ë¯¼ê°í•œ ë°ì´í„° ì•”í˜¸í™”"""
        return self.cipher_suite.encrypt(data.encode()).decode()
    
    def decrypt_sensitive_data(self, encrypted_data):
        """ì•”í˜¸í™”ëœ ë°ì´í„° ë³µí˜¸í™”"""
        return self.cipher_suite.decrypt(encrypted_data.encode()).decode()
    
    def generate_api_key(self):
        """API í‚¤ ìƒì„±"""
        return secrets.token_urlsafe(32)
    
    def hash_password(self, password):
        """ë¹„ë°€ë²ˆí˜¸ í•´ì‹±"""
        salt = secrets.token_hex(16)
        password_hash = hashlib.pbkdf2_hmac('sha256', password.encode(), salt.encode(), 100000)
        return f"{salt}${password_hash.hex()}"
    
    def verify_password(self, password, hashed):
        """ë¹„ë°€ë²ˆí˜¸ ê²€ì¦"""
        salt, stored_hash = hashed.split('$')
        password_hash = hashlib.pbkdf2_hmac('sha256', password.encode(), salt.encode(), 100000)
        return password_hash.hex() == stored_hash
```

### 5.3 ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

```python
# backend/core/monitoring.py
import logging
import time
from functools import wraps

class MonitoringSystem:
    def __init__(self):
        self.setup_logging()
        self.metrics = {}
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('suna.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('suna')
    
    def track_execution_time(self, func_name):
        """ì‹¤í–‰ ì‹œê°„ ì¶”ì  ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = await func(*args, **kwargs)
                    execution_time = time.time() - start_time
                    
                    self.metrics[func_name] = {
                        'last_execution_time': execution_time,
                        'total_executions': self.metrics.get(func_name, {}).get('total_executions', 0) + 1
                    }
                    
                    self.logger.info(f"{func_name} executed in {execution_time:.2f}s")
                    return result
                    
                except Exception as e:
                    self.logger.error(f"Error in {func_name}: {str(e)}")
                    raise
                    
            return wrapper
        return decorator
    
    def log_user_action(self, user_id, action, details=None):
        """ì‚¬ìš©ì ì•¡ì…˜ ë¡œê¹…"""
        self.logger.info(f"User {user_id} performed {action}: {details or ''}")
    
    def get_system_metrics(self):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        return {
            'execution_metrics': self.metrics,
            'timestamp': time.time()
        }
```

## 6. í”„ë¡œë•ì…˜ ë°°í¬

### 6.1 Docker í”„ë¡œë•ì…˜ ì„¤ì •

```yaml
# docker-compose.prod.yaml
version: '3.8'

services:
  backend:
    build: 
      context: ./backend
      dockerfile: Dockerfile.prod
    environment:
      - NODE_ENV=production
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      - REDIS_URL=${REDIS_URL}
    networks:
      - suna-network
    restart: unless-stopped
    
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_BACKEND_URL=${BACKEND_URL}
    networks:
      - suna-network
    restart: unless-stopped
    
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - backend
      - frontend
    networks:
      - suna-network
    restart: unless-stopped
    
  redis:
    image: redis:alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    networks:
      - suna-network
    restart: unless-stopped

networks:
  suna-network:
    driver: bridge

volumes:
  redis_data:
```

### 6.2 Nginx ì„¤ì •

```nginx
# nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream backend {
        server backend:8000;
    }
    
    upstream frontend {
        server frontend:3000;
    }
    
    server {
        listen 80;
        server_name your-domain.com;
        return 301 https://$server_name$request_uri;
    }
    
    server {
        listen 443 ssl;
        server_name your-domain.com;
        
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        
        location /api/ {
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
        
        location / {
            proxy_pass http://frontend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}
```

### 6.3 CI/CD íŒŒì´í”„ë¼ì¸

```yaml
# .github/workflows/deploy.yml
name: Deploy Suna

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements.txt
    
    - name: Run tests
      run: |
        cd backend
        pytest tests/
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
    
    - name: Install and test frontend
      run: |
        cd frontend
        npm ci
        npm run test
        npm run build

  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to server
      uses: appleboy/ssh-action@v0.1.5
      with:
        host: ${{ secrets.HOST }}
        username: ${{ secrets.USERNAME }}
        key: ${{ secrets.PRIVATE_KEY }}
        script: |
          cd /opt/suna
          git pull origin main
          docker-compose -f docker-compose.prod.yaml down
          docker-compose -f docker-compose.prod.yaml build
          docker-compose -f docker-compose.prod.yaml up -d
```

## 7. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ë° FAQ

### 7.1 ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

**ë¬¸ì œ 1: Docker ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹¤íŒ¨**

```bash
# í•´ê²° ë°©ë²•
docker-compose down
docker system prune -a
docker-compose up --build
```

**ë¬¸ì œ 2: Supabase ì—°ê²° ì‹¤íŒ¨**

```bash
# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
echo $SUPABASE_URL
echo $SUPABASE_ANON_KEY

# ì—°ê²° í…ŒìŠ¤íŠ¸
curl -H "apikey: $SUPABASE_ANON_KEY" $SUPABASE_URL/rest/v1/
```

**ë¬¸ì œ 3: ë¸Œë¼ìš°ì € ìë™í™” ì‹¤íŒ¨**

```python
# ë” ì•ˆì •ì ì¸ ë¸Œë¼ìš°ì € ì„¤ì •
browser = await playwright.chromium.launch(
    headless=True,
    args=[
        '--no-sandbox',
        '--disable-dev-shm-usage',
        '--disable-gpu',
        '--disable-background-timer-throttling',
        '--disable-backgrounding-occluded-windows',
        '--disable-renderer-backgrounding'
    ]
)
```

### 7.2 ì„±ëŠ¥ ìµœì í™” íŒ

```python
# 1. ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”
import asyncio

async def optimize_concurrent_tasks():
    """ë™ì‹œ ì‘ì—… ìµœì í™”"""
    semaphore = asyncio.Semaphore(5)  # ìµœëŒ€ 5ê°œ ë™ì‹œ ì‹¤í–‰
    
    async def limited_task(task):
        async with semaphore:
            return await task()
    
    tasks = [limited_task(task) for task in task_list]
    return await asyncio.gather(*tasks)

# 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
import psutil

def monitor_memory():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    process = psutil.Process()
    memory_info = process.memory_info()
    
    print(f"RSS: {memory_info.rss / 1024 / 1024:.1f} MB")
    print(f"VMS: {memory_info.vms / 1024 / 1024:.1f} MB")
```

## 8. ì»¤ìŠ¤í…€ í™•ì¥

### 8.1 ìƒˆë¡œìš´ Agent ì¶”ê°€

```python
# backend/agents/custom_agent.py
from .base_agent import BaseAgent

class CustomAnalysisAgent(BaseAgent):
    def __init__(self):
        super().__init__()
        self.name = "CustomAnalysisAgent"
    
    async def analyze_custom_data(self, data_source, analysis_type):
        """ì»¤ìŠ¤í…€ ë°ì´í„° ë¶„ì„"""
        try:
            # 1. ë°ì´í„° ìˆ˜ì§‘
            raw_data = await self.collect_data(data_source)
            
            # 2. ë¶„ì„ ìˆ˜í–‰
            analysis_result = await self.perform_analysis(raw_data, analysis_type)
            
            # 3. ê²°ê³¼ í¬ë§·íŒ…
            formatted_result = self.format_result(analysis_result)
            
            return formatted_result
            
        except Exception as e:
            self.logger.error(f"Custom analysis failed: {e}")
            raise
    
    async def collect_data(self, source):
        """ë°ì´í„° ìˆ˜ì§‘ ë¡œì§"""
        if source.startswith('http'):
            return await self.api_manager.crawl_website(source)
        else:
            return self.file_agent.read_file(source)
    
    async def perform_analysis(self, data, analysis_type):
        """ë¶„ì„ ìˆ˜í–‰ ë¡œì§"""
        if analysis_type == "sentiment":
            return await self.analyze_sentiment(data)
        elif analysis_type == "keywords":
            return self.extract_keywords(data)
        else:
            return {"error": "Unknown analysis type"}
```

### 8.2 í”ŒëŸ¬ê·¸ì¸ ì‹œìŠ¤í…œ

```python
# backend/plugins/plugin_manager.py
import importlib
import os

class PluginManager:
    def __init__(self):
        self.plugins = {}
        self.plugin_dir = "plugins"
    
    def load_plugins(self):
        """í”ŒëŸ¬ê·¸ì¸ ë¡œë“œ"""
        plugin_files = [f[:-3] for f in os.listdir(self.plugin_dir) 
                       if f.endswith('.py') and f != '__init__.py']
        
        for plugin_name in plugin_files:
            try:
                module = importlib.import_module(f"{self.plugin_dir}.{plugin_name}")
                if hasattr(module, 'Plugin'):
                    self.plugins[plugin_name] = module.Plugin()
                    print(f"Loaded plugin: {plugin_name}")
            except Exception as e:
                print(f"Failed to load plugin {plugin_name}: {e}")
    
    def execute_plugin(self, plugin_name, method, *args, **kwargs):
        """í”ŒëŸ¬ê·¸ì¸ ì‹¤í–‰"""
        if plugin_name in self.plugins:
            plugin = self.plugins[plugin_name]
            if hasattr(plugin, method):
                return getattr(plugin, method)(*args, **kwargs)
        raise ValueError(f"Plugin {plugin_name} or method {method} not found")
```

## 9. ê²°ë¡  ë° í–¥í›„ ë°œì „ ë°©í–¥

### 9.1 Sunaì˜ ê°€ì¹˜ ì œì•ˆ

SunaëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³ ìœ í•œ ê°€ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

- **ì™„ì „í•œ ì˜¤í”ˆì†ŒìŠ¤**: íˆ¬ëª…í•˜ê³  ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
- **ì‹¤ì œ ì—…ë¬´ ìë™í™”**: ë‹¨ìˆœí•œ ì±—ë´‡ì„ ë„˜ì–´ì„  ì‹¤ì§ˆì  ì—…ë¬´ ì²˜ë¦¬
- **ëª¨ë“ˆí˜• ì•„í‚¤í…ì²˜**: í•„ìš”ì— ë”°ë¥¸ í™•ì¥ ê°€ëŠ¥
- **ê¸°ì—…ê¸‰ ë³´ì•ˆ**: í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì•ˆì „í•œ ì‚¬ìš©

### 9.2 í–¥í›„ ë°œì „ ë°©í–¥

**ë‹¨ê¸° ê³„íš:**

- ë” ë§ì€ API í†µí•© (Slack, Microsoft 365, Google Workspace)
- ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥ (AI ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±)
- ëª¨ë°”ì¼ ì•± ì§€ì›

**ì¤‘ì¥ê¸° ê³„íš:**

- ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ (ì´ë¯¸ì§€, ìŒì„± ì²˜ë¦¬)
- ìë™ í•™ìŠµ ë° ê°œì„  ê¸°ëŠ¥
- ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ê´€ë¦¬ ê¸°ëŠ¥

### 9.3 ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬

SunaëŠ” ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ë¡œì„œ ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤:

- **ì½”ë“œ ê¸°ì—¬**: ìƒˆë¡œìš´ ê¸°ëŠ¥ ê°œë°œ, ë²„ê·¸ ìˆ˜ì •
- **ë¬¸ì„œí™”**: ì‚¬ìš© ê°€ì´ë“œ, API ë¬¸ì„œ ê°œì„ 
- **í”¼ë“œë°±**: ì‚¬ìš© ê²½í—˜ ê³µìœ , ê°œì„  ì œì•ˆ

**ê¸°ì—¬ ë°©ë²•:**

1. [GitHub Issues](https://github.com/kortix-ai/suna/issues)ì—ì„œ ë¬¸ì œ ë³´ê³ 
2. Pull Requestë¥¼ í†µí•œ ì½”ë“œ ê¸°ì—¬
3. [Discord](https://discord.gg/suna) ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬

SunaëŠ” AI ì—ì´ì „íŠ¸ ê¸°ìˆ ì˜ ë¯¼ì£¼í™”ë¥¼ í†µí•´ ëˆ„êµ¬ë‚˜ ê°•ë ¥í•œ ìë™í™” ë„êµ¬ë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ë¯¸ë˜ë¥¼ ë§Œë“¤ì–´ê°€ê³  ìˆìŠµë‹ˆë‹¤. ì´ ê°€ì´ë“œë¥¼ í†µí•´ Sunaì˜ ë¬´í•œí•œ ê°€ëŠ¥ì„±ì„ íƒí—˜í•˜ê³ , ì—¬ëŸ¬ë¶„ë§Œì˜ ë…íŠ¹í•œ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ê°œë°œí•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.

---

*ì°¸ê³ ìë£Œ: [Suna GitHub Repository](https://github.com/kortix-ai/suna) | [ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://www.suna.so)*
