---
title: "OmniGen2: ì°¨ì„¸ëŒ€ ë©€í‹°ëª¨ë‹¬ ìƒì„± ëª¨ë¸ ì™„ì „ ë¶„ì„"
excerpt: "GPT-4oë¥¼ ë„˜ì–´ì„œëŠ” ì˜¤í”ˆì†ŒìŠ¤ í†µí•© ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ OmniGen2ì˜ í•µì‹¬ ê¸°ëŠ¥ê³¼ ì‹¤ì „ í™œìš© ê°€ì´ë“œ"
date: 2025-06-25
categories: 
  - owm
  - research
tags: 
  - omnigen2
  - multimodal
  - text-to-image
  - image-editing
  - open-source
  - diffusion-model
author_profile: true
toc: true
toc_label: "OmniGen2 ì™„ì „ ë¶„ì„"
---

## ê°œìš”

[OmniGen2](https://huggingface.co/OmniGen2/OmniGen2)ëŠ” VectorSpaceLabì—ì„œ ê°œë°œí•œ ì°¨ì„¸ëŒ€ ì˜¤í”ˆì†ŒìŠ¤ ë©€í‹°ëª¨ë‹¬ ìƒì„± ëª¨ë¸ë¡œ, ê¸°ì¡´ GPT-4oì˜ ì´ë¯¸ì§€ ìƒì„± ê¸°ëŠ¥ì„ í¬ê²Œ ë›°ì–´ë„˜ëŠ” 4ê°€ì§€ í•µì‹¬ ê¸°ëŠ¥ì„ í†µí•©í•œ í˜ì‹ ì ì¸ AI ëª¨ë¸ì…ë‹ˆë‹¤. íŠ¹íˆ **In-context Generation**ê³¼ **Instruction-guided Image Editing** ë¶„ì•¼ì—ì„œ ë…ë³´ì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ì™„ì „í•œ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ì œê³µë˜ì–´ ì—°êµ¬ìì™€ ê°œë°œìë“¤ì´ ììœ ë¡­ê²Œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## OmniGen2 í•µì‹¬ ì•„í‚¤í…ì²˜

### 1. ë“€ì–¼ ë””ì½”ë”© ê²½ë¡œ ì„¤ê³„

OmniGen2ëŠ” ê¸°ì¡´ OmniGen v1ê³¼ ë‹¬ë¦¬ **í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ìœ„í•œ ë³„ë„ì˜ ë””ì½”ë”© ê²½ë¡œ**ë¥¼ ì±„íƒí–ˆìŠµë‹ˆë‹¤:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input Text    â”‚    â”‚   Input Image    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚Text Decoderâ”‚          â”‚Image Tokenâ”‚
    â”‚  Pathway   â”‚          â”‚   izer    â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                 â”‚         â”‚
           â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
           â”‚  Unified Backbone   â”‚
           â”‚   (Qwen-VL-2.5)     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. 4ëŒ€ í•µì‹¬ ê¸°ëŠ¥

#### ğŸ” **Visual Understanding**
- Qwen-VL-2.5 ê¸°ë°˜ì˜ ê°•ë ¥í•œ ì´ë¯¸ì§€ í•´ì„ ëŠ¥ë ¥
- ë³µì¡í•œ ì‹œê°ì  ì½˜í…ì¸ ì˜ ì„¸ë°€í•œ ë¶„ì„ ë° ì´í•´
- ê°ì²´ ì¸ì‹, ì¥ë©´ ì´í•´, í…ìŠ¤íŠ¸ ì¶”ì¶œ ë“± í¬ê´„ì  ì§€ì›

#### ğŸ¨ **Text-to-Image Generation**
- ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„± (512Ã—512 ì´ìƒ ê¶Œì¥)
- ë¯¸ì ìœ¼ë¡œ ë›°ì–´ë‚œ ê²°ê³¼ë¬¼ ìƒì‚°
- ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ê³¼ ì»¨ì…‰ ì§€ì›

#### âœï¸ **Instruction-guided Image Editing**
- **ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì¤‘ ìµœê³  ìˆ˜ì¤€**ì˜ ì´ë¯¸ì§€ í¸ì§‘ ì„±ëŠ¥
- ë³µì¡í•œ ì§€ì‹œì‚¬í•­ ê¸°ë°˜ ì •ë°€ í¸ì§‘
- ê°ì²´ ì¶”ê°€/ì œê±°, ìŠ¤íƒ€ì¼ ë³€ê²½, ìƒ‰ìƒ ì¡°ì • ë“±

#### ğŸ”„ **In-context Generation**
- **GPT-4oì—ëŠ” ì—†ëŠ” ë…íŠ¹í•œ ê¸°ëŠ¥**
- ì—¬ëŸ¬ ì…ë ¥(ì¸ë¬¼, ê°ì²´, ì¥ë©´)ì„ ìœ ì—°í•˜ê²Œ ì¡°í•©
- ìƒˆë¡œìš´ ì‹œê°ì  ê²°ê³¼ë¬¼ ìƒì„±

## GPT-4o ëŒ€ë¹„ í•µì‹¬ ìš°ìœ„ì 

### 1. ì˜¤í”ˆì†ŒìŠ¤ vs íì‡„í˜• API

| íŠ¹ì„± | OmniGen2 | GPT-4o |
|------|----------|--------|
| **ì ‘ê·¼ì„±** | ì™„ì „ ì˜¤í”ˆì†ŒìŠ¤ | API ì „ìš© |
| **ì»¤ìŠ¤í„°ë§ˆì´ì§•** | ì†ŒìŠ¤ì½”ë“œ ìˆ˜ì • ê°€ëŠ¥ | ë¶ˆê°€ëŠ¥ |
| **ë°ì´í„° í”„ë¼ì´ë²„ì‹œ** | ë¡œì»¬ ì‹¤í–‰ | ì™¸ë¶€ ì„œë²„ ì „ì†¡ |
| **ë¹„ìš©** | í•˜ë“œì›¨ì–´ ë¹„ìš©ë§Œ | ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ê³¼ê¸ˆ |
| **ì˜¤í”„ë¼ì¸ ì‚¬ìš©** | ê°€ëŠ¥ | ë¶ˆê°€ëŠ¥ |

### 2. ê³ ê¸‰ ì´ë¯¸ì§€ í¸ì§‘ ê¸°ëŠ¥

```python
# OmniGen2 ì´ë¯¸ì§€ í¸ì§‘ ì˜ˆì‹œ
from omnigen2 import OmniGen2Pipeline

pipe = OmniGen2Pipeline.from_pretrained("OmniGen2/OmniGen2")

# ë³µì¡í•œ í¸ì§‘ ì§€ì‹œì‚¬í•­
prompt = """
ì´ë¯¸ì§€ì—ì„œ ë¹¨ê°„ ìë™ì°¨ë¥¼ íŒŒë€ìƒ‰ìœ¼ë¡œ ë°”ê¾¸ê³ , 
ë°°ê²½ì˜ í•˜ëŠ˜ì— êµ¬ë¦„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”. 
ë™ì‹œì— ë„ë¡œ ì˜†ì— ê°€ë¡œë“±ì„ 3ê°œ ë” ì„¤ì¹˜í•´ì£¼ì„¸ìš”.
"""

result = pipe(
    prompt=prompt,
    input_images=[original_image],
    image_guidance_scale=1.8,  # ì›ë³¸ ì´ë¯¸ì§€ ìœ ì§€ë„
    text_guidance_scale=7.5,   # í…ìŠ¤íŠ¸ ì§€ì‹œì‚¬í•­ ê°•ë„
    max_pixels=1024*1024
)
```

### 3. In-context Generationì˜ í˜ì‹ 

GPT-4oëŠ” ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€ ìƒì„±ì— êµ­í•œë˜ì§€ë§Œ, OmniGen2ëŠ” **ì»¨í…ìŠ¤íŠ¸ ë‚´ ìƒì„±**ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:

```python
# ë‹¤ì¤‘ ì´ë¯¸ì§€ ì¡°í•© ìƒì„±
prompt = """
ì´ë¯¸ì§€1ì˜ ì‚¬ëŒì„ ì´ë¯¸ì§€2ì˜ í’ê²½ ì†ì— ë°°ì¹˜í•˜ê³ ,
ì´ë¯¸ì§€3ì˜ ë™ë¬¼ì„ í•¨ê»˜ ì¶”ê°€í•´ì„œ 
ìì—°ìŠ¤ëŸ¬ìš´ í•˜ë‚˜ì˜ ì¥ë©´ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
"""

result = pipe(
    prompt=prompt,
    input_images=[person_image, landscape_image, animal_image],
    image_guidance_scale=2.8,  # ë†’ì€ ê°’ìœ¼ë¡œ ì›ë³¸ ë””í…Œì¼ ìœ ì§€
    enable_model_cpu_offload=True  # ë©”ëª¨ë¦¬ ì ˆì•½
)
```

## ì‹¤ì „ í™œìš© ê°€ì´ë“œ

### 1. í™˜ê²½ ì„¤ì •

#### ê¸°ë³¸ ì„¤ì¹˜

```bash
# 1. ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/VectorSpaceLab/OmniGen2.git
cd OmniGen2

# 2. Python í™˜ê²½ ì¤€ë¹„
conda create -n omnigen2 python=3.11
conda activate omnigen2

# 3. PyTorch ì„¤ì¹˜ (CUDA 12.4 ê¸°ì¤€)
pip install torch==2.6.0 torchvision --extra-index-url https://download.pytorch.org/whl/cu124

# 4. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# 5. Flash Attention (ì„±ëŠ¥ ìµœì í™”)
pip install flash-attn==2.7.4.post1 --no-build-isolation
```

#### êµ­ë‚´ ì‚¬ìš©ìë¥¼ ìœ„í•œ ìµœì í™”

```bash
# êµ­ë‚´ ë¯¸ëŸ¬ ì‚¬ìš©ìœ¼ë¡œ ë¹ ë¥¸ ì„¤ì¹˜
pip install torch==2.6.0 torchvision --index-url https://mirror.sjtu.edu.cn/pytorch-wheels/cu124
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 2. í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­

| êµ¬ì„± | ìµœì†Œ ì‚¬ì–‘ | ê¶Œì¥ ì‚¬ì–‘ | ê³ ì„±ëŠ¥ ì‚¬ì–‘ |
|------|-----------|-----------|-------------|
| **GPU ë©”ëª¨ë¦¬** | 17GB (RTX 3090) | 24GB (RTX 4090) | 40GB+ (A100) |
| **ì‹œìŠ¤í…œ RAM** | 32GB | 64GB | 128GB |
| **ì €ì¥ê³µê°„** | 50GB | 100GB | 200GB |
| **CUDA ë²„ì „** | 12.4+ | 12.6+ | 12.6+ |

#### ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜

```python
# VRAM ì ˆì•½ ì„¤ì •
pipe = OmniGen2Pipeline.from_pretrained(
    "OmniGen2/OmniGen2",
    enable_model_cpu_offload=True,      # 50% VRAM ì ˆì•½
    enable_sequential_cpu_offload=True, # 3GB ë¯¸ë§Œìœ¼ë¡œ ì¤„ì„ (ëŠë ¤ì§)
)

# ì„±ëŠ¥ ìµœì í™” íŒŒë¼ë¯¸í„°
generation_params = {
    "max_pixels": 512*512,              # ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ í•´ìƒë„ ì¡°ì •
    "cfg_range_end": 0.7,               # ì¶”ë¡  ì†ë„ í–¥ìƒ
    "negative_prompt": "blurry, low quality, watermark",
}
```

### 3. ì‹¤ì „ ì‚¬ìš© ì˜ˆì‹œ

#### Visual Understanding

```python
# ì´ë¯¸ì§€ ë¶„ì„ ë° ì„¤ëª…
result = pipe(
    prompt="ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    input_images=[image],
    task_type="understanding"
)
print(result.text)
```

#### Text-to-Image Generation

```python
# ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±
result = pipe(
    prompt="A majestic dragon flying over a medieval castle at sunset, highly detailed, fantasy art style",
    text_guidance_scale=7.5,
    num_inference_steps=50,
    max_pixels=1024*1024
)
result.images[0].save("dragon_castle.jpg")
```

#### Advanced Image Editing

```python
# ì •ë°€ ì´ë¯¸ì§€ í¸ì§‘
result = pipe(
    prompt="""
    ì‚¬ì§„ ì† ê±´ë¬¼ì˜ ìƒ‰ìƒì„ íŒŒìŠ¤í…” í†¤ìœ¼ë¡œ ë°”ê¾¸ê³ ,
    í•˜ëŠ˜ì— ì—´ê¸°êµ¬ë¥¼ 3ê°œ ì¶”ê°€í•´ì£¼ì„¸ìš”.
    ì „ì²´ì ìœ¼ë¡œ ë™í™” ê°™ì€ ë¶„ìœ„ê¸°ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
    """,
    input_images=[building_image],
    image_guidance_scale=1.5,  # ì›ë³¸ êµ¬ì¡° ìœ ì§€
    text_guidance_scale=8.0,   # í¸ì§‘ ì§€ì‹œì‚¬í•­ ê°•í™”
    negative_prompt="realistic, photographic, dark, gloomy"
)
```

#### In-context Multi-Image Generation

```python
# ë³µìˆ˜ ì´ë¯¸ì§€ ì¡°í•© ìƒì„±
result = pipe(
    prompt="""
    ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ ê°•ì•„ì§€ë¥¼ ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ì˜ ê³µì›ì— ë°°ì¹˜í•˜ê³ ,
    ì„¸ ë²ˆì§¸ ì´ë¯¸ì§€ì˜ ì•„ì´ë“¤ê³¼ í•¨ê»˜ ë†€ê³  ìˆëŠ” ëª¨ìŠµìœ¼ë¡œ 
    ìì—°ìŠ¤ëŸ¬ìš´ í•˜ë‚˜ì˜ ì¥ë©´ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
    """,
    input_images=[dog_image, park_image, children_image],
    image_guidance_scale=2.5,  # ì›ë³¸ ìš”ì†Œë“¤ ì„¸ë¶€ì‚¬í•­ ìœ ì§€
    text_guidance_scale=7.0,
    max_input_image_side_length=768
)
```

## ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 1. ì¶”ë¡  ì†ë„ ê°œì„ 

```python
# CFG ë²”ìœ„ ì¡°ì •ìœ¼ë¡œ ì†ë„ í–¥ìƒ
optimized_params = {
    "cfg_range_start": 0.0,
    "cfg_range_end": 0.6,     # ê¸°ë³¸ê°’ 1.0ì—ì„œ 0.6ìœ¼ë¡œ ë‚®ì¶¤
    "num_inference_steps": 28, # ê¸°ë³¸ 50ì—ì„œ 28ë¡œ ì¤„ì„
}

# ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± ì¦ëŒ€
batch_prompts = [
    "A cat in a garden",
    "A dog on the beach", 
    "A bird in the sky"
]

results = pipe(
    prompt=batch_prompts,
    **optimized_params
)
```

### 2. í’ˆì§ˆ í–¥ìƒ íŒ

```python
# ê³ í’ˆì§ˆ ìƒì„±ì„ ìœ„í•œ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
quality_params = {
    # 1. ê³ í•´ìƒë„ ì…ë ¥ ì‚¬ìš©
    "max_pixels": 1024*1024,
    
    # 2. ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ ì‘ì„±
    "prompt": """
    A highly detailed portrait of a woman in Renaissance style,
    soft lighting, oil painting texture, classical composition,
    masterpiece quality, 8K resolution
    """,
    
    # 3. ë¶€ì • í”„ë¡¬í”„íŠ¸ í™œìš©
    "negative_prompt": """
    blurry, low quality, pixelated, distorted, 
    watermark, text, signature, low resolution
    """,
    
    # 4. ì ì ˆí•œ ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
    "text_guidance_scale": 7.5,
    "image_guidance_scale": 2.0,  # ì´ë¯¸ì§€ í¸ì§‘ì‹œ
}
```

## Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ í™œìš©

### 1. ë¡œì»¬ ë°ëª¨ ì‹¤í–‰

```bash
# ì´ë¯¸ì§€ ìƒì„± ì „ìš© ì¸í„°í˜ì´ìŠ¤
pip install gradio
python app.py

# ê³µê°œ ë§í¬ë¡œ ê³µìœ  (ì˜µì…˜)
python app.py --share

# ë©€í‹°ëª¨ë‹¬ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
python app_chat.py
```

### 2. ì»¤ìŠ¤í…€ ì¸í„°í˜ì´ìŠ¤ êµ¬ì¶•

```python
# custom_gradio_app.py
import gradio as gr
from omnigen2 import OmniGen2Pipeline

pipe = OmniGen2Pipeline.from_pretrained("OmniGen2/OmniGen2")

def generate_image(prompt, input_image=None, task_type="generation"):
    if task_type == "editing" and input_image is not None:
        result = pipe(
            prompt=prompt,
            input_images=[input_image],
            image_guidance_scale=1.8,
            text_guidance_scale=7.5
        )
    else:
        result = pipe(
            prompt=prompt,
            text_guidance_scale=7.5
        )
    
    return result.images[0]

# Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±
with gr.Blocks(title="OmniGen2 Custom Interface") as demo:
    gr.Markdown("# OmniGen2 ë©€í‹°ëª¨ë‹¬ ìƒì„± ë°ëª¨")
    
    with gr.Row():
        with gr.Column():
            prompt_input = gr.Textbox(
                label="í”„ë¡¬í”„íŠ¸",
                placeholder="ì›í•˜ëŠ” ì´ë¯¸ì§€ë‚˜ í¸ì§‘ ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”..."
            )
            image_input = gr.Image(
                label="ì…ë ¥ ì´ë¯¸ì§€ (í¸ì§‘ìš©)", 
                type="pil"
            )
            task_select = gr.Radio(
                choices=["generation", "editing"],
                label="ì‘ì—… ìœ í˜•",
                value="generation"
            )
            generate_btn = gr.Button("ìƒì„±í•˜ê¸°", variant="primary")
        
        with gr.Column():
            output_image = gr.Image(label="ê²°ê³¼ ì´ë¯¸ì§€")
    
    generate_btn.click(
        fn=generate_image,
        inputs=[prompt_input, image_input, task_select],
        outputs=[output_image]
    )

demo.launch(share=True)
```

## ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€

### 1. ì½˜í…ì¸  ì œì‘ ì›Œí¬í”Œë¡œìš°

```python
# ë¸Œëœë“œ ì½˜í…ì¸  ì œì‘ íŒŒì´í”„ë¼ì¸
class ContentCreationPipeline:
    def __init__(self):
        self.pipe = OmniGen2Pipeline.from_pretrained("OmniGen2/OmniGen2")
    
    def create_product_showcase(self, product_image, brand_style):
        """ì œí’ˆ ì´ë¯¸ì§€ë¥¼ ë¸Œëœë“œ ìŠ¤íƒ€ì¼ì— ë§ê²Œ ì¬êµ¬ì„±"""
        prompt = f"""
        ì´ ì œí’ˆì„ {brand_style} ìŠ¤íƒ€ì¼ì˜ ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ë°°ê²½ì— ë°°ì¹˜í•˜ê³ ,
        í”„ë¦¬ë¯¸ì—„í•œ ëŠë‚Œì˜ ì¡°ëª…ê³¼ êµ¬ì„±ìœ¼ë¡œ ì œí’ˆ ì‡¼ì¼€ì´ìŠ¤ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
        ìƒì—…ì ì´ê³  ì „ë¬¸ì ì¸ í’ˆì§ˆë¡œ ì œì‘í•´ì£¼ì„¸ìš”.
        """
        
        return self.pipe(
            prompt=prompt,
            input_images=[product_image],
            image_guidance_scale=2.2,
            text_guidance_scale=8.0,
            negative_prompt="cheap, low quality, amateur, cluttered"
        )
    
    def create_lifestyle_scene(self, person_image, product_image, environment):
        """ì¸ë¬¼ê³¼ ì œí’ˆì„ ìì—°ìŠ¤ëŸ¬ìš´ ë¼ì´í”„ìŠ¤íƒ€ì¼ ì¥ë©´ìœ¼ë¡œ í•©ì„±"""
        prompt = f"""
        ì´ ì‚¬ëŒì´ {environment}ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ ì œí’ˆì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”
        ë¼ì´í”„ìŠ¤íƒ€ì¼ ì‚¬ì§„ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”. 
        ìì—°ìŠ¤ëŸ½ê³  ì¼ìƒì ì¸ ëŠë‚Œìœ¼ë¡œ ì—°ì¶œí•´ì£¼ì„¸ìš”.
        """
        
        return self.pipe(
            prompt=prompt,
            input_images=[person_image, product_image],
            image_guidance_scale=2.5,
            text_guidance_scale=7.0
        )

# ì‚¬ìš© ì˜ˆì‹œ
pipeline = ContentCreationPipeline()

# ì œí’ˆ ì‡¼ì¼€ì´ìŠ¤ ìƒì„±
showcase = pipeline.create_product_showcase(
    product_image=watch_image,
    brand_style="ë¯¸ë‹ˆë©€ë¦¬ìŠ¤íŠ¸ ëŸ­ì…”ë¦¬"
)

# ë¼ì´í”„ìŠ¤íƒ€ì¼ ì¥ë©´ ìƒì„±
lifestyle = pipeline.create_lifestyle_scene(
    person_image=model_image,
    product_image=watch_image,
    environment="ì„¸ë ¨ëœ ì¹´í˜"
)
```

### 2. êµìœ¡ ì½˜í…ì¸  ì œì‘

```python
# êµìœ¡ìš© ì‹œê° ìë£Œ ìƒì„±
def create_educational_content(concept, style="educational illustration"):
    """êµìœ¡ ê°œë…ì„ ì‹œê°í™”"""
    prompt = f"""
    {concept}ì„ ì„¤ëª…í•˜ëŠ” {style} ìŠ¤íƒ€ì¼ì˜ êµìœ¡ìš© ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
    ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš°ë©°, í•™ìŠµì— ë„ì›€ì´ ë˜ëŠ” ì‹œê°ì  ìš”ì†Œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
    ìƒ‰ìƒì€ ë°ê³  ì¹œê·¼í•˜ê²Œ, í…ìŠ¤íŠ¸ë‚˜ ë‹¤ì´ì–´ê·¸ë¨ ìš”ì†Œë„ í¬í•¨í•´ì£¼ì„¸ìš”.
    """
    
    return pipe(
        prompt=prompt,
        text_guidance_scale=8.0,
        negative_prompt="confusing, dark, unclear, complex"
    )

# ê³¼í•™ ê°œë… ì‹œê°í™”
science_visual = create_educational_content(
    "íƒœì–‘ê³„ì˜ êµ¬ì¡°ì™€ í–‰ì„±ë“¤ì˜ ê¶¤ë„",
    "ê³¼í•™ êµìœ¡ìš© ë‹¤ì´ì–´ê·¸ë¨"
)

# ì—­ì‚¬ ì¥ë©´ ì¬í˜„
history_visual = create_educational_content(
    "ì¡°ì„ ì‹œëŒ€ í•œì–‘ ë„ì„±ì˜ ëª¨ìŠµ",
    "ì—­ì‚¬ êµìœ¡ìš© ì¼ëŸ¬ìŠ¤íŠ¸"
)
```

## ë²¤ì¹˜ë§ˆí¬ ë° ì„±ëŠ¥ ë¶„ì„

### 1. ì •ëŸ‰ì  ì„±ëŠ¥ ì§€í‘œ

| ë©”íŠ¸ë¦­ | OmniGen2 | GPT-4o | Midjourney | DALL-E 3 |
|--------|----------|--------|-----------|----------|
| **ì´ë¯¸ì§€ í’ˆì§ˆ (FIDâ†“)** | 15.2 | 18.5 | 12.8 | 16.3 |
| **í…ìŠ¤íŠ¸ ì •í•©ì„±** | 89.2% | 85.7% | 88.1% | 87.4% |
| **í¸ì§‘ ì •í™•ë„** | 92.3% | N/A | N/A | N/A |
| **ì¶”ë¡  ì†ë„** | 2.8ì´ˆ | 4.1ì´ˆ | 3.5ì´ˆ | 5.2ì´ˆ |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** | 17GB | N/A | N/A | N/A |

### 2. ì‹¤ì œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```python
# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì½”ë“œ
import time
import torch
from PIL import Image

def benchmark_omnigen2():
    """OmniGen2 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    pipe = OmniGen2Pipeline.from_pretrained("OmniGen2/OmniGen2")
    
    test_prompts = [
        "A photorealistic portrait of a woman in natural lighting",
        "A fantasy landscape with dragons and castles",
        "A minimalist product design on white background",
        "An abstract artistic composition with vibrant colors"
    ]
    
    results = {
        "generation_times": [],
        "memory_usage": [],
        "image_quality_scores": []
    }
    
    for prompt in test_prompts:
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        torch.cuda.reset_peak_memory_stats()
        start_memory = torch.cuda.memory_allocated()
        
        # ìƒì„± ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        
        result = pipe(
            prompt=prompt,
            text_guidance_scale=7.5,
            num_inference_steps=50
        )
        
        end_time = time.time()
        peak_memory = torch.cuda.max_memory_allocated()
        
        # ê²°ê³¼ ê¸°ë¡
        results["generation_times"].append(end_time - start_time)
        results["memory_usage"].append((peak_memory - start_memory) / 1024**3)  # GB
        
        # í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
        quality_score = evaluate_image_quality(result.images[0])
        results["image_quality_scores"].append(quality_score)
    
    return results

def evaluate_image_quality(image):
    """ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ (ì˜ˆì‹œ)"""
    # ì‹¤ì œë¡œëŠ” CLIP Score, FID ë“±ì„ ì‚¬ìš©
    return 0.85  # ì˜ˆì‹œ ì ìˆ˜

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
benchmark_results = benchmark_omnigen2()
print(f"í‰ê·  ìƒì„± ì‹œê°„: {np.mean(benchmark_results['generation_times']):.2f}ì´ˆ")
print(f"í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {np.mean(benchmark_results['memory_usage']):.2f}GB")
print(f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {np.mean(benchmark_results['image_quality_scores']):.3f}")
```

## í–¥í›„ ë°œì „ ë°©í–¥ ë° ë¡œë“œë§µ

### 1. ì˜ˆì •ëœ ì—…ë°ì´íŠ¸

OmniGen2 íŒ€ì´ ê³µê°œí•œ ë¡œë“œë§µì— ë”°ë¥´ë©´:

- **âœ… ì™„ë£Œ**: ê¸°ìˆ  ë³´ê³ ì„œ ë°œí‘œ (2025-06-24)
- **ğŸ”„ ì§„í–‰ì¤‘**: In-context generation ë²¤ì¹˜ë§ˆí¬ "OmniContext" ê°œë°œ
- **ğŸ“‹ ì˜ˆì •**: 
  - CPU ì˜¤í”„ë¡œë“œ ë° ì¶”ë¡  íš¨ìœ¨ì„± ê°œì„ 
  - Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ í†µí•©
  - í›ˆë ¨ ë°ì´í„° ë° ìŠ¤í¬ë¦½íŠ¸ ê³µê°œ
  - ë°ì´í„° êµ¬ì¶• íŒŒì´í”„ë¼ì¸ ê³µê°œ
  - ComfyUI ë°ëª¨ (ì»¤ë®¤ë‹ˆí‹° ì§€ì›)

### 2. ì»¤ë®¤ë‹ˆí‹° í™•ì¥ ê³„íš

```python
# ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬ ì˜ˆì‹œ - ComfyUI ë…¸ë“œ ê°œë°œ
class OmniGen2Node:
    """ComfyUIìš© OmniGen2 ì»¤ìŠ¤í…€ ë…¸ë“œ"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "prompt": ("STRING", {"multiline": True}),
                "image_guidance_scale": ("FLOAT", {"default": 2.0, "min": 0.1, "max": 5.0}),
                "text_guidance_scale": ("FLOAT", {"default": 7.5, "min": 1.0, "max": 20.0}),
            },
            "optional": {
                "input_image": ("IMAGE",),
                "negative_prompt": ("STRING", {"multiline": True}),
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "generate"
    CATEGORY = "OmniGen2"
    
    def generate(self, prompt, image_guidance_scale, text_guidance_scale, 
                input_image=None, negative_prompt=""):
        # OmniGen2 ìƒì„± ë¡œì§
        result = self.pipe(
            prompt=prompt,
            input_images=[input_image] if input_image is not None else None,
            image_guidance_scale=image_guidance_scale,
            text_guidance_scale=text_guidance_scale,
            negative_prompt=negative_prompt
        )
        return (result.images[0],)

# ë…¸ë“œ ë“±ë¡
NODE_CLASS_MAPPINGS = {
    "OmniGen2Node": OmniGen2Node
}
```

## ì‹¤ë¬´ í™œìš© ì‹œ ê³ ë ¤ì‚¬í•­

### 1. ë¼ì´ì„¼ìŠ¤ ë° ìƒì—…ì  ì´ìš©

OmniGen2ëŠ” **Apache 2.0 ë¼ì´ì„¼ìŠ¤**ë¡œ ì œê³µë˜ì–´:
- âœ… ìƒì—…ì  ì´ìš© ê°€ëŠ¥
- âœ… ìˆ˜ì • ë° ì¬ë°°í¬ ê°€ëŠ¥  
- âœ… íŠ¹í—ˆ ê¶Œë¦¬ ë¶€ì—¬
- âš ï¸ ì €ì‘ê¶Œ ê³ ì§€ í•„ìš”

### 2. ìœ¤ë¦¬ì  ì‚¬ìš© ê°€ì´ë“œë¼ì¸

```python
# ì•ˆì „í•œ ì‚¬ìš©ì„ ìœ„í•œ ì½˜í…ì¸  í•„í„°ë§
class SafetyFilter:
    def __init__(self):
        self.blocked_keywords = [
            "harmful", "violent", "adult", "inappropriate"
        ]
    
    def check_prompt(self, prompt):
        """í”„ë¡¬í”„íŠ¸ ì•ˆì „ì„± ê²€ì‚¬"""
        for keyword in self.blocked_keywords:
            if keyword.lower() in prompt.lower():
                return False, f"ë¶€ì ì ˆí•œ í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤: {keyword}"
        return True, "ì•ˆì „í•¨"
    
    def filter_result(self, generated_image):
        """ìƒì„±ëœ ì´ë¯¸ì§€ í›„ì²˜ë¦¬ ê²€ì‚¬"""
        # NSFW íƒì§€ ëª¨ë¸ ë“±ì„ í™œìš©
        return True  # ì˜ˆì‹œ

# ì‚¬ìš© ì˜ˆì‹œ
safety = SafetyFilter()
is_safe, message = safety.check_prompt(user_prompt)

if is_safe:
    result = pipe(prompt=user_prompt)
    if safety.filter_result(result.images[0]):
        return result
else:
    print(f"ìš”ì²­ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤: {message}")
```

## ê²°ë¡ 

OmniGen2ëŠ” GPT-4oì˜ ì´ë¯¸ì§€ ìƒì„± ê¸°ëŠ¥ì„ ë›°ì–´ë„˜ëŠ” **4ê°€ì§€ í†µí•© ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥**ì„ ì œê³µí•˜ëŠ” í˜ì‹ ì ì¸ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì…ë‹ˆë‹¤. íŠ¹íˆ **In-context Generation**ê³¼ **ê³ ê¸‰ ì´ë¯¸ì§€ í¸ì§‘** ê¸°ëŠ¥ì€ ê¸°ì¡´ ìƒìš© ëª¨ë¸ë“¤ì´ ì œê³µí•˜ì§€ ëª»í•˜ëŠ” ë…íŠ¹í•œ ê°€ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### ğŸ¯ **í•µì‹¬ ì¥ì  ìš”ì•½**

1. **ì™„ì „í•œ ì œì–´ê¶Œ**: ì˜¤í”ˆì†ŒìŠ¤ë¡œ ëª¨ë“  ê¸°ëŠ¥ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
2. **ë…ì°½ì  ê¸°ëŠ¥**: In-context generationìœ¼ë¡œ ë‹¤ì¤‘ ì´ë¯¸ì§€ ì¡°í•©
3. **ê³ ê¸‰ í¸ì§‘**: ë³µì¡í•œ ì§€ì‹œì‚¬í•­ ê¸°ë°˜ ì •ë°€ ì´ë¯¸ì§€ í¸ì§‘
4. **ë¹„ìš© íš¨ìœ¨ì„±**: ë¡œì»¬ ì‹¤í–‰ìœ¼ë¡œ API ë¹„ìš© ì—†ìŒ
5. **í”„ë¼ì´ë²„ì‹œ**: ë°ì´í„°ê°€ ì™¸ë¶€ë¡œ ì „ì†¡ë˜ì§€ ì•ŠìŒ

### ğŸš€ **í™œìš© ì¶”ì²œ ë¶„ì•¼**

- **ì½˜í…ì¸  ì œì‘**: ë§ˆì¼€íŒ…, ê´‘ê³ , ì†Œì…œë¯¸ë””ì–´
- **êµìœ¡**: ì‹œê° ìë£Œ, êµìœ¡ ì½˜í…ì¸  ì œì‘
- **ì—°êµ¬**: ë©€í‹°ëª¨ë‹¬ AI ì—°êµ¬, ì»´í“¨í„° ë¹„ì „
- **ì—”í„°í…Œì¸ë¨¼íŠ¸**: ê²Œì„, ì˜í™”, ì• ë‹ˆë©”ì´ì…˜
- **ì „ììƒê±°ë˜**: ì œí’ˆ ì´ë¯¸ì§€ í¸ì§‘, ê°€ìƒ í”¼íŒ…

OmniGen2ëŠ” ë‹¨ìˆœí•œ ì´ë¯¸ì§€ ìƒì„±ì„ ë„˜ì–´ **ì°¨ì„¸ëŒ€ ë©€í‹°ëª¨ë‹¬ AIì˜ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±**ì„ ì œì‹œí•˜ëŠ” ëª¨ë¸ë¡œ, íŠ¹íˆ í•œêµ­ì˜ AI ì—°êµ¬ìì™€ ê°œë°œìë“¤ì—ê²Œ ë…ë¦½ì ì¸ AI ê¸°ìˆ  ê°œë°œì˜ ê¸°íšŒë¥¼ ì œê³µí•©ë‹ˆë‹¤. 