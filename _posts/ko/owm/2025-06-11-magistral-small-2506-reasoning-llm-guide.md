---
title: "Magistral-Small-2506: Mistral AIì˜ 24B ì¶”ë¡  íŠ¹í™” ì–¸ì–´ ëª¨ë¸ ì™„ì „ ê°€ì´ë“œ"
date: 2025-06-11
categories: 
  - owm
  - AI
tags: 
  - Mistral AI
  - Magistral-Small
  - Reasoning LLM
  - Open Source
  - Apache License
  - vLLM
  - Local Deployment
  - Multi-language
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
---

Mistral AIê°€ ìƒˆë¡­ê²Œ ì„ ë³´ì¸ **Magistral-Small-2506**ì€ ê¸°ì¡´ Mistral Small 3.1ì„ ê¸°ë°˜ìœ¼ë¡œ ê°•í™”ëœ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°–ì¶˜ í˜ì‹ ì ì¸ 24B íŒŒë¼ë¯¸í„° ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. ë‹¨ì¼ RTX 4090ì´ë‚˜ 32GB MacBookì—ì„œë„ ë¡œì»¬ ë°°í¬ê°€ ê°€ëŠ¥í•œ ì´ ëª¨ë¸ì€ Apache 2.0 ë¼ì´ì„¼ìŠ¤ í•˜ì— ìƒì—…ì  ì´ìš©ê¹Œì§€ í—ˆìš©í•˜ëŠ” ì™„ì „í•œ ì˜¤í”ˆì†ŒìŠ¤ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.

## ëª¨ë¸ ê°œìš”

### í•µì‹¬ íŠ¹ì§•

**Magistral-Small-2506**ì€ Mistral Small 3.1 (2503)ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ í˜ì‹ ì  íŠ¹ì§•ì„ ì œê³µí•©ë‹ˆë‹¤:

- **ğŸ§  ê°•í™”ëœ ì¶”ë¡  ëŠ¥ë ¥**: Magistral Mediumì˜ ì¶”ë¡  íŠ¸ë ˆì´ìŠ¤ë¥¼ í™œìš©í•œ SFTì™€ ê°•í™”í•™ìŠµì„ í†µí•´ ê¸´ ì¶”ë¡  ì²´ì¸ ìˆ˜í–‰ ê°€ëŠ¥
- **ğŸŒ 24ê°œ ì–¸ì–´ ì§€ì›**: ì˜ì–´, í•œêµ­ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´, ì•„ëì–´, íŒë””ì–´ ë“± ì „ ì„¸ê³„ ì£¼ìš” ì–¸ì–´ ì™„ë²½ ì§€ì›
- **ğŸ“„ Apache 2.0 ë¼ì´ì„¼ìŠ¤**: ìƒì—…ì /ë¹„ìƒì—…ì  ìš©ë„ ëª¨ë‘ ììœ ë¡œìš´ ì‚¬ìš© ë° ìˆ˜ì • í—ˆìš©
- **ğŸ’» ë¡œì»¬ ë°°í¬ ìµœì í™”**: RTX 4090 ë˜ëŠ” 32GB RAM MacBookì—ì„œ ì–‘ìí™” í›„ ë°°í¬ ê°€ëŠ¥

### ê¸°ìˆ  ì‚¬ì–‘

| í•­ëª© | ì‚¬ì–‘ |
|------|------|
| **íŒŒë¼ë¯¸í„° ìˆ˜** | 24B |
| **ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°** | 128k (ê¶Œì¥ 40k) |
| **ì§€ì› ì–¸ì–´** | 24ê°œ ì–¸ì–´ |
| **ë¼ì´ì„¼ìŠ¤** | Apache 2.0 |
| **ë² ì´ìŠ¤ ëª¨ë¸** | Mistral Small 3.1 (2503) |
| **íŠ¹í™” ë¶„ì•¼** | ì¶”ë¡ (Reasoning), ë‹¤êµ­ì–´ ëŒ€í™” |

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

Magistral-Small-2506ì€ ì£¼ìš” ì¶”ë¡  ë° ì½”ë”© ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:

### ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

| ëª¨ë¸ | AIME24 pass@1 | AIME25 pass@1 | GPQA Diamond | Livecodebench (v5) |
|------|---------------|---------------|--------------|-------------------|
| **Magistral Medium** | 73.59% | 64.95% | 70.83% | 59.36% |
| **Magistral Small** | **70.68%** | **62.76%** | **68.18%** | **55.84%** |

### ì„±ëŠ¥ ë¶„ì„

- **AIME ë²¤ì¹˜ë§ˆí¬**: ìˆ˜í•™ ë¬¸ì œ í•´ê²°ì—ì„œ 70% ì´ìƒì˜ ë†’ì€ ì •í™•ë„
- **GPQA Diamond**: ê³¼í•™ ì§€ì‹ ì¶”ë¡ ì—ì„œ 68% ë‹¬ì„±
- **Livecodebench**: ì‹¤ì‹œê°„ ì½”ë”© ë¬¸ì œì—ì„œ 55% ì´ìƒì˜ ì„±ëŠ¥

ì´ëŠ” 24B íŒŒë¼ë¯¸í„° ëª¨ë¸ ì¤‘ì—ì„œëŠ” ìµœìƒê¸‰ ì„±ëŠ¥ìœ¼ë¡œ, ë” í° ëª¨ë¸ë“¤ê³¼ ê²¬ì£¼ì–´ë„ ì†ìƒ‰ì—†ëŠ” ê²°ê³¼ì…ë‹ˆë‹¤.

## ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### vLLMì„ í†µí•œ ì„¤ì¹˜ (ê¶Œì¥)

Mistral AIëŠ” í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ **vLLM** ì‚¬ìš©ì„ ê°•ë ¥íˆ ê¶Œì¥í•©ë‹ˆë‹¤:

```bash
# vLLM ìµœì‹  ë²„ì „ ì„¤ì¹˜
pip install -U vllm \
    --pre \
    --extra-index-url https://wheels.vllm.ai/nightly

# mistral_common ë²„ì „ í™•ì¸ (1.6.0 ì´ìƒ í•„ìš”)
python -c "import mistral_common; print(mistral_common.__version__)"
```

### Docker í™˜ê²½

```bash
# Docker ì´ë¯¸ì§€ ì‚¬ìš©
docker pull vllm/vllm-openai:latest
```

### ëª¨ë¸ ì„œë¹™

```bash
# vLLM ì„œë²„ ì‹¤í–‰
vllm serve mistralai/Magistral-Small-2506 \
    --tokenizer_mode mistral \
    --config_format mistral \
    --load_format mistral \
    --tool-call-parser mistral \
    --enable-auto-tool-choice \
    --tensor-parallel-size 2
```

## ì‚¬ìš©ë²• ë° ì˜ˆì œ

### ê¸°ë³¸ ì±„íŒ… í…œí”Œë¦¿

Magistral-Small-2506ì€ ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” íŠ¹ë³„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
SYSTEM_PROMPT = """
A user will ask you to solve a task. You should first draft your thinking process (inner monologue) until you have derived the final answer. Afterwards, write a self-contained summary of your thoughts (i.e. your summary should be succinct but contain all the critical steps you needed to reach the conclusion). You should use Markdown to format your response. Write both your thoughts and summary in the same language as the task posed by the user. NEVER use \\boxed{} in your response.

Your thinking process must follow the template below:
<think>
Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate a correct answer.
</think>

Here, provide a concise summary that reflects your reasoning and presents a clear final answer to the user. Don't mention that this is a summary.
"""
```

### Python í´ë¼ì´ì–¸íŠ¸ ì˜ˆì œ

```python
from openai import OpenAI
from huggingface_hub import hf_hub_download

# vLLM ì„œë²„ì— ì—°ê²°
client = OpenAI(
    api_key="EMPTY",
    base_url="http://localhost:8000/v1",
)

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
def load_system_prompt(repo_id: str, filename: str) -> str:
    file_path = hf_hub_download(repo_id=repo_id, filename=filename)
    with open(file_path, "r") as file:
        return file.read()

SYSTEM_PROMPT = load_system_prompt("mistralai/Magistral-Small-2506", "SYSTEM_PROMPT.txt")

# ì¶”ë¡  ë¬¸ì œ ì˜ˆì œ
query = "í”„ë‘ìŠ¤ í˜ëª…ì´ ì‹œì‘ëœ ì§€ ì •í™•íˆ ë©°ì¹ ì´ ì§€ë‚¬ë‚˜ìš”? ì˜¤ëŠ˜ì€ 2025ë…„ 6ì›” 11ì¼ì…ë‹ˆë‹¤."

messages = [
    {"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": query}
]

# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
stream = client.chat.completions.create(
    model="mistralai/Magistral-Small-2506",
    messages=messages,
    stream=True,
    temperature=0.7,
    top_p=0.95,
    max_tokens=40960,
)

for chunk in stream:
    if hasattr(chunk.choices[0].delta, "content"):
        content = chunk.choices[0].delta.content
        if content:
            print(content, end="", flush=True)
```

### ìˆ˜í•™ ë¬¸ì œ í•´ê²° ì˜ˆì œ

```python
# ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œ
math_query = """
5ê°œì˜ ë¬´ì‘ìœ„ ìˆ«ìë¥¼ ìƒê°í•´ë³´ì„¸ìš”. 
ë§ì…ˆ, ê³±ì…ˆ, ëº„ì…ˆ, ë‚˜ëˆ—ì…ˆì„ ì‚¬ìš©í•´ì„œ 
ì´ ìˆ«ìë“¤ì„ ì¡°í•©í•˜ì—¬ 133ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
"""

# ëª¨ë¸ì€ <think> íƒœê·¸ ì•ˆì—ì„œ ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì •ì„ ë³´ì—¬ì£¼ê³ 
# ìµœì¢… ë‹µì•ˆì„ ëª…í™•í•˜ê²Œ ì œì‹œí•©ë‹ˆë‹¤
```

## ë°°í¬ ì˜µì…˜

### 1. ë¡œì»¬ ë°°í¬

**RTX 4090 í™˜ê²½:**

```bash
# ì–‘ìí™” ë²„ì „ ì‚¬ìš© ê¶Œì¥
# GGUF í˜•ì‹ìœ¼ë¡œ ë¡œë“œ
```

**MacBook (32GB RAM):**

```bash
# MLX ë˜ëŠ” llama.cpp í™œìš©
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–‘ìí™” í•„ìš”
```

### 2. í´ë¼ìš°ë“œ ë°°í¬

```yaml
# Kubernetes ë°°í¬ ì˜ˆì œ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: magistral-small-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: magistral-small
  template:
    metadata:
      labels:
        app: magistral-small
    spec:
      containers:
      - name: vllm-server
        image: vllm/vllm-openai:latest
        command: ["vllm", "serve", "mistralai/Magistral-Small-2506"]
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: "32Gi"
          limits:
            nvidia.com/gpu: 1
            memory: "64Gi"
```

## ì§€ì› í”„ë ˆì„ì›Œí¬

### ì¶”ë¡  í”„ë ˆì„ì›Œí¬

- **vLLM** (ê¶Œì¥): í”„ë¡œë•ì…˜ ë°°í¬ ìµœì í™”
- **llama.cpp**: CPU ì¶”ë¡  ë° ì–‘ìí™” ì§€ì›
- **Ollama**: ë¡œì»¬ ê°œë°œ í™˜ê²½ ìµœì í™”
- **LM Studio**: GUI ê¸°ë°˜ ë¡œì»¬ ì‹¤í–‰

### íŒŒì¸íŠœë‹ í”„ë ˆì„ì›Œí¬

- **Unsloth**: íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹ ì§€ì›
- **Axolotl**: ê³ ê¸‰ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì œê³µ

### ì–‘ìí™” ë²„ì „

| í”„ë ˆì„ì›Œí¬ | ë§í¬ | íŠ¹ì§• |
|------------|------|------|
| **llama.cpp** | [GGUF ë²„ì „](https://huggingface.co/mistralai/Magistral-Small-2506_gguf) | CPU ìµœì í™” |
| **Ollama** | [Ollama ë¼ì´ë¸ŒëŸ¬ë¦¬](https://ollama.com/library/magistral) | ì›í´ë¦­ ì„¤ì¹˜ |
| **Unsloth** | [GGUF ë²„ì „](https://huggingface.co/unsloth/Magistral-Small-2506-GGUF) | í›ˆë ¨ ìµœì í™” |

## ê¶Œì¥ ì„¤ì •

### ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°

ìµœì ì˜ ì„±ëŠ¥ì„ ìœ„í•´ ë‹¤ìŒ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

```python
RECOMMENDED_PARAMS = {
    "temperature": 0.7,
    "top_p": 0.95,
    "max_tokens": 40960,  # 40k ê¶Œì¥ (128k ì§€ì›í•˜ì§€ë§Œ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥)
}
```

### ë©”ëª¨ë¦¬ ìµœì í™”

```python
# GPU ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™”
MODEL_CONFIG = {
    "max_model_len": 40960,  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
    "gpu_memory_utilization": 0.85,  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
    "tensor_parallel_size": 2,  # ë©€í‹° GPU ì‚¬ìš© ì‹œ
}
```

## ì‹¤ì œ í™œìš© ì‚¬ë¡€

### 1. êµìœ¡ ë„êµ¬

```python
# ìˆ˜í•™ ë¬¸ì œ ë‹¨ê³„ë³„ í•´ì„¤
educational_prompt = """
ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í•´ê²°í•´ì£¼ì„¸ìš”:
"í•œ ìƒìì— ì‚¬ê³¼ 12ê°œê°€ ë“¤ì–´ìˆìŠµë‹ˆë‹¤. 
ì „ì²´ ì‚¬ê³¼ì˜ 3/4ì„ ë¨¹ì—ˆë‹¤ë©´ ë‚¨ì€ ì‚¬ê³¼ëŠ” ëª‡ ê°œì¼ê¹Œìš”?"
"""
```

### 2. ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸

```python
# ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œ í•´ê²°
coding_prompt = """
Pythonìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì˜ në²ˆì§¸ í•­ì„ êµ¬í•˜ëŠ” 
íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì‘ì„±í•˜ê³ , ì‹œê°„ ë³µì¡ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""
```

### 3. ë‹¤êµ­ì–´ ì§€ì›

```python
# í•œêµ­ì–´ ì¶”ë¡  ë¬¸ì œ
korean_prompt = """
ë§Œì•½ í•œ ì‹œê°„ì— 12ë²Œì˜ Tì…”ì¸ ë¥¼ í–‡ë¹›ì— ë§ë¦´ ìˆ˜ ìˆë‹¤ë©´, 
33ë²Œì˜ Tì…”ì¸ ë¥¼ ë§ë¦¬ëŠ” ë° ì–¼ë§ˆë‚˜ ê±¸ë¦´ê¹Œìš”?
"""
```

## ì„±ëŠ¥ ìµœì í™” íŒ

### 1. í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­

```bash
# ìµœì†Œ ìš”êµ¬ì‚¬í•­
- GPU: RTX 4090 (24GB VRAM) ë˜ëŠ” ë™ê¸‰
- RAM: 32GB ì´ìƒ
- ì €ì¥ê³µê°„: 50GB ì´ìƒ

# ê¶Œì¥ ìš”êµ¬ì‚¬í•­  
- GPU: H100 (80GB VRAM) ë˜ëŠ” A100
- RAM: 64GB ì´ìƒ
- NVMe SSD: 100GB ì´ìƒ
```

### 2. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```python
# íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
async def process_batch(queries, batch_size=4):
    results = []
    for i in range(0, len(queries), batch_size):
        batch = queries[i:i+batch_size]
        batch_results = await process_queries(batch)
        results.extend(batch_results)
    return results
```

## ë¹„êµ ë¶„ì„

### ë‹¤ë¥¸ ëª¨ë¸ ëŒ€ë¹„ ì¥ì 

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ì¶”ë¡  ëŠ¥ë ¥ | ë‹¤êµ­ì–´ | ë¼ì´ì„¼ìŠ¤ | ë¡œì»¬ ë°°í¬ |
|------|----------|-----------|--------|----------|-----------|
| **Magistral-Small** | 24B | âœ… ê°•í™”ë¨ | âœ… 24ê°œêµ­ | âœ… Apache 2.0 | âœ… ê°€ëŠ¥ |
| GPT-4o mini | ë¯¸ê³µê°œ | âœ… ìš°ìˆ˜ | âœ… ë‹¤ìˆ˜ | âŒ ìƒì—…ì  | âŒ ë¶ˆê°€ |
| Claude 3 Haiku | ë¯¸ê³µê°œ | âœ… ìš°ìˆ˜ | âœ… ë‹¤ìˆ˜ | âŒ ìƒì—…ì  | âŒ ë¶ˆê°€ |
| Llama 3.1 8B | 8B | âš ï¸ ê¸°ë³¸ | âœ… ë‹¤ìˆ˜ | âœ… Llama 2 | âœ… ê°€ëŠ¥ |

## í•œê³„ ë° ì£¼ì˜ì‚¬í•­

### ì„±ëŠ¥ ì œì•½

- **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: 128k ì§€ì›í•˜ì§€ë§Œ 40k ì´í›„ ì„±ëŠ¥ ì €í•˜
- **ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰**: ì–‘ìí™” ì—†ì´ëŠ” 48GB+ VRAM í•„ìš”
- **ì¶”ë¡  ì†ë„**: ì¶”ë¡  ê³¼ì • í‘œì‹œë¡œ ì¸í•œ í† í° ìƒì„±ëŸ‰ ì¦ê°€

### ì‚¬ìš© ì‹œ ê³ ë ¤ì‚¬í•­

```python
# ê¸´ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì‹œ ì£¼ì˜
if context_length > 40000:
    print("ê²½ê³ : 40k ì´ìƒì˜ ì»¨í…ìŠ¤íŠ¸ì—ì„œëŠ” ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
# ì¶”ë¡  ê³¼ì • ì œì–´
if need_fast_response:
    # <think> íƒœê·¸ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í”„ë¡¬í”„íŠ¸ ê³ ë ¤
    prompt = "ê°„ë‹¨íˆ ë‹µë³€í•´ì£¼ì„¸ìš”: " + user_query
```

## í–¥í›„ ì „ë§

Magistral-Small-2506ì€ ì˜¤í”ˆì†ŒìŠ¤ ì¶”ë¡  ëª¨ë¸ì˜ ìƒˆë¡œìš´ ê¸°ì¤€ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤:

### ê¸°ëŒ€ë˜ëŠ” ë°œì „ ë°©í–¥

- **ì„±ëŠ¥ í–¥ìƒ**: ë” í° ì»¨í…ìŠ¤íŠ¸ì—ì„œë„ ì•ˆì •ì ì¸ ì„±ëŠ¥ ìœ ì§€
- **íš¨ìœ¨ì„± ê°œì„ **: ì¶”ë¡  ê³¼ì • ìµœì í™”ë¡œ ì†ë„ í–¥ìƒ
- **íŠ¹í™” ë²„ì „**: ë„ë©”ì¸ë³„ íŠ¹í™” ëª¨ë¸ ì¶œì‹œ ê°€ëŠ¥ì„±
- **ìƒíƒœê³„ í™•ì¥**: ë” ë§ì€ ë„êµ¬ ë° í”„ë ˆì„ì›Œí¬ ì§€ì›

## ê²°ë¡ 

**Magistral-Small-2506**ì€ ì˜¤í”ˆì†ŒìŠ¤ LLM ìƒíƒœê³„ì— ìƒˆë¡œìš´ ì´ì •í‘œë¥¼ ì„¸ìš´ ëª¨ë¸ì…ë‹ˆë‹¤. ê°•í™”ëœ ì¶”ë¡  ëŠ¥ë ¥ê³¼ ë›°ì–´ë‚œ ë‹¤êµ­ì–´ ì§€ì›, ê·¸ë¦¬ê³  Apache 2.0 ë¼ì´ì„¼ìŠ¤ì˜ ììœ ë¡œì›€ê¹Œì§€ ê°–ì¶˜ ì´ ëª¨ë¸ì€ ì—°êµ¬ìë¶€í„° ê¸°ì—…ê¹Œì§€ í­ë„“ì€ ì‚¬ìš©ìì¸µì—ê²Œ ì‹¤ì§ˆì ì¸ ê°€ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

íŠ¹íˆ ë¡œì»¬ ë°°í¬ê°€ ê°€ëŠ¥í•œ 24B íŒŒë¼ë¯¸í„° ëª¨ë¸ë¡œì„œëŠ” ìµœê³  ìˆ˜ì¤€ì˜ ì¶”ë¡  ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, í”„ë¼ì´ë²„ì‹œê°€ ì¤‘ìš”í•œ í™˜ê²½ì´ë‚˜ ì œí•œëœ ë¦¬ì†ŒìŠ¤ í™˜ê²½ì—ì„œë„ ê°•ë ¥í•œ AI ê¸°ëŠ¥ì„ í™œìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ

- **ëª¨ë¸ í˜ì´ì§€**: [Hugging Face - Magistral-Small-2506](https://huggingface.co/mistralai/Magistral-Small-2506)
- **ê³µì‹ ë¸”ë¡œê·¸**: Mistral AI ê³µì‹ ë¸”ë¡œê·¸
- **ê¸°ìˆ  ë¬¸ì„œ**: [mistral-common ë¬¸ì„œ](https://github.com/mistralai/mistral-common)
- **vLLM ê°€ì´ë“œ**: [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/)

---

*ì´ í¬ìŠ¤íŠ¸ëŠ” 2025ë…„ 1ì›” 16ì¼ ê¸°ì¤€ ìµœì‹  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì„±ëŠ¥ ë° ê¸°ëŠ¥ì€ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.*
