---
title: "FantasyPortrait: ë‹¤ì¤‘ ìºë¦­í„° ì´ˆìƒí™” ì• ë‹ˆë©”ì´ì…˜ì„ ìœ„í•œ í‘œì • ì¦ê°• í™•ì‚° ë³€í™˜ê¸° ì™„ì „ ê°€ì´ë“œ"
excerpt: "Alibaba AMAP CV Labì˜ FantasyPortraitë¡œ ë‹¤ì¤‘ ì¸ë¬¼ ì´ˆìƒí™” ì• ë‹ˆë©”ì´ì…˜ ìƒì„±í•˜ê¸°. Multi-Expr ë°ì´í„°ì…‹ê³¼ í™•ì‚° ë³€í™˜ê¸°ë¥¼ í™œìš©í•œ ì›Œí¬í”Œë¡œìš° ìë™í™”"
seo_title: "FantasyPortrait ë‹¤ì¤‘ ìºë¦­í„° ì• ë‹ˆë©”ì´ì…˜ ê°€ì´ë“œ - í™•ì‚° ë³€í™˜ê¸° í™œìš© - Thaki Cloud"
seo_description: "FantasyPortraitë¥¼ í™œìš©í•œ ë‹¤ì¤‘ ì¸ë¬¼ ì´ˆìƒí™” ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì›Œí¬í”Œë¡œìš°. ì„¤ì¹˜ë¶€í„° Multi-Expr ë°ì´í„°ì…‹ í™œìš©ê¹Œì§€ í¬í•¨í•œ ì™„ì „ ê°€ì´ë“œ"
date: 2025-08-15
last_modified_at: 2025-08-15
categories:
  - owm
  - research
tags:
  - fantasyportrait
  - multi-character-animation
  - diffusion-transformers
  - expression-augmented
  - portrait-animation
  - alibaba-amap
  - multi-expr-dataset
  - video-generation
  - face-animation
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/owm/fantasyportrait-multi-character-animation-diffusion-transformers-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 12ë¶„

## ì„œë¡ 

ë‹¤ì¤‘ ì¸ë¬¼ì˜ ì´ˆìƒí™”ì— ìƒë™ê° ìˆëŠ” í‘œì •ê³¼ ì›€ì§ì„ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì€ ë””ì§€í„¸ ì½˜í…ì¸  ì œì‘ì—ì„œ ì˜¤ë«ë™ì•ˆ ë„ì „ ê³¼ì œì˜€ìŠµë‹ˆë‹¤. [FantasyPortrait](https://huggingface.co/acvlab/FantasyPortrait)ëŠ” Alibaba AMAP CV Labì—ì„œ ê°œë°œí•œ í˜ì‹ ì ì¸ ëª¨ë¸ë¡œ, **í‘œì • ì¦ê°• í™•ì‚° ë³€í™˜ê¸°(Expression-Augmented Diffusion Transformers)**ë¥¼ í™œìš©í•˜ì—¬ ì´ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

ì´ ëª¨ë¸ì€ ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ì‹œì‘í•˜ì—¬ ë‹¤ì¤‘ ìºë¦­í„°ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í‘œì • ë³€í™”ì™€ ì• ë‹ˆë©”ì´ì…˜ì„ ìƒì„±í•  ìˆ˜ ìˆìœ¼ë©°, íŠ¹íˆ **Multi-Expr Dataset**ì´ë¼ëŠ” ìµœì´ˆì˜ ë‹¤ì¤‘ ì´ˆìƒí™” í‘œì • ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ì„ ê³µê°œí•˜ì—¬ ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ì— í° ê¸°ì—¬ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### ì´ ê°€ì´ë“œì—ì„œ ë°°ìš¸ ë‚´ìš©

- FantasyPortraitì˜ í•µì‹¬ ê¸°ìˆ ê³¼ ì•„í‚¤í…ì²˜
- Multi-Expr Datasetì˜ íŠ¹ì§•ê³¼ í™œìš© ë°©ë²•
- ê°œë°œ í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ì„¤ì¹˜
- ë‹¨ì¼ ë° ë‹¤ì¤‘ ì¸ë¬¼ ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì›Œí¬í”Œë¡œìš°
- ì„±ëŠ¥ ìµœì í™” ë° VRAM íš¨ìœ¨ì  ì‚¬ìš©ë²•
- ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ í™œìš© ì‚¬ë¡€

### ê¸°ìˆ  ìš”êµ¬ì‚¬í•­

- **GPU**: NVIDIA A100 ê¶Œì¥ (ìµœì†Œ 5GB VRAM)
- **Python**: 3.8+
- **PyTorch**: 2.0.0+
- **CUDA**: 11.8+
- **ë©”ëª¨ë¦¬**: ìµœì†Œ 32GB RAM

## FantasyPortrait ê¸°ìˆ  ê°œìš”

### í•µì‹¬ í˜ì‹  ì‚¬í•­

FantasyPortraitëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ìˆ ì  í˜ì‹ ì„ í†µí•´ ê¸°ì¡´ ì´ˆìƒí™” ì• ë‹ˆë©”ì´ì…˜ì˜ í•œê³„ë¥¼ ê·¹ë³µí•©ë‹ˆë‹¤:

#### 1. í‘œì • ì¦ê°• í™•ì‚° ë³€í™˜ê¸° (Expression-Augmented Diffusion Transformers)

```python
# í•µì‹¬ ì•„í‚¤í…ì²˜ ê°œë…
class ExpressionAugmentedDiT:
    def __init__(self):
        self.expression_encoder = ExpressionEncoder()
        self.diffusion_transformer = DiffusionTransformer()
        self.multi_character_handler = MultiCharacterHandler()
    
    def forward(self, input_image, expression_guidance):
        # í‘œì • íŠ¹ì§• ì¶”ì¶œ
        expr_features = self.expression_encoder(expression_guidance)
        
        # ë‹¤ì¤‘ ìºë¦­í„° ì²˜ë¦¬
        char_features = self.multi_character_handler(input_image)
        
        # í™•ì‚° ë³€í™˜ê¸°ë¥¼ í†µí•œ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
        animation = self.diffusion_transformer(
            char_features, 
            expr_features
        )
        
        return animation
```

#### 2. Multi-Expr Dataset

FantasyPortraitëŠ” ìµœì´ˆì˜ ë‹¤ì¤‘ ì´ˆìƒí™” í‘œì • ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ì„ ê³µê°œí–ˆìŠµë‹ˆë‹¤:

- **ë‹¤ì–‘í•œ í‘œì •**: ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸, ë†€ë¼ì›€ ë“± ë³µì¡í•œ ê°ì • í‘œí˜„
- **ë‹¤ì¤‘ ì¸ë¬¼ ì§€ì›**: í•œ ì¥ë©´ì—ì„œ ì—¬ëŸ¬ ì‚¬ëŒì˜ í‘œì • ë³€í™” ë™ì‹œ ì²˜ë¦¬
- **ê³ í’ˆì§ˆ ë¹„ë””ì˜¤**: 720p í•´ìƒë„ì˜ ë¶€ë“œëŸ¬ìš´ ì• ë‹ˆë©”ì´ì…˜
- **í’ë¶€í•œ ë©”íƒ€ë°ì´í„°**: ê° í”„ë ˆì„ë³„ í‘œì • ë¼ë²¨ê³¼ ê°•ë„ ì •ë³´

#### 3. ê³„ì¸µì  ì²˜ë¦¬ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Image    â”‚    â”‚  Expression     â”‚    â”‚  Multi-Characterâ”‚
â”‚  Processing     â”‚â”€â”€â”€â”€â”‚  Guidance       â”‚â”€â”€â”€â”€â”‚  Animation      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Face Detection â”‚    â”‚  Expression     â”‚    â”‚  Temporal       â”‚
â”‚  & Segmentation â”‚    â”‚  Encoding       â”‚    â”‚  Consistency    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## í™˜ê²½ ì„¤ì • ë° ì„¤ì¹˜

### 1ë‹¨ê³„: ê¸°ë³¸ í™˜ê²½ ì¤€ë¹„

```bash
# ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
nvidia-smi  # CUDA ì§€ì› GPU í™•ì¸
python --version  # Python 3.8+ í™•ì¸

# ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (Ubuntu/Debian)
sudo apt-get update
sudo apt-get install -y ffmpeg git wget curl

# macOSì˜ ê²½ìš°
brew install ffmpeg git wget
```

### 2ë‹¨ê³„: FantasyPortrait ì €ì¥ì†Œ í´ë¡ 

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/ai-projects/fantasy-portrait
cd ~/ai-projects/fantasy-portrait

# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/Fantasy-AMAP/fantasy-portrait.git
cd fantasy-portrait

# ë¸Œëœì¹˜ í™•ì¸
git branch -a
git checkout main  # ë˜ëŠ” ìµœì‹  release ë¸Œëœì¹˜
```

### 3ë‹¨ê³„: Python ê°€ìƒí™˜ê²½ ì„¤ì •

```bash
# conda í™˜ê²½ ìƒì„± (ê¶Œì¥)
conda create -n fantasy-portrait python=3.9
conda activate fantasy-portrait

# ë˜ëŠ” venv ì‚¬ìš©
python -m venv fantasy-portrait-env
source fantasy-portrait-env/bin/activate  # Linux/macOS
# fantasy-portrait-env\Scripts\activate  # Windows
```

### 4ë‹¨ê³„: ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# PyTorch ì„¤ì¹˜ (CUDA ë²„ì „ì— ë§ê²Œ)
pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118

# í”„ë¡œì íŠ¸ ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# Flash Attention ì„¤ì¹˜ (í•„ìˆ˜)
pip install flash-attn --no-build-isolation

# ì¶”ê°€ ìœ í‹¸ë¦¬í‹°
pip install accelerate transformers diffusers
```

### 5ë‹¨ê³„: ì„¤ì¹˜ ê²€ì¦

```python
# test_installation.py
import torch
import torch.nn.functional as F
from diffusers import AutoencoderKL
import flash_attn

def test_installation():
    print("ğŸ” ì„¤ì¹˜ í™•ì¸ ì¤‘...")
    
    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    print(f"âœ… CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        print(f"   í˜„ì¬ GPU: {torch.cuda.get_device_name()}")
        print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    # Flash Attention í™•ì¸
    try:
        import flash_attn
        print("âœ… Flash Attention ì„¤ì¹˜ë¨")
    except ImportError:
        print("âŒ Flash Attention ì„¤ì¹˜ ì‹¤íŒ¨")
        return False
    
    # Diffusers í™•ì¸
    try:
        from diffusers import AutoencoderKL
        print("âœ… Diffusers ì„¤ì¹˜ë¨")
    except ImportError:
        print("âŒ Diffusers ì„¤ì¹˜ ì‹¤íŒ¨")
        return False
    
    print("ğŸ‰ ëª¨ë“  ì˜ì¡´ì„±ì´ ì„±ê³µì ìœ¼ë¡œ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤!")
    return True

if __name__ == "__main__":
    test_installation()
```

## ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •

### ìë™ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# download_models.sh

set -e

echo "ğŸš€ FantasyPortrait ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘..."

# ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ./models

# Hugging Face CLI ì„¤ì¹˜ í™•ì¸
if ! command -v huggingface-cli &> /dev/null; then
    echo "ğŸ“¦ Hugging Face CLI ì„¤ì¹˜ ì¤‘..."
    pip install "huggingface_hub[cli]"
fi

# ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (Wan2.1-I2V-14B-720P)
echo "ğŸ“¥ ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-720P \
    --local-dir ./models/Wan2.1-I2V-14B-720P \
    --resume-download

# FantasyPortrait ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ
echo "ğŸ“¥ FantasyPortrait ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ì¤‘..."
huggingface-cli download acvlab/FantasyPortrait \
    --local-dir ./models/FantasyPortrait \
    --resume-download

# ë‹¤ìš´ë¡œë“œ í™•ì¸
echo "âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!"
echo "ğŸ“ ëª¨ë¸ ìœ„ì¹˜:"
echo "   ë² ì´ìŠ¤ ëª¨ë¸: ./models/Wan2.1-I2V-14B-720P"
echo "   FantasyPortrait: ./models/FantasyPortrait"

# ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
echo "ğŸ’¾ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰:"
du -sh ./models/*
```

### ModelScopeë¥¼ í†µí•œ ë‹¤ìš´ë¡œë“œ (ì¤‘êµ­ ì‚¬ìš©ììš©)

```bash
#!/bin/bash
# download_models_modelscope.sh

echo "ğŸš€ ModelScopeë¥¼ í†µí•œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ..."

# ModelScope CLI ì„¤ì¹˜
pip install modelscope

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
modelscope download Wan-AI/Wan2.1-I2V-14B-720P \
    --local_dir ./models/Wan2.1-I2V-14B-720P

modelscope download amap_cvlab/FantasyPortrait \
    --local_dir ./models/FantasyPortrait

echo "âœ… ModelScope ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!"
```

### ì„¤ì • íŒŒì¼ ìƒì„±

```yaml
# config/fantasy_portrait.yaml
model:
  base_model_path: "./models/Wan2.1-I2V-14B-720P"
  fantasy_portrait_path: "./models/FantasyPortrait"
  
generation:
  resolution: [720, 1280]  # height, width
  num_frames: 16
  fps: 8
  guidance_scale: 7.5
  num_inference_steps: 20

optimization:
  torch_dtype: "bfloat16"  # bfloat16, float16, float32
  num_persistent_param_in_dit: 7000000000  # 7B for 20GB VRAM
  enable_model_offload: true
  enable_sequential_cpu_offload: false

output:
  output_dir: "./outputs"
  save_intermediate_frames: false
  video_codec: "libx264"
  video_quality: "high"
```

## Multi-Expr Dataset í™œìš©

### ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

```python
# download_dataset.py
import os
from huggingface_hub import snapshot_download

def download_multi_expr_dataset():
    """Multi-Expr Dataset ë‹¤ìš´ë¡œë“œ"""
    print("ğŸ“Š Multi-Expr Dataset ë‹¤ìš´ë¡œë“œ ì¤‘...")
    
    # ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
    dataset_path = snapshot_download(
        repo_id="acvlab/FantasyPortrait-Multi-Expr",
        repo_type="dataset",
        local_dir="./datasets/multi-expr",
        resume_download=True
    )
    
    print(f"âœ… ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {dataset_path}")
    
    # ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
    for root, dirs, files in os.walk(dataset_path):
        level = root.replace(dataset_path, '').count(os.sep)
        indent = ' ' * 2 * level
        print(f"{indent}{os.path.basename(root)}/")
        subindent = ' ' * 2 * (level + 1)
        for file in files[:5]:  # ì²˜ìŒ 5ê°œ íŒŒì¼ë§Œ í‘œì‹œ
            print(f"{subindent}{file}")
        if len(files) > 5:
            print(f"{subindent}... ì™¸ {len(files)-5}ê°œ íŒŒì¼")

if __name__ == "__main__":
    download_multi_expr_dataset()
```

### ë°ì´í„°ì…‹ êµ¬ì¡° ë¶„ì„

```python
# analyze_dataset.py
import json
import cv2
import numpy as np
from pathlib import Path

class MultiExprAnalyzer:
    def __init__(self, dataset_path="./datasets/multi-expr"):
        self.dataset_path = Path(dataset_path)
        self.metadata = self.load_metadata()
    
    def load_metadata(self):
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        metadata_file = self.dataset_path / "metadata.json"
        if metadata_file.exists():
            with open(metadata_file) as f:
                return json.load(f)
        return {}
    
    def analyze_videos(self):
        """ë¹„ë””ì˜¤ íŒŒì¼ ë¶„ì„"""
        video_files = list(self.dataset_path.glob("**/*.mp4"))
        
        analysis = {
            "total_videos": len(video_files),
            "resolutions": {},
            "durations": [],
            "fps_values": [],
            "expression_types": {}
        }
        
        for video_file in video_files[:50]:  # ìƒ˜í”Œ ë¶„ì„
            cap = cv2.VideoCapture(str(video_file))
            
            # ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = frame_count / fps if fps > 0 else 0
            
            # í†µê³„ ìˆ˜ì§‘
            resolution = f"{width}x{height}"
            analysis["resolutions"][resolution] = analysis["resolutions"].get(resolution, 0) + 1
            analysis["durations"].append(duration)
            analysis["fps_values"].append(fps)
            
            cap.release()
        
        # í†µê³„ ìš”ì•½
        if analysis["durations"]:
            analysis["avg_duration"] = np.mean(analysis["durations"])
            analysis["avg_fps"] = np.mean(analysis["fps_values"])
        
        return analysis
    
    def get_expression_categories(self):
        """í‘œì • ì¹´í…Œê³ ë¦¬ ë¶„ì„"""
        categories = {
            "happy": ["smile", "laugh", "joy", "cheerful"],
            "sad": ["cry", "tear", "sorrow", "melancholy"],
            "angry": ["frown", "rage", "mad", "furious"],
            "surprised": ["shock", "amazed", "astonished"],
            "neutral": ["calm", "normal", "default"],
            "fear": ["scared", "afraid", "worried"],
            "disgust": ["disgusted", "repulsed"]
        }
        
        return categories
    
    def print_analysis(self):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        analysis = self.analyze_videos()
        
        print("ğŸ“Š Multi-Expr Dataset ë¶„ì„ ê²°ê³¼")
        print("=" * 50)
        print(f"ì´ ë¹„ë””ì˜¤ ìˆ˜: {analysis['total_videos']}")
        print(f"í‰ê·  ê¸¸ì´: {analysis.get('avg_duration', 0):.2f}ì´ˆ")
        print(f"í‰ê·  FPS: {analysis.get('avg_fps', 0):.1f}")
        
        print("\nğŸ“ í•´ìƒë„ ë¶„í¬:")
        for resolution, count in analysis['resolutions'].items():
            print(f"  {resolution}: {count}ê°œ")
        
        print(f"\nğŸ­ í‘œì • ì¹´í…Œê³ ë¦¬: {len(self.get_expression_categories())}ê°œ")
        for category, expressions in self.get_expression_categories().items():
            print(f"  {category}: {', '.join(expressions[:3])}{'...' if len(expressions) > 3 else ''}")

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    analyzer = MultiExprAnalyzer()
    analyzer.print_analysis()
```

## ë‹¨ì¼ ì¸ë¬¼ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±

### ê¸°ë³¸ ì›Œí¬í”Œë¡œìš°

```python
# single_portrait_inference.py
import torch
import cv2
import numpy as np
from PIL import Image
from pathlib import Path
import yaml

class SinglePortraitGenerator:
    def __init__(self, config_path="config/fantasy_portrait.yaml"):
        self.config = self.load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        
    def load_config(self, config_path):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ”„ FantasyPortrait ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        # ì—¬ê¸°ì„œ ì‹¤ì œ ëª¨ë¸ ë¡œë”© ë¡œì§ êµ¬í˜„
        # (ì‹¤ì œ êµ¬í˜„ì€ FantasyPortraitì˜ ê³µì‹ ì½”ë“œ ì°¸ì¡°)
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def preprocess_image(self, image_path):
        """ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        image = Image.open(image_path).convert("RGB")
        
        # í•´ìƒë„ ì¡°ì •
        target_size = tuple(self.config['generation']['resolution'])
        image = image.resize(target_size[::-1], Image.LANCZOS)  # PILì€ (width, height)
        
        # í…ì„œ ë³€í™˜
        image_array = np.array(image) / 255.0
        image_tensor = torch.from_numpy(image_array).float()
        image_tensor = image_tensor.permute(2, 0, 1).unsqueeze(0)  # BCHW í˜•íƒœ
        
        return image_tensor.to(self.device)
    
    def generate_animation(self, image_path, expression_guidance=None, output_path=None):
        """ë‹¨ì¼ ì¸ë¬¼ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±"""
        if self.model is None:
            self.load_model()
        
        # ì…ë ¥ ì´ë¯¸ì§€ ì²˜ë¦¬
        input_tensor = self.preprocess_image(image_path)
        
        # í‘œì • ê°€ì´ë˜ìŠ¤ ì„¤ì •
        if expression_guidance is None:
            expression_guidance = {
                "expression_type": "happy",
                "intensity": 0.8,
                "transition_frames": 8
            }
        
        print(f"ğŸ¬ ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì¤‘...")
        print(f"   ì…ë ¥ ì´ë¯¸ì§€: {image_path}")
        print(f"   í‘œì • íƒ€ì…: {expression_guidance['expression_type']}")
        print(f"   ê°•ë„: {expression_guidance['intensity']}")
        
        # ì‹¤ì œ ì¶”ë¡  (êµ¬í˜„ í•„ìš”)
        # output_frames = self.model.generate(
        #     input_tensor, 
        #     expression_guidance,
        #     num_frames=self.config['generation']['num_frames'],
        #     guidance_scale=self.config['generation']['guidance_scale']
        # )
        
        # ì„ì‹œ ê²°ê³¼ ìƒì„± (ë°ëª¨ìš©)
        output_frames = self.create_demo_frames(input_tensor)
        
        # ë¹„ë””ì˜¤ ì €ì¥
        if output_path is None:
            output_path = f"./outputs/single_portrait_{Path(image_path).stem}.mp4"
        
        self.save_video(output_frames, output_path)
        
        return output_path
    
    def create_demo_frames(self, input_tensor):
        """ë°ëª¨ìš© í”„ë ˆì„ ìƒì„±"""
        num_frames = self.config['generation']['num_frames']
        frames = []
        
        # ê°„ë‹¨í•œ ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼ (ì‹¤ì œë¡œëŠ” ëª¨ë¸ ì¶œë ¥ ì‚¬ìš©)
        base_frame = input_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()
        
        for i in range(num_frames):
            # ê°„ë‹¨í•œ ë³€í˜• íš¨ê³¼
            frame = base_frame.copy()
            # ì—¬ê¸°ì„œ ì‹¤ì œë¡œëŠ” ëª¨ë¸ì´ ìƒì„±í•œ í”„ë ˆì„ì„ ì‚¬ìš©
            frames.append(frame)
        
        return np.stack(frames)
    
    def save_video(self, frames, output_path):
        """í”„ë ˆì„ì„ ë¹„ë””ì˜¤ë¡œ ì €ì¥"""
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        height, width = frames.shape[1:3]
        fps = self.config['generation']['fps']
        
        # OpenCV ë¹„ë””ì˜¤ ë¼ì´í„° ì„¤ì •
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
        
        for frame in frames:
            # RGB to BGR ë³€í™˜
            frame_bgr = cv2.cvtColor((frame * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)
            writer.write(frame_bgr)
        
        writer.release()
        print(f"âœ… ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {output_path}")

# ì‚¬ìš© ì˜ˆì œ
def run_single_portrait_demo():
    generator = SinglePortraitGenerator()
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë“¤
    test_images = [
        "./samples/portrait1.jpg",
        "./samples/portrait2.jpg"
    ]
    
    expressions = [
        {"expression_type": "happy", "intensity": 0.8},
        {"expression_type": "surprised", "intensity": 0.6},
        {"expression_type": "sad", "intensity": 0.7}
    ]
    
    for image_path in test_images:
        if Path(image_path).exists():
            for expr in expressions:
                output_path = generator.generate_animation(
                    image_path, 
                    expression_guidance=expr
                )
                print(f"ğŸ¥ ìƒì„±ëœ ë¹„ë””ì˜¤: {output_path}")

if __name__ == "__main__":
    run_single_portrait_demo()
```

### ì‹¤ì œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# infer_single.sh

echo "ğŸ¬ ë‹¨ì¼ ì¸ë¬¼ ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì‹œì‘..."

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export CUDA_VISIBLE_DEVICES=0
export TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6"

# Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python scripts/inference_single.py \
    --input_image "./samples/input_portrait.jpg" \
    --output_dir "./outputs/single" \
    --expression_type "happy" \
    --intensity 0.8 \
    --num_frames 16 \
    --guidance_scale 7.5 \
    --seed 42 \
    --torch_dtype "bfloat16"

echo "âœ… ë‹¨ì¼ ì¸ë¬¼ ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì™„ë£Œ!"
```

## ë‹¤ì¤‘ ì¸ë¬¼ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±

### ë‹¤ì¤‘ ì¸ë¬¼ ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜

```python
# multi_portrait_inference.py
import torch
import numpy as np
from PIL import Image, ImageDraw
import cv2
import mediapipe as mp
from typing import List, Dict, Tuple

class MultiPortraitGenerator:
    def __init__(self, config_path="config/fantasy_portrait.yaml"):
        self.config = self.load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # MediaPipe ì–¼êµ´ ê°ì§€ ì´ˆê¸°í™”
        self.mp_face_detection = mp.solutions.face_detection
        self.mp_drawing = mp.solutions.drawing_utils
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=1, 
            min_detection_confidence=0.5
        )
        
    def load_config(self, config_path):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        import yaml
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    
    def detect_faces(self, image_path):
        """ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ê°ì§€"""
        image = cv2.imread(image_path)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        results = self.face_detection.process(image_rgb)
        
        faces = []
        if results.detections:
            for i, detection in enumerate(results.detections):
                bbox = detection.location_data.relative_bounding_box
                h, w, _ = image.shape
                
                # ì ˆëŒ€ ì¢Œí‘œë¡œ ë³€í™˜
                x = int(bbox.xmin * w)
                y = int(bbox.ymin * h)
                width = int(bbox.width * w)
                height = int(bbox.height * h)
                
                faces.append({
                    "id": i,
                    "bbox": [x, y, width, height],
                    "confidence": detection.score[0],
                    "center": [x + width//2, y + height//2]
                })
        
        return faces, image_rgb
    
    def extract_face_regions(self, image, faces):
        """ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ"""
        face_regions = []
        
        for face in faces:
            x, y, w, h = face["bbox"]
            
            # ì–¼êµ´ ì˜ì—­ í™•ì¥ (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
            margin = 0.2
            x_margin = int(w * margin)
            y_margin = int(h * margin)
            
            x1 = max(0, x - x_margin)
            y1 = max(0, y - y_margin)
            x2 = min(image.shape[1], x + w + x_margin)
            y2 = min(image.shape[0], y + h + y_margin)
            
            face_region = image[y1:y2, x1:x2]
            
            face_regions.append({
                "id": face["id"],
                "region": face_region,
                "original_bbox": [x1, y1, x2-x1, y2-y1],
                "face_bbox": face["bbox"]
            })
        
        return face_regions
    
    def generate_multi_portrait_animation(
        self, 
        image_path: str, 
        expression_configs: List[Dict],
        output_path: str = None
    ):
        """ë‹¤ì¤‘ ì¸ë¬¼ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±"""
        print("ğŸ­ ë‹¤ì¤‘ ì¸ë¬¼ ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì‹œì‘...")
        
        # ì–¼êµ´ ê°ì§€
        faces, image = self.detect_faces(image_path)
        print(f"ğŸ‘¥ ê°ì§€ëœ ì–¼êµ´ ìˆ˜: {len(faces)}")
        
        if len(faces) == 0:
            raise ValueError("ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # í‘œì • ì„¤ì • ë§¤ì¹­
        if len(expression_configs) == 1:
            # ëª¨ë“  ì–¼êµ´ì— ê°™ì€ í‘œì • ì ìš©
            expression_configs = expression_configs * len(faces)
        elif len(expression_configs) != len(faces):
            raise ValueError(f"í‘œì • ì„¤ì • ìˆ˜({len(expression_configs)})ì™€ ì–¼êµ´ ìˆ˜({len(faces)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        
        # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
        face_regions = self.extract_face_regions(image, faces)
        
        # ê° ì–¼êµ´ì— ëŒ€í•´ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
        animated_faces = []
        for i, (face_region, expr_config) in enumerate(zip(face_regions, expression_configs)):
            print(f"ğŸ¬ ì–¼êµ´ {i+1}/{len(faces)} ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì¤‘...")
            
            # ê°œë³„ ì–¼êµ´ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
            face_animation = self.generate_single_face_animation(
                face_region, 
                expr_config
            )
            
            animated_faces.append({
                "id": face_region["id"],
                "animation": face_animation,
                "bbox": face_region["original_bbox"]
            })
        
        # ì• ë‹ˆë©”ì´ì…˜ í•©ì„±
        final_animation = self.composite_animations(
            image, 
            animated_faces
        )
        
        # ë¹„ë””ì˜¤ ì €ì¥
        if output_path is None:
            output_path = f"./outputs/multi_portrait_{Path(image_path).stem}.mp4"
        
        self.save_video(final_animation, output_path)
        
        return output_path
    
    def generate_single_face_animation(self, face_region, expression_config):
        """ë‹¨ì¼ ì–¼êµ´ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” FantasyPortrait ëª¨ë¸ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ë°ëª¨ìš© ê°„ë‹¨í•œ ë³€í˜• ì ìš©
        
        region = face_region["region"]
        num_frames = self.config['generation']['num_frames']
        
        animation_frames = []
        for frame_idx in range(num_frames):
            # ê°„ë‹¨í•œ ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼ (ì‹¤ì œë¡œëŠ” ëª¨ë¸ ì¶œë ¥)
            frame = region.copy()
            
            # í‘œì •ì— ë”°ë¥¸ ë³€í˜• (ë°ëª¨ìš©)
            if expression_config["expression_type"] == "happy":
                # ë°ê¸° ì¦ê°€ íš¨ê³¼
                frame = cv2.convertScaleAbs(frame, alpha=1.1, beta=10)
            elif expression_config["expression_type"] == "sad":
                # ì±„ë„ ê°ì†Œ íš¨ê³¼
                frame = cv2.convertScaleAbs(frame, alpha=0.9, beta=-10)
            
            animation_frames.append(frame)
        
        return np.stack(animation_frames)
    
    def composite_animations(self, background_image, animated_faces):
        """ì• ë‹ˆë©”ì´ì…˜ í•©ì„±"""
        num_frames = self.config['generation']['num_frames']
        h, w = background_image.shape[:2]
        
        composite_frames = []
        
        for frame_idx in range(num_frames):
            # ë°°ê²½ ì´ë¯¸ì§€ë¡œ ì‹œì‘
            composite_frame = background_image.copy()
            
            # ê° ì–¼êµ´ ì• ë‹ˆë©”ì´ì…˜ í•©ì„±
            for animated_face in animated_faces:
                face_frame = animated_face["animation"][frame_idx]
                x, y, face_w, face_h = animated_face["bbox"]
                
                # ì–¼êµ´ ì˜ì—­ í¬ê¸° ì¡°ì •
                face_frame_resized = cv2.resize(face_frame, (face_w, face_h))
                
                # ì˜ì—­ í™•ì¸ ë° ì¡°ì •
                x2, y2 = x + face_w, y + face_h
                if x2 <= w and y2 <= h and x >= 0 and y >= 0:
                    composite_frame[y:y2, x:x2] = face_frame_resized
            
            composite_frames.append(composite_frame)
        
        return np.stack(composite_frames)
    
    def save_video(self, frames, output_path):
        """ë¹„ë””ì˜¤ ì €ì¥"""
        from pathlib import Path
        
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        height, width = frames.shape[1:3]
        fps = self.config['generation']['fps']
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
        
        for frame in frames:
            # RGB to BGR ë³€í™˜ (í•„ìš”ì‹œ)
            if frame.shape[2] == 3:  # RGB
                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            else:
                frame_bgr = frame
            
            writer.write(frame_bgr)
        
        writer.release()
        print(f"âœ… ë‹¤ì¤‘ ì¸ë¬¼ ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {output_path}")

# ì‚¬ìš© ì˜ˆì œ
def run_multi_portrait_demo():
    generator = MultiPortraitGenerator()
    
    # ë‹¤ì¤‘ ì¸ë¬¼ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸
    image_path = "./samples/group_photo.jpg"
    
    # ê° ì¸ë¬¼ë³„ í‘œì • ì„¤ì •
    expression_configs = [
        {"expression_type": "happy", "intensity": 0.8},
        {"expression_type": "surprised", "intensity": 0.6},
        {"expression_type": "neutral", "intensity": 0.5}
    ]
    
    # ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
    output_path = generator.generate_multi_portrait_animation(
        image_path, 
        expression_configs
    )
    
    print(f"ğŸ¥ ë‹¤ì¤‘ ì¸ë¬¼ ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì™„ë£Œ: {output_path}")

if __name__ == "__main__":
    run_multi_portrait_demo()
```

### ë°°ì¹˜ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°

```python
# batch_processing.py
import os
import json
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp

class BatchProcessor:
    def __init__(self, config_path="config/fantasy_portrait.yaml"):
        self.config = self.load_config(config_path)
        self.max_workers = min(mp.cpu_count(), 4)  # GPU ë©”ëª¨ë¦¬ ê³ ë ¤
        
    def load_config(self, config_path):
        import yaml
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    
    def process_batch_job(self, job_config):
        """ë°°ì¹˜ ì‘ì—… ì²˜ë¦¬"""
        input_dir = Path(job_config["input_dir"])
        output_dir = Path(job_config["output_dir"])
        processing_type = job_config.get("type", "single")  # single or multi
        
        # ì…ë ¥ ì´ë¯¸ì§€ ìˆ˜ì§‘
        image_extensions = [".jpg", ".jpeg", ".png", ".bmp"]
        image_files = []
        
        for ext in image_extensions:
            image_files.extend(input_dir.glob(f"*{ext}"))
            image_files.extend(input_dir.glob(f"*{ext.upper()}"))
        
        print(f"ğŸ“ ë°œê²¬ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì‘ì—… ëª©ë¡ ìƒì„±
        tasks = []
        for image_file in image_files:
            task = {
                "input_path": str(image_file),
                "output_path": str(output_dir / f"{image_file.stem}_animated.mp4"),
                "type": processing_type,
                "expression_config": job_config.get("expression_config", {
                    "expression_type": "happy",
                    "intensity": 0.8
                })
            }
            tasks.append(task)
        
        # ë³‘ë ¬ ì²˜ë¦¬
        if job_config.get("parallel", True):
            self.process_parallel(tasks)
        else:
            self.process_sequential(tasks)
        
        # ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
        self.generate_report(tasks, output_dir)
    
    def process_parallel(self, tasks):
        """ë³‘ë ¬ ì²˜ë¦¬"""
        print(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ì›Œì»¤ ìˆ˜: {self.max_workers})")
        
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self.process_single_task, task) for task in tasks]
            
            for i, future in enumerate(futures):
                try:
                    result = future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                    print(f"âœ… ì‘ì—… {i+1}/{len(tasks)} ì™„ë£Œ: {result}")
                except Exception as e:
                    print(f"âŒ ì‘ì—… {i+1}/{len(tasks)} ì‹¤íŒ¨: {e}")
    
    def process_sequential(self, tasks):
        """ìˆœì°¨ ì²˜ë¦¬"""
        print("ğŸ”„ ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘")
        
        for i, task in enumerate(tasks):
            try:
                result = self.process_single_task(task)
                print(f"âœ… ì‘ì—… {i+1}/{len(tasks)} ì™„ë£Œ: {result}")
            except Exception as e:
                print(f"âŒ ì‘ì—… {i+1}/{len(tasks)} ì‹¤íŒ¨: {e}")
    
    def process_single_task(self, task):
        """ë‹¨ì¼ ì‘ì—… ì²˜ë¦¬"""
        import torch
        
        # GPU ë©”ëª¨ë¦¬ ê´€ë¦¬
        torch.cuda.empty_cache()
        
        if task["type"] == "single":
            generator = SinglePortraitGenerator()
            return generator.generate_animation(
                task["input_path"],
                task["expression_config"],
                task["output_path"]
            )
        elif task["type"] == "multi":
            generator = MultiPortraitGenerator()
            return generator.generate_multi_portrait_animation(
                task["input_path"],
                [task["expression_config"]],
                task["output_path"]
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì²˜ë¦¬ íƒ€ì…: {task['type']}")
    
    def generate_report(self, tasks, output_dir):
        """ì²˜ë¦¬ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            "total_tasks": len(tasks),
            "successful_tasks": 0,
            "failed_tasks": 0,
            "output_files": [],
            "processing_time": None
        }
        
        for task in tasks:
            output_path = Path(task["output_path"])
            if output_path.exists():
                report["successful_tasks"] += 1
                report["output_files"].append(str(output_path))
            else:
                report["failed_tasks"] += 1
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = output_dir / "processing_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nğŸ“Š ì²˜ë¦¬ ì™„ë£Œ ë³´ê³ ì„œ:")
        print(f"   ì´ ì‘ì—…: {report['total_tasks']}")
        print(f"   ì„±ê³µ: {report['successful_tasks']}")
        print(f"   ì‹¤íŒ¨: {report['failed_tasks']}")
        print(f"   ë³´ê³ ì„œ: {report_path}")

# ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ
def run_batch_processing():
    processor = BatchProcessor()
    
    job_config = {
        "input_dir": "./samples/batch_input",
        "output_dir": "./outputs/batch_output",
        "type": "single",  # ë˜ëŠ” "multi"
        "parallel": True,
        "expression_config": {
            "expression_type": "happy",
            "intensity": 0.8
        }
    }
    
    processor.process_batch_job(job_config)

if __name__ == "__main__":
    run_batch_processing()
```

## ì„±ëŠ¥ ìµœì í™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬

### VRAM ìµœì í™” ì „ëµ

```python
# optimization.py
import torch
import gc
from contextlib import contextmanager

class MemoryOptimizer:
    def __init__(self):
        self.optimization_strategies = {
            "ultra_low": {
                "torch_dtype": torch.float16,
                "num_persistent_param_in_dit": 0,
                "enable_model_offload": True,
                "enable_sequential_cpu_offload": True,
                "enable_attention_slicing": True,
                "batch_size": 1
            },
            "balanced": {
                "torch_dtype": torch.bfloat16,
                "num_persistent_param_in_dit": 7e9,  # 7B
                "enable_model_offload": True,
                "enable_sequential_cpu_offload": False,
                "enable_attention_slicing": False,
                "batch_size": 2
            },
            "high_performance": {
                "torch_dtype": torch.bfloat16,
                "num_persistent_param_in_dit": None,  # ë¬´ì œí•œ
                "enable_model_offload": False,
                "enable_sequential_cpu_offload": False,
                "enable_attention_slicing": False,
                "batch_size": 4
            }
        }
    
    def get_optimization_config(self, available_vram_gb):
        """ì‚¬ìš© ê°€ëŠ¥í•œ VRAMì— ë”°ë¥¸ ìµœì í™” ì„¤ì • ë°˜í™˜"""
        if available_vram_gb < 8:
            return self.optimization_strategies["ultra_low"]
        elif available_vram_gb < 24:
            return self.optimization_strategies["balanced"]
        else:
            return self.optimization_strategies["high_performance"]
    
    @contextmanager
    def memory_efficient_inference(self):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì¶”ë¡  ì»¨í…ìŠ¤íŠ¸"""
        try:
            # ì‚¬ì „ ì •ë¦¬
            torch.cuda.empty_cache()
            gc.collect()
            
            yield
            
        finally:
            # ì‚¬í›„ ì •ë¦¬
            torch.cuda.empty_cache()
            gc.collect()
    
    def monitor_memory_usage(self):
        """VRAM ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            total = torch.cuda.get_device_properties(0).total_memory / 1e9
            
            print(f"ğŸ’¾ VRAM ì‚¬ìš©ëŸ‰:")
            print(f"   í• ë‹¹ë¨: {allocated:.1f}GB")
            print(f"   ì˜ˆì•½ë¨: {reserved:.1f}GB")
            print(f"   ì „ì²´: {total:.1f}GB")
            print(f"   ì‚¬ìš©ë¥ : {(allocated/total)*100:.1f}%")
            
            return {
                "allocated_gb": allocated,
                "reserved_gb": reserved,
                "total_gb": total,
                "usage_percent": (allocated/total)*100
            }
        
        return None

class OptimizedFantasyPortrait:
    def __init__(self, optimization_level="balanced"):
        self.optimizer = MemoryOptimizer()
        
        # ì‹œìŠ¤í…œ VRAM í™•ì¸
        if torch.cuda.is_available():
            vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
        else:
            vram_gb = 0
        
        if optimization_level == "auto":
            self.config = self.optimizer.get_optimization_config(vram_gb)
        else:
            self.config = self.optimizer.optimization_strategies[optimization_level]
        
        print(f"ğŸ”§ ìµœì í™” ë ˆë²¨: {optimization_level}")
        print(f"   ì‚¬ìš© ê°€ëŠ¥ VRAM: {vram_gb:.1f}GB")
        print(f"   Torch dtype: {self.config['torch_dtype']}")
        print(f"   ëª¨ë¸ ì˜¤í”„ë¡œë“œ: {self.config['enable_model_offload']}")
    
    def generate_with_optimization(self, input_data, **kwargs):
        """ìµœì í™”ëœ ì¶”ë¡  ì‹¤í–‰"""
        with self.optimizer.memory_efficient_inference():
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            before_memory = self.optimizer.monitor_memory_usage()
            
            # ì‹¤ì œ ì¶”ë¡  ì‹¤í–‰ (êµ¬í˜„ í•„ìš”)
            result = self.run_inference(input_data, **kwargs)
            
            # í›„ì²˜ë¦¬ ë©”ëª¨ë¦¬ í™•ì¸
            after_memory = self.optimizer.monitor_memory_usage()
            
            if before_memory and after_memory:
                memory_diff = after_memory["allocated_gb"] - before_memory["allocated_gb"]
                print(f"ğŸ“ˆ ì¶”ë¡  ì¤‘ ë©”ëª¨ë¦¬ ì¦ê°€: {memory_diff:.1f}GB")
            
            return result
    
    def run_inference(self, input_data, **kwargs):
        """ì‹¤ì œ ì¶”ë¡  ë¡œì§ (êµ¬í˜„ í•„ìš”)"""
        # ì—¬ê¸°ì„œ FantasyPortrait ëª¨ë¸ ì‹¤í–‰
        pass

# ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬
class OptimizedBatchProcessor:
    def __init__(self, optimization_level="balanced"):
        self.optimizer = MemoryOptimizer()
        self.model = OptimizedFantasyPortrait(optimization_level)
        
    def process_batch_with_memory_management(self, image_paths, batch_size=None):
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ê°€ í¬í•¨ëœ ë°°ì¹˜ ì²˜ë¦¬"""
        if batch_size is None:
            batch_size = self.model.config["batch_size"]
        
        results = []
        
        for i in range(0, len(image_paths), batch_size):
            batch_paths = image_paths[i:i+batch_size]
            
            print(f"ğŸ”„ ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(batch_paths)}ê°œ ì´ë¯¸ì§€)")
            
            with self.optimizer.memory_efficient_inference():
                batch_results = []
                
                for image_path in batch_paths:
                    try:
                        result = self.model.generate_with_optimization(image_path)
                        batch_results.append(result)
                    except torch.cuda.OutOfMemoryError:
                        print("âš ï¸ CUDA OOM ë°œìƒ, ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì¬ì‹œë„")
                        torch.cuda.empty_cache()
                        gc.collect()
                        
                        # ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì¬ì‹œë„
                        result = self.model.generate_with_optimization(image_path)
                        batch_results.append(result)
                
                results.extend(batch_results)
        
        return results

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # ìë™ ìµœì í™”
    processor = OptimizedBatchProcessor("auto")
    
    # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    image_paths = ["./samples/image1.jpg", "./samples/image2.jpg"]
    results = processor.process_batch_with_memory_management(image_paths)
    
    print(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
```

## ì‹¤ì œ í™œìš© ì‚¬ë¡€ ë° ì›Œí¬í”Œë¡œìš°

### ë””ì§€í„¸ ì½˜í…ì¸  ì œì‘ íŒŒì´í”„ë¼ì¸

```python
# production_pipeline.py
import os
import json
from pathlib import Path
from datetime import datetime
import logging

class ContentProductionPipeline:
    def __init__(self, project_config):
        self.config = project_config
        self.setup_logging()
        self.setup_directories()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_dir = Path(self.config["output_dir"]) / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / f"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
    
    def setup_directories(self):
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì •"""
        base_dir = Path(self.config["output_dir"])
        
        self.directories = {
            "raw": base_dir / "01_raw",
            "processed": base_dir / "02_processed", 
            "animated": base_dir / "03_animated",
            "final": base_dir / "04_final",
            "metadata": base_dir / "metadata"
        }
        
        for dir_path in self.directories.values():
            dir_path.mkdir(parents=True, exist_ok=True)
    
    def process_character_batch(self, character_data):
        """ìºë¦­í„° ë°°ì¹˜ ì²˜ë¦¬"""
        self.logger.info(f"ğŸ­ ìºë¦­í„° ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(character_data)}ê°œ")
        
        results = []
        
        for i, character in enumerate(character_data):
            try:
                self.logger.info(f"ìºë¦­í„° {i+1}/{len(character_data)} ì²˜ë¦¬ ì¤‘: {character['name']}")
                
                # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                processed_image = self.preprocess_character_image(character)
                
                # 2. í‘œì • ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
                animations = self.generate_character_animations(
                    processed_image, 
                    character["expressions"]
                )
                
                # 3. í›„ì²˜ë¦¬ ë° í’ˆì§ˆ ìµœì í™”
                final_animations = self.postprocess_animations(animations)
                
                # 4. ë©”íƒ€ë°ì´í„° ì €ì¥
                metadata = self.save_character_metadata(character, final_animations)
                
                results.append({
                    "character_id": character["id"],
                    "name": character["name"],
                    "animations": final_animations,
                    "metadata": metadata,
                    "status": "success"
                })
                
            except Exception as e:
                self.logger.error(f"ìºë¦­í„° {character['name']} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                results.append({
                    "character_id": character["id"],
                    "name": character["name"],
                    "status": "failed",
                    "error": str(e)
                })
        
        return results
    
    def preprocess_character_image(self, character):
        """ìºë¦­í„° ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        from PIL import Image, ImageEnhance
        
        image_path = character["image_path"]
        image = Image.open(image_path).convert("RGB")
        
        # í’ˆì§ˆ í–¥ìƒ
        if character.get("enhance_quality", True):
            # ì„ ëª…ë„ í–¥ìƒ
            enhancer = ImageEnhance.Sharpness(image)
            image = enhancer.enhance(1.2)
            
            # ëŒ€ë¹„ í–¥ìƒ
            enhancer = ImageEnhance.Contrast(image)
            image = enhancer.enhance(1.1)
        
        # í•´ìƒë„ ì¡°ì •
        target_size = character.get("target_resolution", [720, 1280])
        image = image.resize(target_size[::-1], Image.LANCZOS)
        
        # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ì €ì¥
        processed_path = self.directories["processed"] / f"{character['id']}_processed.jpg"
        image.save(processed_path, quality=95)
        
        return processed_path
    
    def generate_character_animations(self, image_path, expressions):
        """ìºë¦­í„° ì• ë‹ˆë©”ì´ì…˜ ìƒì„±"""
        from multi_portrait_inference import MultiPortraitGenerator
        
        generator = MultiPortraitGenerator()
        animations = {}
        
        for expr_name, expr_config in expressions.items():
            self.logger.info(f"  í‘œì • '{expr_name}' ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì¤‘...")
            
            output_path = self.directories["animated"] / f"{Path(image_path).stem}_{expr_name}.mp4"
            
            try:
                result_path = generator.generate_multi_portrait_animation(
                    str(image_path),
                    [expr_config],
                    str(output_path)
                )
                
                animations[expr_name] = {
                    "path": result_path,
                    "config": expr_config,
                    "status": "success"
                }
                
            except Exception as e:
                self.logger.error(f"  í‘œì • '{expr_name}' ìƒì„± ì‹¤íŒ¨: {e}")
                animations[expr_name] = {
                    "status": "failed",
                    "error": str(e)
                }
        
        return animations
    
    def postprocess_animations(self, animations):
        """ì• ë‹ˆë©”ì´ì…˜ í›„ì²˜ë¦¬"""
        import subprocess
        
        final_animations = {}
        
        for expr_name, animation in animations.items():
            if animation["status"] != "success":
                final_animations[expr_name] = animation
                continue
            
            input_path = animation["path"]
            output_path = self.directories["final"] / f"{Path(input_path).stem}_final.mp4"
            
            # FFmpegë¥¼ ì‚¬ìš©í•œ í’ˆì§ˆ ìµœì í™”
            ffmpeg_cmd = [
                "ffmpeg", "-i", input_path,
                "-c:v", "libx264",
                "-preset", "medium",
                "-crf", "23",
                "-pix_fmt", "yuv420p",
                "-movflags", "+faststart",
                "-y", str(output_path)
            ]
            
            try:
                subprocess.run(ffmpeg_cmd, check=True, capture_output=True)
                
                final_animations[expr_name] = {
                    **animation,
                    "final_path": str(output_path),
                    "file_size": os.path.getsize(output_path)
                }
                
                self.logger.info(f"  '{expr_name}' í›„ì²˜ë¦¬ ì™„ë£Œ: {output_path}")
                
            except subprocess.CalledProcessError as e:
                self.logger.error(f"  '{expr_name}' í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                final_animations[expr_name] = animation
        
        return final_animations
    
    def save_character_metadata(self, character, animations):
        """ìºë¦­í„° ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata = {
            "character_id": character["id"],
            "name": character["name"],
            "original_image": character["image_path"],
            "processing_timestamp": datetime.now().isoformat(),
            "animations": {},
            "total_animations": len(animations),
            "successful_animations": len([a for a in animations.values() if a["status"] == "success"])
        }
        
        for expr_name, animation in animations.items():
            metadata["animations"][expr_name] = {
                "status": animation["status"],
                "config": animation.get("config", {}),
                "output_path": animation.get("final_path", animation.get("path")),
                "file_size_mb": animation.get("file_size", 0) / (1024 * 1024)
            }
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥
        metadata_path = self.directories["metadata"] / f"{character['id']}_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        return metadata
    
    def generate_project_report(self, results):
        """í”„ë¡œì íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        total_characters = len(results)
        successful = len([r for r in results if r["status"] == "success"])
        failed = total_characters - successful
        
        report = {
            "project_summary": {
                "total_characters": total_characters,
                "successful": successful,
                "failed": failed,
                "success_rate": (successful / total_characters) * 100 if total_characters > 0 else 0
            },
            "processing_details": results,
            "output_directories": {k: str(v) for k, v in self.directories.items()},
            "generated_timestamp": datetime.now().isoformat()
        }
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = self.directories["metadata"] / "project_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ìš”ì•½ ì¶œë ¥
        print(f"\nğŸ“Š í”„ë¡œì íŠ¸ ì™„ë£Œ ë³´ê³ ì„œ")
        print(f"=" * 50)
        print(f"ì´ ìºë¦­í„°: {total_characters}")
        print(f"ì„±ê³µ: {successful}")
        print(f"ì‹¤íŒ¨: {failed}")
        print(f"ì„±ê³µë¥ : {report['project_summary']['success_rate']:.1f}%")
        print(f"ë³´ê³ ì„œ ìœ„ì¹˜: {report_path}")
        
        return report

# í”„ë¡œë•ì…˜ íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ì˜ˆì œ
def run_production_pipeline():
    # í”„ë¡œì íŠ¸ ì„¤ì •
    project_config = {
        "project_name": "Fantasy_Portrait_Demo",
        "output_dir": "./production_output",
        "optimization_level": "balanced"
    }
    
    # ìºë¦­í„° ë°ì´í„° ì •ì˜
    character_data = [
        {
            "id": "char_001",
            "name": "ì£¼ì¸ê³µ",
            "image_path": "./samples/protagonist.jpg",
            "expressions": {
                "happy": {"expression_type": "happy", "intensity": 0.8},
                "sad": {"expression_type": "sad", "intensity": 0.7},
                "angry": {"expression_type": "angry", "intensity": 0.9}
            },
            "target_resolution": [720, 1280],
            "enhance_quality": True
        },
        {
            "id": "char_002", 
            "name": "ì¡°ì—°",
            "image_path": "./samples/supporting.jpg",
            "expressions": {
                "surprised": {"expression_type": "surprised", "intensity": 0.8},
                "neutral": {"expression_type": "neutral", "intensity": 0.5}
            },
            "target_resolution": [720, 1280],
            "enhance_quality": True
        }
    ]
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = ContentProductionPipeline(project_config)
    results = pipeline.process_character_batch(character_data)
    report = pipeline.generate_project_report(results)
    
    return report

if __name__ == "__main__":
    run_production_pipeline()
```

## ì»¤ë®¤ë‹ˆí‹° í™œìš© ë° í™•ì¥

### ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬ ê°€ì´ë“œ

FantasyPortraitëŠ” í™œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# community_extensions.py
class CommunityExtensions:
    def __init__(self):
        self.extension_registry = {}
    
    def register_extension(self, name, extension_class):
        """ì»¤ë®¤ë‹ˆí‹° í™•ì¥ ë“±ë¡"""
        self.extension_registry[name] = extension_class
        print(f"âœ… í™•ì¥ '{name}' ë“±ë¡ë¨")
    
    def load_extension(self, name):
        """í™•ì¥ ë¡œë“œ"""
        if name in self.extension_registry:
            return self.extension_registry[name]()
        else:
            raise ValueError(f"í™•ì¥ '{name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# ì»¤ë®¤ë‹ˆí‹° í™•ì¥ ì˜ˆì œ
class EmotionIntensityController:
    """í‘œì • ê°•ë„ ì„¸ë°€ ì œì–´ í™•ì¥"""
    
    def __init__(self):
        self.emotion_mappings = {
            "joy": {"base_intensity": 0.8, "variation_range": 0.3},
            "sorrow": {"base_intensity": 0.7, "variation_range": 0.4},
            "anger": {"base_intensity": 0.9, "variation_range": 0.2}
        }
    
    def apply_emotion_curve(self, base_config, curve_type="linear"):
        """í‘œì • ë³€í™” ê³¡ì„  ì ìš©"""
        curves = {
            "linear": lambda x: x,
            "ease_in": lambda x: x * x,
            "ease_out": lambda x: 1 - (1 - x) ** 2,
            "bounce": lambda x: 1 - abs(1 - 2 * x)
        }
        
        curve_func = curves.get(curve_type, curves["linear"])
        
        # í‘œì • ê°•ë„ ê³¡ì„  ê³„ì‚°
        modified_config = base_config.copy()
        modified_config["intensity_curve"] = curve_func
        
        return modified_config

class StyleTransferExtension:
    """ìŠ¤íƒ€ì¼ ì „ì´ í™•ì¥"""
    
    def apply_artistic_style(self, animation_config, style="anime"):
        """ì˜ˆìˆ ì  ìŠ¤íƒ€ì¼ ì ìš©"""
        style_configs = {
            "anime": {
                "color_saturation": 1.3,
                "contrast_boost": 1.2,
                "eye_enhancement": True
            },
            "realistic": {
                "color_saturation": 1.0,
                "contrast_boost": 1.0,
                "skin_smoothing": True
            },
            "cartoon": {
                "color_saturation": 1.5,
                "contrast_boost": 1.4,
                "edge_enhancement": True
            }
        }
        
        style_config = style_configs.get(style, style_configs["realistic"])
        
        modified_config = animation_config.copy()
        modified_config["style_transfer"] = style_config
        
        return modified_config

# í™•ì¥ ì‚¬ìš© ì˜ˆì œ
def use_community_extensions():
    extensions = CommunityExtensions()
    
    # í™•ì¥ ë“±ë¡
    extensions.register_extension("emotion_controller", EmotionIntensityController)
    extensions.register_extension("style_transfer", StyleTransferExtension)
    
    # í™•ì¥ ì‚¬ìš©
    emotion_ctrl = extensions.load_extension("emotion_controller")
    style_transfer = extensions.load_extension("style_transfer")
    
    # ê¸°ë³¸ ì„¤ì •
    base_config = {
        "expression_type": "happy",
        "intensity": 0.8
    }
    
    # í‘œì • ê³¡ì„  ì ìš©
    emotion_config = emotion_ctrl.apply_emotion_curve(base_config, "ease_in")
    
    # ìŠ¤íƒ€ì¼ ì ìš©
    final_config = style_transfer.apply_artistic_style(emotion_config, "anime")
    
    print("ğŸ¨ í™•ì¥ì´ ì ìš©ëœ ìµœì¢… ì„¤ì •:")
    print(json.dumps(final_config, indent=2))

if __name__ == "__main__":
    use_community_extensions()
```

## ê²°ë¡ 

FantasyPortraitëŠ” ë‹¤ì¤‘ ìºë¦­í„° ì´ˆìƒí™” ì• ë‹ˆë©”ì´ì…˜ ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ëŒíŒŒêµ¬ë¥¼ ì œì‹œí•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œë¥¼ í†µí•´ ë‹¤ìŒì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤:

### í•µì‹¬ ì„±ê³¼

- **ìµœì²¨ë‹¨ ê¸°ìˆ **: í‘œì • ì¦ê°• í™•ì‚° ë³€í™˜ê¸°ë¥¼ í™œìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
- **í¬ê´„ì  ë°ì´í„°ì…‹**: ìµœì´ˆì˜ Multi-Expr Datasetìœ¼ë¡œ ë‹¤ì–‘í•œ í‘œì • í‘œí˜„ ì§€ì›
- **ì‹¤ìš©ì  ì›Œí¬í”Œë¡œìš°**: ë‹¨ì¼/ë‹¤ì¤‘ ì¸ë¬¼ ì²˜ë¦¬ë¶€í„° ë°°ì¹˜ ì‘ì—…ê¹Œì§€ ì™„ì „ ìë™í™”
- **ìµœì í™”ëœ ì„±ëŠ¥**: VRAM íš¨ìœ¨ì  ì‚¬ìš©ìœ¼ë¡œ ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ í™˜ê²½ ì§€ì›

### í–¥í›„ ë°œì „ ë°©í–¥

1. **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° í™˜ê²½ì—ì„œì˜ ì‹¤ì‹œê°„ í‘œì • ì• ë‹ˆë©”ì´ì…˜
2. **ë©€í‹°ëª¨ë‹¬ í™•ì¥**: ìŒì„±, í…ìŠ¤íŠ¸ì™€ ì—°ë™í•œ í†µí•© ìºë¦­í„° ìƒì„±
3. **ê³ í•´ìƒë„ ì§€ì›**: 4K, 8K í•´ìƒë„ì—ì„œì˜ ê³ í’ˆì§ˆ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
4. **ëª¨ë°”ì¼ ìµœì í™”**: ìŠ¤ë§ˆíŠ¸í°ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê²½ëŸ‰í™” ëª¨ë¸ ê°œë°œ

### í™œìš© ë¶„ì•¼

- **ì—”í„°í…Œì¸ë¨¼íŠ¸**: ê²Œì„, ì• ë‹ˆë©”ì´ì…˜, VR/AR ì½˜í…ì¸ 
- **êµìœ¡**: ì¸í„°ë™í‹°ë¸Œ í•™ìŠµ ë„êµ¬, ê°€ìƒ ê°•ì‚¬
- **ë§ˆì¼€íŒ…**: ê°œì¸í™”ëœ ê´‘ê³ , ë¸Œëœë“œ ìºë¦­í„°
- **ì†Œì…œë¯¸ë””ì–´**: ê°œì¸ ì•„ë°”íƒ€, ê°ì • í‘œí˜„ ìŠ¤í‹°ì»¤

### ìœ ìš©í•œ ë¦¬ì†ŒìŠ¤

- [FantasyPortrait Hugging Face ëª¨ë¸](https://huggingface.co/acvlab/FantasyPortrait)
- [Multi-Expr Dataset](https://huggingface.co/datasets/acvlab/FantasyPortrait-Multi-Expr)
- [arXiv ë…¼ë¬¸](https://arxiv.org/abs/2507.12956)
- [GitHub ì €ì¥ì†Œ](https://github.com/Fantasy-AMAP/fantasy-portrait)

FantasyPortraitë¡œ ì°½ì˜ì ì¸ ë””ì§€í„¸ ì½˜í…ì¸  ì œì‘ì˜ ìƒˆë¡œìš´ ì§€í‰ì„ ì—´ì–´ë³´ì„¸ìš”! ğŸ­âœ¨

---

_ì´ ê°€ì´ë“œê°€ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ [Hugging Face](https://huggingface.co/acvlab/FantasyPortrait)ì—ì„œ â¤ï¸ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”._
