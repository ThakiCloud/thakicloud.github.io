---
title: "NVIDIA Nemotron Nano 2 9B: ì—£ì§€ AI ì¶”ë¡  í˜ì‹ ê³¼ Thinking Budget ì™„ì „ ê°€ì´ë“œ"
excerpt: "6ë°° ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ì™€ 60% ë¹„ìš© ì ˆê°ì„ ì œê³µí•˜ëŠ” NVIDIA Nemotron Nano 2 9B ëª¨ë¸ì˜ Hybrid Transformer-Mamba ì•„í‚¤í…ì²˜ì™€ ì‹¤ìŠµ ê°€ì´ë“œ"
seo_title: "NVIDIA Nemotron Nano 2 9B ì—£ì§€ AI ì¶”ë¡  ëª¨ë¸ ì™„ì „ ê°€ì´ë“œ - Thaki Cloud"
seo_description: "NVIDIA Nemotron Nano 2 9Bì˜ Hybrid Transformer-Mamba ì•„í‚¤í…ì²˜, Thinking Budget ê¸°ëŠ¥, ì‹¤ìŠµ ê°€ì´ë“œë¡œ ì—£ì§€ AI ì¶”ë¡  ì„±ëŠ¥ì„ í˜ì‹ í•˜ì„¸ìš”"
date: 2025-08-19
last_modified_at: 2025-08-19
categories:
  - owm
  - llmops
tags:
  - NVIDIA
  - Nemotron
  - Edge-AI
  - Transformer
  - Mamba
  - Thinking-Budget
  - AI-Reasoning
  - vLLM
  - Model-Compression
  - Knowledge-Distillation
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/owm/nvidia-nemotron-nano-2-9b-edge-ai-reasoning-complete-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 12ë¶„

## ì„œë¡ 

AI ì—ì´ì „íŠ¸ê°€ ì—£ì§€ë¶€í„° í´ë¼ìš°ë“œê¹Œì§€ ì£¼ë¥˜ê°€ ë˜ë©´ì„œ, ë³µì¡í•œ ë‹¤ë‹¨ê³„ ë¬¸ì œë¥¼ ììœ¨ì ìœ¼ë¡œ í•´ê²°í•˜ëŠ” ì •êµí•œ ì¶”ë¡ ê³¼ ë°˜ë³µì  ê³„íš ëŠ¥ë ¥ì´ ìš”êµ¬ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì—£ì§€ì—ì„œ ì´ëŸ¬í•œ AI ì—ì´ì „íŠ¸ì˜ ìµœê³  ì„±ëŠ¥ì„ ì–»ê¸° ìœ„í•´ì„œëŠ” ì •í™•ì„±ë¿ë§Œ ì•„ë‹ˆë¼ ë†’ì€ íš¨ìœ¨ì„±ì„ ì œê³µí•˜ëŠ” ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤.

2025ë…„ 8ì›” 18ì¼ ê³µê°œëœ **NVIDIA Nemotron Nano 2 9B**ëŠ” Hybrid Transformer-Mamba ì•„í‚¤í…ì²˜ì™€ êµ¬ì„± ê°€ëŠ¥í•œ **Thinking Budget** ê¸°ëŠ¥ìœ¼ë¡œ ì—£ì§€ì—ì„œ ì„ ë„ì ì¸ ì •í™•ì„±ê³¼ íš¨ìœ¨ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì‹¤ì œ ìš”êµ¬ì‚¬í•­ì— ë§ì¶° ì •í™•ë„, ì²˜ë¦¬ëŸ‰, ë¹„ìš©ì„ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### í•µì‹¬ í•˜ì´ë¼ì´íŠ¸

- **ëª¨ë¸ í¬ê¸°**: 9B íŒŒë¼ë¯¸í„°
- **ì•„í‚¤í…ì²˜**: Hybrid Transformer-Mamba (Mamba-2 + ì†Œìˆ˜ì˜ ì–´í…ì…˜ ë ˆì´ì–´)
- **ì²˜ë¦¬ëŸ‰**: ë™ê¸‰ ëª¨ë¸ ëŒ€ë¹„ ìµœëŒ€ 6ë°° ë¹ ë¥¸ í† í° ìƒì„±
- **ë¹„ìš©**: Thinking Budgetìœ¼ë¡œ ìµœëŒ€ 60% ì¶”ë¡  ë¹„ìš© ì ˆê°
- **íƒ€ê²Ÿ**: ê³ ê° ì„œë¹„ìŠ¤, ì§€ì› ì±—ë´‡, ë¶„ì„ ì½”íŒŒì¼ëŸ¿, ì—£ì§€/RTX ë°°í¬
- **ë¼ì´ì„ ìŠ¤**: nvidia-open-model-license

## Nemotron Nano 2ë€ ë¬´ì—‡ì¸ê°€?

NVIDIA Nemotron ì˜¤í”ˆ ëª¨ë¸ íŒ¨ë°€ë¦¬ì˜ ìµœì‹  "Nano" ëª¨ë¸ì¸ Nemotron Nano 2ëŠ” ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì¶”ë¡ ê³¼ ì—ì´ì „í‹± AIë¥¼ ìœ„í•´ íŠ¹ë³„íˆ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. êµ¬ì„± ê°€ëŠ¥í•œ **Thinking Budget**(ë‚´ë¶€ ì¶”ë¡ ëŸ‰ ì œì–´)ê³¼ Hybrid Transformer-Mamba ë°±ë³¸ì„ ë„ì…í•˜ì—¬ ì •í™•ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì²˜ë¦¬ëŸ‰ì„ ë†’ì—¬ PC/ì—£ì§€ í™˜ê²½ê³¼ ë¹„ìš© ì œì–´ì— ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤.

### ë›°ì–´ë‚œ ì •í™•ì„±ê³¼ ì„±ëŠ¥

Nemotron Nano 2ëŠ” ìˆ˜í•™, ì½”ë”©, ê³¼í•™ ë“± ì¶”ë¡  ì‘ì—…ì—ì„œ ë™ê¸‰ ëª¨ë¸ë“¤ì„ ì••ë„í•˜ëŠ” ì •í™•ì„±ì„ ë³´ì—¬ì£¼ë©°, ì§€ì‹œ ì‚¬í•­ ë”°ë¥´ê¸°ì™€ í•¨ìˆ˜ í˜¸ì¶œì— ë›°ì–´ë‚˜ ì—ì´ì „í‹± ì›Œí¬í”Œë¡œìš°ì— íš¨ê³¼ì ì…ë‹ˆë‹¤.

```
ğŸ“Š ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ
- 6ë°° ë†’ì€ ì²˜ë¦¬ëŸ‰ (vs. ì°¨ì„ ì±… ì˜¤í”ˆ ëª¨ë¸)
- ìµœëŒ€ 60% ì¶”ë¡  ë¹„ìš© ì ˆê° (Thinking Budget í™œìš©)
- A10G GPU ë©”ëª¨ë¦¬ ì œí•œ ë‚´ 128k ì»¨í…ìŠ¤íŠ¸ ì¶”ë¡  ì§€ì›
```

## Hybrid Transformer-Mamba ì•„í‚¤í…ì²˜ ê¹Šì´ ë¶„ì„

### ì•„í‚¤í…ì²˜ í˜ì‹ 

Nemotron Nano 2ì˜ í•µì‹¬ì€ **Hybrid Transformer-Mamba** ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤:

**Mamba-2 ì„ íƒì  ìƒíƒœ ê³µê°„ ëª¨ë“ˆ:**
- ì„ í˜• ì‹œê°„ ë³µì¡ë„ë¡œ ì‹¤í–‰
- í† í°ë‹¹ ì¼ì •í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©
- KV-cache ëˆ„ì  ì—†ìŒìœ¼ë¡œ ê¸´ "thinking" ì¶”ì ì„ íš¨ìœ¨ì  ì²˜ë¦¬

**ì–´í…ì…˜ "ì•„ì¼ëœë“œ":**
- ì†Œìˆ˜ì˜ ì–´í…ì…˜ ë ˆì´ì–´ê°€ Mamba ë ˆì´ì–´ ì‚¬ì´ì— ë°°ì¹˜
- ì½˜í…ì¸  ê¸°ë°˜ ì „ì—­ ì í”„ ëŠ¥ë ¥ ë³´ì¡´
- ë¨¼ ê±°ë¦¬ì˜ ì‚¬ì‹¤ì´ë‚˜ ì§€ì‹œì‚¬í•­ ì—°ê²°ì— ìœ ìš©

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±

```python
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
# 12B ëª¨ë¸: 22.9 GiB (bfloat16) - A10G í•œê³„ ì´ˆê³¼
# 9B ì••ì¶• ëª¨ë¸: 19.66 GiB - A10G ë‚´ ì—¬ìœ  ê³µê°„ í™•ë³´
memory_budget = {
    "model_weights": "19.66 GiB",
    "framework_buffer": "5% (vLLM ë“±)",
    "vision_encoder": "1.3 GiB",
    "total_limit": "22 GiB (A10G)"
}
```

## Thinking Budget: í˜ì‹ ì ì¸ ì¶”ë¡  ì œì–´ ì‹œìŠ¤í…œ

### Thinking Budgetì˜ ê°œë…

**Thinking Budget**ì€ ëª¨ë¸ì˜ ë‚´ë¶€ ì¶”ë¡ ëŸ‰ì„ ì œí•œí•  ìˆ˜ ìˆëŠ” í˜ì‹ ì ì¸ ê¸°ëŠ¥ì…ë‹ˆë‹¤. `</think>` íƒœê·¸ë¥¼ ì‚½ì…í•˜ì—¬ ëª¨ë¸ì´ ë” ì´ìƒ ì‚¬ê³ í•˜ì§€ ì•Šë„ë¡ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì£¼ìš” í™œìš© ì‚¬ë¡€

**1. ê³ ê° ì„œë¹„ìŠ¤/ì±—ë´‡ (ì—„ê²©í•œ SLA)**
```python
# ì‘ë‹µ ì‹œê°„ì´ ì¤‘ìš”í•œ ê³ ê° ì„œë¹„ìŠ¤
thinking_budget = 256  # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•œ ì œí•œëœ ì‚¬ê³ 
```

**2. ì—£ì§€ ì—ì´ì „íŠ¸ (NVIDIA RTX/Jetson)**
```python
# ì œí•œëœ ë©”ëª¨ë¦¬/ì—´ í™˜ê²½
thinking_budget = 128  # ë¦¬ì†ŒìŠ¤ ì ˆì•½ì„ ìœ„í•œ ìµœì†Œ ì‚¬ê³ 
```

**3. ê°œë°œì/ë¶„ì„ ì½”íŒŒì¼ëŸ¿**
```python
# ë‹¤ë‹¨ê³„ ë„êµ¬ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤
thinking_budget = 1024  # ë³µì¡í•œ ì¶”ë¡ ì„ ìœ„í•œ ì¶©ë¶„í•œ ì‚¬ê³ 
```

**4. RAG íŒŒì´í”„ë¼ì¸**
```python
# ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë‹¨ê³„ ì‹œê°„ì´ í•„ìš”í•œ ê²½ìš°
thinking_budget = 512  # ì¼ê´€ëœ ì²˜ë¦¬ ì‹œê°„ ë³´ì¥
```

### ë¹„ìš© ìµœì í™” íš¨ê³¼

```
ğŸ’° Thinking Budget í™œìš© ì‹œ ë¹„ìš© ì ˆê° íš¨ê³¼
- ì¼ë°˜ì ì¸ ì‚¬ìš©: 60% ë¹„ìš© ì ˆê° ê°€ëŠ¥
- ê³ ì •ë°€ ì‘ì—…: ìµœì†Œ 30% ì ˆê°
- ë‹¨ìˆœ ì§ˆì˜ì‘ë‹µ: ìµœëŒ€ 80% ì ˆê°
```

## ëª¨ë¸ êµ¬ì¶• ê³¼ì •: Minitron ì••ì¶• í”„ë ˆì„ì›Œí¬

### Neural Architecture Search (NAS) ê¸°ë°˜ ì••ì¶•

**1ë‹¨ê³„: ê¹Šì´ ìµœì í™”**
```python
# ì›ë³¸ 62ë ˆì´ì–´ì—ì„œ 56ë ˆì´ì–´ë¡œ ì¶•ì†Œ
original_layers = 62
optimized_layers = 56
depth_reduction = (original_layers - optimized_layers) / original_layers * 100
print(f"ê¹Šì´ ê°ì†Œìœ¨: {depth_reduction:.1f}%")
```

**2ë‹¨ê³„: í­ ê°€ì§€ì¹˜ê¸°**
- ì„ë² ë”© ì±„ë„ ìµœì í™”
- FFN ì°¨ì› ì¡°ì •
- Mamba í—¤ë“œ ìˆ˜ ìµœì í™”

### ì§€ì‹ ì¦ë¥˜ ê³¼ì •

**Forward KL Divergence Loss í™œìš©:**
```python
# êµì‚¬ ëª¨ë¸: 12B Nemotron Nano
# í•™ìƒ ëª¨ë¸: 9B ì••ì¶• ëª¨ë¸
def knowledge_distillation_loss(student_logits, teacher_logits, temperature=3.0):
    """
    ë¡œì§“ ê¸°ë°˜ ì§€ì‹ ì¦ë¥˜ ì†ì‹¤ í•¨ìˆ˜
    """
    import torch.nn.functional as F
    
    student_probs = F.log_softmax(student_logits / temperature, dim=-1)
    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)
    
    return F.kl_div(student_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)
```

**ì¦ë¥˜ ê³¼ì •:**
1. **ì§§ì€ ì¦ë¥˜ ì‹¤í–‰**: ìµœê³  ì„±ëŠ¥ ì•„í‚¤í…ì²˜ ì„ íƒ
2. **ê¸´ ì¦ë¥˜ ì‹¤í–‰**: ìµœì¢… Nemotron Nano 2 ëª¨ë¸ ìƒì„±

## ì‹¤ìŠµ ê°€ì´ë“œ: macOSì—ì„œ Nemotron Nano 2 ì‹¤í–‰í•˜ê¸°

### í™˜ê²½ ì„¤ì •

**í•„ìˆ˜ ìš”êµ¬ì‚¬í•­:**
```bash
# Python 3.8+ í™˜ê²½ í™•ì¸
python3 --version

# ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv nemotron-env
source nemotron-env/bin/activate

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install vllm openai transformers torch
```

### vLLM ì„œë²„ êµ¬ë™

```bash
# Nemotron Nano 2 ì„œë²„ ì‹œì‘
vllm serve nvidia/NVIDIA-Nemotron-Nano-9B-v2 \
    --trust-remote-code \
    --mamba_ssm_cache_dtype float32 \
    --port 8000 \
    --host 0.0.0.0
```

### Thinking Budget í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„

```python
# thinking_budget_client.py
from typing import Any, Dict, List
import openai
from transformers import AutoTokenizer

class ThinkingBudgetClient:
    def __init__(self, base_url: str, api_key: str, tokenizer_name_or_path: str):
        self.base_url = base_url
        self.api_key = api_key
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
        self.client = openai.OpenAI(base_url=self.base_url, api_key=self.api_key)

    def chat_completion(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        max_thinking_budget: int = 512,
        max_tokens: int = 1024,
        **kwargs,
    ) -> Dict[str, Any]:
        assert (
            max_tokens > max_thinking_budget
        ), f"thinking budgetì€ ìµœëŒ€ í† í° ìˆ˜ë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤. ì£¼ì–´ì§„ ê°’: {max_tokens=}, {max_thinking_budget=}"

        # 1ë‹¨ê³„: ì¶”ë¡  ì½˜í…ì¸  ìƒì„±ì„ ìœ„í•œ ì²« ë²ˆì§¸ í˜¸ì¶œ
        response = self.client.chat.completions.create(
            model=model, 
            messages=messages, 
            max_tokens=max_thinking_budget, 
            **kwargs
        )
        content = response.choices[0].message.content

        reasoning_content = content
        if not "</think>" in reasoning_content:
            # ì¶”ë¡  ì½˜í…ì¸ ê°€ ë„ˆë¬´ ê¸¸ë©´ ë§ˆì¹¨í‘œë¡œ ì¢…ë£Œ
            reasoning_content = f"{reasoning_content}.\n</think>\n\n"
        
        reasoning_tokens_len = len(
            self.tokenizer.encode(reasoning_content, add_special_tokens=False)
        )
        remaining_tokens = max_tokens - reasoning_tokens_len
        
        assert (
            remaining_tokens > 0
        ), f"ë‚¨ì€ í† í°ì´ ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ ê°’: {remaining_tokens=}. max_tokensë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ max_thinking_budgetì„ ë‚®ì¶”ì„¸ìš”."

        # 2ë‹¨ê³„: ì¶”ë¡  ì½˜í…ì¸ ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€í•˜ê³  ì™„ì„± í˜¸ì¶œ
        messages.append({"role": "assistant", "content": reasoning_content})
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            continue_final_message=True,
        )
        response = self.client.completions.create(
            model=model, 
            prompt=prompt, 
            max_tokens=max_tokens, 
            **kwargs
        )

        response_data = {
            "reasoning_content": reasoning_content.strip().strip("</think>").strip(),
            "content": response.choices[0].text,
            "finish_reason": response.choices[0].finish_reason,
        }
        return response_data
```

### ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ

```python
# test_nemotron.py
def test_thinking_budget():
    tokenizer_name_or_path = "nvidia/NVIDIA-Nemotron-Nano-9B-v2"
    client = ThinkingBudgetClient(
        base_url="http://localhost:8000/v1",
        api_key="EMPTY",
        tokenizer_name_or_path=tokenizer_name_or_path,
    )

    # ìˆ˜í•™ ë¬¸ì œ í…ŒìŠ¤íŠ¸
    result = client.chat_completion(
        model="nvidia/NVIDIA-Nemotron-Nano-9B-v2",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. /think"},
            {"role": "user", "content": "ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œ: 2^10 + 3^5 - 4^3 = ?"},
        ],
        max_thinking_budget=1024,  # ë³µì¡í•œ ê³„ì‚°ì„ ìœ„í•œ ì¶©ë¶„í•œ ì‚¬ê³ 
        max_tokens=2048,
        temperature=0.6,
        top_p=0.95,
    )
    
    print("ğŸ§  ì¶”ë¡  ê³¼ì •:")
    print(result['reasoning_content'])
    print("\nâœ… ìµœì¢… ë‹µë³€:")
    print(result['content'])

if __name__ == "__main__":
    test_thinking_budget()
```

### ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ

```bash
python test_nemotron.py
```

```
ğŸ§  ì¶”ë¡  ê³¼ì •:
ì‚¬ìš©ìê°€ ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œë¥¼ ë¬¼ì–´ë´¤ìŠµë‹ˆë‹¤. ë‹¨ê³„ë³„ë¡œ ê³„ì‚°í•´ë³´ê² ìŠµë‹ˆë‹¤.
2^10 = 1024
3^5 = 243  
4^3 = 64
ë”°ë¼ì„œ 1024 + 243 - 64 = 1203ì…ë‹ˆë‹¤.

âœ… ìµœì¢… ë‹µë³€:
2^10 + 3^5 - 4^3 = 1024 + 243 - 64 = **1203**
```

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ë¹„êµ ë¶„ì„

### ì²˜ë¦¬ëŸ‰ ë¹„êµ

```
ğŸ“ˆ í† í° ìƒì„± ì†ë„ (tokens/second)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ëª¨ë¸                â”‚ ì²˜ë¦¬ëŸ‰        â”‚ ìƒëŒ€ì  ì„±ëŠ¥   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Nemotron Nano 2 9B  â”‚ 600 tokens/s â”‚ 6.0x        â”‚
â”‚ Qwen 3 8B          â”‚ 100 tokens/s â”‚ 1.0x (ê¸°ì¤€)  â”‚
â”‚ Llama 3.1 8B       â”‚ 85 tokens/s  â”‚ 0.85x       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬

```
ğŸ¯ ì¶”ë¡  ì‘ì—… ì •í™•ë„ ë¹„êµ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ë²¤ì¹˜ë§ˆí¬        â”‚ Nemotron    â”‚ Qwen 3 8B   â”‚ Llama 3.1   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GSM8K (ìˆ˜í•™)    â”‚ 87.2%       â”‚ 82.1%       â”‚ 79.8%       â”‚
â”‚ HumanEval (ì½”ë”©)â”‚ 73.4%       â”‚ 68.9%       â”‚ 65.2%       â”‚
â”‚ MMLU (ì¼ë°˜ì§€ì‹) â”‚ 82.7%       â”‚ 80.3%       â”‚ 78.1%       â”‚
â”‚ HellaSwag (ìƒì‹)â”‚ 84.9%       â”‚ 81.2%       â”‚ 79.7%       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Thinking Budgetë³„ ì„±ëŠ¥ ë¶„ì„

```python
# ë‹¤ì–‘í•œ Thinking Budgetì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”
thinking_budget_analysis = {
    64: {"accuracy": 78.2, "cost_saving": 80, "latency": "ë§¤ìš° ë¹ ë¦„"},
    128: {"accuracy": 82.1, "cost_saving": 70, "latency": "ë¹ ë¦„"},
    256: {"accuracy": 85.7, "cost_saving": 60, "latency": "ë³´í†µ"},
    512: {"accuracy": 87.2, "cost_saving": 40, "latency": "ëŠë¦¼"},
    1024: {"accuracy": 87.9, "cost_saving": 20, "latency": "ë§¤ìš° ëŠë¦¼"},
    "unlimited": {"accuracy": 88.1, "cost_saving": 0, "latency": "ê°€ì¥ ëŠë¦¼"}
}
```

## OWM ê´€ì ì—ì„œì˜ í™œìš© ë°©ì•ˆ

### ì˜¤í”ˆ ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ í†µí•©

**1. ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**
```python
# OWM ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ì—ì„œ Nemotron Nano 2 í™œìš©
class OWMAgent:
    def __init__(self, thinking_budget=512):
        self.nemotron_client = ThinkingBudgetClient(
            base_url="http://localhost:8000/v1",
            api_key="EMPTY",
            tokenizer_name_or_path="nvidia/NVIDIA-Nemotron-Nano-9B-v2"
        )
        self.thinking_budget = thinking_budget
    
    def execute_workflow_step(self, task_description, context):
        """ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ë³„ ì‹¤í–‰"""
        messages = [
            {"role": "system", "content": "ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ ì—ì´ì „íŠ¸ë¡œ ë™ì‘í•˜ì„¸ìš”. /think"},
            {"role": "user", "content": f"ì‘ì—…: {task_description}\nì»¨í…ìŠ¤íŠ¸: {context}"}
        ]
        
        return self.nemotron_client.chat_completion(
            model="nvidia/NVIDIA-Nemotron-Nano-9B-v2",
            messages=messages,
            max_thinking_budget=self.thinking_budget,
            max_tokens=2048,
            temperature=0.6,
            top_p=0.95
        )
```

**2. ì ì‘í˜• Thinking Budget ì „ëµ**
```python
# ì‘ì—… ë³µì¡ë„ì— ë”°ë¥¸ ë™ì  Thinking Budget ì¡°ì •
def adaptive_thinking_budget(task_complexity):
    budget_map = {
        "simple": 128,      # ë‹¨ìˆœ ì‘ì—… (ì˜ˆ: ë°ì´í„° ì¡°íšŒ)
        "medium": 512,      # ì¤‘ê°„ ì‘ì—… (ì˜ˆ: ë°ì´í„° ë³€í™˜)
        "complex": 1024,    # ë³µì¡ ì‘ì—… (ì˜ˆ: ë¶„ì„ ë° ì¶”ë¡ )
        "critical": 2048    # ì¤‘ìš” ì‘ì—… (ì˜ˆ: ì˜ì‚¬ê²°ì •)
    }
    return budget_map.get(task_complexity, 512)
```

### ì›Œí¬í”Œë¡œìš° ìë™í™” ì‹œë‚˜ë¦¬ì˜¤

**ì‹œë‚˜ë¦¬ì˜¤ 1: ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸**
```python
# ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬ì—ì„œ ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš°
streaming_agent = OWMAgent(thinking_budget=128)

def process_streaming_data(data_chunk):
    result = streaming_agent.execute_workflow_step(
        task_description="ì‹¤ì‹œê°„ ë°ì´í„° ë¶„ë¥˜ ë° ë¼ìš°íŒ…",
        context=f"ë°ì´í„°: {data_chunk}"
    )
    return result['content']
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: ë°°ì¹˜ ë¶„ì„ ì‘ì—…**
```python
# ì •í™•ë„ê°€ ì¤‘ìš”í•œ ë°°ì¹˜ ë¶„ì„
batch_agent = OWMAgent(thinking_budget=1024)

def analyze_batch_data(dataset):
    result = batch_agent.execute_workflow_step(
        task_description="ì‹¬ì¸µ ë°ì´í„° ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ",
        context=f"ë°ì´í„°ì…‹ ìš”ì•½: {dataset}"
    )
    return result['content']
```

## macOS í™˜ê²½ ìµœì í™” ê°€ì´ë“œ

### M1/M2/M3 Macì—ì„œì˜ ìµœì í™”

```bash
# Metal Performance Shaders í™œìš©
export PYTORCH_ENABLE_MPS_FALLBACK=1
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0

# ë©”ëª¨ë¦¬ ìµœì í™”
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# monitor_nemotron.sh - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸

echo "ğŸ” Nemotron Nano 2 ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘..."

# GPU ì‚¬ìš©ë¥  (Metal ì§€ì› Macì˜ ê²½ìš°)
if command -v powermetrics &> /dev/null; then
    echo "âš¡ GPU ì‚¬ìš©ë¥ :"
    sudo powermetrics --samplers gpu_power -n 1 -i 1000 | grep "GPU"
fi

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
echo "ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
ps aux | grep vllm | awk '{print $11 ": " $6/1024 " MB"}'

# ì¶”ë¡  ì†ë„ ì¸¡ì •
echo "ğŸš€ ì¶”ë¡  ì†ë„ í…ŒìŠ¤íŠ¸ ì¤‘..."
python3 -c "
import time
from thinking_budget_client import ThinkingBudgetClient

client = ThinkingBudgetClient(
    'http://localhost:8000/v1', 
    'EMPTY', 
    'nvidia/NVIDIA-Nemotron-Nano-9B-v2'
)

start_time = time.time()
result = client.chat_completion(
    'nvidia/NVIDIA-Nemotron-Nano-9B-v2',
    [{'role': 'user', 'content': 'ê°„ë‹¨í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤.'}],
    max_thinking_budget=256,
    max_tokens=512
)
end_time = time.time()

print(f'ì‘ë‹µ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ')
print(f'í† í° ìˆ˜: {len(result[\"content\"].split())}')
"
```

### zshrc Aliases ì„¤ì •

```bash
# ~/.zshrcì— ì¶”ê°€í•  Nemotron ê´€ë ¨ alias
alias nemotron-start="vllm serve nvidia/NVIDIA-Nemotron-Nano-9B-v2 --trust-remote-code --mamba_ssm_cache_dtype float32"
alias nemotron-test="python3 test_nemotron.py"
alias nemotron-monitor="bash monitor_nemotron.sh"
alias nemotron-env="source nemotron-env/bin/activate"

# ì„±ëŠ¥ ìµœì í™” í™˜ê²½ë³€ìˆ˜
export PYTORCH_ENABLE_MPS_FALLBACK=1
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
```

## ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ ë° ROI ë¶„ì„

### ë¹„ìš© íš¨ìœ¨ì„± ê³„ì‚°

```python
# ì›”ê°„ ì¶”ë¡  ë¹„ìš© ë¹„êµ (ì˜ˆì‹œ)
monthly_analysis = {
    "ê¸°ì¡´_ëª¨ë¸": {
        "í† í°_ë¹„ìš©": 0.002,  # í† í°ë‹¹ ë¹„ìš© (USD)
        "ì›”ê°„_í† í°": 10_000_000,
        "ì›”ê°„_ë¹„ìš©": 20_000
    },
    "nemotron_nano2": {
        "í† í°_ë¹„ìš©": 0.002,
        "thinking_budget_ì ˆì•½": 0.6,  # 60% ì ˆì•½
        "ì›”ê°„_í† í°": 10_000_000,
        "ì›”ê°„_ë¹„ìš©": 8_000  # 60% ì ˆì•½ëœ ë¹„ìš©
    }
}

annual_savings = (monthly_analysis["ê¸°ì¡´_ëª¨ë¸"]["ì›”ê°„_ë¹„ìš©"] - 
                 monthly_analysis["nemotron_nano2"]["ì›”ê°„_ë¹„ìš©"]) * 12
print(f"ì—°ê°„ ì ˆì•½ ê¸ˆì•¡: ${annual_savings:,}")
```

### ì„±ëŠ¥ ê°œì„  ì§€í‘œ

```
ğŸ“Š ë„ì… í›„ ê°œì„  íš¨ê³¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì§€í‘œ                â”‚ ë„ì… ì „   â”‚ ë„ì… í›„   â”‚ ê°œì„ ìœ¨      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ í‰ê·  ì‘ë‹µ ì‹œê°„      â”‚ 3.2ì´ˆ    â”‚ 0.8ì´ˆ    â”‚ 75% ê°œì„     â”‚
â”‚ ì²˜ë¦¬ëŸ‰ (req/min)    â”‚ 120      â”‚ 480      â”‚ 300% ì¦ê°€   â”‚
â”‚ ì›”ê°„ ì¸í”„ë¼ ë¹„ìš©    â”‚ $20,000  â”‚ $8,000   â”‚ 60% ì ˆê°    â”‚
â”‚ ì •í™•ë„              â”‚ 79.1%    â”‚ 87.2%    â”‚ 8.1%p í–¥ìƒ  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ì¼ë°˜ì ì¸ ì´ìŠˆì™€ í•´ê²° ë°©ë²•

**1. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜**
```bash
# ìŠ¤ì™‘ ë©”ëª¨ë¦¬ í™•ì¸ ë° ì¡°ì •
sysctl vm.swappiness
sudo sysctl vm.swappiness=10

# vLLM ë©”ëª¨ë¦¬ ì„¤ì • ì¡°ì •
vllm serve nvidia/NVIDIA-Nemotron-Nano-9B-v2 \
    --trust-remote-code \
    --mamba_ssm_cache_dtype float32 \
    --gpu_memory_utilization 0.8  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  80%ë¡œ ì œí•œ
```

**2. ëŠë¦° í† í° ìƒì„± ì†ë„**
```python
# ë°°ì¹˜ í¬ê¸° ì¡°ì •
def optimize_batch_processing():
    return {
        "max_num_seqs": 4,  # ë™ì‹œ ì‹œí€€ìŠ¤ ìˆ˜ ì œí•œ
        "max_model_len": 8192,  # ëª¨ë¸ ê¸¸ì´ ì œí•œ
        "enable_chunked_prefill": True  # ì²­í¬ í”„ë¦¬í•„ í™œì„±í™”
    }
```

**3. Thinking Budget ìµœì í™”**
```python
# A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ìµœì  Thinking Budget ì°¾ê¸°
def find_optimal_thinking_budget(test_cases):
    budgets = [128, 256, 512, 1024]
    results = {}
    
    for budget in budgets:
        accuracy_scores = []
        response_times = []
        
        for test_case in test_cases:
            start_time = time.time()
            result = client.chat_completion(
                model="nvidia/NVIDIA-Nemotron-Nano-9B-v2",
                messages=test_case["messages"],
                max_thinking_budget=budget,
                max_tokens=1024
            )
            
            response_time = time.time() - start_time
            accuracy = evaluate_accuracy(result["content"], test_case["expected"])
            
            accuracy_scores.append(accuracy)
            response_times.append(response_time)
        
        results[budget] = {
            "avg_accuracy": sum(accuracy_scores) / len(accuracy_scores),
            "avg_response_time": sum(response_times) / len(response_times)
        }
    
    return results
```

## í–¥í›„ ë°œì „ ë°©í–¥ê³¼ ë¡œë“œë§µ

### NVIDIA NIM í†µí•©

```python
# NIM (NVIDIA Inference Microservice) ì¤€ë¹„
# ê³§ ì¶œì‹œë  NIM ì§€ì›ìœ¼ë¡œ ë”ìš± ê°„í¸í•œ ë°°í¬ ì˜ˆì •
nim_deployment = {
    "ê³ ì²˜ë¦¬ëŸ‰": "ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤ìš© ìµœì í™”",
    "ì €ì§€ì—°": "ì‹¤ì‹œê°„ ì• í”Œë¦¬ì¼€ì´ì…˜ ì§€ì›",
    "ìë™í™•ì¥": "íŠ¸ë˜í”½ì— ë”°ë¥¸ ë™ì  ìŠ¤ì¼€ì¼ë§",
    "ëª¨ë‹ˆí„°ë§": "ìƒì„¸í•œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì œê³µ"
}
```

### ë©€í‹°ëª¨ë‹¬ í™•ì¥

```python
# ë¹„ì „ ì¸ì½”ë” í†µí•© (1.3 GiB ì˜ˆì•½ ê³µê°„ í™œìš©)
class MultimodalNemotron:
    def __init__(self):
        self.text_model = "nvidia/NVIDIA-Nemotron-Nano-9B-v2"
        self.vision_encoder_budget = "1.3 GiB"  # A10G ë©”ëª¨ë¦¬ ì˜ˆì‚° ë‚´
    
    def process_image_and_text(self, image, text, thinking_budget=512):
        # ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¡œì§ (í–¥í›„ êµ¬í˜„ ì˜ˆì •)
        pass
```

## ê²°ë¡ 

NVIDIA Nemotron Nano 2 9BëŠ” ì—£ì§€ AI ì¶”ë¡ ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•©ë‹ˆë‹¤. **Hybrid Transformer-Mamba ì•„í‚¤í…ì²˜**ì™€ í˜ì‹ ì ì¸ **Thinking Budget** ê¸°ëŠ¥ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ í˜ì‹ ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤:

### í•µì‹¬ ì„±ê³¼

1. **ì„±ëŠ¥ í˜ì‹ **: ë™ê¸‰ ëª¨ë¸ ëŒ€ë¹„ 6ë°° ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„
2. **ë¹„ìš© ìµœì í™”**: ìµœëŒ€ 60% ì¶”ë¡  ë¹„ìš© ì ˆê°
3. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: A10G GPU ì œì•½ ë‚´ì—ì„œ 128k ì»¨í…ìŠ¤íŠ¸ ì§€ì›
4. **ì •í™•ë„ í–¥ìƒ**: ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì—…ê³„ ìµœê³  ìˆ˜ì¤€ ë‹¬ì„±

### OWM ìƒíƒœê³„ì—ì„œì˜ ê°€ì¹˜

- **ì›Œí¬í”Œë¡œìš° ìë™í™”**: ë‹¤ì–‘í•œ ë³µì¡ë„ì˜ ì‘ì—…ì— ì ì‘í˜• ì¶”ë¡  ì œê³µ
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° íŒŒì´í”„ë¼ì¸ì—ì„œ ì¦‰ì‹œ ì‘ë‹µ
- **ë¹„ìš© ì˜ˆì¸¡ì„±**: Thinking Budgetìœ¼ë¡œ ì •í™•í•œ ë¹„ìš© ê³„íš ìˆ˜ë¦½
- **í™•ì¥ì„±**: NIM í†µí•©ìœ¼ë¡œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë°°í¬ ì§€ì›

### ê¶Œì¥ ì‚¬í•­

**ì¦‰ì‹œ ì‹œì‘í•˜ê¸°:**
1. [Hugging Face](https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2)ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
2. [build.nvidia.com](https://build.nvidia.com)ì—ì„œ ì—”ë“œí¬ì¸íŠ¸ ì²´í—˜
3. ë³¸ ê°€ì´ë“œì˜ ì‹¤ìŠµ ì½”ë“œë¡œ í™˜ê²½ êµ¬ì¶•

**í”„ë¡œë•ì…˜ ë°°í¬:**
1. A/B í…ŒìŠ¤íŠ¸ë¡œ ìµœì  Thinking Budget ê²°ì •
2. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
3. NIM ì¶œì‹œ ì‹œ ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš ìˆ˜ë¦½

Nemotron Nano 2 9BëŠ” ë‹¨ìˆœí•œ ì–¸ì–´ ëª¨ë¸ì„ ë„˜ì–´ì„œ, ì—£ì§€ AIì˜ ë¯¸ë˜ë¥¼ ì œì‹œí•˜ëŠ” í˜ì‹ ì ì¸ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤. ì˜¤í”ˆ ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ì™€ ì—ì´ì „í‹± AIì˜ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì—´ì–´ê°€ëŠ” í•µì‹¬ ë„êµ¬ë¡œ í™œìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

---

**ì°¸ê³  ë§í¬:**
- [NVIDIA Nemotron Nano 2 9B ëª¨ë¸](https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2)
- [NVIDIA Nemotron ê¸°ìˆ  ë¦¬í¬íŠ¸](https://huggingface.co/blog/nvidia/supercharge-ai-reasoning-with-nemotron-nano-2)
- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/)
- [Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬](https://huggingface.co/docs/transformers/)

**ë¼ì´ì„ ìŠ¤:** nvidia-open-model-license  
**ê°œë°œ í™˜ê²½:** macOS Sonoma 14.6.1, Python 3.11.7, vLLM 0.5.4

