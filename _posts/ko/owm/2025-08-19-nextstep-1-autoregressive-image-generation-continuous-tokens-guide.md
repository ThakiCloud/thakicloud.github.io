---
title: "NextStep-1: 14B íŒŒë¼ë¯¸í„° ìë™íšŒê·€ ì´ë¯¸ì§€ ìƒì„±ì˜ í˜ì‹ "
excerpt: "ì—°ì† í† í°ê³¼ 14B íŒŒë¼ë¯¸í„°ë¡œ êµ¬í˜„í•œ StepFunì˜ ì°¨ì„¸ëŒ€ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ NextStep-1ì˜ ì•„í‚¤í…ì²˜ì™€ OWM ì ìš© ê°€ì´ë“œ"
seo_title: "NextStep-1 ìë™íšŒê·€ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ì™„ì „ ê°€ì´ë“œ - Thaki Cloud"
seo_description: "StepFun NextStep-1 14B ìë™íšŒê·€ ëª¨ë¸ê³¼ 157M flow matching headë¡œ êµ¬í˜„í•˜ëŠ” ì—°ì† í† í° ê¸°ë°˜ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œ êµ¬ì¶• ë°©ë²•"
date: 2025-08-19
last_modified_at: 2025-08-19
categories:
  - owm
  - llmops
tags:
  - NextStep-1
  - StepFun
  - autoregressive
  - text-to-image
  - continuous-tokens
  - flow-matching
  - image-generation
  - 14B-parameters
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/owm/nextstep-1-autoregressive-image-generation-continuous-tokens-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 8ë¶„

## ì„œë¡ 

StepFun AIì—ì„œ ì¶œì‹œí•œ **NextStep-1**ì€ 14B íŒŒë¼ë¯¸í„° ìë™íšŒê·€ ëª¨ë¸ê³¼ 157M flow matching headë¥¼ ê²°í•©í•˜ì—¬ ì—°ì† í† í° ê¸°ë°˜ ì´ë¯¸ì§€ ìƒì„±ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë””ìŠ¤í¬ë¦¬íŠ¸ í† í° ë°©ì‹ê³¼ ë‹¬ë¦¬ ì—°ì† í† í°ì„ í™œìš©í•¨ìœ¼ë¡œì¨ ë”ìš± ìì—°ìŠ¤ëŸ½ê³  ê³ í’ˆì§ˆì˜ ì´ë¯¸ì§€ ìƒì„±ì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.

ì´ ê¸€ì—ì„œëŠ” NextStep-1ì˜ í•µì‹¬ ì•„í‚¤í…ì²˜ë¶€í„° Open Workflow Management(OWM) í™˜ê²½ì—ì„œì˜ ì‹¤ì œ êµ¬í˜„ê³¼ í™œìš© ë°©ë²•ê¹Œì§€ ìƒì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤.

## NextStep-1 ì•„í‚¤í…ì²˜ ë¶„ì„

### í•˜ì´ë¸Œë¦¬ë“œ í† í° ì²˜ë¦¬ ì‹œìŠ¤í…œ

NextStep-1ì˜ ê°€ì¥ í˜ì‹ ì ì¸ íŠ¹ì§•ì€ **í…ìŠ¤íŠ¸ëŠ” ë””ìŠ¤í¬ë¦¬íŠ¸ í† í°, ì´ë¯¸ì§€ëŠ” ì—°ì† í† í°**ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ì…ë‹ˆë‹¤.

```python
# NextStep-1 í† í° ì²˜ë¦¬ êµ¬ì¡°
{
    "text_tokens": {
        "type": "discrete",
        "vocab_size": 50000,
        "encoding": "BPE"
    },
    "image_tokens": {
        "type": "continuous", 
        "dimension": 768,
        "flow_matching": True
    }
}
```

**ì•„í‚¤í…ì²˜ êµ¬ì„± ìš”ì†Œ:**

1. **14B ë©”ì¸ ëª¨ë¸**: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ í† í° í†µí•© ì²˜ë¦¬
2. **157M Flow Matching Head**: ì—°ì† í† í° ìƒì„± ìµœì í™”
3. **VAE**: ì´ë¯¸ì§€-í† í° ë³€í™˜ ì¸í„°í˜ì´ìŠ¤
4. **Next-token Prediction**: í†µì¼ëœ í•™ìŠµ ëª©í‘œ

### ìë™íšŒê·€ ìƒì„± í”„ë¡œì„¸ìŠ¤

```mermaid
graph TD
    A[í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸] --> B[í…ìŠ¤íŠ¸ í† í°í™”]
    B --> C[14B ìë™íšŒê·€ ëª¨ë¸]
    C --> D[ì—°ì† ì´ë¯¸ì§€ í† í° ìƒì„±]
    D --> E[157M Flow Matching Head]
    E --> F[VAE ë””ì½”ë”]
    F --> G[ìµœì¢… ì´ë¯¸ì§€]
    
    H[ì´ì „ í† í°ë“¤] --> C
    I[ìœ„ì¹˜ ì„ë² ë”©] --> C
```

## macOS í™˜ê²½ ì„¤ì • ë° ì„¤ì¹˜

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

```bash
# GPU ë©”ëª¨ë¦¬: ìµœì†Œ 24GB VRAM ê¶Œì¥
# RAM: 32GB ì´ìƒ
# ë””ìŠ¤í¬: 50GB ì—¬ìœ  ê³µê°„
```

### Conda í™˜ê²½ êµ¬ì„±

```bash
#!/bin/bash
# íŒŒì¼: ~/scripts/setup-nextstep1.sh

echo "ğŸš€ NextStep-1 ì„¤ì¹˜ ì‹œì‘..."

# Conda í™˜ê²½ ìƒì„±
conda create -n nextstep python=3.11 -y
conda activate nextstep

# uv íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì„¤ì¹˜ (ì„ íƒì‚¬í•­)
pip install uv

# ëª¨ë¸ ì €ì¥ì†Œ í´ë¡ 
echo "ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/stepfun-ai/NextStep-1-Large-Pretrain
cd NextStep-1-Large-Pretrain

# ì˜ì¡´ì„± ì„¤ì¹˜
if command -v uv &> /dev/null; then
    uv pip install -r requirements.txt
else
    pip install -r requirements.txt
fi

# VAE ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ
echo "ğŸ”§ VAE ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."
hf download stepfun-ai/NextStep-1-Large-Pretrain "vae/checkpoint.pt" --local-dir ./

echo "âœ… NextStep-1 ì„¤ì¹˜ ì™„ë£Œ!"
echo "í™œì„±í™”: conda activate nextstep"
```

### ê¶Œí•œ ì„¤ì • ë° ì‹¤í–‰

```bash
chmod +x ~/scripts/setup-nextstep1.sh
~/scripts/setup-nextstep1.sh
```

### macOS í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

```bash
# í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd tutorials/nextstep-test

# ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
./setup-nextstep-test.sh

# í¸ì˜ ëª…ë ¹ì–´ ì„¤ì •
source setup-nextstep-aliases.sh
```

**ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ì˜ ì£¼ìš” ê¸°ëŠ¥:**

1. **ì‹œìŠ¤í…œ í™˜ê²½ ìë™ ê°ì§€**: GPU(CUDA/MPS), ë©”ëª¨ë¦¬, ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
2. **Conda í™˜ê²½ êµ¬ì„±**: Python 3.11 ê¸°ë°˜ ë…ë¦½ í™˜ê²½ ìƒì„±
3. **ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ**: Git LFSë¥¼ í†µí•œ íš¨ìœ¨ì  ëŒ€ìš©ëŸ‰ íŒŒì¼ ê´€ë¦¬
4. **ì˜ì¡´ì„± ìµœì í™”**: GPU íƒ€ì…ì— ë”°ë¥¸ PyTorch ë²„ì „ ìë™ ì„ íƒ
5. **í…ŒìŠ¤íŠ¸ ìë™í™”**: ì„¤ì¹˜ ì™„ë£Œ í›„ ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ ìŠ¤í¬ë¦½íŠ¸ ì œê³µ

## ê¸°ë³¸ ì‚¬ìš©ë²• ë° êµ¬í˜„

### íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

```python
import torch
from transformers import AutoTokenizer, AutoModel
from models.gen_pipeline import NextStepPipeline

class NextStepGenerator:
    """NextStep-1 ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_path="stepfun-ai/NextStep-1-Large-Pretrain"):
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "mps"
        self.setup_model()
        
    def setup_model(self):
        """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©"""
        print(f"ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, 
            local_files_only=True, 
            trust_remote_code=True
        )
        
        self.model = AutoModel.from_pretrained(
            self.model_path, 
            local_files_only=True, 
            trust_remote_code=True
        )
        
        self.pipeline = NextStepPipeline(
            tokenizer=self.tokenizer, 
            model=self.model
        ).to(device=self.device, dtype=torch.bfloat16)
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
```

### ê³ ê¸‰ ì´ë¯¸ì§€ ìƒì„±

```python
def generate_high_quality_image(
    generator, 
    prompt, 
    image_size=512,
    guidance_scale=7.5,
    steps=28,
    seed=None
):
    """ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜"""
    
    # í”„ë¡¬í”„íŠ¸ ìµœì í™”
    positive_prompt = "masterpiece, film grained, best quality, highly detailed, ultra sharp"
    negative_prompt = (
        "lowres, bad anatomy, bad hands, text, error, missing fingers, "
        "extra digit, fewer digits, cropped, worst quality, low quality, "
        "normal quality, jpeg artifacts, signature, watermark, blurry"
    )
    
    # ì‹œë“œ ì„¤ì •
    if seed is None:
        seed = torch.randint(0, 2**32, (1,)).item()
    
    print(f"ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
    print(f"í”„ë¡¬í”„íŠ¸: {prompt}")
    print(f"ì‹œë“œ: {seed}")
    
    # ì´ë¯¸ì§€ ìƒì„±
    image = generator.pipeline.generate_image(
        prompt,
        hw=(image_size, image_size),
        num_images_per_caption=1,
        positive_prompt=positive_prompt,
        negative_prompt=negative_prompt,
        cfg=guidance_scale,
        cfg_img=1.0,
        cfg_schedule="constant",
        use_norm=False,
        num_sampling_steps=steps,
        timesteps_shift=1.0,
        seed=seed,
    )[0]
    
    return image, seed

# ì‚¬ìš© ì˜ˆì‹œ
generator = NextStepGenerator()

image, used_seed = generate_high_quality_image(
    generator,
    "A serene mountain landscape at sunset with cherry blossoms",
    image_size=1024,
    guidance_scale=8.0,
    steps=32
)

image.save("nextstep_output.jpg")
print(f"âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ - ì‹œë“œ: {used_seed}")
```

## OWM ì›Œí¬í”Œë¡œìš° í†µí•©

### ìë™í™”ëœ ì»¨í…ì¸  ìƒì„± íŒŒì´í”„ë¼ì¸

```python
import asyncio
import json
from datetime import datetime
from pathlib import Path

class NextStepOWMPipeline:
    """OWMìš© NextStep-1 ìë™í™” íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config_path="nextstep_config.json"):
        self.config = self.load_config(config_path)
        self.generator = NextStepGenerator()
        self.output_dir = Path("generated_content")
        self.output_dir.mkdir(exist_ok=True)
        
    def load_config(self, config_path):
        """ì„¤ì • íŒŒì¼ ë¡œë”©"""
        default_config = {
            "batch_size": 4,
            "quality_presets": {
                "draft": {"size": 512, "steps": 20, "guidance": 6.0},
                "standard": {"size": 768, "steps": 28, "guidance": 7.5},
                "premium": {"size": 1024, "steps": 40, "guidance": 8.5}
            },
            "output_formats": ["jpg", "png"],
            "metadata_tracking": True
        }
        
        if Path(config_path).exists():
            with open(config_path, 'r') as f:
                user_config = json.load(f)
                default_config.update(user_config)
        
        return default_config
    
    async def batch_generate(self, prompts, quality="standard"):
        """ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„±"""
        preset = self.config["quality_presets"][quality]
        results = []
        
        for i, prompt in enumerate(prompts):
            print(f"ğŸ“¸ {i+1}/{len(prompts)} ìƒì„± ì¤‘...")
            
            image, seed = generate_high_quality_image(
                self.generator,
                prompt,
                image_size=preset["size"],
                guidance_scale=preset["guidance"],
                steps=preset["steps"]
            )
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "timestamp": datetime.now().isoformat(),
                "prompt": prompt,
                "seed": seed,
                "quality": quality,
                "settings": preset
            }
            
            # íŒŒì¼ ì €ì¥
            filename = f"nextstep_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i:03d}"
            
            for fmt in self.config["output_formats"]:
                image_path = self.output_dir / f"{filename}.{fmt}"
                image.save(image_path)
                
                if self.config["metadata_tracking"]:
                    metadata_path = self.output_dir / f"{filename}_metadata.json"
                    with open(metadata_path, 'w') as f:
                        json.dump(metadata, f, indent=2)
            
            results.append({
                "image": image,
                "metadata": metadata,
                "filename": filename
            })
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if i % 5 == 0:
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        return results
```

### ë¸Œëœë“œ ì•„ì´ë´í‹°í‹° ìƒì„± ì›Œí¬í”Œë¡œìš°

```python
def create_brand_identity_workflow(company_name, industry, style_preference):
    """ë¸Œëœë“œ ì•„ì´ë´í‹°í‹° ìë™ ìƒì„±"""
    
    prompts = [
        f"Professional logo design for {company_name}, {industry} company, {style_preference} style, clean and modern",
        f"Business card design layout for {company_name}, {style_preference} aesthetic, professional typography",
        f"Website header banner for {company_name}, {industry} theme, {style_preference} color scheme",
        f"Social media profile image for {company_name}, circular format, {style_preference} branding",
        f"Marketing brochure cover design for {company_name}, {industry} focus, {style_preference} visual style"
    ]
    
    pipeline = NextStepOWMPipeline()
    results = asyncio.run(pipeline.batch_generate(prompts, quality="premium"))
    
    # ê²°ê³¼ ì •ë¦¬
    brand_package = {
        "company": company_name,
        "industry": industry,
        "style": style_preference,
        "assets": results,
        "generated_at": datetime.now().isoformat()
    }
    
    # ë¸Œëœë“œ íŒ¨í‚¤ì§€ ë³´ê³ ì„œ ìƒì„±
    with open(f"brand_identity_{company_name.lower().replace(' ', '_')}.json", 'w') as f:
        json.dump(brand_package, f, indent=2, default=str)
    
    return brand_package

# ì‚¬ìš© ì˜ˆì‹œ
brand_assets = create_brand_identity_workflow(
    company_name="TechFlow Solutions",
    industry="software development", 
    style_preference="minimalist tech"
)
```

## ì„±ëŠ¥ ìµœì í™” ë° ìŠ¤ì¼€ì¼ë§

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 

```python
class OptimizedNextStepGenerator:
    """ë©”ëª¨ë¦¬ ìµœì í™”ëœ NextStep-1 ìƒì„±ê¸°"""
    
    def __init__(self, low_memory_mode=True):
        self.low_memory_mode = low_memory_mode
        self.setup_optimized_model()
    
    def setup_optimized_model(self):
        """ìµœì í™”ëœ ëª¨ë¸ ì„¤ì •"""
        model_kwargs = {
            "torch_dtype": torch.bfloat16,
            "low_cpu_mem_usage": True,
            "trust_remote_code": True
        }
        
        if self.low_memory_mode:
            model_kwargs.update({
                "device_map": "auto",
                "max_memory": {0: "20GB"} if torch.cuda.is_available() else None
            })
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            "stepfun-ai/NextStep-1-Large-Pretrain",
            local_files_only=True,
            trust_remote_code=True
        )
        
        self.model = AutoModel.from_pretrained(
            "stepfun-ai/NextStep-1-Large-Pretrain",
            local_files_only=True,
            **model_kwargs
        )
        
        self.pipeline = NextStepPipeline(
            tokenizer=self.tokenizer,
            model=self.model
        )
        
        if not self.low_memory_mode:
            device = "cuda" if torch.cuda.is_available() else "mps"
            self.pipeline = self.pipeline.to(device=device, dtype=torch.bfloat16)
    
    def generate_with_cleanup(self, prompt, **kwargs):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ í¬í•¨í•œ ìƒì„±"""
        try:
            image = self.pipeline.generate_image(prompt, **kwargs)[0]
            return image
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                torch.mps.empty_cache()
```

### ë¶„ì‚° ì²˜ë¦¬ êµ¬í˜„

```python
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor
import queue

def distributed_generation_worker(prompt_queue, result_queue, gpu_id):
    """ë¶„ì‚° ì²˜ë¦¬ ì›Œì»¤"""
    import os
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    
    generator = OptimizedNextStepGenerator(low_memory_mode=True)
    
    while True:
        try:
            prompt_data = prompt_queue.get(timeout=5)
            if prompt_data is None:  # ì¢…ë£Œ ì‹ í˜¸
                break
                
            prompt, settings, task_id = prompt_data
            
            image = generator.generate_with_cleanup(
                prompt,
                hw=(settings["size"], settings["size"]),
                cfg=settings["guidance"],
                num_sampling_steps=settings["steps"],
                seed=settings.get("seed")
            )
            
            result_queue.put({
                "task_id": task_id,
                "image": image,
                "prompt": prompt,
                "status": "success"
            })
            
        except queue.Empty:
            continue
        except Exception as e:
            result_queue.put({
                "task_id": task_id if 'task_id' in locals() else None,
                "error": str(e),
                "status": "error"
            })

class DistributedNextStepPipeline:
    """ë¶„ì‚° NextStep-1 íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, num_gpus=None):
        self.num_gpus = num_gpus or torch.cuda.device_count()
        self.prompt_queue = mp.Queue()
        self.result_queue = mp.Queue()
        self.workers = []
    
    def start_workers(self):
        """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì‹œì‘"""
        for gpu_id in range(self.num_gpus):
            worker = mp.Process(
                target=distributed_generation_worker,
                args=(self.prompt_queue, self.result_queue, gpu_id)
            )
            worker.start()
            self.workers.append(worker)
    
    def submit_batch(self, prompts, settings):
        """ë°°ì¹˜ ì‘ì—… ì œì¶œ"""
        task_ids = []
        for i, prompt in enumerate(prompts):
            task_id = f"task_{int(time.time())}_{i}"
            self.prompt_queue.put((prompt, settings, task_id))
            task_ids.append(task_id)
        return task_ids
    
    def collect_results(self, expected_count, timeout=300):
        """ê²°ê³¼ ìˆ˜ì§‘"""
        results = {}
        start_time = time.time()
        
        while len(results) < expected_count:
            if time.time() - start_time > timeout:
                break
                
            try:
                result = self.result_queue.get(timeout=5)
                results[result["task_id"]] = result
            except queue.Empty:
                continue
        
        return results
    
    def shutdown(self):
        """ì›Œì»¤ ì¢…ë£Œ"""
        for _ in self.workers:
            self.prompt_queue.put(None)
        
        for worker in self.workers:
            worker.join()
```

## ì‹¤ì „ í…ŒìŠ¤íŠ¸ ê²°ê³¼

### macOS í™˜ê²½ í…ŒìŠ¤íŠ¸

ì‹¤ì œë¡œ macOSì—ì„œ NextStep-1ì„ í…ŒìŠ¤íŠ¸í•œ ê²°ê³¼ë¥¼ ê³µìœ í•©ë‹ˆë‹¤:

```bash
# í™˜ê²½ í™œì„±í™”
nextstep-env

# ë¹ ë¥¸ ì‹œìŠ¤í…œ ì ê²€
nextstep-quick
# ì¶œë ¥:
# PyTorch: 2.1.0
# CUDA Available: False
# MPS Available: True

# ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
nextstep-test
```

**í…ŒìŠ¤íŠ¸ í™˜ê²½:**
- **ì‹œìŠ¤í…œ**: macOS Sonoma 14.x (Apple Silicon M2)
- **ë©”ëª¨ë¦¬**: 32GB Unified Memory  
- **ê°€ì†**: Metal Performance Shaders (MPS)
- **ì„¤ì¹˜ ì‹œê°„**: ì•½ 15-20ë¶„ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í¬í•¨)

**ì„±ëŠ¥ ê²°ê³¼:**
- **512x512 ì´ë¯¸ì§€**: ì•½ 45-60ì´ˆ (MPS)
- **1024x1024 ì´ë¯¸ì§€**: ì•½ 2-3ë¶„ (MPS)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: í”¼í¬ ì•½ 12-15GB
- **í’ˆì§ˆ ì ìˆ˜**: í‰ê·  0.78/1.0 (ìì²´ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ)

### í¸ì˜ ëª…ë ¹ì–´ í™œìš©

ì„¤ì¹˜ëœ aliasë¥¼ í™œìš©í•œ ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ:

```bash
# ë¹ ë¥¸ ì´ë¯¸ì§€ ìƒì„±
nextstep-generate "A peaceful zen garden with cherry blossoms" 1024 35

# ë°°ì¹˜ ìƒì„±ì„ ìœ„í•œ ì„¤ì • íŒŒì¼ ìƒì„±
cat > my_batch.json << 'EOF'
{
    "prompts": [
        "Professional corporate headshot, business attire, clean background",
        "Modern minimalist logo design, tech company, blue and white",
        "Product photography, smartphone, studio lighting, white background",
        "Artistic illustration, abstract geometric shapes, vibrant colors"
    ],
    "settings": {
        "size": 1024,
        "steps": 32,
        "guidance": 8.0,
        "quality": "premium"
    }
}
EOF

# ë°°ì¹˜ ìƒì„± ì‹¤í–‰
nextstep-batch my_batch.json

# ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
nextstep-status
```

## ì‹¤ì „ í™œìš© ì‚¬ë¡€

### ë§ˆì¼€íŒ… ì»¨í…ì¸  ìë™ ìƒì„±

```python
def marketing_content_generator(product_info, target_audience, campaign_theme):
    """ë§ˆì¼€íŒ… ì»¨í…ì¸  ìë™ ìƒì„± ì‹œìŠ¤í…œ"""
    
    # ì»¨í…ì¸  ë³€í˜• ìƒì„±
    variations = [
        f"{product_info['name']} product photography, {campaign_theme} theme, targeting {target_audience}",
        f"Instagram story template for {product_info['name']}, {campaign_theme} aesthetic, {target_audience} demographic",
        f"Facebook ad visual for {product_info['name']}, {campaign_theme} style, appeals to {target_audience}",
        f"Email newsletter header for {product_info['name']}, {campaign_theme} design, {target_audience} focused",
        f"YouTube thumbnail for {product_info['name']} review, {campaign_theme} vibe, {target_audience} appeal"
    ]
    
    # A/B í…ŒìŠ¤íŠ¸ìš© ì„¤ì •
    test_configs = [
        {"guidance": 7.0, "steps": 25, "style": "conservative"},
        {"guidance": 8.5, "steps": 35, "style": "bold"}
    ]
    
    all_results = []
    
    for config in test_configs:
        pipeline = NextStepOWMPipeline()
        results = asyncio.run(pipeline.batch_generate(
            variations, 
            quality="premium"
        ))
        
        # A/B í…ŒìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for result in results:
            result["metadata"]["ab_test_config"] = config
            result["metadata"]["product_info"] = product_info
            result["metadata"]["target_audience"] = target_audience
            result["metadata"]["campaign_theme"] = campaign_theme
        
        all_results.extend(results)
    
    return all_results

# ì‹¤ì‚¬ìš© ì˜ˆì‹œ
product = {
    "name": "EcoSmart Water Bottle",
    "category": "sustainable products",
    "features": ["recyclable", "temperature control", "leak-proof"]
}

campaign_results = marketing_content_generator(
    product_info=product,
    target_audience="environmentally conscious millennials",
    campaign_theme="sustainable living"
)
```

### êµìœ¡ ì»¨í…ì¸  ì‹œê°í™”

```python
def educational_content_visualizer(topic, difficulty_level, learning_style):
    """êµìœ¡ ì»¨í…ì¸  ì‹œê°í™” ì‹œìŠ¤í…œ"""
    
    content_types = {
        "visual": [
            f"Infographic explaining {topic}, {difficulty_level} level, clean educational design",
            f"Diagram illustration for {topic}, {difficulty_level} complexity, student-friendly layout",
            f"Flowchart visualization of {topic}, {difficulty_level} detail, educational poster style"
        ],
        "conceptual": [
            f"Abstract representation of {topic}, {difficulty_level} concepts, artistic educational approach",
            f"Metaphorical illustration for {topic}, {difficulty_level} understanding, creative learning aid",
            f"Symbolic diagram of {topic}, {difficulty_level} abstraction, thought-provoking design"
        ],
        "practical": [
            f"Real-world application of {topic}, {difficulty_level} examples, practical demonstration",
            f"Step-by-step visual guide for {topic}, {difficulty_level} instructions, hands-on approach",
            f"Case study illustration of {topic}, {difficulty_level} analysis, practical scenario"
        ]
    }
    
    selected_prompts = content_types.get(learning_style, content_types["visual"])
    
    # êµìœ¡ìš© íŠ¹í™” ì„¤ì •
    edu_config = {
        "quality_presets": {
            "educational": {
                "size": 1024, 
                "steps": 35, 
                "guidance": 8.0
            }
        }
    }
    
    pipeline = NextStepOWMPipeline()
    pipeline.config.update(edu_config)
    
    results = asyncio.run(pipeline.batch_generate(
        selected_prompts, 
        quality="educational"
    ))
    
    # êµìœ¡ ë©”íƒ€ë°ì´í„° ì¶”ê°€
    for result in results:
        result["metadata"].update({
            "educational_context": {
                "topic": topic,
                "difficulty": difficulty_level,
                "learning_style": learning_style,
                "content_type": "visual_aid"
            }
        })
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
physics_visuals = educational_content_visualizer(
    topic="quantum mechanics",
    difficulty_level="undergraduate",
    learning_style="conceptual"
)
```

## ëª¨ë‹ˆí„°ë§ ë° í’ˆì§ˆ ê´€ë¦¬

### ìƒì„± í’ˆì§ˆ ìë™ í‰ê°€

```python
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
import numpy as np

class NextStepQualityAssessment:
    """NextStep-1 ìƒì„± í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.transform = transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    def assess_image_quality(self, image):
        """ì´ë¯¸ì§€ í’ˆì§ˆ ì¢…í•© í‰ê°€"""
        if isinstance(image, Image.Image):
            image_tensor = self.transform(image).unsqueeze(0)
        else:
            image_tensor = image
        
        metrics = {
            "sharpness": self.calculate_sharpness(image_tensor),
            "color_diversity": self.calculate_color_diversity(image_tensor),
            "composition_balance": self.calculate_composition_balance(image_tensor),
            "artifact_score": self.detect_artifacts(image_tensor)
        }
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        overall_score = (
            metrics["sharpness"] * 0.3 +
            metrics["color_diversity"] * 0.2 +
            metrics["composition_balance"] * 0.3 +
            (1 - metrics["artifact_score"]) * 0.2
        )
        
        metrics["overall_score"] = overall_score
        return metrics
    
    def calculate_sharpness(self, image_tensor):
        """ì„ ëª…ë„ ê³„ì‚°"""
        gray = torch.mean(image_tensor, dim=1, keepdim=True)
        
        # Laplacian í•„í„° ì ìš©
        laplacian_kernel = torch.tensor([[[
            [0, -1, 0],
            [-1, 4, -1], 
            [0, -1, 0]
        ]]], dtype=torch.float32)
        
        edges = F.conv2d(gray, laplacian_kernel, padding=1)
        sharpness = torch.var(edges).item()
        
        return min(sharpness / 100.0, 1.0)  # ì •ê·œí™”
    
    def calculate_color_diversity(self, image_tensor):
        """ìƒ‰ìƒ ë‹¤ì–‘ì„± ê³„ì‚°"""
        # RGB ì±„ë„ë³„ ë¶„ì‚° ê³„ì‚°
        rgb_variance = torch.var(image_tensor, dim=[2, 3])
        diversity_score = torch.mean(rgb_variance).item()
        
        return min(diversity_score * 10, 1.0)  # ì •ê·œí™”
    
    def calculate_composition_balance(self, image_tensor):
        """êµ¬ì„± ê· í˜• í‰ê°€"""
        # ì´ë¯¸ì§€ë¥¼ 9ê°œ ì˜ì—­ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ê· í˜• í‰ê°€
        h, w = image_tensor.shape[2], image_tensor.shape[3]
        grid_h, grid_w = h // 3, w // 3
        
        regions = []
        for i in range(3):
            for j in range(3):
                region = image_tensor[:, :, 
                    i*grid_h:(i+1)*grid_h, 
                    j*grid_w:(j+1)*grid_w
                ]
                regions.append(torch.mean(region).item())
        
        # ì˜ì—­ê°„ ë¶„ì‚°ì´ ì ì„ìˆ˜ë¡ ê· í˜•ì´ ì¢‹ìŒ
        balance_score = 1.0 - min(np.var(regions) * 5, 1.0)
        return balance_score
    
    def detect_artifacts(self, image_tensor):
        """ì•„í‹°íŒ©íŠ¸ ê°ì§€"""
        # ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆ ê°ì§€
        gray = torch.mean(image_tensor, dim=1, keepdim=True)
        
        # ê³ ì£¼íŒŒ í•„í„°
        high_freq_kernel = torch.tensor([[[
            [-1, -1, -1],
            [-1,  8, -1],
            [-1, -1, -1]
        ]]], dtype=torch.float32)
        
        high_freq = F.conv2d(gray, high_freq_kernel, padding=1)
        artifact_level = torch.mean(torch.abs(high_freq)).item()
        
        return min(artifact_level / 2.0, 1.0)  # ì •ê·œí™”

# í’ˆì§ˆ ê¸°ë°˜ ìë™ í•„í„°ë§
def quality_filtered_generation(generator, prompts, quality_threshold=0.7):
    """í’ˆì§ˆ ê¸°ì¤€ ìë™ í•„í„°ë§ ìƒì„±"""
    assessor = NextStepQualityAssessment()
    high_quality_results = []
    
    for prompt in prompts:
        max_attempts = 3
        for attempt in range(max_attempts):
            image, seed = generate_high_quality_image(
                generator, 
                prompt,
                seed=None  # ë§¤ë²ˆ ë‹¤ë¥¸ ì‹œë“œ ì‚¬ìš©
            )
            
            quality_metrics = assessor.assess_image_quality(image)
            
            if quality_metrics["overall_score"] >= quality_threshold:
                high_quality_results.append({
                    "image": image,
                    "prompt": prompt,
                    "seed": seed,
                    "quality_metrics": quality_metrics,
                    "attempt": attempt + 1
                })
                break
            
            print(f"í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ (ì ìˆ˜: {quality_metrics['overall_score']:.3f}), ì¬ì‹œë„ ì¤‘... ({attempt+1}/{max_attempts})")
    
    return high_quality_results
```

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ë¹„êµ

### NextStep-1 vs ë‹¤ë¥¸ ëª¨ë¸ ë¹„êµ

```python
def benchmark_nextstep_performance():
    """NextStep-1 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    test_prompts = [
        "A photorealistic portrait of a person reading a book in a cozy library",
        "Abstract geometric art with vibrant colors and flowing shapes", 
        "A detailed landscape painting of mountains reflected in a calm lake",
        "Modern architecture building with glass facades and innovative design",
        "Still life composition with fruits and flowers on a wooden table"
    ]
    
    benchmark_results = {
        "model": "NextStep-1",
        "parameters": "14B + 157M",
        "architecture": "Autoregressive + Flow Matching",
        "results": []
    }
    
    generator = NextStepGenerator()
    assessor = NextStepQualityAssessment()
    
    total_time = 0
    
    for i, prompt in enumerate(test_prompts):
        print(f"ë²¤ì¹˜ë§ˆí¬ {i+1}/{len(test_prompts)}: {prompt[:50]}...")
        
        start_time = time.time()
        image, seed = generate_high_quality_image(
            generator,
            prompt,
            image_size=1024,
            steps=28
        )
        generation_time = time.time() - start_time
        total_time += generation_time
        
        quality_metrics = assessor.assess_image_quality(image)
        
        result = {
            "prompt": prompt,
            "generation_time": generation_time,
            "quality_score": quality_metrics["overall_score"],
            "detailed_metrics": quality_metrics,
            "seed": seed
        }
        
        benchmark_results["results"].append(result)
        
        print(f"  ìƒì„± ì‹œê°„: {generation_time:.2f}s")
        print(f"  í’ˆì§ˆ ì ìˆ˜: {quality_metrics['overall_score']:.3f}")
    
    # ìš”ì•½ í†µê³„
    benchmark_results["summary"] = {
        "avg_generation_time": total_time / len(test_prompts),
        "avg_quality_score": np.mean([r["quality_score"] for r in benchmark_results["results"]]),
        "total_time": total_time,
        "images_per_minute": len(test_prompts) / (total_time / 60)
    }
    
    # ê²°ê³¼ ì €ì¥
    with open("nextstep_benchmark.json", "w") as f:
        json.dump(benchmark_results, f, indent=2, default=str)
    
    return benchmark_results

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
benchmark_data = benchmark_nextstep_performance()
print(f"\nğŸ“Š NextStep-1 ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
print(f"í‰ê·  ìƒì„± ì‹œê°„: {benchmark_data['summary']['avg_generation_time']:.2f}ì´ˆ")
print(f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {benchmark_data['summary']['avg_quality_score']:.3f}")
print(f"ë¶„ë‹¹ ì´ë¯¸ì§€ ìƒì„±: {benchmark_data['summary']['images_per_minute']:.1f}ì¥")
```

## ê²°ë¡ 

NextStep-1ì€ ì—°ì† í† í° ê¸°ë°˜ ìë™íšŒê·€ ì´ë¯¸ì§€ ìƒì„±ì˜ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì œì‹œí•˜ëŠ” í˜ì‹ ì ì¸ ëª¨ë¸ì…ë‹ˆë‹¤. 14B íŒŒë¼ë¯¸í„°ì˜ ë©”ì¸ ëª¨ë¸ê³¼ 157M flow matching headì˜ ì¡°í•©ì€ ê¸°ì¡´ ë””ìŠ¤í¬ë¦¬íŠ¸ í† í° ë°©ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , ë”ìš± ìì—°ìŠ¤ëŸ½ê³  ê³ í’ˆì§ˆì˜ ì´ë¯¸ì§€ ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

**ì£¼ìš” ì¥ì :**

1. **ì—°ì† í† í°ì˜ í‘œí˜„ë ¥**: ë””ìŠ¤í¬ë¦¬íŠ¸ í† í° ëŒ€ë¹„ ë” í’ë¶€í•œ ì´ë¯¸ì§€ í‘œí˜„
2. **í†µí•©ëœ ì•„í‚¤í…ì²˜**: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ ëª¨ë¸ì—ì„œ ì²˜ë¦¬
3. **ìŠ¤ì¼€ì¼ëŸ¬ë¸”í•œ ì„±ëŠ¥**: 14B íŒŒë¼ë¯¸í„°ì˜ ê°•ë ¥í•œ ìƒì„± ëŠ¥ë ¥
4. **OWM ì¹œí™”ì **: ìë™í™” ì›Œí¬í”Œë¡œìš°ì— ìµœì í™”ëœ êµ¬ì¡°

NextStep-1ì„ í™œìš©í•˜ì—¬ ë§ˆì¼€íŒ… ì»¨í…ì¸  ìë™ ìƒì„±, êµìœ¡ ìë£Œ ì‹œê°í™”, ë¸Œëœë“œ ì•„ì´ë´í‹°í‹° ê°œë°œ ë“± ë‹¤ì–‘í•œ ì°½ì‘ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œê³¼ ë¶„ì‚° ì²˜ë¦¬ ê¸°ëŠ¥ì„ í†µí•´ ëŒ€ê·œëª¨ ìƒìš© í™˜ê²½ì—ì„œë„ ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì•ìœ¼ë¡œ NextStep-1ì˜ ì—°ì† í† í° ì ‘ê·¼ë²•ì€ ë©€í‹°ëª¨ë‹¬ AIì˜ ìƒˆë¡œìš´ í‘œì¤€ì´ ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ë©°, ë”ìš± ìì—°ìŠ¤ëŸ½ê³  ì°½ì˜ì ì¸ AI ìƒì„± ì»¨í…ì¸ ì˜ ì‹œëŒ€ë¥¼ ì—´ì–´ê°ˆ ê²ƒì…ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ

- [NextStep-1 Hugging Face ëª¨ë¸](https://huggingface.co/stepfun-ai/NextStep-1-Large-Pretrain)
- [NextStep-1 ë…¼ë¬¸](https://arxiv.org/abs/2508.10711)
- [StepFun AI ê³µì‹ í™ˆí˜ì´ì§€](https://stepfun.com)
- [NextStep-1 GitHub ì €ì¥ì†Œ](https://github.com/stepfun-ai/NextStep)
