---
title: "Microsoft Phi-4-mini-flash-reasoning: ê²½ëŸ‰ ìˆ˜í•™ ì¶”ë¡  AIì˜ í˜ì‹ ì  ëŒíŒŒêµ¬"
excerpt: "3.8B íŒŒë¼ë¯¸í„°ë¡œ ëŒ€í˜• ëª¨ë¸ ì„±ëŠ¥ì— ê·¼ì ‘í•œ Microsoft Phi-4-mini-flash-reasoningì˜ í˜ì‹ ì  ì•„í‚¤í…ì²˜ì™€ ì‹¤ìš©ì  í™œìš© ë°©ì•ˆì„ ìƒì„¸íˆ ì‚´í´ë´…ë‹ˆë‹¤."
seo_title: "Microsoft Phi-4-mini-flash-reasoning ì™„ì „ ê°€ì´ë“œ - Thaki Cloud"
seo_description: "Microsoft Phi-4-mini-flash-reasoningì˜ í•˜ì´ë¸Œë¦¬ë“œ SambaY ì•„í‚¤í…ì²˜, ìˆ˜í•™ ì¶”ë¡  ëŠ¥ë ¥, ê·¸ë¦¬ê³  ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ í™œìš© ë°©ë²•ì„ ìƒì„¸íˆ ì†Œê°œí•©ë‹ˆë‹¤."
date: 2025-07-11
last_modified_at: 2025-07-11
categories:
  - owm
tags:
  - microsoft
  - phi-4-mini
  - flash-reasoning
  - math-reasoning
  - small-language-model
  - state-space-model
  - mamba
  - differential-attention
  - deepseek-r1
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/owm/microsoft-phi-4-mini-flash-reasoning-complete-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 22ë¶„

## ì„œë¡ 

ì†Œí˜• ì–¸ì–´ ëª¨ë¸(Small Language Model)ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•˜ê³  ìˆëŠ” [Microsoft Phi-4-mini-flash-reasoning](https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning)ì€ ë‹¨ 3.8B íŒŒë¼ë¯¸í„°ë¡œ ëŒ€í˜• ëª¨ë¸ì— ê·¼ì ‘í•œ ìˆ˜í•™ ì¶”ë¡  ì„±ëŠ¥ì„ ë‹¬ì„±í•œ í˜ì‹ ì ì¸ ëª¨ë¸ì…ë‹ˆë‹¤.

ì´ ëª¨ë¸ì€ **í•˜ì´ë¸Œë¦¬ë“œ SambaY ì•„í‚¤í…ì²˜**ì™€ **State Space Model(SSM)**ì„ í™œìš©í•˜ì—¬ ì¶”ë¡  íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ë©´ì„œë„ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Phi-4-mini-flash-reasoningì˜ ê¸°ìˆ ì  í˜ì‹ ë¶€í„° ì‹¤ì œ í™œìš© ë°©ì•ˆê¹Œì§€ ìƒì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

## Phi-4-mini-flash-reasoning ëª¨ë¸ ê°œìš”

### ê¸°ë³¸ ì •ë³´

Microsoft Phi-4-mini-flash-reasoningì€ ìˆ˜í•™ì  ì¶”ë¡ ì— íŠ¹í™”ëœ ê²½ëŸ‰ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.

**í•µì‹¬ ìŠ¤í™:**
- **ëª¨ë¸ í¬ê¸°**: 3.8B íŒŒë¼ë¯¸í„°
- **ì–´íœ˜ í¬ê¸°**: 200,064 í† í°
- **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: 64K í† í°
- **ì•„í‚¤í…ì²˜**: í•˜ì´ë¸Œë¦¬ë“œ SambaY with Differential Attention
- **íŠ¹í™” ë¶„ì•¼**: ìˆ˜í•™ ì¶”ë¡ , ë…¼ë¦¬ì  ì‚¬ê³ 
- **ë¼ì´ì„ ìŠ¤**: MIT

### í˜ì‹ ì  ì•„í‚¤í…ì²˜

```mermaid
graph TB
    A[ì…ë ¥ í† í°] --> B[ì„ë² ë”© ë ˆì´ì–´]
    B --> C[í•˜ì´ë¸Œë¦¬ë“œ SambaY ë¸”ë¡]
    C --> D[State Space Model<br/>Mamba ë ˆì´ì–´]
    C --> E[Differential Attention<br/>íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´]
    D --> F[Gated Memory Sharing]
    E --> F
    F --> G[ê³µìœ  KV ìºì‹œ]
    G --> H[ì¶œë ¥ ìƒì„±]
    
    style A fill:#e1f5fe
    style H fill:#c8e6c9
    style C fill:#fff3e0
    style F fill:#f3e5f5
```

## í•µì‹¬ ê¸°ìˆ  í˜ì‹ 

### 1. í•˜ì´ë¸Œë¦¬ë“œ SambaY ì•„í‚¤í…ì²˜

Phi-4-mini-flash-reasoningì˜ ê°€ì¥ í˜ì‹ ì ì¸ íŠ¹ì§•ì€ **í•˜ì´ë¸Œë¦¬ë“œ SambaY ì•„í‚¤í…ì²˜**ì…ë‹ˆë‹¤:

```python
# í•˜ì´ë¸Œë¦¬ë“œ SambaY ì•„í‚¤í…ì²˜ êµ¬ì¡°
class HybridSambaYBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # State Space Model (Mamba) êµ¬ì„±ìš”ì†Œ
        self.mamba_layer = MambaLayer(
            d_model=config.hidden_size,
            d_state=config.state_size,
            d_conv=config.conv_size,
            expand=config.expand_factor
        )
        
        # Differential Attention êµ¬ì„±ìš”ì†Œ
        self.diff_attention = DifferentialAttention(
            hidden_size=config.hidden_size,
            num_heads=config.num_attention_heads,
            attention_dropout=config.attention_dropout
        )
        
        # Gated Memory Sharing ë©”ì»¤ë‹ˆì¦˜
        self.memory_gate = nn.Linear(config.hidden_size, config.hidden_size)
        self.memory_norm = nn.LayerNorm(config.hidden_size)
        
    def forward(self, hidden_states, attention_mask=None):
        # SSM ê²½ë¡œ
        ssm_output = self.mamba_layer(hidden_states)
        
        # Attention ê²½ë¡œ
        attention_output = self.diff_attention(
            hidden_states, 
            attention_mask=attention_mask
        )
        
        # ë©”ëª¨ë¦¬ ê³µìœ  ë©”ì»¤ë‹ˆì¦˜
        gate_weights = torch.sigmoid(self.memory_gate(hidden_states))
        combined_output = gate_weights * ssm_output + (1 - gate_weights) * attention_output
        
        return self.memory_norm(combined_output)
```

### 2. State Space Model (Mamba) í†µí•©

```python
# Mamba ë ˆì´ì–´ êµ¬í˜„
class MambaLayer(nn.Module):
    def __init__(self, d_model, d_state, d_conv, expand):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        
        # ì„ íƒì  ìƒíƒœ ê³µê°„ íŒŒë¼ë¯¸í„°
        self.A_log = nn.Parameter(torch.randn(d_state, d_model))
        self.D = nn.Parameter(torch.randn(d_model))
        
        # ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
        self.conv1d = nn.Conv1d(
            in_channels=d_model,
            out_channels=d_model,
            kernel_size=d_conv,
            padding=d_conv - 1,
            groups=d_model
        )
        
        # í”„ë¡œì ì…˜ ë ˆì´ì–´
        self.in_proj = nn.Linear(d_model, d_model * expand * 2)
        self.out_proj = nn.Linear(d_model * expand, d_model)
        
    def forward(self, x):
        """
        x: (batch, seq_len, d_model)
        """
        batch, seq_len, d_model = x.shape
        
        # ì…ë ¥ í”„ë¡œì ì…˜
        x_proj = self.in_proj(x)  # (batch, seq_len, d_model * expand * 2)
        
        # ë¶„í• 
        x_mamba, x_gate = x_proj.chunk(2, dim=-1)
        
        # ì»¨ë³¼ë£¨ì…˜ ì ìš©
        x_conv = self.conv1d(x_mamba.transpose(1, 2)).transpose(1, 2)
        
        # ì„ íƒì  ìƒíƒœ ê³µê°„ ì²˜ë¦¬
        A = -torch.exp(self.A_log)  # (d_state, d_model)
        
        # ìƒíƒœ ê³µê°„ ì²˜ë¦¬ (ê°„ì†Œí™”ëœ ë²„ì „)
        x_ssm = self._selective_scan(x_conv, A, self.D)
        
        # ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜
        x_gated = x_ssm * torch.sigmoid(x_gate)
        
        # ì¶œë ¥ í”„ë¡œì ì…˜
        output = self.out_proj(x_gated)
        
        return output
    
    def _selective_scan(self, x, A, D):
        """ì„ íƒì  ìƒíƒœ ê³µê°„ ìŠ¤ìº”"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ë³µì¡í•œ ì„ íƒì  ìŠ¤ìº” ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ê°„ì†Œí™”ëœ ë²„ì „ ì œì‹œ
        return x * D.unsqueeze(0).unsqueeze(0)
```

### 3. Differential Attention ë©”ì»¤ë‹ˆì¦˜

```python
class DifferentialAttention(nn.Module):
    def __init__(self, hidden_size, num_heads, attention_dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        # ë‘ ê°œì˜ ì–´í…ì…˜ í—¤ë“œ ê·¸ë£¹
        self.q_proj_1 = nn.Linear(hidden_size, hidden_size)
        self.k_proj_1 = nn.Linear(hidden_size, hidden_size)
        self.v_proj_1 = nn.Linear(hidden_size, hidden_size)
        
        self.q_proj_2 = nn.Linear(hidden_size, hidden_size)
        self.k_proj_2 = nn.Linear(hidden_size, hidden_size)
        self.v_proj_2 = nn.Linear(hidden_size, hidden_size)
        
        self.out_proj = nn.Linear(hidden_size, hidden_size)
        self.dropout = nn.Dropout(attention_dropout)
        
        # ì°¨ë¶„ ê°€ì¤‘ì¹˜
        self.lambda_q1 = nn.Parameter(torch.randn(num_heads, self.head_dim))
        self.lambda_k1 = nn.Parameter(torch.randn(num_heads, self.head_dim))
        self.lambda_q2 = nn.Parameter(torch.randn(num_heads, self.head_dim))
        self.lambda_k2 = nn.Parameter(torch.randn(num_heads, self.head_dim))
        
    def forward(self, hidden_states, attention_mask=None):
        batch_size, seq_len, _ = hidden_states.shape
        
        # ì²« ë²ˆì§¸ ì–´í…ì…˜ ê·¸ë£¹
        q1 = self.q_proj_1(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k1 = self.k_proj_1(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim)
        v1 = self.v_proj_1(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # ë‘ ë²ˆì§¸ ì–´í…ì…˜ ê·¸ë£¹
        q2 = self.q_proj_2(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k2 = self.k_proj_2(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim)
        v2 = self.v_proj_2(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # ì°¨ë¶„ ì–´í…ì…˜ ê³„ì‚°
        attn_1 = self._compute_attention(q1, k1, v1, attention_mask)
        attn_2 = self._compute_attention(q2, k2, v2, attention_mask)
        
        # ì°¨ë¶„ ë©”ì»¤ë‹ˆì¦˜ ì ìš©
        diff_attn = attn_1 - attn_2
        
        # ì¶œë ¥ í”„ë¡œì ì…˜
        output = self.out_proj(diff_attn.view(batch_size, seq_len, self.hidden_size))
        
        return output
    
    def _compute_attention(self, q, k, v, attention_mask):
        """í‘œì¤€ ì–´í…ì…˜ ê³„ì‚°"""
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        if attention_mask is not None:
            scores = scores.masked_fill(attention_mask == 0, -1e9)
        
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        attn_output = torch.matmul(attn_weights, v)
        return attn_output
```

## ì„±ëŠ¥ ë¶„ì„ ë° ë²¤ì¹˜ë§ˆí¬

### 1. ìˆ˜í•™ ì¶”ë¡  ì„±ëŠ¥

Phi-4-mini-flash-reasoningì€ ìˆ˜í•™ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ íƒì›”í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:

| ë²¤ì¹˜ë§ˆí¬ | Phi-4-mini-flash (3.8B) | DeepSeek-R1-7B | Llama-3.1-8B | GPT-4o-mini |
|---------|-------------------------|----------------|--------------|-------------|
| **AIME 2024** | 52.29% | 53.70% | 43.96% | 63.6% |
| **AIME 2025** | 33.59% | 35.94% | 27.34% | 54.8% |
| **Math500** | 92.45% | 93.03% | 87.48% | 90.0% |
| **GPQA Diamond** | 45.08% | 47.85% | 45.83% | 60.0% |

### 2. íš¨ìœ¨ì„± ë¹„êµ

```python
# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì½”ë“œ
import time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def benchmark_model_efficiency():
    model_id = "microsoft/Phi-4-mini-flash-reasoning"
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°
    test_problems = [
        "3x^2 + 4x + 5 = 1ì„ í’€ì–´ì£¼ì„¸ìš”.",
        "ë¯¸ë¶„ d/dx[x^3 + 2x^2 + x + 1]ì„ ê³„ì‚°í•´ì£¼ì„¸ìš”.",
        "í–‰ë ¬ [[1, 2], [3, 4]]ì˜ ê³ ìœ ê°’ì„ êµ¬í•´ì£¼ì„¸ìš”.",
        "ì‚¼ê°í˜•ì˜ ë„“ì´ê°€ 12ì´ê³  ë°‘ë³€ì´ 6ì¼ ë•Œ ë†’ì´ë¥¼ êµ¬í•´ì£¼ì„¸ìš”."
    ]
    
    total_time = 0
    total_tokens = 0
    
    for problem in test_problems:
        messages = [{
            "role": "user",
            "content": problem
        }]
        
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs.to(model.device),
                max_new_tokens=512,
                temperature=0.6,
                top_p=0.95,
                do_sample=True
            )
        
        end_time = time.time()
        
        response_time = end_time - start_time
        token_count = outputs.shape[1] - inputs["input_ids"].shape[1]
        
        total_time += response_time
        total_tokens += token_count
        
        print(f"ë¬¸ì œ: {problem}")
        print(f"ì‘ë‹µ ì‹œê°„: {response_time:.2f}ì´ˆ")
        print(f"ìƒì„± í† í°: {token_count}")
        print(f"í† í°/ì´ˆ: {token_count/response_time:.2f}")
        print("-" * 50)
    
    avg_time = total_time / len(test_problems)
    avg_tokens = total_tokens / len(test_problems)
    
    print(f"\ní‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.2f}ì´ˆ")
    print(f"í‰ê·  í† í° ìˆ˜: {avg_tokens:.0f}")
    print(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {avg_tokens/avg_time:.2f} í† í°/ì´ˆ")

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
benchmark_model_efficiency()
```

### 3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±

```python
def analyze_memory_efficiency():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„"""
    import psutil
    import torch
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
    process = psutil.Process()
    initial_memory = process.memory_info().rss / 1024 / 1024  # MB
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        "microsoft/Phi-4-mini-flash-reasoning",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    model_memory = process.memory_info().rss / 1024 / 1024 - initial_memory
    gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
    
    print(f"ëª¨ë¸ CPU ë©”ëª¨ë¦¬: {model_memory:.2f} MB")
    print(f"ëª¨ë¸ GPU ë©”ëª¨ë¦¬: {gpu_memory:.2f} MB")
    print(f"ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {model_memory + gpu_memory:.2f} MB")
    
    # ì¶”ë¡  ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    test_input = "ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œë¥¼ í’€ì–´ì£¼ì„¸ìš”: âˆ«(x^2 + 3x + 2)dx"
    
    with torch.no_grad():
        inputs = tokenizer(test_input, return_tensors="pt").to(model.device)
        max_memory_before = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        outputs = model.generate(**inputs, max_new_tokens=512)
        
        max_memory_after = torch.cuda.max_memory_allocated() / 1024 / 1024
        inference_memory = max_memory_after - max_memory_before
        
    print(f"ì¶”ë¡  ì‹œ ì¶”ê°€ ë©”ëª¨ë¦¬: {inference_memory:.2f} MB")
    
    return {
        "model_memory_mb": model_memory,
        "gpu_memory_mb": gpu_memory,
        "inference_memory_mb": inference_memory
    }
```

## ì‹¤ìš©ì  í™œìš© ì‚¬ë¡€

### 1. ìˆ˜í•™ êµìœ¡ ì–´ì‹œìŠ¤í„´íŠ¸

```python
class MathEducationAssistant:
    def __init__(self):
        self.model = AutoModelForCausalLM.from_pretrained(
            "microsoft/Phi-4-mini-flash-reasoning",
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            "microsoft/Phi-4-mini-flash-reasoning"
        )
        
    def solve_step_by_step(self, problem, difficulty="intermediate"):
        """ë‹¨ê³„ë³„ ìˆ˜í•™ ë¬¸ì œ í•´ê²°"""
        difficulty_prompts = {
            "beginner": "ì´ˆë“±í•™êµ ìˆ˜ì¤€ìœ¼ë¡œ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "intermediate": "ì¤‘ê³ ë“±í•™êµ ìˆ˜ì¤€ìœ¼ë¡œ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "advanced": "ëŒ€í•™êµ ìˆ˜ì¤€ìœ¼ë¡œ ìƒì„¸í•œ ì¦ëª…ê³¼ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        }
        
        prompt = f"""
        ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”:
        {problem}
        
        {difficulty_prompts[difficulty]}
        
        í•´ê²° ê³¼ì •ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:
        1. ë¬¸ì œ ì´í•´
        2. ì ‘ê·¼ ë°©ë²•
        3. ë‹¨ê³„ë³„ ê³„ì‚°
        4. ë‹µ í™•ì¸
        5. ê²°ë¡ 
        """
        
        messages = [{"role": "user", "content": prompt}]
        
        inputs = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs.to(self.model.device),
                max_new_tokens=1024,
                temperature=0.6,
                top_p=0.95,
                do_sample=True
            )
        
        response = self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return response
    
    def generate_practice_problems(self, topic, count=5):
        """ìœ ì‚¬í•œ ì—°ìŠµ ë¬¸ì œ ìƒì„±"""
        prompt = f"""
        ë‹¤ìŒ ì£¼ì œì— ëŒ€í•œ ì—°ìŠµ ë¬¸ì œë¥¼ {count}ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:
        ì£¼ì œ: {topic}
        
        ê° ë¬¸ì œëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:
        ë¬¸ì œ 1: [ë¬¸ì œ ë‚´ìš©]
        ë‹µ: [ì •ë‹µ]
        
        ë¬¸ì œ 2: [ë¬¸ì œ ë‚´ìš©]
        ë‹µ: [ì •ë‹µ]
        
        (ê³„ì†...)
        """
        
        messages = [{"role": "user", "content": prompt}]
        
        inputs = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs.to(self.model.device),
                max_new_tokens=800,
                temperature=0.8,
                top_p=0.95,
                do_sample=True
            )
        
        response = self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return response

# ì‚¬ìš© ì˜ˆì‹œ
math_assistant = MathEducationAssistant()

# ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œ í•´ê²°
problem = "í–‰ë ¬ A = [[2, 1], [1, 2]]ì— ëŒ€í•´ A^nì˜ ì¼ë°˜í•­ì„ êµ¬í•˜ì„¸ìš”."
solution = math_assistant.solve_step_by_step(problem, "advanced")
print(solution)

# ì—°ìŠµ ë¬¸ì œ ìƒì„±
practice_problems = math_assistant.generate_practice_problems("ì´ì°¨ë°©ì •ì‹", 3)
print(practice_problems)
```

### 2. ì½”ë”© ë¬¸ì œ í•´ê²° ë„êµ¬

```python
class CodingProblemSolver:
    def __init__(self):
        self.model = AutoModelForCausalLM.from_pretrained(
            "microsoft/Phi-4-mini-flash-reasoning",
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            "microsoft/Phi-4-mini-flash-reasoning"
        )
    
    def solve_algorithm_problem(self, problem_description, language="python"):
        """ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œ í•´ê²°"""
        prompt = f"""
        ë‹¤ìŒ ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œë¥¼ {language}ë¡œ í•´ê²°í•´ì£¼ì„¸ìš”:
        
        ë¬¸ì œ: {problem_description}
        
        í•´ê²° ë°©ë²•ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:
        1. ë¬¸ì œ ë¶„ì„
        2. ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„
        3. ì‹œê°„ ë³µì¡ë„ ë¶„ì„
        4. ì½”ë“œ êµ¬í˜„
        5. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        """
        
        messages = [{"role": "user", "content": prompt}]
        
        inputs = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs.to(self.model.device),
                max_new_tokens=1200,
                temperature=0.6,
                top_p=0.95,
                do_sample=True
            )
        
        response = self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return response
    
    def optimize_code(self, code, optimization_target="performance"):
        """ì½”ë“œ ìµœì í™”"""
        optimization_prompts = {
            "performance": "ì‹¤í–‰ ì†ë„ë¥¼ ìµœì í™”í•´ì£¼ì„¸ìš”.",
            "memory": "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì í™”í•´ì£¼ì„¸ìš”.",
            "readability": "ì½”ë“œ ê°€ë…ì„±ì„ ê°œì„ í•´ì£¼ì„¸ìš”."
        }
        
        prompt = f"""
        ë‹¤ìŒ ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  {optimization_prompts[optimization_target]}
        
        ê¸°ì¡´ ì½”ë“œ:
        ```python
        {code}
        ```
        
        ê°œì„  ì‚¬í•­ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:
        1. í˜„ì¬ ì½”ë“œ ë¶„ì„
        2. ê°œì„  í¬ì¸íŠ¸
        3. ìµœì í™”ëœ ì½”ë“œ
        4. ê°œì„  íš¨ê³¼ ì„¤ëª…
        """
        
        messages = [{"role": "user", "content": prompt}]
        
        inputs = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs.to(self.model.device),
                max_new_tokens=1000,
                temperature=0.6,
                top_p=0.95,
                do_sample=True
            )
        
        response = self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return response

# ì‚¬ìš© ì˜ˆì‹œ
coding_solver = CodingProblemSolver()

# ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œ í•´ê²°
problem = "ì£¼ì–´ì§„ ë°°ì—´ì—ì„œ ë‘ ìˆ˜ì˜ í•©ì´ targetê°’ê³¼ ê°™ì€ ì¸ë±ìŠ¤ë¥¼ ì°¾ëŠ” ë¬¸ì œ"
solution = coding_solver.solve_algorithm_problem(problem)
print(solution)
```

### 3. ë…¼ë¦¬ ì¶”ë¡  ì—”ì§„

```python
class LogicalReasoningEngine:
    def __init__(self):
        self.model = AutoModelForCausalLM.from_pretrained(
            "microsoft/Phi-4-mini-flash-reasoning",
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            "microsoft/Phi-4-mini-flash-reasoning"
        )
    
    def solve_logical_puzzle(self, puzzle_description):
        """ë…¼ë¦¬ í¼ì¦ í•´ê²°"""
        prompt = f"""
        ë‹¤ìŒ ë…¼ë¦¬ í¼ì¦ì„ í•´ê²°í•´ì£¼ì„¸ìš”:
        
        {puzzle_description}
        
        í•´ê²° ê³¼ì •ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:
        1. ì£¼ì–´ì§„ ì¡°ê±´ ì •ë¦¬
        2. ë…¼ë¦¬ì  ì¶”ë¡  ê³¼ì •
        3. ì¤‘ê°„ ê²°ë¡ ë“¤
        4. ìµœì¢… ë‹µì•ˆ
        5. ê²€ì¦
        """
        
        messages = [{"role": "user", "content": prompt}]
        
        inputs = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs.to(self.model.device),
                max_new_tokens=1000,
                temperature=0.6,
                top_p=0.95,
                do_sample=True
            )
        
        response = self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return response
    
    def analyze_argument(self, argument_text):
        """ë…¼ì¦ ë¶„ì„"""
        prompt = f"""
        ë‹¤ìŒ ë…¼ì¦ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
        
        "{argument_text}"
        
        ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:
        1. ì „ì œë“¤ ì‹ë³„
        2. ê²°ë¡  ì‹ë³„
        3. ë…¼ì¦ êµ¬ì¡° ë¶„ì„
        4. ë…¼ë¦¬ì  íƒ€ë‹¹ì„± ê²€í† 
        5. ì ì¬ì  ì˜¤ë¥˜ ì§€ì 
        6. ë…¼ì¦ ê°•í™” ë°©ì•ˆ
        """
        
        messages = [{"role": "user", "content": prompt}]
        
        inputs = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs.to(self.model.device),
                max_new_tokens=800,
                temperature=0.6,
                top_p=0.95,
                do_sample=True
            )
        
        response = self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return response

# ì‚¬ìš© ì˜ˆì‹œ
reasoning_engine = LogicalReasoningEngine()

# ë…¼ë¦¬ í¼ì¦ í•´ê²°
puzzle = """
ì„¸ ì‚¬ëŒ A, B, Cê°€ ìˆìŠµë‹ˆë‹¤.
- AëŠ” í•­ìƒ ê±°ì§“ë§ì„ í•©ë‹ˆë‹¤.
- BëŠ” í•­ìƒ ì§„ì‹¤ì„ ë§í•©ë‹ˆë‹¤.
- CëŠ” ë•Œë•Œë¡œ ê±°ì§“ë§ì„ í•©ë‹ˆë‹¤.

Aê°€ ë§í–ˆìŠµë‹ˆë‹¤: "BëŠ” ê±°ì§“ë§ìŸì´ì…ë‹ˆë‹¤."
Bê°€ ë§í–ˆìŠµë‹ˆë‹¤: "CëŠ” ì˜¤ëŠ˜ ì§„ì‹¤ì„ ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤."
Cê°€ ë§í–ˆìŠµë‹ˆë‹¤: "Aì™€ B ì¤‘ ì ì–´ë„ í•œ ëª…ì€ ê±°ì§“ë§ìŸì´ì…ë‹ˆë‹¤."

ê°ìì˜ ì •ì²´ë¥¼ ë°í˜€ì£¼ì„¸ìš”.
"""

solution = reasoning_engine.solve_logical_puzzle(puzzle)
print(solution)
```

## ê³ ê¸‰ í™œìš© ë° ìµœì í™”

### 1. í”„ë¡œë•ì…˜ í™˜ê²½ ë°°í¬

```python
# FastAPI ì„œë²„ êµ¬ì¶•
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import asyncio
from typing import Optional

app = FastAPI(title="Phi-4-mini-flash-reasoning API")

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-4-mini-flash-reasoning",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-flash-reasoning")

class MathRequest(BaseModel):
    problem: str
    difficulty: Optional[str] = "intermediate"
    max_tokens: Optional[int] = 512
    temperature: Optional[float] = 0.6

class ReasoningRequest(BaseModel):
    question: str
    context: Optional[str] = ""
    max_tokens: Optional[int] = 800
    temperature: Optional[float] = 0.6

@app.post("/solve-math")
async def solve_math_problem(request: MathRequest):
    """ìˆ˜í•™ ë¬¸ì œ í•´ê²° API"""
    try:
        prompt = f"""
        ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”:
        {request.problem}
        
        ë‚œì´ë„: {request.difficulty}
        
        ë‹¨ê³„ë³„ë¡œ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        """
        
        messages = [{"role": "user", "content": prompt}]
        
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs.to(model.device),
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=0.95,
                do_sample=True
            )
        
        response = tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return {
            "status": "success",
            "problem": request.problem,
            "solution": response,
            "model": "phi-4-mini-flash-reasoning"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/logical-reasoning")
async def logical_reasoning(request: ReasoningRequest):
    """ë…¼ë¦¬ì  ì¶”ë¡  API"""
    try:
        context_prompt = f"ì°¸ê³  ì •ë³´: {request.context}\n\n" if request.context else ""
        
        prompt = f"""
        {context_prompt}ì§ˆë¬¸: {request.question}
        
        ë…¼ë¦¬ì ìœ¼ë¡œ ë‹¨ê³„ë³„ë¡œ ì¶”ë¡ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
        """
        
        messages = [{"role": "user", "content": prompt}]
        
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs.to(model.device),
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=0.95,
                do_sample=True
            )
        
        response = tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return {
            "status": "success",
            "question": request.question,
            "reasoning": response,
            "model": "phi-4-mini-flash-reasoning"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/model-info")
async def get_model_info():
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    return {
        "model_name": "microsoft/Phi-4-mini-flash-reasoning",
        "parameters": "3.8B",
        "architecture": "Hybrid SambaY with Differential Attention",
        "context_length": "64K tokens",
        "specialization": "Mathematical reasoning and logic",
        "memory_usage": f"{torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”

```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            "request_count": 0,
            "total_response_time": 0,
            "error_count": 0,
            "token_count": 0
        }
    
    def log_request(self, response_time, token_count, error=False):
        """ìš”ì²­ ë¡œê¹…"""
        self.metrics["request_count"] += 1
        self.metrics["total_response_time"] += response_time
        self.metrics["token_count"] += token_count
        
        if error:
            self.metrics["error_count"] += 1
    
    def get_stats(self):
        """í†µê³„ ì¡°íšŒ"""
        if self.metrics["request_count"] == 0:
            return {"message": "No requests processed yet"}
        
        avg_response_time = self.metrics["total_response_time"] / self.metrics["request_count"]
        avg_tokens = self.metrics["token_count"] / self.metrics["request_count"]
        error_rate = self.metrics["error_count"] / self.metrics["request_count"]
        
        return {
            "total_requests": self.metrics["request_count"],
            "avg_response_time": f"{avg_response_time:.2f}s",
            "avg_tokens_per_request": f"{avg_tokens:.0f}",
            "error_rate": f"{error_rate:.2%}",
            "throughput": f"{self.metrics['request_count'] / (self.metrics['total_response_time'] / 3600):.2f} req/hour"
        }

# ëª¨ë‹ˆí„°ë§ ì¸ìŠ¤í„´ìŠ¤
monitor = PerformanceMonitor()

# API ì—”ë“œí¬ì¸íŠ¸ì— ëª¨ë‹ˆí„°ë§ ì¶”ê°€
@app.middleware("http")
async def add_monitoring(request, call_next):
    start_time = time.time()
    
    try:
        response = await call_next(request)
        process_time = time.time() - start_time
        
        # ì„±ê³µ ìš”ì²­ ë¡œê¹…
        monitor.log_request(process_time, 0)  # í† í° ìˆ˜ëŠ” ì‹¤ì œ êµ¬í˜„ì—ì„œ ê³„ì‚°
        
        return response
        
    except Exception as e:
        process_time = time.time() - start_time
        monitor.log_request(process_time, 0, error=True)
        raise e

@app.get("/stats")
async def get_performance_stats():
    """ì„±ëŠ¥ í†µê³„ ì¡°íšŒ"""
    return monitor.get_stats()
```

### 3. ë°°ì¹˜ ì²˜ë¦¬ ë° ìºì‹±

```python
import asyncio
from typing import List
import hashlib
import redis

class BatchProcessor:
    def __init__(self, batch_size=4, cache_ttl=3600):
        self.batch_size = batch_size
        self.cache = redis.Redis(host='localhost', port=6379, db=0)
        self.cache_ttl = cache_ttl
        self.pending_requests = []
        
    def _generate_cache_key(self, request_text):
        """ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(request_text.encode()).hexdigest()
    
    def _get_from_cache(self, cache_key):
        """ìºì‹œì—ì„œ ì¡°íšŒ"""
        try:
            cached_result = self.cache.get(cache_key)
            if cached_result:
                return cached_result.decode()
            return None
        except:
            return None
    
    def _save_to_cache(self, cache_key, result):
        """ìºì‹œì— ì €ì¥"""
        try:
            self.cache.setex(cache_key, self.cache_ttl, result)
        except:
            pass
    
    async def process_batch(self, requests: List[str]):
        """ë°°ì¹˜ ì²˜ë¦¬"""
        results = []
        
        for request in requests:
            cache_key = self._generate_cache_key(request)
            cached_result = self._get_from_cache(cache_key)
            
            if cached_result:
                results.append(cached_result)
            else:
                # ì‹¤ì œ ëª¨ë¸ ì¶”ë¡ 
                messages = [{"role": "user", "content": request}]
                
                inputs = tokenizer.apply_chat_template(
                    messages,
                    add_generation_prompt=True,
                    return_dict=True,
                    return_tensors="pt"
                )
                
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs.to(model.device),
                        max_new_tokens=512,
                        temperature=0.6,
                        top_p=0.95,
                        do_sample=True
                    )
                
                response = tokenizer.decode(
                    outputs[0][inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
                
                # ìºì‹œì— ì €ì¥
                self._save_to_cache(cache_key, response)
                results.append(response)
        
        return results

# ë°°ì¹˜ ì²˜ë¦¬ ì¸ìŠ¤í„´ìŠ¤
batch_processor = BatchProcessor()

@app.post("/batch-solve")
async def batch_solve_problems(problems: List[str]):
    """ë°°ì¹˜ ë¬¸ì œ í•´ê²°"""
    try:
        results = await batch_processor.process_batch(problems)
        
        return {
            "status": "success",
            "batch_size": len(problems),
            "results": [
                {"problem": problem, "solution": solution}
                for problem, solution in zip(problems, results)
            ]
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

## í•œê³„ì  ë° ê°œì„  ë°©ì•ˆ

### 1. ì£¼ìš” í•œê³„ì 

```python
def analyze_model_limitations():
    """ëª¨ë¸ í•œê³„ì  ë¶„ì„"""
    limitations = {
        "factual_knowledge": {
            "description": "ì‘ì€ ëª¨ë¸ í¬ê¸°ë¡œ ì¸í•œ ì œí•œì  ì§€ì‹ ì €ì¥",
            "impact": "ì¼ë°˜ ìƒì‹ì´ë‚˜ ìµœì‹  ì •ë³´ì— ëŒ€í•œ ë¶€ì •í™•í•œ ë‹µë³€ ê°€ëŠ¥",
            "mitigation": "RAG(Retrieval-Augmented Generation) í™œìš©"
        },
        "long_context": {
            "description": "64K í† í° ì œí•œ",
            "impact": "ë§¤ìš° ê¸´ ë¬¸ì„œë‚˜ ë³µì¡í•œ ì¶”ë¡  ê³¼ì • ì²˜ë¦¬ í•œê³„",
            "mitigation": "ì²­í¬ ê¸°ë°˜ ì²˜ë¦¬ ë˜ëŠ” ìš”ì•½ í›„ ì²˜ë¦¬"
        },
        "multimodal": {
            "description": "í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬ ê°€ëŠ¥",
            "impact": "ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹° ì²˜ë¦¬ ë¶ˆê°€",
            "mitigation": "ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹° ì „ìš© ëª¨ë¸ê³¼ ì¡°í•©"
        },
        "language_diversity": {
            "description": "ì˜ì–´ ì¤‘ì‹¬ í›ˆë ¨",
            "impact": "ë‹¤ë¥¸ ì–¸ì–´ì—ì„œì˜ ì„±ëŠ¥ ì €í•˜",
            "mitigation": "ë‹¤êµ­ì–´ íŒŒì¸íŠœë‹ í•„ìš”"
        }
    }
    
    return limitations

# í•œê³„ì  ë¶„ì„ ì‹¤í–‰
limitations = analyze_model_limitations()
for key, value in limitations.items():
    print(f"{key.upper()}:")
    print(f"  ì„¤ëª…: {value['description']}")
    print(f"  ì˜í–¥: {value['impact']}")
    print(f"  ì™„í™” ë°©ì•ˆ: {value['mitigation']}")
    print()
```

### 2. ê°œì„  ë°©ì•ˆ

```python
class ModelEnhancer:
    def __init__(self):
        self.base_model = AutoModelForCausalLM.from_pretrained(
            "microsoft/Phi-4-mini-flash-reasoning",
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            "microsoft/Phi-4-mini-flash-reasoning"
        )
        
    def add_rag_capability(self, knowledge_base_path):
        """RAG ê¸°ëŠ¥ ì¶”ê°€"""
        from sentence_transformers import SentenceTransformer
        import faiss
        import numpy as np
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        
        # ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ë° ì¸ë±ì‹±
        with open(knowledge_base_path, 'r', encoding='utf-8') as f:
            self.knowledge_base = f.read().split('\n\n')
        
        # ì„ë² ë”© ìƒì„±
        embeddings = self.embedder.encode(self.knowledge_base)
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        self.index = faiss.IndexFlatIP(embeddings.shape[1])
        self.index.add(embeddings.astype('float32'))
        
    def retrieve_relevant_context(self, query, top_k=3):
        """ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        if not hasattr(self, 'index'):
            return ""
        
        query_embedding = self.embedder.encode([query])
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        relevant_contexts = [self.knowledge_base[idx] for idx in indices[0]]
        return "\n\n".join(relevant_contexts)
    
    def enhanced_generation(self, question, use_rag=True):
        """í–¥ìƒëœ ìƒì„± ê¸°ëŠ¥"""
        if use_rag and hasattr(self, 'index'):
            context = self.retrieve_relevant_context(question)
            prompt = f"""
            ì°¸ê³  ì •ë³´:
            {context}
            
            ì§ˆë¬¸: {question}
            
            ìœ„ì˜ ì°¸ê³  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
            """
        else:
            prompt = question
        
        messages = [{"role": "user", "content": prompt}]
        
        inputs = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = self.base_model.generate(
                **inputs.to(self.base_model.device),
                max_new_tokens=800,
                temperature=0.6,
                top_p=0.95,
                do_sample=True
            )
        
        response = self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return response

# ëª¨ë¸ í–¥ìƒ ì‚¬ìš© ì˜ˆì‹œ
enhancer = ModelEnhancer()

# RAG ê¸°ëŠ¥ ì¶”ê°€ (ì§€ì‹ ë² ì´ìŠ¤ íŒŒì¼ í•„ìš”)
# enhancer.add_rag_capability("knowledge_base.txt")

# í–¥ìƒëœ ìƒì„±
question = "ì–‘ì ì»´í“¨í„°ì˜ ì›ë¦¬ì™€ ì‘ìš© ë¶„ì•¼ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
enhanced_answer = enhancer.enhanced_generation(question, use_rag=True)
print(enhanced_answer)
```

## ê²°ë¡ 

Microsoft Phi-4-mini-flash-reasoningì€ ì†Œí˜• ì–¸ì–´ ëª¨ë¸ì˜ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì œì‹œí•˜ëŠ” í˜ì‹ ì ì¸ ëª¨ë¸ì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ê°€ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

### ğŸ¯ ì£¼ìš” ê°•ì 

1. **íš¨ìœ¨ì„±**: 3.8B íŒŒë¼ë¯¸í„°ë¡œ ëŒ€í˜• ëª¨ë¸ê¸‰ ì„±ëŠ¥
2. **í˜ì‹ ì  ì•„í‚¤í…ì²˜**: í•˜ì´ë¸Œë¦¬ë“œ SambaYì™€ Differential Attention
3. **ìˆ˜í•™ íŠ¹í™”**: ë›°ì–´ë‚œ ìˆ˜í•™ì  ì¶”ë¡  ëŠ¥ë ¥
4. **ì‹¤ìš©ì„±**: ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ë°”ë¡œ í™œìš© ê°€ëŠ¥

### ğŸ”® ë¯¸ë˜ ì „ë§

- **ì—£ì§€ ì»´í“¨íŒ…**: ì œí•œëœ ë¦¬ì†ŒìŠ¤ í™˜ê²½ì—ì„œì˜ AI í™œìš©
- **êµìœ¡ í˜ì‹ **: ê°œì¸í™”ëœ ìˆ˜í•™ êµìœ¡ ì„œë¹„ìŠ¤
- **ê³¼í•™ ì—°êµ¬**: ë³µì¡í•œ ê³„ì‚° ë° ì¶”ë¡  ì§€ì›
- **ë¹„ìš© íš¨ìœ¨ì„±**: í´ë¼ìš°ë“œ ë¹„ìš© ëŒ€ë¹„ ë†’ì€ ì„±ëŠ¥

### ğŸ› ï¸ í™œìš© ê¶Œì¥ì‚¬í•­

1. **ìˆ˜í•™ êµìœ¡**: ë‹¨ê³„ë³„ ë¬¸ì œ í•´ê²° ë° ê°œë… ì„¤ëª…
2. **ì½”ë”© ì§€ì›**: ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œ í•´ê²° ë° ì½”ë“œ ìµœì í™”
3. **ë…¼ë¦¬ ì¶”ë¡ **: ë³µì¡í•œ ë…¼ë¦¬ ë¬¸ì œ í•´ê²°
4. **ì—°êµ¬ ì§€ì›**: ê³¼í•™ì  ê³„ì‚° ë° ë¶„ì„ ë³´ì¡°

Phi-4-mini-flash-reasoningì€ ë‹¨ìˆœí•œ ì†Œí˜• ëª¨ë¸ì„ ë„˜ì–´ì„œ AIì˜ ë¯¼ì£¼í™”ì™€ íš¨ìœ¨ì„±ì„ ë™ì‹œì— ë‹¬ì„±í•œ ëª¨ë¸ì…ë‹ˆë‹¤. íŠ¹íˆ êµìœ¡, ì—°êµ¬, ê°œë°œ ë¶„ì•¼ì—ì„œ ê°•ë ¥í•œ ë„êµ¬ë¡œ í™œìš©ë  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

---

**ì°¸ê³  ë§í¬:**
- [Phi-4-mini-flash-reasoning ê³µì‹ í˜ì´ì§€](https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning)
- [Microsoft Phi-4 ê¸°ìˆ  ë³´ê³ ì„œ](https://arxiv.org/abs/2507.06607)
- [Phi-4 ì‹œë¦¬ì¦ˆ í¬í„¸](https://aka.ms/phi4)
- [Azure AI Studioì—ì„œ Phi-4 ì‚¬ìš©í•˜ê¸°](https://azure.microsoft.com/services/ai-studio/) 