---
title: "2025년 강화학습 후훈련 연구의 최전선: 필독 논문 10선 종합 분석"
excerpt: "2025년 4월부터 발표된 강화학습 후훈련 분야의 핵심 연구들을 심층 분석합니다. Kimi k1.5부터 AlphaMed까지, LLM의 추론 능력을 혁신적으로 향상시키는 최신 기법들을 한국어로 상세히 소개합니다."
seo_title: "2025년 강화학습 후훈련 연구 TOP 10 논문 리뷰 - 추론 능력 혁신의 최전선 - Thaki Cloud"
seo_description: "Kimi k1.5, Microsoft RPT, Agent Lightning 등 2025년 최신 강화학습 후훈련 연구 10선을 한국어로 상세 분석. LLM 추론 능력 향상의 핵심 기법과 실무 적용 인사이트를 제공합니다."
date: 2025-01-28
last_modified_at: 2025-01-28
categories:
  - research
tags:
  - 강화학습
  - 후훈련
  - LLM
  - 추론능력
  - AI연구
  - 논문리뷰
  - 머신러닝
  - 인공지능
author_profile: true
toc: true
toc_label: "목차"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/research/reinforcement-learning-post-training-top-10-papers-review/"
reading_time: true
---

⏱️ **예상 읽기 시간**: 25분

## 서론: 강화학습 후훈련이 열어가는 AI의 새로운 지평

2025년은 대규모 언어모델(LLM)의 추론 능력을 혁신적으로 향상시키는 강화학습 후훈련(Reinforcement Learning Post-Training) 연구가 폭발적으로 발전한 해입니다. OpenAI의 o1 모델이 보여준 놀라운 추론 성능은 더 이상 폐쇄형 모델만의 전유물이 아닙니다. 

이번 글에서는 2025년 4월부터 현재까지 발표된 강화학습 후훈련 분야의 가장 중요한 연구 10편을 심층 분석합니다. 이 연구들은 7B-13B 규모의 오픈 모델을 GPT-4 수준의 추론 능력까지 끌어올리는 구체적인 방법론부터, 의료·법률 등 전문 도메인에서의 특화된 응용까지 다양한 혁신을 제시하고 있습니다.

각 논문이 제시하는 핵심 아이디어와 실무적 시사점을 통해, 강화학습이 어떻게 LLM의 한계를 뛰어넘는 새로운 패러다임을 만들어가고 있는지 살펴보겠습니다.

## 1. Kimi k1.5: 대규모 강화학습으로 추론 능력의 새로운 기준 제시

### 핵심 기여와 혁신점

Kimi k1.5는 2025년 강화학습 후훈련 연구의 랜드마크라 할 수 있는 성과를 달성했습니다. 이 연구의 가장 놀라운 점은 **복잡한 MCTS(Monte Carlo Tree Search)나 가치 네트워크 없이도**, 상대적으로 단순한 RL 프레임워크만으로 OpenAI의 o1 모델과 동등한 성능을 달성했다는 것입니다.

멀티모달 LLM을 대규모 강화학습으로 훈련한 이 모델은 긴 컨텍스트 처리와 개선된 정책 최적화를 통해 수학과 코딩 벤치마크에서 최첨단 추론 결과를 보여줍니다. 특히 **7B-13B 파라미터 모델을 GPT-4 수준의 추론 태스크까지 끌어올린** 것은 오픈 모델의 잠재력을 극명하게 보여주는 사례입니다.

### Long-to-Short CoT 전이의 혁신

이 연구에서 주목할 만한 기법 중 하나는 **Long-to-Short Chain-of-Thought(CoT) 전이**입니다. 이는 긴 형태의 사고 과정을 학습한 후 이를 압축된 형태로 전이시키는 방법으로, 추론 정확도를 극적으로 향상시킵니다. 

\[
P(\text{short CoT} | \text{input}) = \sum_{\text{long CoT}} P(\text{short CoT} | \text{long CoT}, \text{input}) \cdot P(\text{long CoT} | \text{input})
\]

이러한 접근법은 모델이 복잡한 추론 과정을 내재화한 후, 실제 추론 시에는 효율적인 형태로 압축하여 사용할 수 있게 합니다.

### 실무적 시사점

Kimi k1.5의 성공은 **순수 RL 기반 후훈련이 확장 가능함**을 증명합니다. 이는 오픈 모델을 활용한 고성능 도메인 전문가 시스템 구축에 실질적인 가이드라인을 제공합니다. 특히 정책 최적화 트릭과 인프라 구축에 대한 실용적 인사이트는 기업 환경에서 즉시 활용 가능한 지식입니다.

## 2. Microsoft RPT: 사전훈련 패러다임의 근본적 혁신

### 강화학습 사전훈련의 새로운 지평

Microsoft의 Reinforcement Pre-Training(RPT) 연구는 LLM의 사전훈련 패러다임 자체를 재정의하는 혁신적 접근법을 제시합니다. 기존의 다음 토큰 예측 방식을 넘어서, **검증 가능한 보상을 가진 토큰 예측을 "추론" 태스크로 취급**하여 강화학습으로 훈련하는 것입니다.

### 범용 강화학습의 가능성

RPT의 핵심은 광대한 레이블이 없는 텍스트를 범용 RL에 활용한다는 점입니다. 이는 언어 모델링 정확도를 크게 향상시킬 뿐만 아니라, 후속 파인튜닝을 위한 더 강력한 베이스 모델을 만들어냅니다.

\[
\mathcal{L}_{RPT} = \mathbb{E}_{s,a \sim \pi}[R(s,a) - \beta \log \pi(a|s)]
\]

여기서 \(R(s,a)\)는 토큰 예측의 검증 가능한 보상이고, \(\beta\)는 엔트로피 정규화 계수입니다.

### 지속적 개선의 가능성

RPT의 성공은 **더 많은 컴퓨팅 자원을 투입할수록 일관된 성능 향상**을 보인다는 점에서 특히 중요합니다. 이는 지도 학습 데이터의 한계를 넘어선 지속적 개선이 가능함을 시사하며, 셀프플레이나 피드백 루프를 통해 훈련되는 대규모 베이스 모델의 미래를 암시합니다.

## 3. 추론 능력의 한계에 대한 비판적 성찰

### RL이 정말 새로운 추론을 유도하는가?

Yue 등의 연구는 현재 RL 기반 파인튜닝의 한계에 대한 중요한 질문을 제기합니다. **현재의 RL 방법들이 사전훈련된 베이스 모델이 이미 알고 있는 것 너머의 근본적으로 새로운 추론 스킬을 유도하지 못한다**는 발견은 충격적입니다.

### 기존 능력의 재가중화 vs 새로운 능력 창발

연구진은 RL로 파인튜닝된 모델들이 top-1 정확도에서는 뛰어나지만, 여러 번의 시도를 허용하면 원래 베이스 모델이 동등하거나 더 나은 성능을 보인다는 점을 발견했습니다. 이는 RL이 주로 **기존 능력의 재가중화(reweighting)**를 수행하고 있음을 의미합니다.

\[
P_{RL}(\text{correct} | \text{single attempt}) > P_{base}(\text{correct} | \text{single attempt})
\]
\[
P_{RL}(\text{correct} | \text{multiple attempts}) \approx P_{base}(\text{correct} | \text{multiple attempts})
\]

### 이론적 상한과의 격차

놀랍게도 모든 인기 있는 RL 알고리즘들이 유사한 성능을 보였으며, 베이스 모델의 이론적 상한에는 훨씬 못 미치는 결과를 보였습니다. 이는 **현재의 RLHF/RLVR이 베이스 모델의 추론을 확장하기보다는 증폭**하고 있음을 시사합니다.

## 4. 오프라인과 온라인 RL의 효과적 결합

### Meta의 체계적 비교 연구

Meta의 연구는 파인튜닝 전략(오프라인 vs 준온라인 vs 완전온라인)과 목적함수(DPO vs PPO 기반 GRPO)에 대한 심도 있는 경험적 비교를 제공합니다. 핵심 발견은 **순수 오프라인 DPO가 상당히 뒤처지지만, DPO를 온라인 또는 반복적 루프로 수행하면 온라인 GRPO(PPO)의 성능에 맞출 수 있다**는 점입니다.

### 온라인 학습의 우월성

준온라인 DPO와 완전온라인 DPO 모두 PPO 기반 방법들과 비교 가능한 결과를 달성했으며, 모든 온라인 방법들이 정적 오프라인 훈련을 크게 앞섰습니다.

\[
\text{Performance}: \text{Online Methods} >> \text{Offline Methods}
\]

### 검증 가능한 보상과 선호도 보상의 조합

특히 주목할 만한 점은 검증 가능한 보상(수학 등)과 검증 불가능한 보상(선호도)을 혼합했을 때 최상의 결과를 얻었다는 것입니다. 이는 실무진에게 **제한된 인간 데이터로 파인튜닝할 때 언제 오프라인 vs 온라인 RL을 사용할지에 대한 귀중한 인사이트**를 제공합니다.

## 5. 도메인 전이의 현실과 한계

### UIUC의 도메인 일반화 연구

UIUC의 "Breaking Barriers" 연구는 기업 환경에서 매우 실용적인 질문을 다룹니다: **한 도메인에서 RL 튜닝한 모델이 다른 도메인에서도 도움이 될까?** 

### 도메인별 전이의 비대칭성

연구 결과는 명확합니다. **RL 개선사항은 좁은 범위**에 국한되어 있습니다. 모델들은 RL 훈련 데이터와 유사한 태스크에서는 뛰어나지만, 다른 추론 패턴을 가진 태스크에서는 이득이 사라지는 경우가 많습니다.

- **수학/코딩 RL 튜닝**: 구조화된 태스크 간에는 잘 일반화
- **비구조화된 도메인 RL 튜닝**: 다른 비구조화된 도메인으로도 일반화되지 않음
- **흥미롭게도**: 비구조화된 도메인 RL이 때로는 구조화된 태스크로 전이되지만 그 반대는 성립하지 않음

### 도메인별 특화의 필요성

이는 **금융, 법률, 의료 에이전트를 구축하려면 해당 도메인을 대표하는 데이터와 보상을 사용한 도메인별 RL 파인튜닝**이 필수적임을 강조합니다.

## 6. Agent Lightning: 범용 에이전트 RL 훈련 프레임워크

### 기존 에이전트와의 완벽한 호환성

Microsoft의 Agent Lightning은 **기존 에이전트 코드를 변경하지 않고도 모든 에이전트에 RL을 적용**할 수 있는 혁신적인 해결책을 제시합니다. LangChain, OpenAI Functions, 커스텀 코드 등으로 구축된 어떤 에이전트든 연결하여 RL로 파인튜닝할 수 있습니다.

### 계층적 RL과 신용 할당

에이전트의 다단계 실행을 MDP(Markov Decision Process)로 공식화하고, 궤적에 대한 통합 인터페이스를 제공함으로써 **계층적 RL과 단계별 신용 할당**을 가능하게 합니다.

\[
G_t = \sum_{k=0}^{T-t} \gamma^k R_{t+k+1}
\]

여기서 \(G_t\)는 시점 \(t\)에서의 할인된 누적 보상입니다.

### 실무적 가치

Agent Lightning은 **실제 워크플로우에서 에이전트를 지속적으로 개선하는 실용적 솔루션**을 제공합니다. text-to-SQL, 검색 증강 QA, 도구 활용 수학 문제 등에서 안정적인 성능 향상을 보여주며, 베이스 LLM을 신뢰할 수 있는 도메인 특화 에이전틱 시스템으로 변환하는 "RL 파인튜닝 레이어"를 제공합니다.

## 7. ASearcher: 장기 도구 사용의 새로운 지평

### 웹 검색 특화 에이전트의 혁신

Tsinghua와 Ant의 ASearcher는 **웹 검색과 브라우징에 특화된 오픈소스 RL 훈련 에이전트**로, 전문가 수준의 "검색 지능"을 목표로 합니다. 이 연구의 핵심 기여는 두 가지입니다:

1. **매우 긴 도구 사용 시퀀스까지 확장 가능한 완전 비동기 RL 훈련 시스템**
2. **에이전트가 스스로 어려운 QA 챌린지를 생성하여 자가 훈련하는 프롬프트 기반 셀프플레이**

### 극단적 규모의 도구 사용

온라인 RL을 통해 32B 모델이 웹 검색 벤치마크 xBench에서 **+46.7%라는 엄청난 성능 향상**을 달성했습니다. 더 놀라운 것은 **40개 이상의 순차적 도구 호출과 150k 이상의 출력 토큰**을 처리하는 검색을 학습했다는 점입니다.

\[
\text{Performance Gain} = \frac{\text{Score}_{RL} - \text{Score}_{base}}{\text{Score}_{base}} = +46.7\%
\]

### 장기 추론의 실현

ASearcher는 **정적 모델들이 어려워하는 장기 다단계 추론 능력을 RL로 해제**할 수 있음을 보여줍니다. 금융, 법률, 연구 분야에서 정보를 부지런히 검색하고 분석하며 교차 검증하는 에이전트는 매우 가치 있는 자산입니다.

## 8. ARTIST: 추론과 도구 통합의 긴밀한 결합

### 적응적 도구 사용 전략의 학습

MSR의 ARTIST는 **다단계 추론과 도구 사용에 대한 의사결정을 긴밀하게 결합**하여 모두 RL로 최적화하는 프레임워크입니다. LLM이 고정된 프롬프트나 규칙에 의존하지 않고, 사고의 연쇄 내에서 외부 도구(계산기, 코드 인터프리터, 검색 API 등)를 언제 어떻게 호출할지 학습합니다.

### 결과 기반 보상의 힘

단계별 지도 없이 **결과 기반 보상**만을 사용하여, ARTIST는 견고하고 적응적인 도구 사용 전략을 개발합니다. 어려운 수학 추론과 함수 호출 태스크에서 최첨단 베이스라인을 **최대 22% 절대 정확도로 압도**하는 성과를 보였습니다.

\[
\text{Accuracy Improvement} = \text{Acc}_{ARTIST} - \text{Acc}_{baseline} \leq 22\%
\]

### 자율적 문제 해결 에이전트로의 진화

ARTIST의 접근법은 **기업 LLM이 자율적 문제 해결 에이전트가 되는 방법**을 보여줍니다. 필요할 때 도구를 호출하는 법을 학습함으로써 모델은 훨씬 더 신뢰할 수 있고 해석 가능해집니다.

## 9. AlphaMed: 의료 영역 특화의 혁신적 성공

### 순수 RL 기반 의료 추론의 돌파구

Imperial/TUM/HKUST의 AlphaMed는 **도메인 특화 분야의 혁신**을 보여줍니다. 이는 단순한 규칙 기반 보상을 가진 RL만으로 추론 능력을 획득한 최초의 의료 LLM입니다.

### 지도학습 없는 접근법의 성공

값비싼 GPT-4 데이터에 의존하는 지도학습 사고의 연쇄 파인튜닝을 전혀 사용하지 않고, **공개 의료 QA 데이터셋에 기본 정확성 보상으로 RL을 적용**했습니다. 놀랍게도 AlphaMed는 6개 의료 질의응답 벤치마크에서 최첨단 정확도를 달성했습니다.

### 대형 모델을 뛰어넘는 성과

가장 어려운 데이터셋(MedXpert)에서는 **훨씬 큰 폐쇄형 모델들(DeepSeek-V3 671B, Claude 3.5)도 능가**하는 성과를 보였습니다. 이는 도메인 특화에서 규모보다는 최적화 방법이 더 중요할 수 있음을 시사합니다.

\[
\text{Performance}: \text{AlphaMed} > \text{DeepSeek-V3 (671B)} > \text{Claude 3.5}
\]

### 수직적 LLM 구축의 새로운 패러다임

AlphaMed의 성공은 **신중하게 설계된 보상(여기서는 정보성 있고 단계별인 답변을 규칙 기반 체크로 장려)과 함께 RL만으로도 전문 도메인에서 견고한 추론을 유도**할 수 있음을 증명합니다.

## 10. General-Reasoner: 전 영역 추론의 통합적 접근

### 수학/코딩 편향을 넘어선 도전

Waterloo 대학의 General-Reasoner는 **기존 RL 노력들이 수학/코딩에만 국한되어 있던 편향을 극복**하는 새로운 훈련 패러다임을 제안합니다. 다양한 도메인에 걸쳐 RL 기반 추론을 확장하는 것이 목표입니다.

### 다학제 검증 가능한 데이터셋

핵심 기여는 **물리, 화학, 금융 등을 아우르는 검증 가능한 답변을 가진 대규모 다학제 질문 데이터셋 구축**과 **취약한 규칙 체커를 대체하는 생성적 사고의 연쇄 검증기** 도입입니다.

### Zero-RL 설정의 성공

"Zero-RL"(지도학습 사전 파인튜닝 없음) 설정에서 이 데이터와 검증기로 훈련함으로써, General-Reasoner는 **수학 너머 12개 벤치마크에서 견고하고 일반화 가능한 성능**을 달성했습니다. 광범위한 지식 벤치마크에서 약 10% 향상을 보이면서도 수학 태스크에서는 수학 특화 프레임워크와 동등한 성능을 유지했습니다.

### 도메인 간 시너지의 활용

가장 놀라운 점은 **14B 모델이 여러 학술 벤치마크에서 GPT-4와 경쟁**하는 수준에 도달했다는 것입니다. General-Reasoner는 본질적으로 **사일로를 깨뜨리는 연구**로, RL을 사용하여 도메인 간 시너지를 활용하면서도 수학/코드 능력을 잃지 않고 모든 도메인에서 잘 추론하는 LLM 에이전트를 훈련하는 경로를 보여줍니다.

## 결론: 강화학습이 열어가는 AI의 미래

### 패러다임의 전환점

2025년 강화학습 후훈련 연구들은 단순히 기술적 개선을 넘어서 **AI 개발 패러다임의 근본적 전환**을 보여주고 있습니다. Kimi k1.5가 증명한 순수 RL의 확장성부터 Microsoft RPT가 제시한 사전훈련의 재정의까지, 이 연구들은 오픈 모델이 폐쇄형 모델과 동등하거나 그 이상의 성능을 달성할 수 있는 구체적인 경로를 제시합니다.

### 실무적 함의와 한계 인식

동시에 Yue 등의 비판적 연구가 보여주듯, **현재 RL 방법들의 한계를 정확히 인식하는 것도 중요**합니다. RL이 주로 기존 능력의 재가중화를 수행한다는 발견은 우리가 보다 혁신적인 RL 기법을 개발해야 할 필요성을 시사합니다.

### 도메인 특화의 중요성

UIUC의 도메인 전이 연구와 AlphaMed의 의료 특화 성공은 **범용성과 전문성 사이의 균형**에 대한 중요한 통찰을 제공합니다. 기업이 특정 도메인에서 최고 성능을 원한다면, 해당 도메인에 특화된 RL 훈련이 필수적입니다.

### 장기 추론과 도구 사용의 혁신

ASearcher와 ARTIST가 보여준 장기 추론과 적응적 도구 사용 능력은 **AI 에이전트가 단순한 질의응답을 넘어 복잡한 문제 해결과 의사결정을 수행**할 수 있는 가능성을 열어줍니다.

### 향후 전망: 통합적 접근의 필요성

General-Reasoner가 제시한 다영역 통합 접근법은 **미래 AI 시스템이 나아가야 할 방향**을 보여줍니다. 특정 도메인에서의 깊이와 다영역에 걸친 넓이를 동시에 확보하는 것이 차세대 AI 에이전트의 핵심 과제가 될 것입니다.

### 마무리: 새로운 가능성의 지평

이 10편의 연구는 강화학습이 단순히 모델을 개선하는 도구를 넘어서 **AI의 추론 능력을 근본적으로 변화시키는 패러다임**임을 증명합니다. 오픈 모델의 민주화된 접근성과 결합된 이러한 혁신들은 AI 개발의 새로운 황금기를 열어가고 있습니다.

앞으로의 연구는 현재 방법들의 한계를 극복하면서도 실무적 적용 가능성을 높이는 방향으로 진화할 것입니다. 강화학습 후훈련은 더 이상 연구실의 실험이 아닌, **기업과 개발자들이 실제로 활용할 수 있는 강력한 도구**로 자리잡아 가고 있습니다.
