---
title: "ê¸°ì—…ìš© RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ì‚°ì—…ë³„ ê³µê°œ ë°ì´í„°ì…‹ ì™„ë²½ ê°€ì´ë“œ: ì€í–‰ë¶€í„° ì¦ê¶Œê¹Œì§€"
excerpt: "ì€í–‰, ë³´í—˜, íšŒê³„, ë²•ë¥ , ì˜ë£Œ, ìë™ì°¨, ì¦ê¶Œ ë¶„ì•¼ì—ì„œ RAG ê¸°ë°˜ LLM ì±—ë´‡ êµ¬ì¶•ì— í™œìš© ê°€ëŠ¥í•œ ê³µê°œ ë°ì´í„°ì…‹ê³¼ ì‹¤ì œ êµ¬í˜„ ë°©ë²•ì„ ì¢…í•© ì •ë¦¬"
seo_title: "RAG ì±—ë´‡ ì‚°ì—…ë³„ ë°ì´í„°ì…‹ ê°€ì´ë“œ - ì€í–‰ ë³´í—˜ ì˜ë£Œ ë²•ë¥  - Thaki Cloud"
seo_description: "ê¸°ì—…ìš© RAG ì‹œìŠ¤í…œ êµ¬ì¶•ì„ ìœ„í•œ 7ê°œ ì‚°ì—… ë¶„ì•¼ë³„ ê³µê°œ ë°ì´í„°ì…‹ ëª©ë¡ê³¼ ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ. FDIC, SEC, MIMIC-IV, CourtListener ë“± ê²€ì¦ëœ ë°ì´í„°ì…‹ í™œìš©ë²•"
date: 2025-07-19
last_modified_at: 2025-07-19
categories:
  - datasets
tags:
  - RAG
  - LLM
  - ì±—ë´‡
  - ê¸°ì—…ìš©
  - ê¸ˆìœµ
  - ì˜ë£Œ
  - ë²•ë¥ 
  - ë°ì´í„°ì…‹
  - FDIC
  - SEC
  - MIMIC-IV
  - OpenAI
  - LangChain
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/datasets/enterprise-rag-datasets-industry-specific-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 20ë¶„

## ì„œë¡ 

ê¸°ì—…ìš© RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œ êµ¬ì¶•ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ **í’ˆì§ˆ ë†’ì€ ë„ë©”ì¸ íŠ¹í™” ë°ì´í„°**ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” 7ê°œ ì£¼ìš” ì‚°ì—… ë¶„ì•¼ì—ì„œ ì‹¤ì œ í™œìš© ê°€ëŠ¥í•œ **ê²€ì¦ëœ ê³µê°œ ë°ì´í„°ì…‹**ê³¼ **ì‹¤ì œ êµ¬í˜„ ë°©ë²•**ì„ ì œì‹œí•©ë‹ˆë‹¤.

### ì´ ê°€ì´ë“œì˜ íŠ¹ì§•

- âœ… **ë¬´ë£Œ ì ‘ê·¼**: ëª¨ë“  ë°ì´í„°ì…‹ì´ ë¬´ë£Œë¡œ ì´ìš© ê°€ëŠ¥
- âœ… **êµ¬ì¡°í™”ëœ ìŠ¤í‚¤ë§ˆ**: RAG ì‹œìŠ¤í…œ ì„¤ê³„ê°€ ìš©ì´í•œ ëª…í™•í•œ êµ¬ì¡°
- âœ… **PoC ì í•©**: ê¸°ì—… ê³ ê° ì‹œì—°ì— ë°”ë¡œ í™œìš© ê°€ëŠ¥
- âœ… **ì‹¤ì œ ì½”ë“œ**: ê° ë°ì´í„°ì…‹ë³„ êµ¬í˜„ ì˜ˆì‹œ ì œê³µ
- âœ… **í”„ë¡œë•ì…˜ ê²€ì¦**: ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ê²€ì¦ëœ ë°ì´í„°ì…‹ë§Œ ì„ ë³„

## ì‚°ì—…ë³„ ë°ì´í„°ì…‹ ìš”ì•½

| ë¶„ì•¼ | ì¶”ì²œ ë°ì´í„°ì…‹ | ë°ì´í„° ê·œëª¨ | ì£¼ìš” í™œìš© ì‚¬ë¡€ |
|------|-------------|------------|-------------|
| **ì€í–‰** | FDIC Call Report | 5,000+ ê¸°ê´€, ë¶„ê¸°ë³„ | ì¬ë¬´ì œí‘œ ì§ˆì˜ì‘ë‹µ, ìœ„í—˜ë„ ë¶„ì„ |
| **ë³´í—˜** | NAIC InsData | 3,000+ ë³´í—˜ì‚¬ | ë³´í—˜ê¸ˆ ì§€ê¸‰ ë¶„ì„, ë¯¼ì› ì²˜ë¦¬ |
| **íšŒê³„** | SEC XBRL | 8,000+ ìƒì¥ì‚¬ | ì¬ë¬´ í•­ëª© ê²€ìƒ‰, ê³µì‹œ ë¶„ì„ |
| **ë²•ë¥ ** | CourtListener | 400ë§Œ+ íŒë¡€ | íŒë¡€ ê²€ìƒ‰, ë²•ë¥  ìë¬¸ |
| **ì˜ë£Œ** | MIMIC-IV | 30ë§Œ+ í™˜ì | ì„ìƒ ì§ˆì˜ì‘ë‹µ, ì§„ë‹¨ ì§€ì› |
| **ìë™ì°¨** | NHTSA API | ìˆ˜ì‹­ë§Œ ê±´ ë¦¬ì½œ | ì•ˆì „ë„ ì¡°íšŒ, ë¦¬ì½œ ì•Œë¦¼ |
| **ì¦ê¶Œ** | NASDAQ Data | 15ë…„+ ì£¼ê°€ | íˆ¬ì ë¶„ì„, íŠ¸ë Œë“œ ì˜ˆì¸¡ |

## 1. ì€í–‰ ë° ì‹ ìš© ë¶„ì„

### 1-1. FDIC Call Report

**ê°œìš”**: ë¯¸êµ­ FDIC ê°ë… í•˜ì˜ ëª¨ë“  ì€í–‰ì´ ë¶„ê¸°ë³„ë¡œ ì œì¶œí•˜ëŠ” ì¬ë¬´ì œí‘œ ë°ì´í„°

**ë°ì´í„° êµ¬ì¡°**:
```json
{
  "bank_name": "First National Bank",
  "cert_id": "123456",
  "quarter": "2024Q3",
  "total_assets": 1234567890,
  "tier1_capital_ratio": 12.5,
  "net_income": 45678901,
  "loan_loss_provisions": 2345678
}
```

**RAG í™œìš© ì˜ˆì‹œ**:
```python
# FDIC ë°ì´í„° RAG êµ¬ì„± ì˜ˆì‹œ
from langchain.document_loaders import CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

class FDICRAGSystem:
    def __init__(self):
        self.loader = CSVLoader("fdic_call_reports.csv")
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n", ","]  # CSV ë°ì´í„° ìµœì í™”
        )
        
    def create_bank_profile_chunks(self, bank_data):
        """ì€í–‰ë³„ í”„ë¡œí•„ ì²­í¬ ìƒì„±"""
        profile_text = f"""
        ì€í–‰ëª…: {bank_data['bank_name']}
        ì´ìì‚°: ${bank_data['total_assets']:,}
        Tier1 ìë³¸ë¹„ìœ¨: {bank_data['tier1_capital_ratio']}%
        ìˆœì´ìµ: ${bank_data['net_income']:,}
        ëŒ€ì†ì¶©ë‹¹ê¸ˆ: ${bank_data['loan_loss_provisions']:,}
        ë¶„ê¸°: {bank_data['quarter']}
        """
        return self.text_splitter.split_text(profile_text)

# ì§ˆì˜ ì˜ˆì‹œ
def query_bank_performance(query: str):
    """
    ì˜ˆì‹œ ì§ˆì˜:
    - "2024ë…„ 3ë¶„ê¸° Tier1 ìë³¸ë¹„ìœ¨ì´ ê°€ì¥ ë†’ì€ ì€í–‰ì€?"
    - "First National Bankì˜ ìµœê·¼ 3ê°œ ë¶„ê¸° ìˆœì´ìµ ì¶”ì´ëŠ”?"
    """
    pass
```

**í™œìš© ì‹œë‚˜ë¦¬ì˜¤**:
- ğŸ“Š **ë¦¬ìŠ¤í¬ ë¶„ì„**: "ìë³¸ë¹„ìœ¨ì´ 10% ë¯¸ë§Œì¸ ì€í–‰ë“¤ì˜ ê³µí†µì ì€?"
- ğŸ“ˆ **ì„±ê³¼ ë¹„êµ**: "ìœ ì‚¬ ê·œëª¨ ì€í–‰ë“¤ê³¼ ë¹„êµí•œ ìš°ë¦¬ ì€í–‰ì˜ ìœ„ì¹˜ëŠ”?"
- ğŸ” **ê·œì œ ì¤€ìˆ˜**: "Basel III ê¸°ì¤€ì„ ì¶©ì¡±í•˜ì§€ ëª»í•˜ëŠ” í•­ëª©ë“¤ì€?"

**ë°ì´í„° ì ‘ê·¼**: [FDIC Call Reports](https://www.fdic.gov/analysis/call-reports/)

### 1-2. FRED API (Federal Reserve Economic Data)

**ê°œìš”**: ë¯¸êµ­ ì—°ë°©ì¤€ë¹„ì œë„ì˜ ê²½ì œ ì§€í‘œ ë°ì´í„°ë² ì´ìŠ¤ (80ë§Œ+ ì‹œê³„ì—´)

**ì£¼ìš” ì§€í‘œ**:
- ê¸ˆë¦¬ (Federal Funds Rate, êµ­ì±„ ìˆ˜ìµë¥ )
- í†µí™”ëŸ‰ (M1, M2, M3)
- ê³ ìš© ì§€í‘œ (ì‹¤ì—…ë¥ , ë¹„ë†ì—… ì·¨ì—…ì ìˆ˜)
- ì¸í”Œë ˆì´ì…˜ (CPI, PCE)

**RAG êµ¬í˜„ ì˜ˆì‹œ**:
```python
import pandas as pd
from fredapi import Fred
from datetime import datetime, timedelta

class FREDEconomicRAG:
    def __init__(self, api_key):
        self.fred = Fred(api_key=api_key)
        self.indicators = {
            'FEDFUNDS': 'Federal Funds Rate',
            'UNRATE': 'Unemployment Rate',
            'CPIAUCSL': 'Consumer Price Index',
            'GDP': 'Gross Domestic Product'
        }
    
    def create_economic_context(self, indicator_code, months_back=12):
        """ê²½ì œ ì§€í‘œ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=months_back*30)
        
        data = self.fred.get_series(
            indicator_code, 
            start_date, 
            end_date
        )
        
        context = f"""
        ì§€í‘œëª…: {self.indicators.get(indicator_code, indicator_code)}
        ìµœì‹ ê°’: {data.iloc[-1]:.2f}
        1ë…„ ì „ ëŒ€ë¹„: {((data.iloc[-1] / data.iloc[0]) - 1) * 100:.2f}%
        ìµœê³ ê°’: {data.max():.2f} (ë‚ ì§œ: {data.idxmax().strftime('%Y-%m-%d')})
        ìµœì €ê°’: {data.min():.2f} (ë‚ ì§œ: {data.idxmin().strftime('%Y-%m-%d')})
        """
        return context

# í™œìš© ì˜ˆì‹œ
economic_rag = FREDEconomicRAG(api_key="your_fred_api_key")
context = economic_rag.create_economic_context('FEDFUNDS')
```

**í™œìš© ì‹œë‚˜ë¦¬ì˜¤**:
- ğŸ“Š **ì‹ ìš© ì •ì±…**: "í˜„ì¬ ê¸ˆë¦¬ í™˜ê²½ì—ì„œ ëŒ€ì¶œ í¬íŠ¸í´ë¦¬ì˜¤ ì¡°ì • ë°©í–¥ì€?"
- ğŸ“‰ **ê²½ê¸° ì˜ˆì¸¡**: "ì‹¤ì—…ë¥ ê³¼ GDP ì„±ì¥ë¥  ê¸°ë°˜ ê²½ê¸° ì‚¬ì´í´ ë¶„ì„"
- ğŸ’¹ **ìì‚° ë°°ë¶„**: "ì¸í”Œë ˆì´ì…˜ ìƒìŠ¹ê¸° ìµœì  ìì‚° ë°°ë¶„ ì „ëµì€?"

## 2. ë³´í—˜

### 2-1. NAIC InsData

**ê°œìš”**: ë¯¸êµ­ ë³´í—˜ê·œì œí˜‘íšŒ(NAIC)ê°€ ì œê³µí•˜ëŠ” ë³´í—˜ì‚¬ë³„ ì‹œì¥ ë°ì´í„°

**ë°ì´í„° í¬í•¨ í•­ëª©**:
- ì‹œì¥ ì ìœ ìœ¨ ë° í”„ë¦¬ë¯¸ì—„ ìˆ˜ì…
- ì†í•´ìœ¨ (Loss Ratio) ë° ì‚¬ì—…ë¹„ìœ¨
- ë¯¼ì› ê±´ìˆ˜ ë° ìœ í˜•ë³„ ë¶„ì„
- ì¬ë¬´ ê±´ì „ì„± ì§€í‘œ

**RAG êµ¬í˜„ ì˜ˆì‹œ**:
```python
class InsuranceRAGSystem:
    def __init__(self):
        self.complaint_categories = {
            'claim_handling': 'ë³´í—˜ê¸ˆ ì§€ê¸‰ ê´€ë ¨',
            'policy_service': 'ë³´í—˜ê³„ì•½ ì„œë¹„ìŠ¤',
            'premium_billing': 'ë³´í—˜ë£Œ ì²­êµ¬',
            'underwriting': 'ì–¸ë”ë¼ì´íŒ…',
            'sales_marketing': 'íŒë§¤ ë° ë§ˆì¼€íŒ…'
        }
    
    def create_insurer_profile(self, insurer_data):
        """ë³´í—˜ì‚¬ í”„ë¡œí•„ ìƒì„±"""
        profile = f"""
        ë³´í—˜ì‚¬ëª…: {insurer_data['company_name']}
        ì‹œì¥ì ìœ ìœ¨: {insurer_data['market_share']}%
        ì†í•´ìœ¨: {insurer_data['loss_ratio']}%
        ì‚¬ì—…ë¹„ìœ¨: {insurer_data['expense_ratio']}%
        ì´ ë¯¼ì› ê±´ìˆ˜: {insurer_data['total_complaints']}ê±´
        ì£¼ìš” ë¯¼ì› ìœ í˜•: {insurer_data['top_complaint_type']}
        ì¬ë¬´ë“±ê¸‰: {insurer_data['financial_rating']}
        """
        return profile
    
    def analyze_complaint_trends(self, company_name, time_period):
        """ë¯¼ì› íŠ¸ë Œë“œ ë¶„ì„"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬
        return {
            'trend': 'decreasing',
            'primary_issues': ['claim_delays', 'coverage_disputes'],
            'resolution_rate': 0.87
        }

# ì§ˆì˜ ì˜ˆì‹œ
def insurance_chatbot_query(query: str):
    """
    ì˜ˆì‹œ ì§ˆì˜:
    - "State Farmê³¼ Allstateì˜ ì†í•´ìœ¨ ë¹„êµëŠ”?"
    - "ìë™ì°¨ ë³´í—˜ ë¯¼ì›ì´ ê°€ì¥ ì ì€ ë³´í—˜ì‚¬ëŠ”?"
    - "ìš°ë¦¬ ë³´í—˜ì‚¬ì˜ ë¯¼ì› í•´ê²°ë¥  ê°œì„  ë°©ì•ˆì€?"
    """
    pass
```

**ì‹¤ì œ í™œìš© ì‚¬ë¡€**:
```python
# ë³´í—˜ì‚¬ ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ
class InsuranceBenchmarking:
    def compare_competitors(self, my_company, competitors):
        comparison_data = []
        for competitor in competitors:
            data = {
                'company': competitor,
                'loss_ratio': self.get_loss_ratio(competitor),
                'complaint_rate': self.get_complaint_rate(competitor),
                'market_share': self.get_market_share(competitor)
            }
            comparison_data.append(data)
        
        return self.create_benchmark_report(my_company, comparison_data)
```

### 2-2. OpenFEMA NFIP Claims

**ê°œìš”**: ë¯¸êµ­ ì—°ë°©ì¬í•´ê´€ë¦¬ì²­(FEMA)ì˜ í™ìˆ˜ ë³´í—˜ í´ë ˆì„ ë°ì´í„°

**ë°ì´í„° íŠ¹ì§•**:
- 1978ë…„ë¶€í„° í˜„ì¬ê¹Œì§€ì˜ ëª¨ë“  í™ìˆ˜ ë³´í—˜ í´ë ˆì„
- ì§€ì—­ë³„, ì¬í•´ ìœ í˜•ë³„ ìƒì„¸ ë¶„ë¥˜
- ë³´í—˜ê¸ˆ ì§€ê¸‰ì•¡ ë° ì†í•´ í‰ê°€ ì •ë³´

**í™œìš© ì˜ˆì‹œ**:
```python
import geopandas as gpd
from shapely.geometry import Point

class FloodInsuranceRAG:
    def __init__(self):
        self.claims_data = self.load_nfip_data()
        
    def create_risk_assessment_context(self, zip_code):
        """ìš°í¸ë²ˆí˜¸ë³„ í™ìˆ˜ ìœ„í—˜ë„ í‰ê°€"""
        historical_claims = self.claims_data[
            self.claims_data['zip_code'] == zip_code
        ]
        
        risk_context = f"""
        ì§€ì—­: {zip_code}
        ê³¼ê±° 10ë…„ í´ë ˆì„ ê±´ìˆ˜: {len(historical_claims)}ê±´
        í‰ê·  ì§€ê¸‰ì•¡: ${historical_claims['amount_paid'].mean():,.2f}
        ìµœëŒ€ ì§€ê¸‰ì•¡: ${historical_claims['amount_paid'].max():,.2f}
        ì£¼ìš” ì†í•´ ìœ í˜•: {historical_claims['damage_type'].mode()[0]}
        ìœ„í—˜ë„ ë“±ê¸‰: {self.calculate_risk_grade(historical_claims)}
        """
        return risk_context
```

## 3. íšŒê³„ / ê³µì‹œìë£Œ

### 3-1. SEC XBRL ë°ì´í„°

**ê°œìš”**: ë¯¸êµ­ ì¦ê¶Œê±°ë˜ìœ„ì›íšŒ(SEC)ì˜ ìƒì¥ê¸°ì—… ì¬ë¬´ì œí‘œ í‘œì¤€í™” ë°ì´í„°

**XBRL êµ¬ì¡° ì˜ˆì‹œ**:
```xml
<xbrl>
  <us-gaap:Assets contextRef="FY2024" unitRef="USD">1234567890</us-gaap:Assets>
  <us-gaap:Liabilities contextRef="FY2024" unitRef="USD">987654321</us-gaap:Liabilities>
  <us-gaap:Revenues contextRef="FY2024Q4" unitRef="USD">456789012</us-gaap:Revenues>
</xbrl>
```

**RAG êµ¬í˜„ ì˜ˆì‹œ**:
```python
import xml.etree.ElementTree as ET
from langchain.schema import Document

class SECXBRLProcessor:
    def __init__(self):
        self.namespace = {
            'us-gaap': 'http://fasb.org/us-gaap/2024',
            'dei': 'http://xbrl.sec.gov/dei/2024'
        }
    
    def parse_financial_statement(self, xbrl_file):
        """XBRL íŒŒì¼ì—ì„œ ì¬ë¬´ì œí‘œ íŒŒì‹±"""
        tree = ET.parse(xbrl_file)
        root = tree.getroot()
        
        financial_data = {}
        
        # ì£¼ìš” ì¬ë¬´ í•­ëª© ì¶”ì¶œ
        key_items = [
            'Assets', 'Liabilities', 'StockholdersEquity',
            'Revenues', 'NetIncomeLoss', 'EarningsPerShare'
        ]
        
        for item in key_items:
            element = root.find(f'.//us-gaap:{item}', self.namespace)
            if element is not None:
                financial_data[item] = {
                    'value': float(element.text),
                    'context': element.get('contextRef'),
                    'unit': element.get('unitRef')
                }
        
        return financial_data
    
    def create_financial_summary(self, company_data):
        """ì¬ë¬´ ìš”ì•½ ë¬¸ì„œ ìƒì„±"""
        summary = f"""
        íšŒì‚¬ëª…: {company_data['company_name']}
        ì´ìì‚°: ${company_data['Assets']['value']:,.0f}
        ì´ë¶€ì±„: ${company_data['Liabilities']['value']:,.0f}
        ìë³¸ì´ê³„: ${company_data['StockholdersEquity']['value']:,.0f}
        ë§¤ì¶œì•¡: ${company_data['Revenues']['value']:,.0f}
        ìˆœì´ìµ: ${company_data['NetIncomeLoss']['value']:,.0f}
        ì£¼ë‹¹ìˆœì´ìµ: ${company_data['EarningsPerShare']['value']:.2f}
        """
        return Document(page_content=summary)

# í™œìš© ì˜ˆì‹œ
processor = SECXBRLProcessor()
tesla_data = processor.parse_financial_statement('tesla_10k_2024.xml')
summary_doc = processor.create_financial_summary(tesla_data)
```

**ê³ ê¸‰ ì§ˆì˜ ì²˜ë¦¬**:
```python
class FinancialAnalysisRAG:
    def calculate_financial_ratios(self, financial_data):
        """ì¬ë¬´ë¹„ìœ¨ ê³„ì‚°"""
        ratios = {}
        
        # ìœ ë™ë¹„ìœ¨
        if 'CurrentAssets' in financial_data and 'CurrentLiabilities' in financial_data:
            ratios['current_ratio'] = (
                financial_data['CurrentAssets']['value'] / 
                financial_data['CurrentLiabilities']['value']
            )
        
        # ë¶€ì±„ë¹„ìœ¨
        if 'Liabilities' in financial_data and 'StockholdersEquity' in financial_data:
            ratios['debt_ratio'] = (
                financial_data['Liabilities']['value'] / 
                financial_data['StockholdersEquity']['value']
            )
        
        # ROE (ìê¸°ìë³¸ì´ìµë¥ )
        if 'NetIncomeLoss' in financial_data and 'StockholdersEquity' in financial_data:
            ratios['roe'] = (
                financial_data['NetIncomeLoss']['value'] / 
                financial_data['StockholdersEquity']['value'] * 100
            )
        
        return ratios
```

### 3-2. EDGAR 10-K ìœ„í—˜ìš”ì†Œ ë¶„ì„

**RAG í™œìš© ì˜ˆì‹œ**:
```python
import requests
from bs4 import BeautifulSoup

class EDGARRiskFactorRAG:
    def extract_risk_factors(self, filing_url):
        """10-K ì„œë¥˜ì—ì„œ ìœ„í—˜ìš”ì†Œ ì¶”ì¶œ"""
        response = requests.get(filing_url)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Item 1A. Risk Factors ì„¹ì…˜ ì°¾ê¸°
        risk_section = soup.find(text=lambda text: text and 'risk factors' in text.lower())
        
        if risk_section:
            risk_content = self.extract_section_content(risk_section)
            return self.chunk_risk_factors(risk_content)
        
        return []
    
    def chunk_risk_factors(self, risk_content):
        """ìœ„í—˜ìš”ì†Œë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹"""
        risk_chunks = []
        
        # ìœ„í—˜ìš”ì†Œë³„ë¡œ ë¶„í•  (ë³´í†µ ë²ˆí˜¸ë‚˜ í•­ëª©ìœ¼ë¡œ êµ¬ë¶„ë¨)
        risk_items = risk_content.split('\n\n')
        
        for item in risk_items:
            if len(item.strip()) > 100:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ ìœ„í—˜ìš”ì†Œë§Œ
                risk_chunks.append({
                    'content': item.strip(),
                    'category': self.categorize_risk(item),
                    'severity': self.assess_risk_severity(item)
                })
        
        return risk_chunks
    
    def categorize_risk(self, risk_text):
        """ìœ„í—˜ìš”ì†Œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        categories = {
            'market': ['market', 'competition', 'demand'],
            'operational': ['operation', 'supply chain', 'manufacturing'],
            'financial': ['credit', 'liquidity', 'debt'],
            'regulatory': ['regulation', 'compliance', 'legal'],
            'technology': ['technology', 'cyber', 'data']
        }
        
        for category, keywords in categories.items():
            if any(keyword in risk_text.lower() for keyword in keywords):
                return category
        
        return 'other'
```

## 4. ë²•ë¥ 

### 4-1. CourtListener

**ê°œìš”**: 400ë§Œ+ ë¯¸êµ­ íŒë¡€ë¥¼ í¬í•¨í•œ ë²•ë¥  ë°ì´í„°ë² ì´ìŠ¤

**API í™œìš© ì˜ˆì‹œ**:
```python
import requests
from datetime import datetime

class CourtListenerRAG:
    def __init__(self, api_token):
        self.api_token = api_token
        self.base_url = "https://www.courtlistener.com/api/rest/v3/"
        self.headers = {'Authorization': f'Token {api_token}'}
    
    def search_cases(self, query, court=None, date_range=None):
        """íŒë¡€ ê²€ìƒ‰"""
        params = {
            'q': query,
            'order_by': 'score desc',
            'format': 'json'
        }
        
        if court:
            params['court'] = court
        if date_range:
            params['filed_after'] = date_range['start']
            params['filed_before'] = date_range['end']
        
        response = requests.get(
            f"{self.base_url}search/",
            params=params,
            headers=self.headers
        )
        
        return response.json()
    
    def create_case_summary(self, case_data):
        """íŒë¡€ ìš”ì•½ ìƒì„±"""
        summary = f"""
        ì‚¬ê±´ëª…: {case_data['caseName']}
        ë²•ì›: {case_data['court']}
        íŒê²°ì¼: {case_data['dateFiled']}
        ì¬íŒë¶€: {case_data['panel']}
        ì£¼ìš” ìŸì : {case_data.get('summary', 'N/A')}
        íŒê²° ê²°ê³¼: {case_data.get('disposition', 'N/A')}
        ì¸ìš© íšŸìˆ˜: {case_data.get('citeCount', 0)}
        """
        return summary
    
    def find_similar_cases(self, case_facts, jurisdiction=None):
        """ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰"""
        # ì‚¬ì‹¤ê´€ê³„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.extract_legal_keywords(case_facts)
        
        search_query = ' AND '.join(keywords)
        similar_cases = self.search_cases(
            query=search_query,
            court=jurisdiction
        )
        
        return similar_cases['results'][:10]  # ìƒìœ„ 10ê°œ ìœ ì‚¬ íŒë¡€
```

**ë²•ë¥  ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ**:
```python
class LegalQASystem:
    def __init__(self):
        self.court_listener = CourtListenerRAG(api_token="your_token")
        self.legal_categories = {
            'contract': 'ê³„ì•½ë²•',
            'tort': 'ë¶ˆë²•í–‰ìœ„ë²•',
            'property': 'ì¬ì‚°ë²•',
            'criminal': 'í˜•ë²•',
            'constitutional': 'í—Œë²•'
        }
    
    def answer_legal_question(self, question, context=None):
        """ë²•ë¥  ì§ˆë¬¸ ë‹µë³€"""
        # ì§ˆë¬¸ì—ì„œ ë²•ë¥  ì˜ì—­ ì‹ë³„
        legal_area = self.identify_legal_area(question)
        
        # ê´€ë ¨ íŒë¡€ ê²€ìƒ‰
        relevant_cases = self.court_listener.search_cases(
            query=question,
            court=context.get('jurisdiction') if context else None
        )
        
        # ë‹µë³€ êµ¬ì„±
        answer = self.compose_legal_answer(question, relevant_cases, legal_area)
        return answer
    
    def compose_legal_answer(self, question, cases, legal_area):
        """ë²•ë¥  ë‹µë³€ êµ¬ì„±"""
        answer = f"[{self.legal_categories.get(legal_area, 'ì¼ë°˜')}] ê´€ë ¨ ë‹µë³€:\n\n"
        
        if cases['results']:
            answer += "ê´€ë ¨ íŒë¡€:\n"
            for i, case in enumerate(cases['results'][:3], 1):
                answer += f"{i}. {case['caseName']} ({case['dateFiled']})\n"
                answer += f"   ìš”ì•½: {case.get('summary', 'ìš”ì•½ ì—†ìŒ')}\n\n"
        
        answer += "ì°¸ê³ : êµ¬ì²´ì ì¸ ë²•ë¥  ìë¬¸ì€ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        return answer
```

### 4-2. Caselaw Access Project

**í•˜ë²„ë“œ ë²•ëŒ€ì˜ 360ë…„ íŒë¡€ ë°ì´í„° í™œìš©**:
```python
class CaselawHistoricalRAG:
    def analyze_legal_evolution(self, legal_concept, time_periods):
        """ë²•ë¦¬ ë³€í™” ì¶”ì """
        evolution_data = {}
        
        for period in time_periods:
            cases = self.search_cases_by_period(legal_concept, period)
            evolution_data[period] = {
                'case_count': len(cases),
                'major_decisions': self.identify_landmark_cases(cases),
                'legal_trends': self.extract_legal_trends(cases)
            }
        
        return self.create_evolution_narrative(evolution_data)
    
    def create_evolution_narrative(self, evolution_data):
        """ë²•ë¦¬ ë°œì „ ì„œì‚¬ ìƒì„±"""
        narrative = "ë²•ë¦¬ ë°œì „ ê³¼ì •:\n\n"
        
        for period, data in evolution_data.items():
            narrative += f"ğŸ“… {period}:\n"
            narrative += f"- ê´€ë ¨ íŒë¡€ ìˆ˜: {data['case_count']}ê±´\n"
            
            if data['major_decisions']:
                narrative += f"- ì£¼ìš” íŒê²°: {', '.join(data['major_decisions'])}\n"
            
            narrative += f"- ì£¼ìš” ë™í–¥: {data['legal_trends']}\n\n"
        
        return narrative
```

## 5. ì˜ë£Œ

### 5-1. MIMIC-IV v3.1

**ê°œìš”**: MITì˜ ì¤‘í™˜ìì‹¤ ë°ì´í„°ì…‹ (30ë§Œ+ í™˜ì, ë¹„ì‹ë³„í™” ì™„ë£Œ)

**ë°ì´í„° êµ¬ì¡°**:
```python
class MIMICDataProcessor:
    def __init__(self):
        self.tables = {
            'admissions': 'ì…ì› ì •ë³´',
            'patients': 'í™˜ì ê¸°ë³¸ì •ë³´', 
            'icustays': 'ICU ì¬ì› ì •ë³´',
            'chartevents': 'ì°¨íŠ¸ ê¸°ë¡',
            'labevents': 'ê²€ì‚¬ ê²°ê³¼',
            'prescriptions': 'ì²˜ë°© ì •ë³´',
            'noteevents': 'ì„ìƒ ë…¸íŠ¸'
        }
    
    def create_patient_timeline(self, patient_id):
        """í™˜ìë³„ ì¹˜ë£Œ ê³¼ì • íƒ€ì„ë¼ì¸ ìƒì„±"""
        timeline = []
        
        # ì…ì› ì •ë³´
        admission = self.get_admission_data(patient_id)
        timeline.append({
            'timestamp': admission['admittime'],
            'event': f"ì…ì› - {admission['admission_type']}",
            'detail': f"ì§„ë‹¨: {admission['diagnosis']}"
        })
        
        # ê²€ì‚¬ ê²°ê³¼
        lab_events = self.get_lab_events(patient_id)
        for lab in lab_events:
            timeline.append({
                'timestamp': lab['charttime'],
                'event': f"ê²€ì‚¬ - {lab['itemid']}",
                'detail': f"ê²°ê³¼: {lab['value']} {lab['valueuom']}"
            })
        
        # ì²˜ë°© ì •ë³´
        prescriptions = self.get_prescriptions(patient_id)
        for rx in prescriptions:
            timeline.append({
                'timestamp': rx['startdate'],
                'event': f"ì²˜ë°© ì‹œì‘ - {rx['drug']}",
                'detail': f"ìš©ëŸ‰: {rx['dose_val_rx']} {rx['dose_unit_rx']}"
            })
        
        return sorted(timeline, key=lambda x: x['timestamp'])
```

**ì˜ë£Œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ**:
```python
class MedicalQASystem:
    def __init__(self):
        self.mimic_processor = MIMICDataProcessor()
        self.medical_concepts = self.load_medical_ontology()
    
    def answer_clinical_question(self, question, patient_context=None):
        """ì„ìƒ ì§ˆë¬¸ ë‹µë³€"""
        question_type = self.classify_medical_question(question)
        
        if question_type == 'diagnostic':
            return self.handle_diagnostic_query(question, patient_context)
        elif question_type == 'treatment':
            return self.handle_treatment_query(question, patient_context)
        elif question_type == 'prognosis':
            return self.handle_prognosis_query(question, patient_context)
        else:
            return self.handle_general_query(question)
    
    def handle_diagnostic_query(self, question, patient_context):
        """ì§„ë‹¨ ê´€ë ¨ ì§ˆì˜ ì²˜ë¦¬"""
        # ìœ ì‚¬í•œ ì¦ìƒì„ ê°€ì§„ í™˜ìë“¤ì˜ ì§„ë‹¨ ê³¼ì • ê²€ìƒ‰
        similar_cases = self.find_similar_cases(patient_context)
        
        diagnostic_patterns = []
        for case in similar_cases:
            pattern = {
                'symptoms': case['presenting_symptoms'],
                'lab_results': case['key_lab_values'],
                'final_diagnosis': case['diagnosis'],
                'diagnostic_path': case['diagnostic_steps']
            }
            diagnostic_patterns.append(pattern)
        
        return self.generate_diagnostic_insight(diagnostic_patterns)
```

**ì•½ë¬¼ ìƒí˜¸ì‘ìš© ë¶„ì„**:
```python
class DrugInteractionAnalyzer:
    def __init__(self):
        self.drug_database = self.load_drug_interactions()
    
    def analyze_prescription_safety(self, prescription_list):
        """ì²˜ë°© ì•ˆì „ì„± ë¶„ì„"""
        interactions = []
        
        for i, drug1 in enumerate(prescription_list):
            for drug2 in prescription_list[i+1:]:
                interaction = self.check_drug_interaction(drug1, drug2)
                if interaction:
                    interactions.append({
                        'drug1': drug1['name'],
                        'drug2': drug2['name'],
                        'severity': interaction['severity'],
                        'description': interaction['description'],
                        'recommendation': interaction['recommendation']
                    })
        
        return interactions
    
    def generate_safety_report(self, interactions):
        """ì•ˆì „ì„± ë³´ê³ ì„œ ìƒì„±"""
        if not interactions:
            return "âœ… í™•ì¸ëœ ì•½ë¬¼ ìƒí˜¸ì‘ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        
        report = "âš ï¸ ì•½ë¬¼ ìƒí˜¸ì‘ìš© ê²½ê³ :\n\n"
        
        for interaction in interactions:
            severity_emoji = {
                'major': 'ğŸ”´',
                'moderate': 'ğŸŸ¡', 
                'minor': 'ğŸŸ¢'
            }
            
            report += f"{severity_emoji[interaction['severity']]} "
            report += f"{interaction['drug1']} â†” {interaction['drug2']}\n"
            report += f"ì„¤ëª…: {interaction['description']}\n"
            report += f"ê¶Œì¥ì‚¬í•­: {interaction['recommendation']}\n\n"
        
        return report
```

### 5-2. PubMed Central OA

**ì˜í•™ ë…¼ë¬¸ ê¸°ë°˜ ì¦ê±° ê²€ìƒ‰**:
```python
import xml.etree.ElementTree as ET
from Bio import Entrez

class PubMedRAGSystem:
    def __init__(self, email):
        Entrez.email = email
        self.database = "pmc"
    
    def search_evidence(self, medical_question, max_results=10):
        """ì˜í•™ì  ì§ˆë¬¸ì— ëŒ€í•œ ë…¼ë¬¸ ì¦ê±° ê²€ìƒ‰"""
        # PubMed ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        search_terms = self.extract_medical_terms(medical_question)
        query = ' AND '.join(search_terms)
        
        # PMC ê²€ìƒ‰ ì‹¤í–‰
        handle = Entrez.esearch(
            db=self.database,
            term=query,
            retmax=max_results,
            sort="relevance"
        )
        search_results = Entrez.read(handle)
        
        # ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        evidence_papers = []
        for paper_id in search_results['IdList']:
            paper_details = self.get_paper_details(paper_id)
            evidence_papers.append(paper_details)
        
        return evidence_papers
    
    def get_paper_details(self, paper_id):
        """ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        handle = Entrez.efetch(
            db=self.database,
            id=paper_id,
            rettype="xml"
        )
        
        paper_xml = handle.read()
        root = ET.fromstring(paper_xml)
        
        # ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        title = root.find('.//article-title').text if root.find('.//article-title') is not None else "ì œëª© ì—†ìŒ"
        authors = [author.text for author in root.findall('.//contrib[@contrib-type="author"]//name/surname')]
        abstract = root.find('.//abstract/p').text if root.find('.//abstract/p') is not None else "ì´ˆë¡ ì—†ìŒ"
        
        return {
            'pmcid': paper_id,
            'title': title,
            'authors': authors,
            'abstract': abstract,
            'url': f"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{paper_id}/"
        }
    
    def generate_evidence_summary(self, question, evidence_papers):
        """ì¦ê±° ê¸°ë°˜ ìš”ì•½ ìƒì„±"""
        summary = f"ì§ˆë¬¸: {question}\n\n"
        summary += "ğŸ“š ê´€ë ¨ ì—°êµ¬ ì¦ê±°:\n\n"
        
        for i, paper in enumerate(evidence_papers, 1):
            summary += f"{i}. {paper['title']}\n"
            summary += f"   ì €ì: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}\n"
            summary += f"   ìš”ì•½: {paper['abstract'][:200]}...\n"
            summary += f"   ë§í¬: {paper['url']}\n\n"
        
        summary += "âš ï¸ ì£¼ì˜: ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¤ì œ ì§„ë£Œ ê²°ì •ì€ ë‹´ë‹¹ ì˜ì‚¬ì™€ ìƒì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        return summary
```

## 6. ìë™ì°¨

### 6-1. NHTSA API

**ê°œìš”**: ë¯¸êµ­ ë„ë¡œêµí†µì•ˆì „ì²­ì˜ ìë™ì°¨ ì•ˆì „ ë°ì´í„°

**API í™œìš© ì˜ˆì‹œ**:
```python
import requests
import pandas as pd

class NHTSAVehicleSafetyRAG:
    def __init__(self):
        self.base_url = "https://vpic.nhtsa.dot.gov/api/vehicles"
        self.recall_url = "https://api.nhtsa.gov/recalls/recallsByVehicle"
    
    def get_vehicle_info(self, vin):
        """VIN ê¸°ë°˜ ì°¨ëŸ‰ ì •ë³´ ì¡°íšŒ"""
        url = f"{self.base_url}/DecodeVINValues/{vin}"
        params = {'format': 'json'}
        
        response = requests.get(url, params=params)
        vehicle_data = response.json()
        
        if vehicle_data['Results']:
            return vehicle_data['Results'][0]
        return None
    
    def get_recall_info(self, make, model, year):
        """ì œì¡°ì‚¬/ëª¨ë¸/ì—°ì‹ë³„ ë¦¬ì½œ ì •ë³´ ì¡°íšŒ"""
        params = {
            'make': make,
            'model': model,
            'modelYear': year,
            'format': 'json'
        }
        
        response = requests.get(self.recall_url, params=params)
        recall_data = response.json()
        
        return recall_data.get('results', [])
    
    def create_safety_profile(self, vehicle_info, recalls):
        """ì°¨ëŸ‰ ì•ˆì „ì„± í”„ë¡œí•„ ìƒì„±"""
        safety_profile = f"""
        === ì°¨ëŸ‰ ì•ˆì „ì„± ì •ë³´ ===
        ì œì¡°ì‚¬: {vehicle_info.get('Make', 'N/A')}
        ëª¨ë¸: {vehicle_info.get('Model', 'N/A')}
        ì—°ì‹: {vehicle_info.get('ModelYear', 'N/A')}
        
        ğŸš¨ ë¦¬ì½œ ì •ë³´:
        ì´ ë¦¬ì½œ ê±´ìˆ˜: {len(recalls)}ê±´
        """
        
        if recalls:
            safety_profile += "\nì£¼ìš” ë¦¬ì½œ ì‚¬í•­:\n"
            for i, recall in enumerate(recalls[:5], 1):
                safety_profile += f"{i}. {recall.get('Component', 'N/A')}\n"
                safety_profile += f"   ìœ„í—˜ë„: {recall.get('PotentialUnitsAffected', 'N/A')}ëŒ€ ì˜í–¥\n"
                safety_profile += f"   ì¡°ì¹˜: {recall.get('Remedy', 'N/A')}\n\n"
        else:
            safety_profile += "\nâœ… í˜„ì¬ í™œì„± ë¦¬ì½œì´ ì—†ìŠµë‹ˆë‹¤.\n"
        
        return safety_profile
```

**ì‚¬ê³  ë°ì´í„° ë¶„ì„ (FARS)**:
```python
class FARSAccidentAnalyzer:
    def __init__(self):
        self.accident_factors = {
            'weather': ['clear', 'rain', 'snow', 'fog'],
            'road_surface': ['dry', 'wet', 'icy', 'snowy'],
            'light_condition': ['daylight', 'dark', 'dawn', 'dusk'],
            'alcohol_involvement': ['yes', 'no', 'unknown']
        }
    
    def analyze_accident_patterns(self, vehicle_make_model):
        """íŠ¹ì • ì°¨ëŸ‰ì˜ ì‚¬ê³  íŒ¨í„´ ë¶„ì„"""
        accident_data = self.get_accident_data(vehicle_make_model)
        
        analysis = {
            'total_accidents': len(accident_data),
            'fatality_rate': self.calculate_fatality_rate(accident_data),
            'common_factors': self.identify_common_factors(accident_data),
            'seasonal_trends': self.analyze_seasonal_trends(accident_data)
        }
        
        return self.generate_safety_insights(analysis)
    
    def generate_safety_insights(self, analysis):
        """ì•ˆì „ì„± ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = f"""
        ğŸš— ì°¨ëŸ‰ ì•ˆì „ì„± ë¶„ì„ ê²°ê³¼:
        
        ğŸ“Š ê¸°ë³¸ í†µê³„:
        - ì´ ì‚¬ê³  ê±´ìˆ˜: {analysis['total_accidents']:,}ê±´
        - ì¹˜ëª…ë¥ : {analysis['fatality_rate']:.2%}
        
        âš ï¸ ì£¼ìš” ìœ„í—˜ ìš”ì¸:
        """
        
        for factor, frequency in analysis['common_factors'].items():
            insights += f"- {factor}: {frequency:.1%}\n"
        
        insights += f"\nğŸ“… ê³„ì ˆë³„ ë™í–¥:\n"
        for season, data in analysis['seasonal_trends'].items():
            insights += f"- {season}: {data['accident_count']}ê±´ ({data['severity_score']:.1f}ì )\n"
        
        return insights
```

## 7. ì¦ê¶Œ

### 7-1. NASDAQ ì£¼ì‹ ë°ì´í„°

**í¬ê´„ì  ì£¼ì‹ ë°ì´í„° RAG ì‹œìŠ¤í…œ**:
```python
import yfinance as yf
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

class StockMarketRAG:
    def __init__(self):
        self.technical_indicators = {
            'sma_20': lambda data: data['Close'].rolling(window=20).mean(),
            'sma_50': lambda data: data['Close'].rolling(window=50).mean(),
            'rsi': self.calculate_rsi,
            'macd': self.calculate_macd
        }
    
    def get_stock_data(self, symbol, period="1y"):
        """ì£¼ì‹ ë°ì´í„° ì¡°íšŒ"""
        stock = yf.Ticker(symbol)
        hist_data = stock.history(period=period)
        info = stock.info
        
        return {
            'price_data': hist_data,
            'company_info': info,
            'technical_analysis': self.calculate_technical_indicators(hist_data)
        }
    
    def calculate_technical_indicators(self, price_data):
        """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        indicators = {}
        
        for name, calc_func in self.technical_indicators.items():
            try:
                indicators[name] = calc_func(price_data)
            except Exception as e:
                indicators[name] = None
        
        return indicators
    
    def create_stock_analysis_context(self, symbol):
        """ì£¼ì‹ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        stock_data = self.get_stock_data(symbol)
        
        latest_price = stock_data['price_data']['Close'].iloc[-1]
        prev_close = stock_data['price_data']['Close'].iloc[-2]
        change_pct = ((latest_price - prev_close) / prev_close) * 100
        
        context = f"""
        === {symbol} ì£¼ì‹ ë¶„ì„ ===
        í˜„ì¬ê°€: ${latest_price:.2f}
        ì „ì¼ ëŒ€ë¹„: {change_pct:+.2f}%
        52ì£¼ ìµœê³ ê°€: ${stock_data['price_data']['High'].max():.2f}
        52ì£¼ ìµœì €ê°€: ${stock_data['price_data']['Low'].min():.2f}
        
        ê¸°ìˆ ì  ì§€í‘œ:
        - 20ì¼ ì´í‰ì„ : ${stock_data['technical_analysis']['sma_20'].iloc[-1]:.2f}
        - 50ì¼ ì´í‰ì„ : ${stock_data['technical_analysis']['sma_50'].iloc[-1]:.2f}
        - RSI: {stock_data['technical_analysis']['rsi'].iloc[-1]:.1f}
        
        íšŒì‚¬ ì •ë³´:
        - ì‹œê°€ì´ì•¡: ${stock_data['company_info'].get('marketCap', 0):,}
        - P/E ë¹„ìœ¨: {stock_data['company_info'].get('trailingPE', 'N/A')}
        - ë°°ë‹¹ìˆ˜ìµë¥ : {stock_data['company_info'].get('dividendYield', 0)*100:.2f}%
        """
        
        return context
```

**í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ ì‹œìŠ¤í…œ**:
```python
class PortfolioAnalysisRAG:
    def __init__(self):
        self.risk_free_rate = 0.03  # 3% ë¬´ìœ„í—˜ ìˆ˜ìµë¥  ê°€ì •
    
    def analyze_portfolio(self, portfolio_symbols, weights=None):
        """í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„"""
        if weights is None:
            weights = [1/len(portfolio_symbols)] * len(portfolio_symbols)
        
        portfolio_data = {}
        for symbol in portfolio_symbols:
            stock_data = yf.Ticker(symbol).history(period="2y")
            portfolio_data[symbol] = stock_data['Close'].pct_change().dropna()
        
        portfolio_df = pd.DataFrame(portfolio_data)
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ê³„ì‚°
        portfolio_returns = (portfolio_df * weights).sum(axis=1)
        
        # ìœ„í—˜ ì§€í‘œ ê³„ì‚°
        portfolio_metrics = {
            'annual_return': portfolio_returns.mean() * 252,
            'annual_volatility': portfolio_returns.std() * np.sqrt(252),
            'sharpe_ratio': self.calculate_sharpe_ratio(portfolio_returns),
            'max_drawdown': self.calculate_max_drawdown(portfolio_returns),
            'var_95': np.percentile(portfolio_returns, 5),
            'correlation_matrix': portfolio_df.corr()
        }
        
        return self.generate_portfolio_report(portfolio_metrics, portfolio_symbols, weights)
    
    def calculate_sharpe_ratio(self, returns):
        """ìƒ¤í”„ ë¹„ìœ¨ ê³„ì‚°"""
        excess_returns = returns.mean() - (self.risk_free_rate / 252)
        return (excess_returns * 252) / (returns.std() * np.sqrt(252))
    
    def calculate_max_drawdown(self, returns):
        """ìµœëŒ€ ë‚™í­ ê³„ì‚°"""
        cumulative_returns = (1 + returns).cumprod()
        rolling_max = cumulative_returns.expanding().max()
        drawdown = (cumulative_returns - rolling_max) / rolling_max
        return drawdown.min()
    
    def generate_portfolio_report(self, metrics, symbols, weights):
        """í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""
        ğŸ“Š í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ ë¦¬í¬íŠ¸
        
        êµ¬ì„± ì¢…ëª©: {', '.join(symbols)}
        ë¹„ì¤‘: {', '.join([f'{w:.1%}' for w in weights])}
        
        ğŸ“ˆ ìˆ˜ìµì„± ì§€í‘œ:
        - ì—°ê°„ ìˆ˜ìµë¥ : {metrics['annual_return']:.2%}
        - ìƒ¤í”„ ë¹„ìœ¨: {metrics['sharpe_ratio']:.2f}
        
        âš ï¸ ìœ„í—˜ ì§€í‘œ:
        - ì—°ê°„ ë³€ë™ì„±: {metrics['annual_volatility']:.2%}
        - ìµœëŒ€ ë‚™í­: {metrics['max_drawdown']:.2%}
        - VaR (95%): {metrics['var_95']:.2%}
        
        ğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„:
        """
        
        # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        corr_matrix = metrics['correlation_matrix']
        for i in range(len(symbols)):
            for j in range(i+1, len(symbols)):
                corr_value = corr_matrix.iloc[i, j]
                report += f"- {symbols[i]} â†” {symbols[j]}: {corr_value:.3f}\n"
        
        return report
```

## RAG vs íŒŒì¸íŠœë‹ ì „ëµ ê°€ì´ë“œ

### ì‚°ì—…ë³„ ìµœì  ì ‘ê·¼ë²•

```python
class IndustryRAGStrategy:
    def __init__(self):
        self.industry_configs = {
            'banking': {
                'chunk_size': 512,
                'overlap': 50,
                'embedding_model': 'text-embedding-ada-002',
                'vector_store': 'pinecone',
                'rerank_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2'
            },
            'legal': {
                'chunk_size': 1024,  # ë²•ë¥  ë¬¸ì„œëŠ” ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ í•„ìš”
                'overlap': 100,
                'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',
                'vector_store': 'weaviate',
                'rerank_model': 'cross-encoder/ms-marco-MiniLM-L-12-v2'
            },
            'medical': {
                'chunk_size': 256,  # ì˜ë£Œ ë°ì´í„°ëŠ” ì •í™•ì„±ì„ ìœ„í•´ ì‘ì€ ì²­í¬
                'overlap': 25,
                'embedding_model': 'sentence-transformers/all-mpnet-base-v2',
                'vector_store': 'qdrant',
                'rerank_model': 'ms-marco-MiniLM-L-6-v2'
            }
        }
    
    def get_optimal_config(self, industry, data_characteristics):
        """ì‚°ì—…ë³„ ìµœì  ì„¤ì • ì œì•ˆ"""
        base_config = self.industry_configs.get(industry, self.industry_configs['banking'])
        
        # ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ì¡°ì •
        if data_characteristics.get('avg_document_length', 0) > 2000:
            base_config['chunk_size'] *= 1.5
            base_config['overlap'] *= 1.5
        
        if data_characteristics.get('technical_terminology', False):
            base_config['embedding_model'] = 'sentence-transformers/all-mpnet-base-v2'
        
        return base_config
```

### í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• êµ¬í˜„

```python
class HybridRAGFineTuning:
    def __init__(self, base_model="gpt-3.5-turbo"):
        self.base_model = base_model
        self.rag_system = None
        self.fine_tuned_model = None
    
    def stage1_rag_deployment(self, datasets):
        """1ë‹¨ê³„: RAG ì‹œìŠ¤í…œ ë°°í¬"""
        print("ğŸ”„ 1ë‹¨ê³„: RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì¤‘...")
        
        # ê° ì‚°ì—…ë³„ ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•
        for industry, dataset in datasets.items():
            vector_store = self.create_vector_store(industry, dataset)
            self.rag_system = self.setup_rag_pipeline(vector_store)
        
        print("âœ… RAG ì‹œìŠ¤í…œ ë°°í¬ ì™„ë£Œ")
    
    def stage2_collect_qa_logs(self, monitoring_period_days=30):
        """2ë‹¨ê³„: ì‚¬ìš©ì Q&A ë¡œê·¸ ìˆ˜ì§‘"""
        print(f"ğŸ“Š 2ë‹¨ê³„: {monitoring_period_days}ì¼ê°„ Q&A ë¡œê·¸ ìˆ˜ì§‘...")
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë¡œê¹… ì‹œìŠ¤í…œì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        qa_logs = self.collect_user_interactions()
        quality_scores = self.evaluate_rag_responses(qa_logs)
        
        # í’ˆì§ˆì´ ë‚®ì€ ì§ˆì˜ë“¤ ì‹ë³„
        improvement_candidates = [
            qa for qa, score in zip(qa_logs, quality_scores) 
            if score < 0.7
        ]
        
        return improvement_candidates
    
    def stage3_fine_tuning(self, improvement_data):
        """3ë‹¨ê³„: ì„ ë³„ëœ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹"""
        print("ğŸ”§ 3ë‹¨ê³„: ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œì‘...")
        
        # íŒŒì¸íŠœë‹ ë°ì´í„° ì¤€ë¹„
        training_data = self.prepare_fine_tuning_data(improvement_data)
        
        # OpenAI Fine-tuning API ì‚¬ìš© ì˜ˆì‹œ
        fine_tuning_job = self.create_fine_tuning_job(training_data)
        
        print(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ. ëª¨ë¸ ID: {fine_tuning_job.fine_tuned_model}")
        return fine_tuning_job.fine_tuned_model
    
    def prepare_fine_tuning_data(self, qa_data):
        """íŒŒì¸íŠœë‹ ë°ì´í„° ì¤€ë¹„"""
        training_examples = []
        
        for qa in qa_data:
            # RAGì—ì„œ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì™€ ê°œì„ ëœ ë‹µë³€ì„ ì¡°í•©
            training_example = {
                "messages": [
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë„ë©”ì¸ ì§€ì‹ì„ ê°€ì§„ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": qa['question']},
                    {"role": "assistant", "content": qa['improved_answer']}
                ]
            }
            training_examples.append(training_example)
        
        return training_examples
```

## ë³´ì•ˆ ë° ì»´í”Œë¼ì´ì–¸ìŠ¤ ê³ ë ¤ì‚¬í•­

### ë°ì´í„° ë³´ì•ˆ í”„ë ˆì„ì›Œí¬

```python
class SecureRAGFramework:
    def __init__(self):
        self.access_control = {}
        self.audit_logger = AuditLogger()
        self.data_classifier = DataClassifier()
    
    def setup_row_level_security(self, user_role, organization):
        """í–‰ ìˆ˜ì¤€ ë³´ì•ˆ ì„¤ì •"""
        access_rules = {
            'banking_analyst': {
                'allowed_tables': ['public_filings', 'market_data'],
                'restricted_fields': ['ssn', 'account_numbers'],
                'data_masking': True
            },
            'legal_counsel': {
                'allowed_tables': ['public_cases', 'regulations'],
                'restricted_fields': ['client_names', 'case_details'],
                'data_masking': True
            },
            'medical_researcher': {
                'allowed_tables': ['anonymized_records', 'public_studies'],
                'restricted_fields': ['patient_ids', 'personal_info'],
                'data_masking': True
            }
        }
        
        return access_rules.get(user_role, {})
    
    def implement_data_governance(self, dataset_info):
        """ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ êµ¬í˜„"""
        governance_policy = {
            'data_classification': self.classify_data_sensitivity(dataset_info),
            'retention_period': self.determine_retention_period(dataset_info),
            'access_logging': True,
            'encryption_at_rest': True,
            'encryption_in_transit': True
        }
        
        return governance_policy
    
    def audit_rag_queries(self, user_id, query, results):
        """RAG ì¿¼ë¦¬ ê°ì‚¬ ë¡œê¹…"""
        audit_record = {
            'timestamp': datetime.now().isoformat(),
            'user_id': user_id,
            'query': query,
            'results_count': len(results),
            'data_sources': self.extract_data_sources(results),
            'sensitivity_level': self.assess_query_sensitivity(query)
        }
        
        self.audit_logger.log(audit_record)
```

### GDPR/ê°œì¸ì •ë³´ë³´í˜¸ ì¤€ìˆ˜

```python
class PrivacyCompliantRAG:
    def __init__(self):
        self.pii_detector = PIIDetector()
        self.anonymizer = DataAnonymizer()
    
    def ensure_gdpr_compliance(self, raw_data):
        """GDPR ì¤€ìˆ˜ ë°ì´í„° ì²˜ë¦¬"""
        # 1. PII ì‹ë³„
        pii_detected = self.pii_detector.scan(raw_data)
        
        # 2. ìë™ ìµëª…í™”
        if pii_detected:
            anonymized_data = self.anonymizer.process(raw_data, pii_detected)
            return {
                'processed_data': anonymized_data,
                'pii_removed': len(pii_detected),
                'compliance_status': 'GDPR_COMPLIANT'
            }
        
        return {
            'processed_data': raw_data,
            'pii_removed': 0,
            'compliance_status': 'NO_PII_DETECTED'
        }
    
    def implement_right_to_erasure(self, user_request):
        """ì‚­ì œê¶Œ êµ¬í˜„"""
        affected_records = self.find_user_data(user_request['user_identifier'])
        
        for record in affected_records:
            self.mark_for_deletion(record)
            self.update_vector_embeddings(record['embedding_id'])
        
        return {
            'deleted_records': len(affected_records),
            'status': 'DELETION_COMPLETED'
        }
```

## ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§

### RAG ì„±ëŠ¥ ë©”íŠ¸ë¦­

```python
class RAGPerformanceMonitor:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alerting_system = AlertingSystem()
    
    def measure_rag_performance(self, query, retrieved_docs, final_answer):
        """RAG ì„±ëŠ¥ ì¸¡ì •"""
        metrics = {
            # ê²€ìƒ‰ í’ˆì§ˆ
            'retrieval_precision': self.calculate_retrieval_precision(retrieved_docs),
            'retrieval_recall': self.calculate_retrieval_recall(retrieved_docs),
            'context_relevance': self.measure_context_relevance(query, retrieved_docs),
            
            # ìƒì„± í’ˆì§ˆ
            'answer_relevance': self.measure_answer_relevance(query, final_answer),
            'answer_faithfulness': self.measure_faithfulness(retrieved_docs, final_answer),
            'answer_completeness': self.measure_completeness(query, final_answer),
            
            # ì„±ëŠ¥ ì§€í‘œ
            'response_time': self.measure_response_time(),
            'token_usage': self.count_tokens(final_answer),
            'cost_per_query': self.calculate_cost()
        }
        
        return metrics
    
    def setup_continuous_monitoring(self):
        """ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        monitoring_config = {
            'response_time_threshold': 3.0,  # 3ì´ˆ
            'relevance_score_threshold': 0.7,
            'cost_per_query_threshold': 0.05,  # $0.05
            'alert_channels': ['slack', 'email'],
            'monitoring_interval': 300  # 5ë¶„
        }
        
        return monitoring_config
```

### ìë™ ì„±ëŠ¥ íŠœë‹

```python
class AutoRAGOptimizer:
    def __init__(self):
        self.hyperparameter_tuner = HyperparameterTuner()
        self.a_b_tester = ABTester()
    
    def optimize_chunk_size(self, dataset, test_queries):
        """ì²­í¬ í¬ê¸° ìµœì í™”"""
        chunk_sizes = [256, 512, 1024, 2048]
        results = {}
        
        for chunk_size in chunk_sizes:
            # ê° ì²­í¬ í¬ê¸°ë¡œ RAG ì‹œìŠ¤í…œ êµ¬ì„±
            rag_system = self.create_rag_system(dataset, chunk_size)
            
            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì„±ëŠ¥ í‰ê°€
            performance = self.evaluate_performance(rag_system, test_queries)
            results[chunk_size] = performance
        
        # ìµœì  ì²­í¬ í¬ê¸° ì„ íƒ
        optimal_size = max(results.keys(), key=lambda k: results[k]['f1_score'])
        return optimal_size, results
    
    def optimize_retrieval_parameters(self, rag_system, validation_set):
        """ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ìµœì í™”"""
        parameter_grid = {
            'top_k': [3, 5, 10, 15],
            'similarity_threshold': [0.6, 0.7, 0.8, 0.9],
            'rerank_top_n': [3, 5, 10]
        }
        
        best_params = self.hyperparameter_tuner.grid_search(
            rag_system, 
            parameter_grid, 
            validation_set
        )
        
        return best_params
```

## ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ

### í†µí•© ì‚°ì—…ë³„ RAG í”Œë«í¼

```python
class MultiIndustryRAGPlatform:
    def __init__(self):
        self.industry_processors = {
            'banking': BankingDataProcessor(),
            'insurance': InsuranceDataProcessor(),
            'legal': LegalDataProcessor(),
            'medical': MedicalDataProcessor(),
            'automotive': AutomotiveDataProcessor(),
            'securities': SecuritiesDataProcessor()
        }
        
        self.rag_systems = {}
        self.user_management = UserManagementSystem()
        
    def initialize_industry_rag(self, industry, datasets):
        """ì‚°ì—…ë³„ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        processor = self.industry_processors[industry]
        
        # 1. ë°ì´í„° ì „ì²˜ë¦¬
        processed_data = processor.process_datasets(datasets)
        
        # 2. ë²¡í„° ìŠ¤í† ì–´ êµ¬ì„±
        vector_store = self.create_vector_store(industry, processed_data)
        
        # 3. RAG íŒŒì´í”„ë¼ì¸ ì„¤ì •
        rag_pipeline = self.setup_rag_pipeline(industry, vector_store)
        
        self.rag_systems[industry] = rag_pipeline
        return rag_pipeline
    
    def query_multi_industry(self, user_id, query, industries=None):
        """ë‹¤ì¤‘ ì‚°ì—… ì§ˆì˜"""
        user_permissions = self.user_management.get_permissions(user_id)
        allowed_industries = industries or user_permissions['industries']
        
        results = {}
        for industry in allowed_industries:
            if industry in self.rag_systems:
                industry_result = self.rag_systems[industry].query(query)
                results[industry] = industry_result
        
        # ê²°ê³¼ í†µí•© ë° ë­í‚¹
        integrated_response = self.integrate_multi_industry_results(results)
        return integrated_response
    
    def integrate_multi_industry_results(self, industry_results):
        """ë‹¤ì¤‘ ì‚°ì—… ê²°ê³¼ í†µí•©"""
        all_sources = []
        confidence_scores = []
        
        for industry, result in industry_results.items():
            for source in result['sources']:
                source['industry'] = industry
                all_sources.append(source)
                confidence_scores.append(result['confidence'])
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ ì •ë ¬
        sorted_sources = [
            source for _, source in sorted(
                zip(confidence_scores, all_sources), 
                reverse=True
            )
        ]
        
        return {
            'integrated_answer': self.generate_integrated_answer(sorted_sources),
            'sources_by_industry': industry_results,
            'confidence_score': max(confidence_scores) if confidence_scores else 0
        }
```

## ê²°ë¡ 

ì´ ê°€ì´ë“œì—ì„œ ì œì‹œí•œ **7ê°œ ì‚°ì—… ë¶„ì•¼ì˜ ê²€ì¦ëœ ê³µê°œ ë°ì´í„°ì…‹**ì€ ê¸°ì—…ìš© RAG ì‹œìŠ¤í…œ êµ¬ì¶•ì˜ ì¶œë°œì ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

### ğŸ¯ í•µì‹¬ ì„±ê³µ ìš”ì†Œ

1. **ì ì§„ì  êµ¬ì¶•**: RAG â†’ ëª¨ë‹ˆí„°ë§ â†’ íŒŒì¸íŠœë‹ ìˆœì„œë¡œ ë‹¨ê³„ë³„ ë°œì „
2. **ì‚°ì—…ë³„ ìµœì í™”**: ê° ë„ë©”ì¸ì˜ íŠ¹ì„±ì— ë§ëŠ” ì²­í‚¹ ë° ì„ë² ë”© ì „ëµ
3. **ë³´ì•ˆ ìš°ì„ **: ì²˜ìŒë¶€í„° ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ì™€ ì ‘ê·¼ ì œì–´ ê³ ë ¤
4. **ì§€ì†ì  ê°œì„ **: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ê³¼ ìë™ ìµœì í™” ì²´ê³„ êµ¬ì¶•

### ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. **PoC ë‹¨ê³„**: 1-2ê°œ ì‚°ì—…ìœ¼ë¡œ ì‹œì‘í•˜ì—¬ í•µì‹¬ ê¸°ëŠ¥ ê²€ì¦
2. **í™•ì¥ ë‹¨ê³„**: ì„±ê³µ ì‚¬ë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ë¥¸ ì‚°ì—… ì˜ì—­ ì¶”ê°€
3. **ê³ ë„í™” ë‹¨ê³„**: ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì™€ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì§€ì›

ê° ì‚°ì—…ë³„ ìƒì„¸í•œ êµ¬í˜„ ê°€ì´ë“œë‚˜ íŠ¹ì • ë°ì´í„°ì…‹ í™œìš© ë°©ë²•ì— ëŒ€í•œ ì¶”ê°€ ë¬¸ì˜ê°€ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ¤ 