---
title: "ضبط نموذج gpt-oss للدقة والأداء: دليل التدريب المدرك للكمية"
excerpt: "تعلم كيفية ضبط نموذج gpt-oss من OpenAI بفعالية باستخدام التعلم الموجه والتدريب المدرك للكمية للحفاظ على الدقة مع الاستفادة من فوائد دقة FP4 للنشر في الإنتاج."
seo_title: "دليل ضبط gpt-oss: QAT لنماذج الذكاء الاصطناعي في الإنتاج - Thaki Cloud"
seo_description: "دليل شامل لضبط gpt-oss مع التدريب المدرك للكمية. تعلم سير عمل SFT + QAT، مقارنة MXFP4 مقابل NVFP4، واستراتيجيات النشر في الإنتاج للحصول على أداء مثالي للذكاء الاصطناعي."
date: 2025-08-30
lang: ar
permalink: /ar/llmops/gpt-oss-fine-tuning-quantization-aware-training/
canonical_url: "https://thakicloud.github.io/ar/llmops/gpt-oss-fine-tuning-quantization-aware-training/"
categories:
  - llmops
tags:
  - gpt-oss
  - الكمية
  - الضبط-الدقيق
  - QAT
  - NVIDIA
  - TensorRT
  - تحسين-النماذج
  - FP4
  - الذكاء-الاصطناعي-الإنتاجي
author_profile: true
toc: true
toc_label: "جدول المحتويات"
---

⏱️ **وقت القراءة المقدر**: 12 دقيقة

## مقدمة

يمثل إطلاق gpt-oss معلماً مهماً كأول عائلة نماذج مفتوحة المصدر من مختبر OpenAI منذ GPT-2. يتميز هذا النموذج الثوري بهندسة خليط الخبراء (MoE)، وطول سياق 128K، وقدرات استدلال عميقة قابلة للتعديل. ومع ذلك، فإن دقة MXFP4 الأصلية تقدم تحديات فريدة للضبط الدقيق تتطلب حلولاً مبتكرة.

في هذا الدليل الشامل، سنستكشف سير العمل المثبت من NVIDIA لضبط نماذج gpt-oss الذي يحافظ على الدقة مع الحفاظ على فوائد الأداء لدقة FP4. يجمع هذا النهج بين التعلم الموجه (SFT) والتدريب المدرك للكمية (QAT) باستخدام محسن نماذج NVIDIA TensorRT.

## فهم تحدي gpt-oss

### معضلة دقة MXFP4

كان قرار OpenAI بإطلاق gpt-oss بدقة MXFP4 الأصلية الأول من نوعه في الصناعة، لكنه خلق تحديات كبيرة للممارسين:

- **مشاكل الاستقرار**: لم تثبت دقة MXFP4 الأصلية استقرارها لتراكم التدرج أثناء الضبط الدقيق
- **صعوبات التدريب**: الضبط الدقيق المباشر في FP4 يمكن أن يؤدي إلى مشاكل التقارب وتدهور الدقة
- **متطلبات الإنتاج**: معظم النماذج الأساسية تتطلب تقنيات ما بعد التدريب للنشر الفعال، خاصة في الصناعات منخفضة التسامح مع الأخطاء مثل الرعاية الصحية والمالية

### لماذا يقصر الضبط الدقيق التقليدي

يحقق نموذج gpt-oss-120B أداءً مماثلاً لنماذج OpenAI مغلقة المصدر o3 و o4 في المعايير المفتوحة. ومع ذلك، غالباً ما يُظهر الأداء الجاهز للاستخدام مجالاً للتحسين في مهام محددة:

- **الاستدلال غير الإنجليزي**: درجات أولية حوالي 16% في مجموعات البيانات متعددة اللغات
- **معالجة المطالبات الآمنة**: معدل نجاح 30% في تقليل الرفض غير الضروري للمطالبات الآمنة للمستخدمين
- **المحاذاة الخاصة بالمهام**: النماذج العامة تتطلب تدريباً متخصصاً للتطبيقات الخاصة بالمجال

## حل سير عمل SFT + QAT

### نظرة عامة على النهج

يتضمن حل NVIDIA عملية من مرحلتين تعالج مشاكل الاستقرار مع الحفاظ على الكفاءة:

1. **التعلم الموجه (SFT)**: يُؤدى على نسخة محسنة BF16 لتراكم تدرج مستقر
2. **التدريب المدرك للكمية (QAT)**: يُطبق باستخدام محسن نماذج NVIDIA TensorRT للعودة إلى دقة FP4

يمكّن سير العمل هذا SFT من تعزيز السلوك الخاص بالمهام بينما يكيف QAT الأوزان لتنسيق الدقة المنخفضة المستهدف، مما يوفر كلاً من المحاذاة والأداء للنشر.

### التنفيذ خطوة بخطوة

#### الخطوة 1: رفع نقطة تفتيش MXFP4 الأصلية إلى BF16/FP16

تتضمن الخطوة الأولى الحاسمة تحويل نموذج MXFP4 الأصلي إلى دقة أعلى:

```python
# استخدام Hugging Face Transformers للرفع
from transformers import AutoModelForCausalLM, AutoTokenizer

# تحميل نموذج MXFP4 الأصلي
model = AutoModelForCausalLM.from_pretrained(
    "openai/gpt-oss-20b",
    torch_dtype=torch.bfloat16,  # رفع إلى BF16
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained("openai/gpt-oss-20b")
```

**فوائد الرفع:**
- يوفر تدرجات أكثر استقراراً أثناء التدريب
- يمكّن QAT من استرداد الدقة بفعالية عند إعادة الكمية إلى FP4
- مقايضة مقبولة حيث أن الضبط الدقيق يستخدم رموزاً أقل بكثير من التدريب المسبق

#### الخطوة 2: تنفيذ التعلم الموجه

مع النموذج المحسن، قم بتنفيذ التعلم الموجه القياسي:

```python
import torch
from torch.utils.data import DataLoader
from transformers import TrainingArguments, Trainer

# تكوين معاملات التدريب
training_args = TrainingArguments(
    output_dir="./sft_output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=2e-5,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    fp16=False,  # استخدم BF16 للاستقرار
    bf16=True,
)

# تهيئة المدرب
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
)

# تنفيذ الضبط الدقيق
trainer.train()
```

#### الخطوة 3: الكمية باستخدام محسن نماذج TensorRT

تطبيق الكمية لإعداد النموذج لـ QAT:

```python
import modelopt.torch.quantization as mtq

# تكوين إعدادات الكمية
config = mtq.MXFP4_MLP_WEIGHT_ONLY_CFG

# تعريف حلقة أمامية للمعايرة
def forward_loop(model):
    for data in calib_set:
        model(data)

# كمية النموذج وإعداده لـ QAT
model = mtq.quantize(model, config, forward_loop)
```

#### الخطوة 4: ضبط النموذج المكمم FP4

تتضمن خطوة QAT الأخيرة الضبط الدقيق بمعدل تعلم أصغر:

```python
# تكوين QAT
qat_optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
qat_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    qat_optimizer, T_max=1000
)

# حلقة تدريب QAT
for epoch in range(qat_epochs):
    for batch in train_loader:
        qat_optimizer.zero_grad()
        
        outputs = model(**batch)
        loss = outputs.loss
        
        loss.backward()
        qat_optimizer.step()
        qat_scheduler.step()
```

## تحليل تأثير الأداء

### تحسينات جذرية في المهام اللاحقة

يُظهر سير عمل SFT + QAT فعالية ملحوظة عبر مقاييس التقييم المختلفة:

**قبل الضبط الدقيق:**
- الاستدلال غير الإنجليزي: معدل نجاح 16%
- معالجة المطالبات الآمنة: معدل نجاح 30%

**بعد SFT + QAT:**
- الاستدلال غير الإنجليزي: معدل نجاح 98%
- معالجة المطالبات الآمنة: معدل نجاح 98%

هذا يمثل **تحسناً بـ 6 أضعاف** في الاستدلال غير الإنجليزي و**تحسناً بـ 3.3 أضعاف** في معالجة المطالبات الآمنة.

### مقارنة طرق الكمية

| الطريقة | الاستدلال غير الإنجليزي | معالجة المطالبات الآمنة | كفاءة النشر |
|---------|----------------------|----------------------|-------------|
| الأصلي | 16% | 30% | عالية (FP4) |
| SFT فقط | 85% | 82% | منخفضة (BF16) |
| PTQ | 45% | 52% | عالية (FP4) |
| **SFT + QAT** | **98%** | **98%** | **عالية (FP4)** |

## NVFP4: الجيل القادم

### تقديم تنسيق NVFP4

مع NVIDIA Blackwell، يقدم NVFP4 تنسيق FP4 جديد مبني خصيصاً لكفاءة التدريب والاستنتاج:

```python
# التبديل إلى NVFP4 بسيط مثل تغيير سطر واحد
# من MXFP4
config = mtq.MXFP4_MLP_WEIGHT_ONLY_CFG

# إلى NVFP4
config = mtq.NVFP4_MLP_WEIGHT_ONLY_CFG

# للحصول على أداء أفضل مع كمية الوزن-التفعيل
config = mtq.NVFP4_MLP_ONLY_CFG
```

### مزايا NVFP4

**الفوائد التقنية:**
- دقة تدرج E4M3 FP8 تقلل أخطاء الكمية
- تقارب أفضل أثناء عملية الكمية المزيفة
- خسارة تحقق أفضل بـ 2-3% مقارنة بـ MXFP4
- قدرات محسنة لاسترداد الدقة

**فوائد الأداء:**
- حتى 15 PFLOPS من حوسبة FP4 على NVIDIA Blackwell Ultra
- تعليمات متخصصة في محرك NVIDIA Transformer من الجيل الثاني
- احتفاظ أفضل بأداء دقة النموذج

### مقارنة خسارة التحقق

تُظهر النتائج التجريبية تحسينات متسقة مع NVFP4:

- **المهام متعددة اللغات**: خسارة تحقق أفضل بـ 2-3%
- **مهام الرفض الخاطئ**: استقرار تقارب محسن
- **سيناريوهات الاستدلال العميق**: أداء أفضل تحت عتبات صارمة

## دليل النشر في الإنتاج

### تحويل وتصدير النموذج

بعد إكمال سير عمل SFT + QAT، قم بتحويل النموذج للنشر:

```bash
# تحويل نقطة تفتيش مدربة BF16 إلى MXFP4
python examples/gpt-oss/convert_oai_mxfp4_weight_only.py \
    --model_path qat_model_dir/ \
    --output_path qat_model_mxfp4/
```

### النشر مع TensorRT-LLM

انشر النموذج المحسن باستخدام TensorRT-LLM:

```bash
# استضافة نقطة النهاية مع TensorRT-LLM 1.1.0rc1
trtllm-serve qat_model_mxfp4/ \
    --tokenizer <tokenizer_path> \
    --max_batch_size 32 \
    --max_num_tokens 4096 \
    --max_seq_len 128000 \
    --tp_size 4 \
    --pp_size 2 \
    --host 0.0.0.0 \
    --kv_cache_free_gpu_memory_fraction 0.95
```

### التوافق ودعم الإطار

تم اختبار نقاط تفتيش MXFP4 الناتجة مع:

- **SGLang**: توافق كامل للخدمة
- **TensorRT-LLM**: أداء استنتاج محسن
- **vLLM**: نشر جاهز للإنتاج
- **دعم NVFP4 القادم**: أداء محسن مع هندسة Blackwell

## أفضل الممارسات ونصائح التحسين

### تحسين المعاملات الفائقة

**مرحلة SFT:**
- معدل التعلم: 2e-5 إلى 5e-5
- حجم الدفعة: اضبط حسب ذاكرة GPU
- العصور: 2-5 حسب حجم مجموعة البيانات
- تراكم التدرج: 4-16 خطوة

**مرحلة QAT:**
- معدل التعلم: 1e-5 إلى 5e-6 (أصغر بـ 10 مرات من SFT)
- مدة التدريب: 500-2000 خطوة
- المحسن: Adam مع التلدين الجيبي
- مجموعة بيانات المعايرة: ممثلة للتوزيع المستهدف

### المخاطر الشائعة التي يجب تجنبها

1. **تخطي SFT**: QAT المباشر ينتج عنه دقة أقل بكثير
2. **معدلات تعلم خاطئة**: معدلات تعلم عالية جداً في QAT يمكن أن تزعزع الأوزان المكممة
3. **معايرة غير كافية**: بيانات معايرة ضعيفة تؤدي إلى كمية دون المثلى
4. **تقارب مبكر**: راقب مقاييس التحقق طوال QAT

### المراقبة والتحقق

```python
# المقاييس الأساسية للتتبع أثناء التدريب
metrics_to_monitor = {
    'training_loss': 'هدف التحسين الأساسي',
    'validation_loss': 'مؤشر التعميم', 
    'perplexity': 'جودة نمذجة اللغة',
    'task_specific_accuracy': 'أداء المهام اللاحقة',
    'quantization_error': 'حفظ الدقة'
}
```

## الاتجاهات المستقبلية وخارطة الطريق

### التحسينات القادمة

**توسع نظام NVFP4 البيئي:**
- دعم TensorRT-LLM الأصلي لـ gpt-oss NVFP4
- التكامل مع أطر استنتاج مفتوحة المصدر إضافية
- أدوات محسنة لتحسين نماذج NVFP4

**ابتكارات التدريب:**
- تقنيات تدريب FP4 أصلية لكفاءة محسنة
- خوارزميات QAT متقدمة لاسترداد دقة أفضل
- تحسين متعدد الأهداف لمقايضات الدقة-الكفاءة

### التأثير الصناعي

يمثل سير عمل SFT + QAT لـ gpt-oss تقدماً كبيراً في نشر الذكاء الاصطناعي الإنتاجي:

- **الكفاءة من حيث التكلفة**: يحافظ على فوائد استنتاج FP4 مع ضمان الدقة
- **القابلية للتوسع**: يمكّن النشر في بيئات محدودة الموارد
- **الموثوقية**: يوفر أداءً مستقراً للتطبيقات الحرجة
- **إمكانية الوصول**: يجعل قدرات الذكاء الاصطناعي المتقدمة متاحة لجماهير أوسع

## الخلاصة

ينجح سير عمل الضبط الدقيق لـ gpt-oss باستخدام التعلم الموجه متبوعاً بالتدريب المدرك للكمية في معالجة التحديات الفريدة التي تطرحها دقة MXFP4 الأصلية. يوفر هذا النهج:

- **تحسينات أداء جذرية**: دقة أفضل حتى 6 أضعاف في المهام اللاحقة
- **كفاءة الإنتاج**: يحافظ على فوائد استنتاج FP4 للنشر الفعال من حيث التكلفة
- **هندسة جاهزة للمستقبل**: انتقال سلس إلى NVFP4 لأداء أفضل

يوفر الجمع بين محسن نماذج NVIDIA TensorRT مع منهجيات التدريب المثبتة أساساً قوياً لنشر نماذج gpt-oss في بيئات الإنتاج. مع توسع دعم NVFP4 عبر أطر الاستنتاج، سيفتح سير العمل هذا إمكانات أكبر لتحسين الدقة والكفاءة.

للممارسين الذين يتطلعون إلى الاستفادة من gpt-oss في تطبيقات الإنتاج، يوفر سير عمل SFT + QAT مساراً مثبتاً لتحقيق دقة عالية ونشر فعال. التنفيذ الكامل متاح من خلال [مستودع محسن نماذج NVIDIA](https://github.com/NVIDIA/TensorRT-Model-Optimizer)، مما يوفر وصولاً فورياً لهذه التقنيات المتقدمة للتحسين.

---

**المراجع:**
- [مدونة مطوري NVIDIA: ضبط gpt-oss للدقة والأداء](https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/?linkId=100000380274168)
- [مستودع محسن نماذج NVIDIA TensorRT](https://github.com/NVIDIA/TensorRT-Model-Optimizer)
- [وصفات Hugging Face gpt-oss](https://github.com/huggingface/gpt-oss-recipes)
