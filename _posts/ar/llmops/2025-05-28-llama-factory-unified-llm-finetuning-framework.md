---
title: "LLaMA Factory: إطار العمل الموحد للنماذج اللغوية الكبيرة الذي يضبط أكثر من 100 نموذج بسطر واحد من الكود"
excerpt: "اضبط Llama 3 وQwen 3 وDeepSeek وأكثر من 100 نموذج لغوي كبير متطور بسهولة. إطار عمل مفتوح المصدر يدمج LoRA/QLoRA وFSDP وFlash-Attention 2 وأحدث التقنيات"
seo_title: "LLaMA Factory - دليل شامل لإطار العمل الموحد لضبط النماذج اللغوية الكبيرة"
seo_description: "أتقن LLaMA Factory، الإطار الشامل لضبط أكثر من 100 نموذج لغوي كبير بما في ذلك Llama 3 وQwen 3 وDeepSeek مع LoRA وQLoRA وتقنيات التحسين المتقدمة"
date: 2025-05-28
categories:
  - llmops
tags:
  - LLaMAFactory
  - LoRA
  - QLoRA
  - النماذج-اللغوية-الكبيرة
  - الضبط-الدقيق
  - MLOps
  - إطار-الذكاء-الاصطناعي
  - مفتوح-المصدر
author_profile: true
toc: true
toc_label: "دليل LLaMA Factory"
lang: ar
permalink: /ar/llmops/llama-factory-unified-llm-finetuning-framework/
canonical_url: "https://thakicloud.github.io/ar/llmops/llama-factory-unified-llm-finetuning-framework/"
---

⏱️ **الوقت المقدر للقراءة**: 8 دقائق

> **الملخص** LLaMA Factory هو إطار عمل مفتوح المصدر يمكّن من الضبط الدقيق لأكثر من 100 نموذج لغوي كبير ومتعدد الوسائط متطور بما في ذلك Llama 3 وQwen 3 وDeepSeek وMistral **بسطر واحد من الكود**. يدمج أحدث التقنيات مثل LoRA/QLoRA وFSDP وFlash‑Attention 2 وvLLM وPPO/DPO/KTO/ORPO، بينما يوفر كلاً من CLI وواجهة ويب قائمة على Gradio لسد الفجوة بشكل كبير بين بيئات البحث والإنتاج.

---

## ما هو LLaMA Factory؟

LLaMA Factory هو **إطار عمل مفتوح المصدر للضبط الدقيق للنماذج اللغوية الكبيرة قائم على PyTorch** يقدم الخصائص التالية:

- **دعم أكثر من 100 نموذج** — Llama 3/4 وQwen 3 وDeepSeek وYi وGemma 3 وMixtral‑MoE وLLaVA‑NeXT والمزيد
- **نهج تدريب متنوعة** — Full/Freeze/LoRA/QLoRA و(متعدد الوسائط) SFT ونموذج المكافآت وPPO وDPO وKTO وORPO وSimPO وغيرها
- **تحسين الكفاءة** — FlashAttention‑2 وUnsloth وGaLore وBAdam وAPOLLO وDoRA وLongLoRA وMixture‑of‑Depths وPiSSA
- **واجهة سهلة الاستخدام** — أمر واحد `llamafactory-cli train …` أو واجهة **LLaMA Board** الرسومية القائمة على Gradio
- **جاهز للنشر** — REST API بنمط OpenAI (`/v1/chat/completions`) وخلفيات vLLM·SGLang وأمثلة Docker وK8s

> مستخدم بالفعل في Amazon SageMaker HyperPod وNVIDIA RTX AI Toolkit وAliyun PAI‑DSW ومنصات أخرى.

## لماذا يجب عليك استخدام LLaMA Factory؟

### سد فجوة البحث والإنتاج

يتعامل الإطار مع خط الأنابيب بأكمله من معالجة البيانات المسبقة إلى التدريب والتقييم والاستنتاج والنشر داخل إطار عمل واحد، مما يقلل بشكل كبير من **تعقيد MLOps**.

### استخدام الموارد المرن

من خلال مجموعات LoRA·QLoRA·FSDP، يمكن تدريب حتى **نماذج 70B** على 2×24GB GPU. كما يدعم بيئات Ascend NPU·AMD ROCm.

### دعم اليوم-N لأحدث الخوارزميات

يتم دمج النماذج والخوارزميات الرئيسية خلال D+1 أيام من نشر الورقة البحثية، مثل Llama 4 وGLM‑4 وMuon وGaLore.

## الميزات الرئيسية في لمحة

- **CLI**: يوفر سير عمل كامل من خلال أربعة أوامر: `train/chat/export/api`
- **واجهة الويب (LLaMA Board)**: يصور كل شيء من رفع مجموعة البيانات إلى إعدادات المعاملات الفائقة والسجلات في الوقت الفعلي
- **مجموعة Docker**: يوفر ملفات Compose لبيئات CUDA·NPU·ROCm → بيئة متطابقة في أي مكان، محلياً أو سحابياً
- **حزمة مجموعة البيانات الكبيرة**: أكثر من 80 مجموعة بيانات عامة محددة مسبقاً لـ SFT/RLHF/متعددة الوسائط
- **متتبع التجارب**: تكامل TensorBoard وW&B وSwanLab وMLflow

## لمحة سريعة عن دعم النماذج

يدعم الإطار مجموعة واسعة من النماذج عبر عائلات مختلفة:

**Meta**: Llama 2·3·4 (7B → 402B معامل)
**Alibaba**: Qwen 1‑3 / Qwen 2‑VL (0.5B → 235B معامل)
**Mistral**: Mistral / Mixtral‑MoE (7B → 8×22B معامل)
**Google**: Gemma 3 / PaliGemma 2 (1B → 28B معامل)
**OpenGVLab**: InternVL 3‑8B (8B معامل)

يشمل الدعم الإضافي DeepSeek وYi وPhi 4 وMiniCPM‑V والعديد من النماذج الأخرى. يمكن العثور على القائمة الكاملة وتعيينات القوالب في ملف `constants.py`.

## بداية سريعة في 3 دقائق

يوفر الإطار تجربة بداية بسيطة بشكل لا يصدق:

```bash
# 1) مثال LoRA‑SFT (Llama 3‑8B‑Instruct)
llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml

# 2) الاستنتاج التفاعلي
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml

# 3) دمج محول LoRA وتصدير نقطة التفتيش
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
```

### تنفيذ واجهة Gradio الرسومية

```bash
llamafactory-cli webui  # الوصول إلى localhost:7860
```

### Docker Compose (CUDA)

```bash
cd docker/docker-cuda && docker compose up -d
```

## نصائح التنفيذ العملي

يقدم الإطار عدة استراتيجيات تحسين لتعزيز الأداء:

**FlashAttention‑2**: إعداد `flash_attn: fa2` يوفر تحسناً في السرعة بأكثر من 30% على وحدات معالجة الرسوميات A100.

**QLoRA**: دمج التكميم 4‑بت مع LoRA يمكّن من الضبط الدقيق لنماذج 8B بـ 6GB vRAM فقط.

**NEFTune**: استخدام `neftune_noise_alpha: 5` للتنظيم يحسن سرعة التقارب.

**خدمة vLLM**: إعداد `infer_backend: vllm` يزيد متوسط TPS بـ 2.7×.

## منهجيات التدريب المتقدمة

### الضبط الدقيق المُشرف عليه (SFT)

يتفوق الإطار في تكييف النماذج المدربة مسبقاً مع مهام ومجالات محددة من خلال الضبط الدقيق المُشرف عليه. تتضمن هذه العملية تدريب النماذج على أزواج تعليمات-استجابات منسقة بعناية لتحسين قدرتها على اتباع التعليمات البشرية وتوليد استجابات مناسبة في سياقات محددة.

### التعلم التعزيزي من التغذية الراجعة البشرية (RLHF)

يدعم LLaMA Factory خطوط أنابيب RLHF الشاملة بما في ذلك تدريب نموذج المكافآت وتحسين السياسة من خلال خوارزميات مختلفة. ينفذ الإطار تقنيات تحسين التفضيل المتعددة مثل PPO (تحسين السياسة القريبة) وDPO (تحسين التفضيل المباشر) وKTO (تحسين كانمان-تفيرسكي) وORPO (تحسين التفضيل بنسبة الاحتمالات)، مما يمكّن من التحكم الدقيق في توافق سلوك النموذج مع التفضيلات البشرية.

### الضبط الدقيق الفعال للمعاملات

يوفر الإطار دعماً واسعاً للطرق الفعالة للمعاملات التي تقلل بشكل كبير من متطلبات الحوسبة مع الحفاظ على الأداء. تتضمن هذه التقنيات التكيف منخفض الرتبة (LoRA) ومتغيراته، والتي تحلل تحديثات الأوزان إلى مصفوفات منخفضة الرتبة، مما يقلل بشكل كبير من عدد المعاملات القابلة للتدريب مع الحفاظ على قدرات النموذج.

### قدرات التدريب متعدد الوسائط

بما يتجاوز النماذج النصية فقط، يدعم LLaMA Factory تدريب نماذج الرؤية واللغة التي يمكنها معالجة وفهم المدخلات النصية والبصرية. تمكّن هذه القدرة من تطوير أنظمة ذكاء اصطناعي متطورة يمكنها أداء مهام تتطلب فهم الصور والوثائق والمخططات والمحتوى البصري الآخر إلى جانب معالجة اللغة الطبيعية.

## استراتيجيات النشر الإنتاجي

### تكامل خادم API

يوفر الإطار تكاملاً سلساً مع بيئات الإنتاج من خلال نقاط نهاية API متوافقة مع OpenAI. تضمن هذه الواجهة الموحدة التكامل السهل مع التطبيقات والخدمات الحالية التي تستخدم بالفعل تنسيق API الخاص بـ OpenAI، مما يقلل من تعقيد الهجرة ويمكّن من النشر السريع للنماذج المضبوطة بدقة مخصصة.

### حلول الاستنتاج القابلة للتوسع

يتكامل LLaMA Factory مع محركات الاستنتاج عالية الأداء بما في ذلك vLLM وSGLang، والتي توفر قدرات خدمة محسنة لأحمال العمل الإنتاجية. تنفذ هذه المحركات تقنيات متقدمة مثل التجميع المستمر وآليات الانتباه الفعالة للذاكرة وجدولة الطلبات الديناميكية لتعظيم الإنتاجية وتقليل زمن الاستجابة.

### تنسيق الحاويات

يتضمن الإطار تكوينات نشر Docker وKubernetes شاملة، مما يمكّن من التوسع السلس عبر بيئات البنية التحتية المختلفة. تضمن هذه النشر المحتواة سلوكاً متسقاً عبر بيئات التطوير والاختبار والإنتاج مع توفير المرونة لتوسيع الموارد بناءً على الطلب.

## ضمان الجودة والمراقبة

### تكامل إطار التقييم

يدمج LLaMA Factory قدرات تقييم قوية تمكّن من التقييم المنهجي لأداء النموذج عبر مقاييس ومعايير مختلفة. يشمل هذا دعم المقاييس التلقائية وبروتوكولات التقييم البشري، مما يضمن تقييم الجودة الشامل طوال دورة حياة التطوير.

### تتبع التجارب وإمكانية الإعادة

يوفر الإطار قدرات تسجيل وتتبع تجارب واسعة من خلال التكامل مع منصات MLOps الشائعة. يمكّن هذا من المقارنة المنهجية لتكوينات التدريب المختلفة وإعدادات المعاملات الفائقة ومتغيرات النموذج، مما يسهل اتخاذ القرارات القائمة على البيانات في عمليات تطوير النموذج.

## المجتمع والنظام البيئي

### التعاون مفتوح المصدر

يستفيد LLaMA Factory من مساهمات المجتمع النشطة ويحافظ على التوافق مع النظام البيئي الأوسع لأدوات وإطارات التعلم الآلي. يرحب المشروع بمساهمات الباحثين والممارسين، مما يعزز الابتكار والاعتماد السريع للتقنيات والمنهجيات الجديدة.

### الموارد التعليمية

يوفر الإطار وثائق شاملة ودروساً تعليمية وتكوينات أمثلة تمكّن كلاً من المبتدئين والخبراء من الاستفادة بفعالية من قدراته. تغطي هذه الموارد كل شيء من سير عمل الضبط الدقيق الأساسي إلى تقنيات التحسين المتقدمة واستراتيجيات النشر الإنتاجي.

## روابط المرجع

- **GitHub**: [https://github.com/hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
- **الوثائق**: [https://llamafactory.readthedocs.io/en/latest/](https://llamafactory.readthedocs.io/en/latest/)
- **دفتر Colab**: [https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9)
- **Discord للدعم**: [https://discord.gg/rKfvV9r9FK](https://discord.gg/rKfvV9r9FK)

يجب أن تتوافق تراخيص الإطار والنموذج مع Apache‑2.0 وتراخيص النماذج الفردية المحددة. يرجى الرجوع إلى ملف `LICENSE` في المستودع وبطاقات النموذج للحصول على معلومات مفصلة.

---
