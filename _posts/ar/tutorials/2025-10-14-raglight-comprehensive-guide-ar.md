---
title: "Ø¯Ù„ÙŠÙ„ RAGLight Ø§Ù„Ø´Ø§Ù…Ù„: Ù…Ù† RAG Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¥Ù„Ù‰ Ø³ÙŠØ± Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„ÙˆÙƒÙŠÙ„"
excerpt: "Ø¥ØªÙ‚Ø§Ù† Ø¥Ø·Ø§Ø± RAGLight Ù…Ø¹ Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ© ØªØºØ·ÙŠ RAGØŒ Agentic RAGØŒ RAT pipelinesØŒ ÙˆØªÙƒØ§Ù…Ù„ MCP Ù„Ø¨Ù†Ø§Ø¡ Ø£Ù†Ø¸Ù…Ø© ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø²Ø²Ø© Ø¨Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù‚ÙˆÙŠØ©."
seo_title: "Ø¯Ø±ÙˆØ³ RAGLight: Ø¯Ù„ÙŠÙ„ Ø¥Ø·Ø§Ø± RAG Ø§Ù„ÙƒØ§Ù…Ù„ - Thaki Cloud"
seo_description: "ØªØ¹Ù„Ù… Ø¥Ø·Ø§Ø± RAGLight Ù…Ø¹ Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ©. Ø¨Ù†Ø§Ø¡ RAGØŒ Agentic RAGØŒ Ùˆ RAT pipelines Ø¹Ù„Ù‰ macOS Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ollama Ø£Ùˆ OpenAI Ø£Ùˆ Mistral Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„ÙˆØ§Ø¹ÙŠØ© Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚."
date: 2025-10-14
categories:
  - tutorials
tags:
  - raglight
  - rag
  - agentic-rag
  - ollama
  - python
  - llm
  - vector-database
  - mcp
  - huggingface
author_profile: true
toc: true
toc_label: "Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª"
lang: ar
permalink: /ar/tutorials/raglight-comprehensive-guide/
canonical_url: "https://thakicloud.github.io/ar/tutorials/raglight-comprehensive-guide/"
---

â±ï¸ **ÙˆÙ‚Øª Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù‚Ø¯Ø±**: 15 Ø¯Ù‚ÙŠÙ‚Ø©

## Ù…Ù‚Ø¯Ù…Ø©

**RAGLight** Ù‡Ùˆ Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ Python Ø®ÙÙŠÙ Ø§Ù„ÙˆØ²Ù† ÙˆÙ…Ø¹ÙŠØ§Ø±ÙŠ Ù…ØµÙ…Ù… Ù„ØªØ¨Ø³ÙŠØ· ØªÙ†ÙÙŠØ° **Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø²Ø² Ø¨Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ (Retrieval-Augmented Generation - RAG)**. Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆÙ†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© (Large Language Models - LLM)ØŒ ÙŠØªÙŠØ­ Ù„Ùƒ RAGLight Ø¨Ù†Ø§Ø¡ Ø£Ù†Ø¸Ù…Ø© Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ø¹ÙŠØ© Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø³ØªÙ†Ø¯Ø§ØªÙƒ ÙˆÙ‚ÙˆØ§Ø¹Ø¯ Ù…Ø¹Ø±ÙØªÙƒ Ø§Ù„Ø®Ø§ØµØ©.

ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ØŒ Ø³ØªØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ©:

- Ø¥Ø¹Ø¯Ø§Ø¯ RAGLight Ù…Ø¹ Ù…Ø²ÙˆØ¯ÙŠ LLM Ø§Ù„Ù…Ø®ØªÙ„ÙÙŠÙ† (OllamaØŒ OpenAIØŒ Mistral)
- Ø¨Ù†Ø§Ø¡ Ø®Ø·ÙˆØ· RAG Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
- ØªÙ†ÙÙŠØ° Agentic RAG Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø®Ø·ÙˆØ§Øª
- Ø§Ø³ØªØ®Ø¯Ø§Ù… RAT (Retrieval-Augmented Thinking) Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†
- Ø¯Ù…Ø¬ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… MCP (Model Context Protocol)

### Ù…Ø§ Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¹Ù„ RAGLight Ù…Ù…ÙŠØ²Ø§Ù‹ØŸ

ÙŠØªÙ…ÙŠØ² RAGLight Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:

- **Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©**: Ø³Ù‡ÙˆÙ„Ø© ØªØ¨Ø¯ÙŠÙ„ LLMs ÙˆØ§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª ÙˆÙ…Ø®Ø§Ø²Ù† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
- **Ø¯Ø¹Ù… Ù…ÙˆÙØ±ÙŠÙ† Ù…ØªØ¹Ø¯Ø¯ÙŠÙ†**: OllamaØŒ OpenAIØŒ MistralØŒ LMStudioØŒ vLLMØŒ Google AI
- **Ø®Ø·ÙˆØ· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù…ØªÙ‚Ø¯Ù…Ø©**: RAG Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØŒ Agentic RAGØŒ Ùˆ RAT Ù…Ø¹ Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
- **ØªÙƒØ§Ù…Ù„ MCP**: Ø±Ø¨Ø· Ø§Ù„Ø£Ø¯ÙˆØ§Øª ÙˆÙ…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø¨Ø³Ù„Ø§Ø³Ø©
- **ØªÙƒÙˆÙŠÙ† Ù…Ø±Ù†**: ØªØ®ØµÙŠØµ ÙƒÙ„ Ø¬Ø§Ù†Ø¨ Ù…Ù† Ø¬ÙˆØ§Ù†Ø¨ Ø®Ø· RAG Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ

## Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆÙØ±:

### 1. Ø¨ÙŠØ¦Ø© Python

```bash
# ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥ØµØ¯Ø§Ø± Python (Ù…Ø·Ù„ÙˆØ¨ 3.8 Ø£Ùˆ Ø£Ø¹Ù„Ù‰)
python3 --version

# Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ¦Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© (Ù…ÙˆØµÙ‰ Ø¨Ù‡)
python3 -m venv raglight-env
source raglight-env/bin/activate  # Ø¹Ù„Ù‰ macOS/Linux
# raglight-env\Scripts\activate  # Ø¹Ù„Ù‰ Windows
```

### 2. ØªØ«Ø¨ÙŠØª Ollama (Ù„Ù€ LLM Ø§Ù„Ù…Ø­Ù„ÙŠ)

```bash
# macOS
brew install ollama

# Ø£Ùˆ Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ù…Ù† https://ollama.ai/download

# Ø¨Ø¯Ø¡ Ø®Ø¯Ù…Ø© Ollama
ollama serve

# Ø³Ø­Ø¨ Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ terminal Ø¬Ø¯ÙŠØ¯)
ollama pull llama3.2:3b
```

**Ø§Ù„Ø¨Ø¯ÙŠÙ„**: Ø§Ø³ØªØ®Ø¯Ù… OpenAI Ø£Ùˆ Mistral API Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙØ¶Ù„ LLMs Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©.

### 3. ØªØ«Ø¨ÙŠØª RAGLight

```bash
pip install raglight
```

## Ø§Ù„ØªØ«Ø¨ÙŠØª ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯

### ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø¨ÙŠØ¦Ø©

Ø£Ù†Ø´Ø¦ Ù…Ù„Ù `.env` Ù„ØªØ®Ø²ÙŠÙ† Ù…ÙØ§ØªÙŠØ­ API Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ (Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆÙØ±ÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©):

```bash
# Ù…Ù„Ù .env
OPENAI_API_KEY=your_openai_key_here
MISTRAL_API_KEY=your_mistral_key_here
```

### Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ù„ÙŠÙ„ Ù…Ø´Ø±ÙˆØ¹Ùƒ:

```bash
mkdir raglight-tutorial
cd raglight-tutorial
mkdir data
mkdir knowledge_base
```

### Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©

Ø£Ù†Ø´Ø¦ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±:

```bash
# data/document1.txt
cat > data/document1.txt << 'EOF'
RAGLight Ù‡Ùˆ Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ Python Ù…Ø¹ÙŠØ§Ø±ÙŠ Ù„Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø²Ø² Ø¨Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹.
ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ù…ÙˆÙØ±ÙŠ LLM Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ollama Ùˆ OpenAI Ùˆ Mistral.
ØªØ´Ù…Ù„ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø±Ù† Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ù…Ø¹ ChromaDB Ùˆ FAISS.
EOF

# data/document2.txt
cat > data/document2.txt << 'EOF'
ÙŠÙˆØ³Ø¹ Agentic RAG RAG Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¯Ù…Ø¬ Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ‚Ù„ÙŠÙ†.
ÙŠÙ…ÙƒÙ† Ù„Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª ÙˆØ§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ.
ØªØ´Ù…Ù„ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© ÙˆÙ…Ø³Ø§Ø¹Ø¯ÙŠ Ø§Ù„Ø£Ø¨Ø­Ø§Ø«.
EOF

# data/document3.txt
cat > data/document3.txt << 'EOF'
ÙŠØ¶ÙŠÙ RAT (Retrieval-Augmented Thinking) Ø·Ø¨Ù‚Ø© Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ØªØ®ØµØµØ©.
ÙŠØ³ØªØ®Ø¯Ù… LLMs Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ÙŠØ© Ù„ØªØ¹Ø²ÙŠØ² Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙˆØ§Ù„Ø¹Ù…Ù‚ Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠ.
RAT Ù…Ø«Ø§Ù„ÙŠ Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙŠ ØªØªØ·Ù„Ø¨ ØªÙÙƒÙŠØ±Ø§Ù‹ Ø¹Ù…ÙŠÙ‚Ø§Ù‹ ÙˆØ§Ø³ØªØ¯Ù„Ø§Ù„Ø§Ù‹ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù‚ÙØ²Ø§Øª.
EOF
```

## Ø®Ø· RAG Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ

### ÙÙ‡Ù… Ø¨Ù†ÙŠØ© RAG

ÙŠØªÙƒÙˆÙ† Ø®Ø· RAG Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù…Ù† Ø«Ù„Ø§Ø«Ø© Ù…ÙƒÙˆÙ†Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©:

1. **Ø§Ø³ØªÙŠØ¹Ø§Ø¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª (Document Ingestion)**: ÙŠØªÙ… ØªÙ‚Ø³ÙŠÙ… Ù…Ø³ØªÙ†Ø¯Ø§ØªÙƒ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ†Ø§Øª
2. **Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡ (Vector Storage)**: ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ¬Ù‡Ø§Øª (ChromaDBØŒ FAISSØŒ Ø¥Ù„Ø®)
3. **Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØ§Ù„ØªÙˆÙ„ÙŠØ¯ (Retrieval & Generation)**: Ø¹Ù†Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…ØŒ ÙŠØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© ÙˆØªÙ…Ø±ÙŠØ±Ù‡Ø§ Ø¥Ù„Ù‰ LLM

### Ø§Ù„ØªÙ†ÙÙŠØ°

Ø¥Ù„ÙŠÙƒ Ù…Ø«Ø§Ù„ ÙƒØ§Ù…Ù„ Ù„Ø®Ø· RAG Ø£Ø³Ø§Ø³ÙŠ:

```python
#!/usr/bin/env python3
"""Ø®Ø· RAG Ø£Ø³Ø§Ø³ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RAGLight"""

from raglight.rag.simple_rag_api import RAGPipeline
from raglight.config.rag_config import RAGConfig
from raglight.config.vector_store_config import VectorStoreConfig
from raglight.config.settings import Settings
from raglight.models.data_source_model import FolderSource
from dotenv import load_dotenv

# ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
load_dotenv()

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
Settings.setup_logging()

# ØªÙƒÙˆÙŠÙ† Ù…Ø®Ø²Ù† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
vector_store_config = VectorStoreConfig(
    embedding_model=Settings.DEFAULT_EMBEDDINGS_MODEL,
    api_base=Settings.DEFAULT_OLLAMA_CLIENT,
    provider=Settings.HUGGINGFACE,
    database=Settings.CHROMA,
    persist_directory="./chroma_db",
    collection_name="my_knowledge_base"
)

# ØªÙƒÙˆÙŠÙ† RAG
config = RAGConfig(
    llm="llama3.2:3b",  # Ù†Ù…ÙˆØ°Ø¬ Ollama
    k=5,  # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹
    provider=Settings.OLLAMA,
    system_prompt=Settings.DEFAULT_SYSTEM_PROMPT,
    knowledge_base=[FolderSource(path="./data")]
)

# ØªÙ‡ÙŠØ¦Ø© ÙˆØ¨Ù†Ø§Ø¡ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨
print("ØªÙ‡ÙŠØ¦Ø© Ø®Ø· RAG...")
pipeline = RAGPipeline(config, vector_store_config)

print("Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©...")
pipeline.build()

# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ù† Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨
query = "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù€ RAGLightØŸ"
print(f"\nØ§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query}")

response = pipeline.generate(query)
print(f"\nØ§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©:\n{response}")
```

### Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©

**Ø®ÙŠØ§Ø±Ø§Øª Ù…Ø®Ø²Ù† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª:**
- `database`: CHROMA Ø£Ùˆ FAISS Ø£Ùˆ QDRANT
- `provider`: HUGGINGFACE Ø£Ùˆ OLLAMA Ø£Ùˆ OPENAI Ù„Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
- `persist_directory`: Ù…ÙƒØ§Ù† ØªØ®Ø²ÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª

**Ø®ÙŠØ§Ø±Ø§Øª RAG:**
- `llm`: Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ù…Ø«Ù„ "llama3.2:3b"ØŒ "gpt-4"ØŒ "mistral-large-2411")
- `k`: Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ù„Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹
- `provider`: OLLAMA Ø£Ùˆ OPENAI Ø£Ùˆ MISTRAL Ø£Ùˆ LMSTUDIO Ø£Ùˆ GOOGLE

### Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆÙØ±ÙŠ LLM Ø§Ù„Ù…Ø®ØªÙ„ÙÙŠÙ†

**OpenAI:**
```python
config = RAGConfig(
    llm="gpt-4",
    k=5,
    provider=Settings.OPENAI,
    api_key=Settings.OPENAI_API_KEY,
    knowledge_base=[FolderSource(path="./data")]
)
```

**Mistral:**
```python
config = RAGConfig(
    llm="mistral-large-2411",
    k=5,
    provider=Settings.MISTRAL,
    api_key=Settings.MISTRAL_API_KEY,
    knowledge_base=[FolderSource(path="./data")]
)
```

## Ø®Ø· Agentic RAG

### Ù…Ø§ Ù‡Ùˆ Agentic RAGØŸ

ÙŠÙˆØ³Ø¹ Agentic RAG RAG Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¯Ù…Ø¬ ÙˆÙƒÙŠÙ„ Ù…Ø³ØªÙ‚Ù„ ÙŠÙ…ÙƒÙ†Ù‡:

- Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª
- ØªØ­Ø¯ÙŠØ¯ Ù…ØªÙ‰ ÙŠØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
- Ø§Ù„ØªÙƒØ±Ø§Ø± Ø®Ù„Ø§Ù„ Ø¯ÙˆØ±Ø§Øª Ø§Ø³ØªØ±Ø¬Ø§Ø¹-ØªÙˆÙ„ÙŠØ¯ Ù…ØªØ¹Ø¯Ø¯Ø©
- Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ø§Ù„ØªÙŠ ØªØªØ·Ù„Ø¨ Ù…ØµØ§Ø¯Ø± Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©

### Ø§Ù„ØªÙ†ÙÙŠØ°

```python
"""Ø®Ø· Agentic RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RAGLight"""

from raglight.rag.simple_agentic_rag_api import AgenticRAGPipeline
from raglight.config.agentic_rag_config import AgenticRAGConfig
from raglight.config.vector_store_config import VectorStoreConfig
from raglight.config.settings import Settings
from raglight.models.data_source_model import FolderSource
from dotenv import load_dotenv

load_dotenv()
Settings.setup_logging()

# ØªÙƒÙˆÙŠÙ† Ù…Ø®Ø²Ù† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
vector_store_config = VectorStoreConfig(
    embedding_model=Settings.DEFAULT_EMBEDDINGS_MODEL,
    api_base=Settings.DEFAULT_OLLAMA_CLIENT,
    provider=Settings.HUGGINGFACE,
    database=Settings.CHROMA,
    persist_directory="./agentic_chroma_db",
    collection_name="agentic_knowledge_base"
)

# ØªÙƒÙˆÙŠÙ† Agentic RAG
config = AgenticRAGConfig(
    provider=Settings.MISTRAL,
    model="mistral-large-2411",
    k=10,
    system_prompt=Settings.DEFAULT_AGENT_PROMPT,
    max_steps=4,  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
    api_key=Settings.MISTRAL_API_KEY,
    knowledge_base=[FolderSource(path="./data")]
)

# Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ÙˆØ§Ù„Ø¨Ù†Ø§Ø¡
print("ØªÙ‡ÙŠØ¦Ø© Ø®Ø· Agentic RAG...")
agentic_rag = AgenticRAGPipeline(config, vector_store_config)

print("Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©...")
agentic_rag.build()

# Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø¹Ù‚Ø¯ ÙŠØªØ·Ù„Ø¨ Ø®Ø·ÙˆØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
query = """
Ù‚Ø§Ø±Ù† Ù‚Ø¯Ø±Ø§Øª RAG Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ùˆ Agentic RAG.
Ù…Ø§ Ù‡ÙŠ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø§Ù„ØªÙŠ Ø³ÙŠÙƒÙˆÙ† ÙÙŠÙ‡Ø§ Agentic RAG Ø£ÙƒØ«Ø± ÙØ§Ø¦Ø¯Ø©ØŸ
"""

print(f"\nØ§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query}")
response = agentic_rag.generate(query)
print(f"\nØ§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©:\n{response}")
```

### Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù€ Agentic RAG

**max_steps**: ÙŠØªØ­ÙƒÙ… ÙÙŠ Ø¹Ø¯Ø¯ ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ù„Ù„ÙˆÙƒÙŠÙ„ ØªÙ†ÙÙŠØ°Ù‡Ø§
```python
# Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø³ÙŠØ·: Ø®Ø·ÙˆØ§Øª Ø£Ù‚Ù„ Ù…Ø·Ù„ÙˆØ¨Ø©
config = AgenticRAGConfig(max_steps=2, ...)

# ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ù‚Ø¯: Ø®Ø·ÙˆØ§Øª Ø£ÙƒØ«Ø± Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡Ø§
config = AgenticRAGConfig(max_steps=10, ...)
```

**Ù…ÙˆØ¬Ù‡ ÙˆÙƒÙŠÙ„ Ù…Ø®ØµØµ**: ØªØ®ØµÙŠØµ Ø³Ù„ÙˆÙƒ Ø§Ù„ÙˆÙƒÙŠÙ„
```python
custom_agent_prompt = """
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¨Ø­Ø«. Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:
1. Ù‚Ø³Ù‘Ù… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ø¥Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© ÙØ±Ø¹ÙŠØ©
2. Ø§Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„ ÙØ±Ø¹ÙŠ
3. Ø§Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø©
4. Ø§Ø°ÙƒØ± Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ù…ÙƒØ§Ù†
"""

config = AgenticRAGConfig(
    system_prompt=custom_agent_prompt,
    ...
)
```

## RAT (Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ø¹Ø²Ø² Ø¨Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹)

### ÙÙ‡Ù… RAT

ÙŠØ¶ÙŠÙ RAT Ø·Ø¨Ù‚Ø© Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ØªØ®ØµØµØ© Ø¥Ù„Ù‰ Ø®Ø· RAG:

1. **Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ (Retrieval)**: Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
2. **Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ (Reasoning)**: Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM Ø§Ø³ØªØ¯Ù„Ø§Ù„ÙŠ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹
3. **Ø§Ù„ØªÙÙƒÙŠØ± (Thinking)**: ØªÙˆÙ„ÙŠØ¯ Ø®Ø·ÙˆØ§Øª Ø§Ø³ØªØ¯Ù„Ø§Ù„ ÙˆØ³ÙŠØ·Ø©
4. **Ø§Ù„ØªÙˆÙ„ÙŠØ¯ (Generation)**: Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø¹ Ø³ÙŠØ§Ù‚ Ù…Ø­Ø³Ù‘Ù†

### Ø§Ù„ØªÙ†ÙÙŠØ°

```python
"""Ø®Ø· RAT Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RAGLight"""

from raglight.rat.simple_rat_api import RATPipeline
from raglight.config.rat_config import RATConfig
from raglight.config.vector_store_config import VectorStoreConfig
from raglight.config.settings import Settings
from raglight.models.data_source_model import FolderSource

Settings.setup_logging()

# ØªÙƒÙˆÙŠÙ† Ù…Ø®Ø²Ù† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
vector_store_config = VectorStoreConfig(
    embedding_model=Settings.DEFAULT_EMBEDDINGS_MODEL,
    api_base=Settings.DEFAULT_OLLAMA_CLIENT,
    provider=Settings.HUGGINGFACE,
    database=Settings.CHROMA,
    persist_directory="./rat_chroma_db",
    collection_name="rat_knowledge_base"
)

# ØªÙƒÙˆÙŠÙ† RAT
config = RATConfig(
    cross_encoder_model=Settings.DEFAULT_CROSS_ENCODER_MODEL,
    llm="llama3.2:3b",
    k=Settings.DEFAULT_K,
    provider=Settings.OLLAMA,
    system_prompt=Settings.DEFAULT_SYSTEM_PROMPT,
    reasoning_llm=Settings.DEFAULT_REASONING_LLM,
    reflection=3,  # Ø¹Ø¯Ø¯ ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
    knowledge_base=[FolderSource(path="./data")]
)

# Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ÙˆØ§Ù„Ø¨Ù†Ø§Ø¡
print("ØªÙ‡ÙŠØ¦Ø© Ø®Ø· RAT...")
pipeline = RATPipeline(config, vector_store_config)

print("Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©...")
pipeline.build()

# Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙŠØªØ·Ù„Ø¨ Ø§Ø³ØªØ¯Ù„Ø§Ù„Ø§Ù‹ Ø¹Ù…ÙŠÙ‚Ø§Ù‹
query = """
Ø­Ù„Ù„ Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø¨ÙŠÙ† RAG Ùˆ Agentic RAG Ùˆ RAT.
Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù‚Ø§ÙŠØ¶Ø§Øª Ù…Ù† Ø­ÙŠØ« Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ØŸ
"""

print(f"\nØ§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query}")
response = pipeline.generate(query)
print(f"\nØ§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©:\n{response}")
```

### Ø®ÙŠØ§Ø±Ø§Øª ØªÙƒÙˆÙŠÙ† RAT

**reflection**: Ø¹Ø¯Ø¯ ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
```python
# Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø³Ø±ÙŠØ¹
config = RATConfig(reflection=1, ...)

# ØªÙÙƒÙŠØ± ØªØ­Ù„ÙŠÙ„ÙŠ Ø¹Ù…ÙŠÙ‚
config = RATConfig(reflection=5, ...)
```

**cross_encoder_model**: Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø£ÙØ¶Ù„
```python
config = RATConfig(
    cross_encoder_model="cross-encoder/ms-marco-MiniLM-L-12-v2",
    ...
)
```

## ØªÙƒØ§Ù…Ù„ MCP

### Ù…Ø§ Ù‡Ùˆ MCPØŸ

ÙŠØ³Ù…Ø­ Model Context Protocol (MCP) Ù„Ø®Ø· RAG Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¨Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø¯ÙˆØ§Øª ÙˆØ§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©. Ù‡Ø°Ø§ ÙŠØªÙŠØ­:

- ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙŠØ¨
- Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
- Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª API Ù„Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
- Ø¨ÙŠØ¦Ø§Øª ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙˆØ¯
- ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø®ØµØµØ©

### Ø¥Ø¹Ø¯Ø§Ø¯ Ø®Ø§Ø¯Ù… MCP

Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù‚Ù… Ø¨ØªÙƒÙˆÙŠÙ† Ø®Ø§Ø¯Ù… MCP Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ (Ù…Ø«Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… MCPClient):

```python
"""ØªÙƒÙˆÙŠÙ† Ø®Ø§Ø¯Ù… MCP"""

from raglight.rag.simple_agentic_rag_api import AgenticRAGPipeline
from raglight.config.agentic_rag_config import AgenticRAGConfig
from raglight.config.settings import Settings

# ØªÙƒÙˆÙŠÙ† Ø¹Ù†ÙˆØ§Ù† URL Ù„Ø®Ø§Ø¯Ù… MCP
config = AgenticRAGConfig(
    provider=Settings.OPENAI,
    model="gpt-4o",
    k=10,
    mcp_config=[
        {% raw %}{"url": "http://127.0.0.1:8001/sse"}{% endraw %}  # Ø¹Ù†ÙˆØ§Ù† URL Ù„Ø®Ø§Ø¯Ù… MCP Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ
    ],
    system_prompt=Settings.DEFAULT_AGENT_PROMPT,
    max_steps=4,
    api_key=Settings.OPENAI_API_KEY
)

# Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹ MCP
pipeline = AgenticRAGPipeline(config, vector_store_config)
pipeline.build()

# ÙŠÙ…ÙƒÙ† Ù„Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
query = "Ø§Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙˆÙŠØ¨ Ø¹Ù† Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ø¹Ù„Ù‰ Ø£Ø·Ø± RAG ÙˆÙ„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"
response = pipeline.generate(query)
```

### Ø­Ø§Ù„Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… MCP

**ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙŠØ¨:**
```python
# ÙŠÙ…ÙƒÙ† Ù„Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø¨Ø­Ø« ÙˆØ¯Ù…Ø¬ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙˆÙŠØ¨
query = "Ù…Ø§ Ù‡ÙŠ Ø¢Ø®Ø± Ø§Ù„ØªØ·ÙˆØ±Ø§Øª ÙÙŠ ØªÙ‚Ù†ÙŠØ© RAG ÙÙŠ Ø¹Ø§Ù… 2024ØŸ"
```

**Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:**
```python
# ÙŠÙ…ÙƒÙ† Ù„Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
query = "Ø§Ø³ØªØ±Ø¬Ø¹ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§ ÙˆØ­Ù„Ù„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª"
```

**ØªÙƒØ§Ù…Ù„ API:**
```python
# ÙŠÙ…ÙƒÙ† Ù„Ù„ÙˆÙƒÙŠÙ„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ APIs Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
query = "ØªØ­Ù‚Ù‚ Ù…Ù† API Ø§Ù„Ø·Ù‚Ø³ ÙˆØ£ÙˆØµÙ Ø¨Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª"
```

## Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡

### Ø®ØµØ§Ø¦Øµ Ø®Ø·ÙˆØ· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨

| Ù†ÙˆØ¹ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ | Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ | ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© | Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… |
|----------------|---------|---------------|-----------------|
| **RAG Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ** | Ù…Ù†Ø®ÙØ¶ | Ø³Ø±ÙŠØ¹ (< 5 Ø«ÙˆØ§Ù†ÙŠ) | Q&A Ø¨Ø³ÙŠØ·ØŒ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª |
| **Agentic RAG** | Ù…ØªÙˆØ³Ø· | Ù…Ø¹ØªØ¯Ù„ (5-15 Ø«Ø§Ù†ÙŠØ©) | Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§ØªØŒ Ø¨Ø­Ø« |
| **RAT** | Ø¹Ø§Ù„ÙŠ | Ø¨Ø·ÙŠØ¡ (15-30 Ø«Ø§Ù†ÙŠØ©) | ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ØŒ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…Ø¹Ù‚Ø¯ |
| **RAG + MCP** | Ù…ØªØºÙŠØ± | ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯ÙˆØ§Øª | ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© |

### Ø§Ø®ØªÙŠØ§Ø± Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨

**Ø§Ø³ØªØ®Ø¯Ù… RAG Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¹Ù†Ø¯Ù…Ø§:**
- ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ø³Ø±ÙŠØ¹Ø©
- Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
- Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø³ØªÙ†Ø¯ ÙˆØ§Ø­Ø¯ ÙƒØ§ÙÙ

**Ø§Ø³ØªØ®Ø¯Ù… Agentic RAG Ø¹Ù†Ø¯Ù…Ø§:**
- Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ØªØªØ·Ù„Ø¨ Ø®Ø·ÙˆØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
- ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
- Ø§Ù„Ù…Ù‡Ù…Ø© ØªØªØ¶Ù…Ù† Ø¨Ø­Ø«Ø§Ù‹ Ø£Ùˆ Ø§Ø³ØªÙƒØ´Ø§ÙØ§Ù‹

**Ø§Ø³ØªØ®Ø¯Ù… RAT Ø¹Ù†Ø¯Ù…Ø§:**
- Ù…Ø·Ù„ÙˆØ¨ ØªÙÙƒÙŠØ± ØªØ­Ù„ÙŠÙ„ÙŠ Ø¹Ù…ÙŠÙ‚
- Ø§Ù„Ø¬ÙˆØ¯Ø© Ø£ÙƒØ«Ø± Ø£Ù‡Ù…ÙŠØ© Ù…Ù† Ø§Ù„Ø³Ø±Ø¹Ø©
- Ù…Ø·Ù„ÙˆØ¨ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…Ø¹Ù‚Ø¯ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù‚ÙØ²Ø§Øª

**Ø§Ø³ØªØ®Ø¯Ù… ØªÙƒØ§Ù…Ù„ MCP Ø¹Ù†Ø¯Ù…Ø§:**
- ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ© ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
- Ø§Ù„Ù…Ù‡Ù…Ø© ØªØªØ·Ù„Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª
- Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© Ø¶Ø±ÙˆØ±ÙŠØ©

## Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª

### 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª

**ØªØ­Ø³ÙŠÙ† Ø­Ø¬Ù… Ø§Ù„Ø¬Ø²Ø¡:**
```python
# Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©
chunk_size = 512

# Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø³Ø±Ø¯ÙŠ
chunk_size = 1024
```

**ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª:**
```
knowledge_base/
â”œâ”€â”€ technical_docs/
â”œâ”€â”€ user_manuals/
â”œâ”€â”€ api_reference/
â””â”€â”€ faq/
```

### 2. Ø¥Ø¯Ø§Ø±Ø© Ù…Ø®Ø²Ù† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª

**Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø±ÙŠØ©:**
```python
# Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¯Ø§Ø¦Ù… ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬
vector_store_config = VectorStoreConfig(
    persist_directory="./prod_vectordb",
    collection_name="production_kb"
)
```

**ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª:**
```python
# Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ù†ÙØµÙ„Ø© Ù„Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
collections = {
    "technical": "tech_docs_collection",
    "business": "business_docs_collection",
    "general": "general_knowledge_collection"
}
```

### 3. Ø§Ø®ØªÙŠØ§Ø± LLM

**Ø§Ù„ØªØ·ÙˆÙŠØ±:**
```python
# Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ù„Ù„ØªØ·ÙˆÙŠØ±
config = RAGConfig(
    llm="llama3.2:3b",
    provider=Settings.OLLAMA
)
```

**Ø§Ù„Ø¥Ù†ØªØ§Ø¬:**
```python
# Ø§Ø³ØªØ®Ø¯Ù… Ù†Ù…Ø§Ø°Ø¬ Ø£Ù‚ÙˆÙ‰ Ù„Ù„Ø¥Ù†ØªØ§Ø¬
config = RAGConfig(
    llm="gpt-4",
    provider=Settings.OPENAI
)
```

### 4. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡

```python
"""Ø®Ø· RAG Ù‚ÙˆÙŠ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""

try:
    pipeline = RAGPipeline(config, vector_store_config)
    pipeline.build()
    response = pipeline.generate(query)
except Exception as e:
    print(f"Ø®Ø·Ø£ ÙÙŠ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨: {e}")
    # Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ LLM Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¨Ø¯ÙˆÙ† RAG
    response = fallback_generate(query)
```

### 5. ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…ØªØ¬Ø§Ù‡Ù„Ø©

Ø¹Ù†Ø¯ ÙÙ‡Ø±Ø³Ø© Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª Ø§Ù„ÙƒÙˆØ¯ØŒ Ø§Ø³ØªØ¨Ø¹Ø¯ Ø§Ù„Ø¯Ù„Ø§Ø¦Ù„ ØºÙŠØ± Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:

```python
# Ù…Ø¬Ù„Ø¯Ø§Øª Ù…Ø®ØµØµØ© Ù„Ù„ØªØ¬Ø§Ù‡Ù„
custom_ignore_folders = [
    ".venv",
    "venv",
    "node_modules",
    "__pycache__",
    ".git",
    "build",
    "dist",
    "my_custom_folder_to_ignore"
]

config = AgenticRAGConfig(
    ignore_folders=custom_ignore_folders,
    ...
)
```

### 6. Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØ§Ù„ØªØ³Ø¬ÙŠÙ„

```python
"""ØªÙ…ÙƒÙŠÙ† Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ"""

import logging

# ØªÙƒÙˆÙŠÙ† Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO)

# Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø¥Ø¹Ø¯Ø§Ø¯ RAGLight
Settings.setup_logging()

# Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
import time

start_time = time.time()
response = pipeline.generate(query)
elapsed_time = time.time() - start_time

print(f"ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ {elapsed_time:.2f}Ø«")
```

## Ø§Ù„ØªØ®ØµÙŠØµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…

### Ù…Ù†Ø´Ø¦ Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù…Ø®ØµØµ

```python
"""Ø®Ø· RAG Ù…Ø®ØµØµ Ø¨Ù†Ù…Ø· Ø§Ù„Ù…Ù†Ø´Ø¦"""

from raglight.rag.builder import Builder
from raglight.config.settings import Settings

# Ø¨Ù†Ø§Ø¡ Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù…Ø®ØµØµ
rag = Builder() \
    .with_embeddings(
        Settings.HUGGINGFACE,
        model_name=Settings.DEFAULT_EMBEDDINGS_MODEL
    ) \
    .with_vector_store(
        Settings.CHROMA,
        persist_directory="./custom_db",
        collection_name="custom_collection"
    ) \
    .with_llm(
        Settings.OLLAMA,
        model_name="llama3.2:3b",
        system_prompt_file="./custom_prompt.txt",
        provider=Settings.OLLAMA
    ) \
    .build_rag(k=5)

# Ø§Ø³ØªÙŠØ¹Ø§Ø¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
rag.vector_store.ingest(data_path='./data')

# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
response = rag.generate("Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§")
```

### ÙÙ‡Ø±Ø³Ø© Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„ÙƒÙˆØ¯

```python
"""ÙÙ‡Ø±Ø³Ø© Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª Ø§Ù„ÙƒÙˆØ¯"""

# ÙÙ‡Ø±Ø³Ø© Ø§Ù„ÙƒÙˆØ¯ Ù…Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙˆÙ‚ÙŠØ¹Ø§Øª
rag.vector_store.ingest(repos_path=['./repo1', './repo2'])

# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙˆØ¯
code_results = rag.vector_store.similarity_search("Ù…Ù†Ø·Ù‚ Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø©")

# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªÙˆÙ‚ÙŠØ¹Ø§Øª Ø§Ù„ØµÙ
class_results = rag.vector_store.similarity_search_class("ØªØ¹Ø±ÙŠÙ ØµÙ User")
```

### ØªÙƒØ§Ù…Ù„ Ù…Ø³ØªÙˆØ¯Ø¹ GitHub

```python
"""ÙÙ‡Ø±Ø³Ø© Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª GitHub Ù…Ø¨Ø§Ø´Ø±Ø©"""

from raglight.models.data_source_model import GitHubSource

knowledge_base = [
    GitHubSource(url="https://github.com/Bessouat40/RAGLight"),
    GitHubSource(url="https://github.com/your-org/your-repo")
]

config = RAGConfig(
    knowledge_base=knowledge_base,
    ...
)
```

## Ù†Ø´Ø± Docker

### Ù…Ø«Ø§Ù„ Dockerfile

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# ØªØ«Ø¨ÙŠØª Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Ù†Ø³Ø® Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
COPY . .

# Ø¥Ø¶Ø§ÙØ© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…Ø¶ÙŠÙ Ù„Ù€ Ollama/LMStudio
# Ø§Ù„ØªØ´ØºÙŠÙ„: docker run --add-host=host.docker.internal:host-gateway your-image

CMD ["python", "app.py"]
```

### Ø§Ù„Ø¨Ù†Ø§Ø¡ ÙˆØ§Ù„ØªØ´ØºÙŠÙ„

```bash
# Ø¨Ù†Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø©
docker build -t raglight-app .

# Ø§Ù„ØªØ´ØºÙŠÙ„ Ù…Ø¹ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø´Ø¨ÙƒØ© Ø§Ù„Ù…Ø¶ÙŠÙ (Ù„Ù€ Ollama)
docker run --add-host=host.docker.internal:host-gateway raglight-app
```

## Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ¥ØµÙ„Ø§Ø­Ù‡Ø§

### Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©

**1. Ø®Ø·Ø£ Ø§ØªØµØ§Ù„ Ollama:**
```python
# ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ´ØºÙŠÙ„ Ollama
# macOS/Linux:
ollama serve

# ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© API Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
vector_store_config = VectorStoreConfig(
    api_base="http://localhost:11434",  # Ø¹Ù†ÙˆØ§Ù† URL Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ù€ Ollama
    ...
)
```

**2. Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø©:**
```python
# ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ø¬Ø²Ø¡ ÙˆÙ‚ÙŠÙ…Ø© k
config = RAGConfig(
    k=3,  # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¹Ø¯Ø¯ Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
    ...
)
```

**3. Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø·ÙŠØ¡:**
```python
# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ ØªØ¶Ù…ÙŠÙ† Ø£ØµØºØ±
vector_store_config = VectorStoreConfig(
    embedding_model="all-MiniLM-L6-v2",  # Ù†Ù…ÙˆØ°Ø¬ Ø£ØµØºØ± ÙˆØ£Ø³Ø±Ø¹
    ...
)
```

**4. Ø£Ø®Ø·Ø§Ø¡ Ù…Ø®Ø²Ù† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª:**
```bash
# Ø§Ù„Ù…Ø³Ø­ ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¨Ù†Ø§Ø¡
rm -rf ./chroma_db
python rebuild_kb.py
```

## Ø§Ù„Ø®Ù„Ø§ØµØ©

ÙŠÙˆÙØ± RAGLight Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ Ù‚ÙˆÙŠ ÙˆÙ…Ø±Ù† Ù„Ø¨Ù†Ø§Ø¡ Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø²Ø²Ø© Ø¨Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹. Ø³ÙˆØ§Ø¡ ÙƒÙ†Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Q&A Ø¨Ø³ÙŠØ· Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø£Ùˆ Ø³ÙŠØ± Ø¹Ù…Ù„ ÙˆÙƒÙŠÙ„ Ù…Ø¹Ù‚Ø¯Ø© Ù…Ø¹ ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©ØŒ ÙØ¥Ù† Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ© Ù„Ù€ RAGLight ØªØ¬Ø¹Ù„ Ù…Ù† Ø§Ù„Ø³Ù‡Ù„ Ø§Ù„Ø¨Ù†Ø§Ø¡ ÙˆØ§Ù„ØªÙˆØ³Ø¹.

### Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©

- **Ø§Ø¨Ø¯Ø£ Ø¨Ø¨Ø³Ø§Ø·Ø©**: Ø§Ø¨Ø¯Ø£ Ø¨Ù€ RAG Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙˆÙ‚Ù… Ø¨Ø§Ù„ØªØ±Ù‚ÙŠØ© Ø¥Ù„Ù‰ Agentic RAG Ø£Ùˆ RAT Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©
- **Ø§Ø®ØªØ± Ø¨Ø­ÙƒÙ…Ø©**: Ø§Ø®ØªØ± Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆÙ…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
- **Ø®ØµØµ Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø³Ø¹**: ÙŠØªÙŠØ­ ØªØµÙ…ÙŠÙ… RAGLight Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ Ø§Ù„ØªØ®ØµÙŠØµ Ø§Ù„ÙƒØ§Ù…Ù„
- **Ù‚Ù… Ø¨Ø§Ù„ØªÙˆØ³Ø¹ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹**: Ø§Ø¨Ø¯Ø£ Ù…Ø­Ù„ÙŠØ§Ù‹ Ù…Ø¹ OllamaØŒ Ø«Ù… Ø§Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ Ù…ÙˆÙØ±ÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø© Ù„Ù„Ø¥Ù†ØªØ§Ø¬

### Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©

1. **ØªØ¬Ø±Ø¨Ø©**: Ø¬Ø±Ø¨ Ù…ÙˆÙØ±ÙŠ LLM ÙˆÙ…Ø®Ø§Ø²Ù† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
2. **ØªØ­Ø³ÙŠÙ†**: Ø¶Ø¨Ø· Ù‚ÙŠÙ… k ÙˆØ£Ø­Ø¬Ø§Ù… Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ ÙˆØ§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
3. **Ø¯Ù…Ø¬**: Ø£Ø¶Ù Ø®ÙˆØ§Ø¯Ù… MCP Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
4. **Ù†Ø´Ø±**: Ø§Ø­ØªÙˆÙŠ Ù…Ø¹ Docker Ù„Ù„Ù†Ø´Ø± ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬

### Ø§Ù„Ù…ÙˆØ§Ø±Ø¯

- **RAGLight GitHub**: [https://github.com/Bessouat40/RAGLight](https://github.com/Bessouat40/RAGLight)
- **Ø­Ø²Ù…Ø© PyPI**: [https://pypi.org/project/raglight/](https://pypi.org/project/raglight/)
- **Ollama**: [https://ollama.ai](https://ollama.ai)
- **ChromaDB**: [https://www.trychroma.com](https://www.trychroma.com)
- **Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„ MCP**: Ø§Ø¨Ø­Ø« Ø¹Ù† "Model Context Protocol" Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚

Ø¨Ù†Ø§Ø¡Ù‹ Ø³Ø¹ÙŠØ¯Ø§Ù‹ Ù…Ø¹ RAGLight! ðŸš€

