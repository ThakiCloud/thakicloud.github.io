---
title: "ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุน Docker ู Ollama: ุจูุงุก ุฎุทูุท ุฃูุงุจูุจ ุงูุจุญุซ ุงูุขูู"
excerpt: "ุญูู ุณูุฑ ุนูู ุงูุจุญุซ ุงูุฎุงุต ุจู ูุน ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูู SakanaAI ูุนูู ูู ุจูุฆุฉ OrbStack Docker. ููุถุญ ูุฐุง ุงูุฏููู ุงูุดุงูู ููููุฉ ุฅุนุฏุงุฏ ุฎุทูุท ุฃูุงุจูุจ ุงูุจุญุซ ุงูุขูู ุนูู ูุฏุงุฑ 24/7 ุจุงุณุชุฎุฏุงู ุงูููุงุฐุฌ ุงููุบููุฉ ุงููุญููุฉ ูุซู Ollama ู LM Studio ูุน ุฅุฏุงุฑุฉ ุงูุทูุงุจูุฑ ููุชุดุบูู ุงููุณุชูุฑ."
seo_title: "ุฏููู ุฅุนุฏุงุฏ ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู Docker Ollama - ุจูุงุก ุฎุท ุฃูุงุจูุจ ุงูุจุญุซ ุงูุขูู - Thaki Cloud"
seo_description: "ุจุฑูุงูุฌ ุชุนูููู ูุงูู ุญูู ุฅุนุฏุงุฏ ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูู SakanaAI ูุน OrbStack Docker ู Ollama ู LM Studio ููุจุญุซ ุงูุขูู. ูุชุถูู ุฅุฏุงุฑุฉ ุงูุทูุงุจูุฑ ูุงููุฑุงูุจุฉ ูุฃูุซูุฉ ุงูุชุดุบูู ุนูู ูุฏุงุฑ 24/7 ูุน ุฃุฏูุฉ ุงูุชูููุฐ ุงูุนูููุฉ."
date: 2025-09-02
last_modified_at: 2025-09-02
categories:
  - tutorials
tags:
  - ุนุงูู-ุงูุฐูุงุก-ุงูุงุตุทูุงุนู
  - Docker
  - Ollama
  - LM-Studio
  - ุฃุชูุชุฉ-ุงูุจุญุซ
  - OrbStack
  - ุฅุฏุงุฑุฉ-ุงูุทูุงุจูุฑ
  - ุงูุจุญุซ-ุงูุนููู
author_profile: true
toc: true
toc_label: "ููุฑุณ ุงููุญุชููุงุช"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/ar/tutorials/ai-scientist-docker-ollama-automated-research-pipeline/"
lang: ar
permalink: /ar/tutorials/ai-scientist-docker-ollama-automated-research-pipeline/
---

โฑ๏ธ **ููุช ุงููุฑุงุกุฉ ุงูููุฏุฑ**: 18 ุฏูููุฉ

## ููุฏูุฉ

ุชุฎูู ูุฌูุฏ ุจุงุญุซ ุฐูุงุก ุงุตุทูุงุนู ูุง ููู ููุง ููู ูุนูู ุนูู ูุฏุงุฑ 24/7ุ ููุชุฌ ุงูุฃูุฑุงู ุงูุจุญุซูุฉุ ููุฌุฑู ุงูุชุฌุงุฑุจุ ููุฏูุน ุญุฏูุฏ ุงูุงูุชุดุงู ุงูุนููู ุจูููุง ุฃูุช ูุงุฆู. ูุน **ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูู SakanaAI** ูุงูุจููุฉ ุงูุชุญุชูุฉ ููููุงุฐุฌ ุงููุบููุฉ ุงููุญููุฉุ ูู ุชุนุฏ ูุฐู ุฎูุงูุงู ุนูููุงู - ุจู ูุงูุน ุนููู ููููู ุชูููุฐู ุงูููู.

ุณูุฑุดุฏู ูุฐุง ุงูุฏููู ุงูุดุงูู ุฎูุงู ุฅุนุฏุงุฏ ุฎุท ุฃูุงุจูุจ ุจุญุซ ุขูู ุจุงุณุชุฎุฏุงู:
- **ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูู SakanaAI**: ุฃูู ูุธุงู ูู ุงูุนุงูู ููุงูุชุดุงู ุงูุนููู ุงูุขูู ุจุงููุงูู
- **OrbStack Docker**: ุงูุญุงููุงุช ุฎูููุฉ ุงููุฒู ูููุดุฑ ุงูุณูุณ
- **Ollama ู LM Studio**: ุงูุงุณุชุฏูุงู ุงููุญูู ููููุงุฐุฌ ุงููุบููุฉ ููุจุญุซ ุงููุนุงู ูู ุญูุซ ุงูุชูููุฉ ูุงูุฎุตูุตูุฉ
- **ุฅุฏุงุฑุฉ ุงูุทูุงุจูุฑ**: ุงูุชุดุบูู ุงููุณุชูุฑ ูุน ุฌุฏููุฉ ุงูููุงู ุงูุฐููุฉ

ุจุญููู ููุงูุฉ ูุฐุง ุงูุจุฑูุงูุฌ ุงูุชุนููููุ ุณุชุญุตู ุนูู ุจูุฆุฉ ุจุญุซ ูููุฉ ูููุชููุฉ ุฐุงุชูุงู ูุงุฏุฑุฉ ุนูู ุฅูุชุงุฌ ุงูุฃูุฑุงู ุงูุนูููุฉ ุนุจุฑ ูุฌุงูุงุช ูุชุนุฏุฏุฉ ุฏูู ุชุฏุฎู ุจุดุฑู ูุณุชูุฑ.

## ููู ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู

### ูุง ูุฌุนู ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุซูุฑูุงู

ููุซู [ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูู SakanaAI](https://github.com/SakanaAI/AI-Scientist) ุชุบููุฑุงู ุฌุฐุฑูุงู ูู ุงูุจุญุซ ุงูุขูู. ุนูู ุนูุณ ุฃุฏูุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุงูุชูููุฏูุฉ ุงูุชู ุชุณุงุนุฏ ุงูุจุงุญุซููุ ูุฐุง ุงููุธุงู **ูููู ุจูุดุงุฑูุน ุจุญุซูุฉ ูุงููุฉ ุจุดูู ูุณุชูู**:

- **ุงูุฃุชูุชุฉ ูู ุงูุจุฏุงูุฉ ููููุงูุฉ**: ูู ุชูููุฏ ุงูุฃููุงุฑ ุฅูู ูุชุงุจุฉ ุงูุฃูุฑุงู ููุฑุงุฌุนุฉ ุงูุฃูุฑุงู
- **ุฏุนู ุงูููุงูุจ ุงููุชุนุฏุฏุฉ**: ูุฌุงูุงุช ุจุญุซ NanoGPT ู 2D Diffusion ู Grokking
- **ุงูุชุฌุฑูุจ ุงูุขูู**: ูุตูู ููููุฐ ููุญูู ุงูุชุฌุงุฑุจ
- **ุฅูุชุงุฌ ุฃูุฑุงู LaTeX**: ููุชุฌ ุฃูุฑุงูุงู ุฃูุงุฏูููุฉ ุฌุงูุฒุฉ ูููุดุฑ
- **ูุธุงู ูุฑุงุฌุนุฉ ุงูุฃูุฑุงู**: ุขููุงุช ุชูููู ูุฏูุฌุฉ ูุชูููู ุงูุฌูุฏุฉ

### ูุธุฑุฉ ุนุงูุฉ ุนูู ููุฏุณุฉ ุงููุธุงู

```mermaid
graph TD
    A["๐ฏ ุชูููุฏ ุฃููุงุฑ<br/>ุงูุจุญุซ"] --> B["๐ฌ ุชุตููู<br/>ุงูุชุฌุงุฑุจ"]
    B --> C["โ๏ธ ุชูููุฐ<br/>ุงูููุฏ"]
    C --> D["๐งช ุชูููุฐ<br/>ุงูุชุฌุงุฑุจ"]
    D --> E["๐ ุชุญููู<br/>ุงููุชุงุฆุฌ"]
    E --> F["๐ ูุชุงุจุฉ<br/>ุงููุฑูุฉ"]
    F --> G["๐ ูุฑุงุฌุนุฉ<br/>ุงูุฃูุฑุงู"]
    G --> H["๐ ุงููุฑูุฉ<br/>ุงูููุงุฆูุฉ"]
    
    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#ffebee
    style F fill:#f1f8e9
    style G fill:#fce4ec
    style H fill:#e0f2f1
```

## ุงููุชุทูุจุงุช ุงููุณุจูุฉ ูุฅุนุฏุงุฏ ุงูุจูุฆุฉ

### ูุชุทูุจุงุช ุงููุธุงู

```bash
# ุงูุญุฏ ุงูุฃุฏูู ููุชุทูุจุงุช ุงูุฃุฌูุฒุฉ
- ุงูุฐุงูุฑุฉ: 16GB (32GB ููุตู ุจู ููููุงุฐุฌ ุงูุฃูุจุฑ)
- ุงูุชุฎุฒูู: 50GB ูุณุงุญุฉ ุญุฑุฉ
- ุงููุนุงูุฌ: 8+ ุฃูููุฉ (Apple Silicon ุฃู x86_64)
- ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช: ุงุฎุชูุงุฑูุฉ ููู ููุตู ุจูุง (NVIDIA RTX 3080+ ุฃู Apple M-series)

# ุงูุชุจุนูุงุช ุงูุจุฑูุฌูุฉ
- macOS 13+ ุฃู Linux Ubuntu 20.04+
- OrbStack ุฃู Docker Desktop
- Python 3.8+
- Git
```

### ุชุซุจูุช OrbStack

ูููุฑ OrbStack ุฃุฏุงุกู ูุงุฆูุงู ููุงุฑูุฉ ุจู Docker Desktopุ ุฎุงุตุฉ ุนูู macOS:

```bash
# ุชุซุจูุช OrbStack ุนุจุฑ Homebrew
brew install orbstack

# ุจุฏุก ุฎุฏูุฉ OrbStack
orbstack start

# ุงูุชุญูู ูู ุงูุชุซุจูุช
orbstack --version
```

### ุฅุนุฏุงุฏ Ollama

ูููุฑ Ollama ุญูุงู ููุชุงุฒุงู ููุงุณุชุฏูุงู ุงููุญูู ููููุงุฐุฌ ุงููุบููุฉ:

```bash
# ุชุซุจูุช Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# ุจุฏุก ุฎุฏูุฉ Ollama
ollama serve

# ุชูุฒูู ุงูููุงุฐุฌ ุงูููุตู ุจูุง ููุจุญุซ
ollama pull llama2:70b          # ูููุฐุฌ ุงูุณูุงู ุงููุจูุฑ
ollama pull codellama:34b       # ุชูููุฏ ุงูููุฏ
ollama pull mistral:7b          # ุงูุงุณุชุฏูุงู ุงูุณุฑูุน
ollama pull deepseek-coder:33b  # ุงูุจุฑูุฌุฉ ุงููุชูุฏูุฉ

# ุงูุชุญูู ูู ุงูุชุซุจูุช
ollama list
```

### ุฅุนุฏุงุฏ ุจุฏูู LM Studio

ูุฅุฏุงุฑุฉ ุงูููุงุฐุฌ ุงููุงุฆูุฉ ุนูู ูุงุฌูุฉ ุงููุณุชุฎุฏู ุงูุฑุณูููุฉ:

```bash
# ุชูุฒูู LM Studio ูู https://lmstudio.ai/
# ุชุซุจูุช ูุชูููู ุฎุงุฏู API
# ููุทุฉ ุงูููุงูุฉ ุงูุงูุชุฑุงุถูุฉ ูู API: http://localhost:1234/v1
```

## ุชุซุจูุช ูุชูููู ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู

### ุงุณุชูุณุงุฎ ูุฅุนุฏุงุฏ ุงููุณุชูุฏุน

```bash
# ุงุณุชูุณุงุฎ ูุณุชูุฏุน ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู
git clone https://github.com/SakanaAI/AI-Scientist.git
cd AI-Scientist

# ุฅูุดุงุก ุฏููู ูุฎุตุต ูุฅุนุฏุงุฏูุง
mkdir -p ~/ai-research-lab
cd ~/ai-research-lab

# ูุณุฎ ูููุงุช ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู
cp -r /path/to/AI-Scientist/* .
```

### ุชูููู ุจูุฆุฉ Docker

ุฅูุดุงุก ุฅุนุฏุงุฏ Docker ุดุงูู:

```dockerfile
# Dockerfile ูุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุน ุฏุนู ุงููููุฐุฌ ุงููุบูู ุงููุญูู
FROM python:3.9-slim

# ุชุซุจูุช ุชุจุนูุงุช ุงููุธุงู
RUN apt-get update && apt-get install -y \
    git \
    wget \
    curl \
    build-essential \
    texlive-full \
    pandoc \
    && rm -rf /var/lib/apt/lists/*

# ุชุนููู ุฏููู ุงูุนูู
WORKDIR /app

# ูุณุฎ ุงููุชุทูุจุงุช ูุชุซุจูุช ุชุจุนูุงุช Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ุชุซุจูุช ุญุฒู ุฅุถุงููุฉ ูููุธุงุฆู ุงููุญุณูุฉ
RUN pip install \
    ollama \
    openai \
    anthropic \
    tiktoken \
    matplotlib \
    seaborn \
    jupyter \
    notebook

# ูุณุฎ ููุฏ ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู
COPY . .

# ุฅูุดุงุก ุงูุฃุฏูุฉ ุงูุถุฑูุฑูุฉ
RUN mkdir -p /app/results /app/logs /app/queue

# ุชุนููู ูุชุบูุฑุงุช ุงูุจูุฆุฉ
ENV PYTHONPATH=/app
ENV OLLAMA_HOST=host.docker.internal:11434
ENV LM_STUDIO_BASE_URL=http://host.docker.internal:1234/v1

# ูุชุญ ุงูููุงูุฐ ูู Jupyter ูุงููุฑุงูุจุฉ
EXPOSE 8888 8080

# ุฅูุดุงุก ุณูุฑูุจุช ุงูุจุฏุก
COPY scripts/startup.sh /startup.sh
RUN chmod +x /startup.sh

CMD ["/startup.sh"]
```

### Docker Compose ููููุฏุณ ุงููุงูู

```yaml
# docker-compose.yml
version: '3.8'

services:
  ai-scientist:
    build: .
    container_name: ai-scientist-main
    volumes:
      - ./results:/app/results
      - ./logs:/app/logs
      - ./queue:/app/queue
      - ./templates:/app/templates
    ports:
      - "8888:8888"  # Jupyter
      - "8080:8080"  # ููุญุฉ ุงููุฑุงูุจุฉ
    environment:
      - OLLAMA_HOST=host.docker.internal:11434
      - LM_STUDIO_BASE_URL=http://host.docker.internal:1234/v1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    depends_on:
      - redis
    networks:
      - ai-research-net

  redis:
    image: redis:7-alpine
    container_name: ai-scientist-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - ai-research-net

  queue-manager:
    build: .
    container_name: ai-scientist-queue
    command: python scripts/queue_manager.py
    volumes:
      - ./queue:/app/queue
      - ./logs:/app/logs
    depends_on:
      - redis
      - ai-scientist
    networks:
      - ai-research-net

  monitoring:
    build: .
    container_name: ai-scientist-monitor
    command: python scripts/monitoring_dashboard.py
    ports:
      - "8081:8081"
    volumes:
      - ./logs:/app/logs
      - ./results:/app/results
    networks:
      - ai-research-net

volumes:
  redis_data:

networks:
  ai-research-net:
    driver: bridge
```

## ุชูุงูู ุงูููุงุฐุฌ ุงููุบููุฉ ุงููุญููุฉ

### ุชูุงูู Ollama API

ุฅูุดุงุก ุนููู ูููุฐุฌ ูุบูู ูุฎุตุต ูู Ollama:

```python
# scripts/ollama_client.py
import requests
import json
from typing import Dict, List, Optional
import logging

class OllamaClient:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.logger = logging.getLogger(__name__)
    
    def generate(self, 
                model: str,
                prompt: str,
                temperature: float = 0.7,
                max_tokens: int = 4000,
                **kwargs) -> str:
        """ุชูููุฏ ุงููุต ุจุงุณุชุฎุฏุงู Ollama API"""
        try:
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens,
                    **kwargs
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=300
            )
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "")
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ุชูููุฏ Ollama: {e}")
            raise
    
    def list_models(self) -> List[str]:
        """ูุงุฆูุฉ ุงูููุงุฐุฌ ุงููุชุงุญุฉ"""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            response.raise_for_status()
            
            models = response.json().get("models", [])
            return [model["name"] for model in models]
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ุณุฑุฏ ุงูููุงุฐุฌ: {e}")
            return []
    
    def chat_completion(self,
                       model: str,
                       messages: List[Dict],
                       temperature: float = 0.7,
                       max_tokens: int = 4000) -> str:
        """ุฅููุงู ุงููุญุงุฏุซุฉ ุงููุชูุงูู ูุน OpenAI"""
        try:
            # ุชุญููู ุงูุฑุณุงุฆู ุฅูู ูุทุงูุจุฉ ูุงุญุฏุฉ
            prompt = self._messages_to_prompt(messages)
            return self.generate(model, prompt, temperature, max_tokens)
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ุฅููุงู ุงููุญุงุฏุซุฉ: {e}")
            raise
    
    def _messages_to_prompt(self, messages: List[Dict]) -> str:
        """ุชุญููู ุชูุณูู ุฑุณุงุฆู OpenAI ุฅูู ูุทุงูุจุฉ"""
        prompt_parts = []
        
        for message in messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            if role == "system":
                prompt_parts.append(f"ุงููุธุงู: {content}")
            elif role == "user":
                prompt_parts.append(f"ุงููุณุชุฎุฏู: {content}")
            elif role == "assistant":
                prompt_parts.append(f"ุงููุณุงุนุฏ: {content}")
        
        prompt_parts.append("ุงููุณุงุนุฏ:")
        return "\n\n".join(prompt_parts)

# ุงุฎุชุจุงุฑ ุงูุนููู
if __name__ == "__main__":
    client = OllamaClient()
    print("ุงูููุงุฐุฌ ุงููุชุงุญุฉ:", client.list_models())
    
    test_response = client.generate(
        model="llama2:7b",
        prompt="ุงุดุฑุญ ุงูุญูุณุจุฉ ุงููููุฉ ุจูุตุทูุญุงุช ุจุณูุทุฉ."
    )
    print("ุงุณุชุฌุงุจุฉ ุงูุงุฎุชุจุงุฑ:", test_response[:200] + "...")
```

### ุชูุงูู LM Studio

```python
# scripts/lm_studio_client.py
import openai
from typing import Dict, List
import logging

class LMStudioClient:
    def __init__(self, base_url: str = "http://localhost:1234/v1"):
        self.client = openai.OpenAI(
            base_url=base_url,
            api_key="lm-studio"  # ูุทููุจ ููู ููุชุฌุงูู
        )
        self.logger = logging.getLogger(__name__)
    
    def generate(self, 
                model: str,
                prompt: str,
                temperature: float = 0.7,
                max_tokens: int = 4000,
                **kwargs) -> str:
        """ุชูููุฏ ุงููุต ุจุงุณุชุฎุฏุงู LM Studio API"""
        try:
            messages = [{"role": "user", "content": prompt}]
            
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ุชูููุฏ LM Studio: {e}")
            raise
    
    def chat_completion(self,
                       model: str,
                       messages: List[Dict],
                       temperature: float = 0.7,
                       max_tokens: int = 4000) -> str:
        """ุฅููุงู ุงููุญุงุฏุซุฉ ุงููุจุงุดุฑ"""
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ุฅููุงู ุงููุญุงุฏุซุฉ: {e}")
            raise
    
    def list_models(self) -> List[str]:
        """ูุงุฆูุฉ ุงูููุงุฐุฌ ุงููุชุงุญุฉ"""
        try:
            models = self.client.models.list()
            return [model.id for model in models.data]
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ุณุฑุฏ ุงูููุงุฐุฌ: {e}")
            return []

# ุงุฎุชุจุงุฑ ุงูุนููู
if __name__ == "__main__":
    client = LMStudioClient()
    print("ุงูููุงุฐุฌ ุงููุชุงุญุฉ:", client.list_models())
    
    test_response = client.generate(
        model="local-model",
        prompt="ุงุดุฑุญ ุงูุชุนูู ุงูุขูู ุจูุตุทูุญุงุช ุจุณูุทุฉ."
    )
    print("ุงุณุชุฌุงุจุฉ ุงูุงุฎุชุจุงุฑ:", test_response[:200] + "...")
```

## ูุธุงู ุฅุฏุงุฑุฉ ุงูุทูุงุจูุฑ

### ุชูููุฐ ุงูุทุงุจูุฑ ุงููุงุฆู ุนูู Redis

```python
# scripts/queue_manager.py
import redis
import json
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum

class TaskStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class ResearchTask:
    id: str
    template: str
    model: str
    num_ideas: int
    priority: int = 1
    created_at: datetime = None
    started_at: datetime = None
    completed_at: datetime = None
    status: TaskStatus = TaskStatus.PENDING
    progress: int = 0
    error_message: str = ""
    results_path: str = ""
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

class QueueManager:
    def __init__(self, redis_host: str = "localhost", redis_port: int = 6379):
        self.redis_client = redis.Redis(
            host=redis_host, 
            port=redis_port, 
            decode_responses=True
        )
        self.logger = logging.getLogger(__name__)
        
        # ููุงุชูุญ ุงูุทุงุจูุฑ
        self.pending_queue = "ai_scientist:pending"
        self.running_queue = "ai_scientist:running"
        self.completed_queue = "ai_scientist:completed"
        self.failed_queue = "ai_scientist:failed"
        self.task_data = "ai_scientist:tasks"
    
    def add_task(self, task: ResearchTask) -> str:
        """ุฅุถุงูุฉ ูููุฉ ุจุญุซ ุฌุฏูุฏุฉ ุฅูู ุงูุทุงุจูุฑ"""
        try:
            # ุชุฎุฒูู ุจูุงูุงุช ุงููููุฉ
            task_json = json.dumps(asdict(task), default=str)
            self.redis_client.hset(self.task_data, task.id, task_json)
            
            # ุฅุถุงูุฉ ุฅูู ุทุงุจูุฑ ุงูุงูุชุธุงุฑ ูุน ุงูุฃููููุฉ
            self.redis_client.zadd(
                self.pending_queue, 
                {task.id: task.priority}
            )
            
            self.logger.info(f"ุชู ุฅุถุงูุฉ ุงููููุฉ {task.id} ุฅูู ุงูุทุงุจูุฑ")
            return task.id
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ุฅุถุงูุฉ ุงููููุฉ: {e}")
            raise
    
    def get_next_task(self) -> Optional[ResearchTask]:
        """ุงูุญุตูู ุนูู ุงููููุฉ ุงูุชุงููุฉ ุฐุงุช ุงูุฃููููุฉ ุงูุนููุง"""
        try:
            # ุงูุญุตูู ุนูู ูููุฉ ุงูุฃููููุฉ ุงูุนููุง
            task_ids = self.redis_client.zrevrange(
                self.pending_queue, 0, 0
            )
            
            if not task_ids:
                return None
            
            task_id = task_ids[0]
            
            # ุงูุงูุชูุงู ุฅูู ุทุงุจูุฑ ุงูุชุดุบูู
            self.redis_client.zrem(self.pending_queue, task_id)
            self.redis_client.sadd(self.running_queue, task_id)
            
            # ุงูุญุตูู ุนูู ุจูุงูุงุช ุงููููุฉ
            task_data = self.redis_client.hget(self.task_data, task_id)
            if not task_data:
                return None
            
            task_dict = json.loads(task_data)
            task = ResearchTask(**task_dict)
            task.status = TaskStatus.RUNNING
            task.started_at = datetime.now()
            
            # ุชุญุฏูุซ ุงููููุฉ
            self.update_task(task)
            
            return task
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ุงูุญุตูู ุนูู ุงููููุฉ ุงูุชุงููุฉ: {e}")
            return None
    
    def update_task(self, task: ResearchTask):
        """ุชุญุฏูุซ ุญุงูุฉ ูุจูุงูุงุช ุงููููุฉ"""
        try:
            task_json = json.dumps(asdict(task), default=str)
            self.redis_client.hset(self.task_data, task.id, task_json)
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ุชุญุฏูุซ ุงููููุฉ: {e}")
    
    def complete_task(self, task_id: str, results_path: str = ""):
        """ูุถุน ุนูุงูุฉ ุนูู ุงููููุฉ ูููุชููุฉ"""
        try:
            task = self.get_task(task_id)
            if not task:
                return
            
            # ุงูุงูุชูุงู ุฅูู ุทุงุจูุฑ ุงูููุชููุฉ
            self.redis_client.srem(self.running_queue, task_id)
            self.redis_client.sadd(self.completed_queue, task_id)
            
            # ุชุญุฏูุซ ุงููููุฉ
            task.status = TaskStatus.COMPLETED
            task.completed_at = datetime.now()
            task.progress = 100
            task.results_path = results_path
            
            self.update_task(task)
            self.logger.info(f"ุชู ุฅููุงู ุงููููุฉ {task_id}")
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ุฅููุงู ุงููููุฉ: {e}")
    
    def fail_task(self, task_id: str, error_message: str = ""):
        """ูุถุน ุนูุงูุฉ ุนูู ุงููููุฉ ููุงุดูุฉ"""
        try:
            task = self.get_task(task_id)
            if not task:
                return
            
            # ุงูุงูุชูุงู ุฅูู ุทุงุจูุฑ ุงููุงุดูุฉ
            self.redis_client.srem(self.running_queue, task_id)
            self.redis_client.sadd(self.failed_queue, task_id)
            
            # ุชุญุฏูุซ ุงููููุฉ
            task.status = TaskStatus.FAILED
            task.completed_at = datetime.now()
            task.error_message = error_message
            
            self.update_task(task)
            self.logger.error(f"ูุดูุช ุงููููุฉ {task_id}: {error_message}")
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ูุดู ุงููููุฉ: {e}")
    
    def get_task(self, task_id: str) -> Optional[ResearchTask]:
        """ุงูุญุตูู ุนูู ุงููููุฉ ุจูุงุณุทุฉ ID"""
        try:
            task_data = self.redis_client.hget(self.task_data, task_id)
            if not task_data:
                return None
            
            task_dict = json.loads(task_data)
            return ResearchTask(**task_dict)
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ุงูุญุตูู ุนูู ุงููููุฉ: {e}")
            return None
    
    def get_queue_stats(self) -> Dict[str, int]:
        """ุงูุญุตูู ุนูู ุฅุญุตุงุฆูุงุช ุงูุทุงุจูุฑ"""
        try:
            return {
                "pending": self.redis_client.zcard(self.pending_queue),
                "running": self.redis_client.scard(self.running_queue),
                "completed": self.redis_client.scard(self.completed_queue),
                "failed": self.redis_client.scard(self.failed_queue)
            }
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ุงูุญุตูู ุนูู ุฅุญุตุงุฆูุงุช ุงูุทุงุจูุฑ: {e}")
            return {}
    
    def list_tasks(self, status: TaskStatus = None) -> List[ResearchTask]:
        """ูุงุฆูุฉ ุงูููุงู ุญุณุจ ุงูุญุงูุฉ"""
        try:
            if status == TaskStatus.PENDING:
                task_ids = self.redis_client.zrevrange(self.pending_queue, 0, -1)
            elif status == TaskStatus.RUNNING:
                task_ids = list(self.redis_client.smembers(self.running_queue))
            elif status == TaskStatus.COMPLETED:
                task_ids = list(self.redis_client.smembers(self.completed_queue))
            elif status == TaskStatus.FAILED:
                task_ids = list(self.redis_client.smembers(self.failed_queue))
            else:
                # ุงูุญุตูู ุนูู ุฌููุน ุงูููุงู
                task_ids = list(self.redis_client.hkeys(self.task_data))
            
            tasks = []
            for task_id in task_ids:
                task = self.get_task(task_id)
                if task:
                    tasks.append(task)
            
            return tasks
            
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ุณุฑุฏ ุงูููุงู: {e}")
            return []

# ุนูููุฉ ุงูุนุงูู
class ResearchWorker:
    def __init__(self, queue_manager: QueueManager):
        self.queue_manager = queue_manager
        self.logger = logging.getLogger(__name__)
        self.running = False
    
    def start(self):
        """ุจุฏุก ุนูููุฉ ุงูุนุงูู"""
        self.running = True
        self.logger.info("ุชู ุจุฏุก ุนุงูู ุงูุจุญุซ")
        
        while self.running:
            try:
                task = self.queue_manager.get_next_task()
                
                if task:
                    self.logger.info(f"ูุนุงูุฌุฉ ุงููููุฉ: {task.id}")
                    self.process_task(task)
                else:
                    # ูุง ุชูุฌุฏ ููุงู ูุชุงุญุฉุ ุงูุชุธุงุฑ
                    time.sleep(10)
                    
            except KeyboardInterrupt:
                self.logger.info("ุชู ููุงุทุนุฉ ุงูุนุงูู")
                break
            except Exception as e:
                self.logger.error(f"ุฎุทุฃ ูู ุงูุนุงูู: {e}")
                time.sleep(30)
    
    def process_task(self, task: ResearchTask):
        """ูุนุงูุฌุฉ ูููุฉ ุงูุจุญุซ"""
        try:
            # ุงุณุชูุฑุงุฏ ูุญุฏุงุช ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู
            import subprocess
            import os
            
            # ุชุญุถูุฑ ุงูุฃูุฑ
            cmd = [
                "python", "launch_scientist.py",
                "--model", task.model,
                "--experiment", task.template,
                "--num-ideas", str(task.num_ideas),
                "--out-dir", f"results/{task.id}"
            ]
            
            # ุชุญุฏูุซ ุงูุชูุฏู
            task.progress = 10
            self.queue_manager.update_task(task)
            
            # ุชูููุฐ ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=3600  # ูููุฉ ุฒูููุฉ ุณุงุนุฉ ูุงุญุฏุฉ
            )
            
            if result.returncode == 0:
                # ูุฌุญ
                results_path = f"results/{task.id}"
                self.queue_manager.complete_task(task.id, results_path)
                self.logger.info(f"ุชู ุฅููุงู ุงููููุฉ {task.id} ุจูุฌุงุญ")
            else:
                # ูุดู
                error_msg = result.stderr or "ุฎุทุฃ ุบูุฑ ูุนุฑูู"
                self.queue_manager.fail_task(task.id, error_msg)
                self.logger.error(f"ูุดูุช ุงููููุฉ {task.id}: {error_msg}")
                
        except subprocess.TimeoutExpired:
            self.queue_manager.fail_task(task.id, "ุงูุชูุช ูููุฉ ุงููููุฉ")
        except Exception as e:
            self.queue_manager.fail_task(task.id, str(e))
    
    def stop(self):
        """ุฅููุงู ุนูููุฉ ุงูุนุงูู"""
        self.running = False
        self.logger.info("ุชู ุฅููุงู ุนุงูู ุงูุจุญุซ")

# ุงูุชูููุฐ ุงูุฑุฆูุณู
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # ุชููุฆุฉ ูุฏูุฑ ุงูุทุงุจูุฑ
    queue_manager = QueueManager()
    
    # ุฅูุดุงุก ูุจุฏุก ุงูุนุงูู
    worker = ResearchWorker(queue_manager)
    
    try:
        worker.start()
    except KeyboardInterrupt:
        worker.stop()
```

## ุงููุฑุงูุจุฉ ูุงูุฅุฏุงุฑุฉ

### ููุญุฉ ุงููุนูููุงุช ูู ุงูููุช ุงููุนูู

```python
# scripts/monitoring_dashboard.py
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import time
from datetime import datetime, timedelta
from queue_manager import QueueManager, TaskStatus

st.set_page_config(
    page_title="ููุญุฉ ูุนูููุงุช ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู",
    page_icon="๐งโ๐ฌ",
    layout="wide"
)

class MonitoringDashboard:
    def __init__(self):
        self.queue_manager = QueueManager()
    
    def render_header(self):
        """ุนุฑุถ ุฑุฃุณ ููุญุฉ ุงููุนูููุงุช"""
        st.title("๐งโ๐ฌ ููุญุฉ ูุนูููุงุช ุจุญุซ ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู")
        st.markdown("ูุฑุงูุจุฉ ูู ุงูููุช ุงููุนูู ูุฎุท ุฃูุงุจูุจ ุงูุจุญุซ ุงูุขูู")
        
        # ุฒุฑ ุงูุชุญุฏูุซ
        if st.button("๐ ุชุญุฏูุซ", key="refresh"):
            st.rerun()
    
    def render_queue_stats(self):
        """ุนุฑุถ ุฅุญุตุงุฆูุงุช ุงูุทุงุจูุฑ"""
        stats = self.queue_manager.get_queue_stats()
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("โณ ูู ุงูุงูุชุธุงุฑ", stats.get("pending", 0))
        
        with col2:
            st.metric("๐ ููุฏ ุงูุชุดุบูู", stats.get("running", 0))
        
        with col3:
            st.metric("โ ููุชููุฉ", stats.get("completed", 0))
        
        with col4:
            st.metric("โ ูุงุดูุฉ", stats.get("failed", 0))
    
    def render_task_timeline(self):
        """ุนุฑุถ ูุฎุทุท ุงูุฌุฏูู ุงูุฒููู ููููุงู"""
        st.subheader("๐ ุงูุฌุฏูู ุงูุฒููู ููููุงู")
        
        # ุงูุญุตูู ุนูู ุฌููุน ุงูููุงู
        all_tasks = self.queue_manager.list_tasks()
        
        if not all_tasks:
            st.info("ูู ูุชู ุงูุนุซูุฑ ุนูู ููุงู")
            return
        
        # ุชุญุถูุฑ ุงูุจูุงูุงุช ููุฌุฏูู ุงูุฒููู
        timeline_data = []
        for task in all_tasks:
            timeline_data.append({
                "ูุนุฑู ุงููููุฉ": task.id[:8],
                "ุงููุงูุจ": task.template,
                "ุงููููุฐุฌ": task.model,
                "ุงูุญุงูุฉ": task.status.value,
                "ุชู ุงูุฅูุดุงุก": task.created_at,
                "ุชู ุงูุจุฏุก": task.started_at,
                "ุชู ุงูุฅููุงู": task.completed_at,
                "ุงููุฏุฉ": self._calculate_duration(task)
            })
        
        df = pd.DataFrame(timeline_data)
        
        # ูุฎุทุท ุฏุงุฆุฑู ูุชูุฒูุน ุงูุญุงูุฉ
        col1, col2 = st.columns(2)
        
        with col1:
            status_counts = df["ุงูุญุงูุฉ"].value_counts()
            fig_pie = px.pie(
                values=status_counts.values,
                names=status_counts.index,
                title="ุชูุฒูุน ุญุงูุฉ ุงูููุงู"
            )
            st.plotly_chart(fig_pie, use_container_width=True)
        
        with col2:
            # ููุณุชูุบุฑุงู ุงููุฏุฉ
            completed_tasks = df[df["ุงูุญุงูุฉ"] == "completed"]
            if not completed_tasks.empty:
                fig_hist = px.histogram(
                    completed_tasks,
                    x="ุงููุฏุฉ",
                    title="ุชูุฒูุน ูุฏุฉ ุงูููุงู (ุฏูุงุฆู)",
                    nbins=20
                )
                st.plotly_chart(fig_hist, use_container_width=True)
            else:
                st.info("ูุง ุชูุฌุฏ ููุงู ููุชููุฉ ุจุนุฏ")
    
    def render_task_list(self):
        """ุนุฑุถ ูุงุฆูุฉ ุงูููุงู ุงูุชูุตูููุฉ"""
        st.subheader("๐ ุชูุงุตูู ุงูููุงู")
        
        # ููุชุฑ ุงูุญุงูุฉ
        status_filter = st.selectbox(
            "ุชุตููุฉ ุญุณุจ ุงูุญุงูุฉ",
            ["ุงููู", "pending", "running", "completed", "failed"]
        )
        
        # ุงูุญุตูู ุนูู ุงูููุงู ุงููููุชุฑุฉ
        if status_filter == "ุงููู":
            tasks = self.queue_manager.list_tasks()
        else:
            tasks = self.queue_manager.list_tasks(TaskStatus(status_filter))
        
        if not tasks:
            st.info(f"ูู ูุชู ุงูุนุซูุฑ ุนูู ููุงู {status_filter}")
            return
        
        # ุฅูุดุงุก ุฌุฏูู ุงูููุงู
        task_data = []
        for task in tasks:
            task_data.append({
                "ุงููุนุฑู": task.id[:8] + "...",
                "ุงููุงูุจ": task.template,
                "ุงููููุฐุฌ": task.model,
                "ุงูุฃููุงุฑ": task.num_ideas,
                "ุงูุญุงูุฉ": task.status.value.title(),
                "ุงูุชูุฏู": f"{task.progress}%",
                "ุชู ุงูุฅูุดุงุก": task.created_at.strftime("%Y-%m-%d %H:%M") if task.created_at else "ุบูุฑ ูุชุงุญ",
                "ุงููุฏุฉ": self._calculate_duration(task),
                "ุงูุฎุทุฃ": task.error_message[:50] + "..." if len(task.error_message) > 50 else task.error_message
            })
        
        df = pd.DataFrame(task_data)
        st.dataframe(df, use_container_width=True)
    
    def render_resource_usage(self):
        """ุนุฑุถ ููุงููุณ ุงุณุชุฎุฏุงู ุงูููุงุฑุฏ"""
        st.subheader("๐ป ุงุณุชุฎุฏุงู ุงูููุงุฑุฏ")
        
        # ูุฌุจ ุฃู ูุชุตู ุจููุงููุณ ุงููุธุงู ุงููุนููุฉ
        # ูุฃุบุฑุงุถ ุงูุนุฑุถ ุงูุชูุถูุญูุ ุนุฑุถ ุจูุงูุงุช ููููุฉ
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # ูุญุงูุงุฉ ุงุณุชุฎุฏุงู ุงููุนุงูุฌ
            cpu_usage = 65  # ูุฌุจ ุฃู ูุฃุชู ูู ุงููุฑุงูุจุฉ ุงููุนููุฉ
            fig_cpu = go.Figure(go.Indicator(
                mode="gauge+number",
                value=cpu_usage,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "ุงุณุชุฎุฏุงู ุงููุนุงูุฌ %"},
                gauge={'axis': {'range': [None, 100]},
                       'bar': {'color': "darkblue"},
                       'steps': [
                           {'range': [0, 50], 'color': "lightgray"},
                           {'range': [50, 80], 'color': "yellow"},
                           {'range': [80, 100], 'color': "red"}
                       ]}
            ))
            fig_cpu.update_layout(height=250)
            st.plotly_chart(fig_cpu, use_container_width=True)
        
        with col2:
            # ูุญุงูุงุฉ ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ
            mem_usage = 78
            fig_mem = go.Figure(go.Indicator(
                mode="gauge+number",
                value=mem_usage,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ %"},
                gauge={'axis': {'range': [None, 100]},
                       'bar': {'color': "darkgreen"},
                       'steps': [
                           {'range': [0, 50], 'color': "lightgray"},
                           {'range': [50, 80], 'color': "yellow"},
                           {'range': [80, 100], 'color': "red"}
                       ]}
            ))
            fig_mem.update_layout(height=250)
            st.plotly_chart(fig_mem, use_container_width=True)
        
        with col3:
            # ูุญุงูุงุฉ ุงุณุชุฎุฏุงู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช (ุฅู ูุฌุฏุช)
            gpu_usage = 45
            fig_gpu = go.Figure(go.Indicator(
                mode="gauge+number",
                value=gpu_usage,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "ุงุณุชุฎุฏุงู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช %"},
                gauge={'axis': {'range': [None, 100]},
                       'bar': {'color': "darkred"},
                       'steps': [
                           {'range': [0, 50], 'color': "lightgray"},
                           {'range': [50, 80], 'color': "yellow"},
                           {'range': [80, 100], 'color': "red"}
                       ]}
            ))
            fig_gpu.update_layout(height=250)
            st.plotly_chart(fig_gpu, use_container_width=True)
    
    def render_logs(self):
        """ุนุฑุถ ุงูุณุฌูุงุช ุงูุฃุฎูุฑุฉ"""
        st.subheader("๐ ุงูุณุฌูุงุช ุงูุฃุฎูุฑุฉ")
        
        # ูุฌุจ ุฃู ููุฑุฃ ูู ูููุงุช ุงูุณุฌู ุงููุนููุฉ
        # ูุฃุบุฑุงุถ ุงูุนุฑุถ ุงูุชูุถูุญูุ ุนุฑุถ ุงูุจูุงูุงุช ุงูููููุฉ
        log_entries = [
            "2025-09-02 14:30:15 - ูุนูููุงุช - ุจุฏุฃุช ูุนุงูุฌุฉ ุงููููุฉ 12345678",
            "2025-09-02 14:28:42 - ูุนูููุงุช - ุชู ุชุญููู ูููุฐุฌ Ollama llama2:70b ุจูุฌุงุญ",
            "2025-09-02 14:25:10 - ูุนูููุงุช - ุชู ุฅุถุงูุฉ ูููุฉ ุฌุฏูุฏุฉ ุฅูู ุงูุทุงุจูุฑ: nanoGPT_lite",
            "2025-09-02 14:22:33 - ูุนูููุงุช - ุชู ุฅููุงู ุงููููุฉ 87654321 ุจูุฌุงุญ",
            "2025-09-02 14:20:15 - ุฎุทุฃ - ูุดูุช ุงููููุฉ 11111111: ุงูุชูุช ูููุฉ ุงูุงุชุตุงู"
        ]
        
        for entry in log_entries:
            level = "ูุนูููุงุช" if "ูุนูููุงุช" in entry else "ุฎุทุฃ" if "ุฎุทุฃ" in entry else "ุชุญุฐูุฑ"
            if level == "ูุนูููุงุช":
                st.info(entry)
            elif level == "ุฎุทุฃ":
                st.error(entry)
            else:
                st.warning(entry)
    
    def _calculate_duration(self, task) -> str:
        """ุญุณุงุจ ูุฏุฉ ุงููููุฉ"""
        if task.completed_at and task.started_at:
            duration = task.completed_at - task.started_at
            return f"{duration.total_seconds() / 60:.1f} ุฏูููุฉ"
        elif task.started_at:
            duration = datetime.now() - task.started_at
            return f"{duration.total_seconds() / 60:.1f} ุฏูููุฉ (ุฌุงุฑูุฉ)"
        else:
            return "ูู ุชุจุฏุฃ"
    
    def run(self):
        """ุชุดุบูู ููุญุฉ ุงููุนูููุงุช"""
        self.render_header()
        
        # ุชุญุฏูุซ ุชููุงุฆู ูู 30 ุซุงููุฉ
        if "last_refresh" not in st.session_state:
            st.session_state.last_refresh = time.time()
        
        if time.time() - st.session_state.last_refresh > 30:
            st.session_state.last_refresh = time.time()
            st.rerun()
        
        # ุงููุญุชูู ุงูุฑุฆูุณู
        self.render_queue_stats()
        st.divider()
        
        self.render_task_timeline()
        st.divider()
        
        self.render_task_list()
        st.divider()
        
        self.render_resource_usage()
        st.divider()
        
        self.render_logs()

# ุชุดุบูู ููุญุฉ ุงููุนูููุงุช
if __name__ == "__main__":
    dashboard = MonitoringDashboard()
    dashboard.run()
```

## ุฃูุซูุฉ ุงูุชูููุฐ ุงูุนููู

### ุงููุซุงู 1: ุจุญุซ ูุชุนุฏุฏ ุงูููุงูุจ ุนูู ูุฏุงุฑ 24/7

```bash
#!/bin/bash
# scripts/deploy_ai_scientist_24_7.sh

echo "๐ ูุดุฑ ุฎุท ุฃูุงุจูุจ ุจุญุซ ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุนูู ูุฏุงุฑ 24/7"

# ุชุนููู ูุชุบูุฑุงุช ุงูุจูุฆุฉ
export OLLAMA_HOST="localhost:11434"
export LM_STUDIO_BASE_URL="http://localhost:1234/v1"

# ุจุฏุก OrbStack
echo "ุจุฏุก OrbStack..."
orbstack start

# ุจุฏุก Ollama
echo "ุจุฏุก Ollama..."
ollama serve &
OLLAMA_PID=$!

# ุงูุชุธุงุฑ ุญุชู ูุตุจุญ Ollama ุฌุงูุฒุงู
echo "ุงูุชุธุงุฑ ุญุชู ูุตุจุญ Ollama ุฌุงูุฒุงู..."
sleep 10

# ุณุญุจ ุงูููุงุฐุฌ ุงููุทููุจุฉ ุฅุฐุง ูู ุชูู ููุฌูุฏุฉ
echo "ุงูุชุฃูุฏ ูู ุชููุฑ ุงูููุงุฐุฌ..."
ollama pull llama2:70b &
ollama pull codellama:34b &
ollama pull mistral:7b &
ollama pull deepseek-coder:33b &
wait

# ุจุฏุก ููุฏุณ Docker Compose
echo "ุจุฏุก ููุฏุณ ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู..."
cd ~/ai-research-lab
docker-compose up -d

# ุงูุชุธุงุฑ ุญุชู ุชุตุจุญ ุงูุฎุฏูุงุช ุฌุงูุฒุฉ
echo "ุงูุชุธุงุฑ ุชููุฆุฉ ุงูุฎุฏูุงุช..."
sleep 30

# ุฅุฑุณุงู ุฏูุนุฉ ุงูุจุญุซ ุงูุฃูููุฉ
echo "ุฅุฑุณุงู ููุงู ุงูุจุญุซ ุงูุฃูููุฉ..."
python scripts/task_submitter.py --batch

# ุจุฏุก ุงูุฌุฏููุฉ ุงูุฐููุฉ
echo "ุจุฏุก ุงูุฌุฏููุฉ ุงูุฐููุฉ..."
python scripts/intelligent_scheduler.py &
SCHEDULER_PID=$!

# ุจุฏุก ูุฑุงูุจุฉ ุงูููุงุฑุฏ
echo "ุจุฏุก ูุฑุงูุจุฉ ุงูููุงุฑุฏ..."
python scripts/resource_monitor.py &
MONITOR_PID=$!

echo "โ ุชู ูุดุฑ ุฎุท ุฃูุงุจูุจ ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุนูู ูุฏุงุฑ 24/7 ุจูุฌุงุญ!"
echo ""
echo "๐ ููุงุท ุงููุตูู:"
echo "  - ููุญุฉ ุงููุฑุงูุจุฉ: http://localhost:8081"
echo "  - ุฏูุชุฑ Jupyter: http://localhost:8888"
echo "  - ุฅุญุตุงุฆูุงุช ุงูุทุงุจูุฑ: ุฑุงุฌุน ุงูุณุฌูุงุช ุฃู ููุญุฉ ุงููุนูููุงุช"
echo ""
echo "๐ ูููุฑุงูุจุฉ:"
echo "  docker-compose logs -f"
echo "  tail -f logs/scheduler.log"
echo "  tail -f logs/resource_monitor.log"
echo ""
echo "๐ ููุชููู:"
echo "  docker-compose down"
echo "  kill $OLLAMA_PID $SCHEDULER_PID $MONITOR_PID"

# ุญูุธ PIDs ููุชูุธูู
echo "$OLLAMA_PID $SCHEDULER_PID $MONITOR_PID" > .ai_scientist_pids

echo "ุฎุท ุงูุฃูุงุจูุจ ูุนูู ุงูุขู ุนูู ูุฏุงุฑ 24/7. ุงุถุบุท Ctrl+C ููุชููู."

# ุงูุชุธุงุฑ ุงูููุงุทุนุฉ
trap 'echo "ุฅููุงู ุฎุท ุฃูุงุจูุจ ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู..."; docker-compose down; kill $OLLAMA_PID $SCHEDULER_PID $MONITOR_PID; exit' INT
while true; do sleep 1; done
```

## ุญู ุงููุดุงูู ูุงูุชุญุณูู

### ุงููุดุงูู ุงูุดุงุฆุนุฉ ูุงูุญููู

#### ุงููุดููุฉ 1: ูุดู ุงุชุตุงู Ollama

```bash
# ุชุดุฎูุต ุงุชุตุงู Ollama
curl http://localhost:11434/api/tags

# ูุญุต ุณุฌูุงุช Ollama
journalctl -u ollama --follow

# ุฅุนุงุฏุฉ ุชุดุบูู ุฎุฏูุฉ Ollama
sudo systemctl restart ollama

# ุงูุจุฏูู: ุฅุนุงุฏุฉ ุงูุชุดุบูู ุงููุฏูู
pkill ollama
ollama serve
```

#### ุงููุดููุฉ 2: ูุดุงูู ุฐุงูุฑุฉ Docker

```yaml
# docker-compose.override.yml
version: '3.8'

services:
  ai-scientist:
    deploy:
      resources:
        limits:
          memory: 16G
          cpus: '8'
        reservations:
          memory: 8G
          cpus: '4'
    environment:
      - MALLOC_ARENA_MAX=2
      - PYTHONHASHSEED=0
```

### ูุตุงุฆุญ ุชุญุณูู ุงูุฃุฏุงุก

1. **ุงุณุชุฑุงุชูุฌูุฉ ุงุฎุชูุงุฑ ุงููููุฐุฌ**:
   ```python
   # ุงุณุชุฎุฏุงู ููุงุฐุฌ ุฃุตุบุฑ ููุชุฌุงุฑุจ ุงูุฃูููุฉ
   quick_models = ["mistral:7b", "llama2:13b"]
   
   # ุงุณุชุฎุฏุงู ููุงุฐุฌ ุฃูุจุฑ ููุชุญูู ุงูููุงุฆู
   powerful_models = ["llama2:70b", "deepseek-coder:33b"]
   ```

2. **ุฅุฏุงุฑุฉ ุงูููุงุฑุฏ**:
   ```bash
   # ุชุญุฏูุฏ ุงูููุงู ุงููุชุฒุงููุฉ ุญุณุจ ูุฏุฑุฉ ุงููุธุงู
   export MAX_CONCURRENT_TASKS=2
   
   # ุงุณุชุฎุฏุงู ุชุฎุฒูู ุณุฑูุน ูููููุงุช ุงููุคูุชุฉ
   export TMPDIR=/tmp/ai-scientist-fast
   mkdir -p $TMPDIR
   ```

## ุงูุฎูุงุตุฉ

ูููุฑ ูุฐุง ุงูุฏููู ุงูุดุงูู ูู ูุง ุชุญุชุงุฌู ูุฅุนุฏุงุฏ ุฎุท ุฃูุงุจูุจ ุจุญุซ ูุชุทูุฑ ูุขูู ุจุงุณุชุฎุฏุงู ุนุงูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูู SakanaAI. ุฅู ุงูุฌูุน ุจูู ุงูููุงุฐุฌ ุงููุบููุฉ ุงููุญููุฉ ูุฅุฏุงุฑุฉ ุงูุทูุงุจูุฑ ุงูุฐููุฉ ูุชุฎุตูุต ุงูููุงุฑุฏ ุงูุชูููู ููุดุฆ ูุธุงูุงู ูููุงู ูุงุฏุฑุงู ุนูู ุฅุฌุฑุงุก ุงูุจุญูุซ ุนูู ูุฏุงุฑ ุงูุณุงุนุฉ.

### ุงูููุงุฆุฏ ุงูุฑุฆูุณูุฉ ุงููุญููุฉ

1. **ุชุดุบูู ุงูุจุญุซ ุนูู ูุฏุงุฑ 24/7**: ุงูุชุดุงู ุนููู ูุณุชูุฑ ุฏูู ุชุฏุฎู ุจุดุฑู
2. **ูุนุงููุฉ ูู ุญูุซ ุงูุชูููุฉ**: ุงูุงุณุชุฏูุงู ุงููุญูู ููููุงุฐุฌ ุงููุบููุฉ ููุบู ุชูุงููู API ููุจุญุซ ูุงุณุน ุงููุทุงู
3. **ุงูุฐูุงุก ุงูุชูููู**: ุงููุธุงู ูุชููู ุชููุงุฆูุงู ูุน ุธุฑูู ุงูุฃุฏุงุก
4. **ุงููุฑุงูุจุฉ ุงูุดุงููุฉ**: ุฑุคูุฉ ูู ุงูููุช ุงููุนูู ูุชูุฏู ุงูุจุญุซ ูุตุญุฉ ุงููุธุงู
5. **ุงูููุฏุณุฉ ุงููุงุจูุฉ ููุชุทููุฑ**: ูุงุจูุฉ ููุชูุณุน ุจุณูููุฉ ูุงุณุชูุนุงุจ ูุฌุงูุงุช ุงูุจุญุซ ุงูุฌุฏูุฏุฉ

ูุณุชูุจู ุงูุจุญุซ ุงูุนููู ุงูุขูู ููุงุ ููุน ูุฐุง ุงูุฅุนุฏุงุฏุ ุฃูุช ูู ุงูููุฏูุฉ ูู ูุฐู ุงูุซูุฑุฉ ุงูุชูููููุฌูุฉ. ุงุจุฏุฃ ูุงุฑุงุซูู ุงูุจุญุซ ุงูููู ูุฏุน ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุฏูุน ุญุฏูุฏ ุงููุนุฑูุฉ ุงูุจุดุฑูุฉ ุจูููุง ุฃูุช ูุงุฆู! ๐งโ๐ฌ๐
