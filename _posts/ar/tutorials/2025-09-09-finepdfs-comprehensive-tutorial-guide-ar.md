---
title: "FinePDFs: Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª PDF Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ù…Ù† Hugging Face"
excerpt: "Ø¥ØªÙ‚Ø§Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª FinePDFs Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 4.7 Ù…Ù„ÙŠÙˆÙ† ÙˆØ«ÙŠÙ‚Ø©. ØªØ¹Ù„Ù… Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ§Ù„ØªØµÙÙŠØ© ÙˆØ§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø£Ù…Ø«Ù„Ø© Ø´Ø§Ù…Ù„Ø© ÙˆØ£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª."
seo_title: "Ø´Ø±Ø­ FinePDFs: Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª PDF - Thaki Cloud"
seo_description: "ØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª FinePDFs Ø¨ÙØ¹Ø§Ù„ÙŠØ© Ù…Ø¹ Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ© ÙˆÙ†ØµØ§Ø¦Ø­ ØªØ­Ø³ÙŠÙ† ÙˆØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ø¨Ø­Ø«."
date: 2025-09-09
categories:
  - tutorials
tags:
  - FinePDFs
  - HuggingFace
  - Ù…Ø¹Ø§Ù„Ø¬Ø©-PDF
  - Ù…Ø¬Ù…ÙˆØ¹Ø©-Ø¨ÙŠØ§Ù†Ø§Øª
  - Ø§Ù„ØªØ¹Ù„Ù…-Ø§Ù„Ø¢Ù„ÙŠ
  - Ø§Ø³ØªØ®Ø±Ø§Ø¬-Ø§Ù„Ù†Øµ
  - Ø¹Ù„ÙˆÙ…-Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
author_profile: true
toc: true
toc_label: "Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª"
lang: ar
permalink: /ar/tutorials/finepdfs-comprehensive-tutorial-guide/
canonical_url: "https://thakicloud.github.io/ar/tutorials/finepdfs-comprehensive-tutorial-guide/"
---

â±ï¸ **ÙˆÙ‚Øª Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù‚Ø¯Ø±**: 12 Ø¯Ù‚ÙŠÙ‚Ø©

## Ù…Ù‚Ø¯Ù…Ø©

ØªÙ…Ø«Ù„ FinePDFs ØªÙ‚Ø¯Ù…Ø§Ù‹ Ø«ÙˆØ±ÙŠØ§Ù‹ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙˆØ§Ø³Ø¹Ø© Ø§Ù„Ù†Ø·Ø§Ù‚ØŒ Ø­ÙŠØ« ØªÙ‚Ø¯Ù… 4.7 Ù…Ù„ÙŠÙˆÙ† ÙˆØ«ÙŠÙ‚Ø© PDF Ù…Ø³ØªØ®Ø±Ø¬Ø© Ø¨Ø­Ø¬Ù… Ø¥Ø¬Ù…Ø§Ù„ÙŠ 67.9 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ØµÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©. Ø³ÙŠØ±Ø´Ø¯Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ø¹Ø¨Ø± Ø¬Ù…ÙŠØ¹ Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ù‡Ø°Ù‡.

### Ù…Ø§ ÙŠØ¬Ø¹Ù„ FinePDFs Ù…Ù…ÙŠØ²Ø©ØŸ

ØªØ¨Ø±Ø² [FinePDFs](https://huggingface.co/datasets/HuggingFaceFW/finepdfs) ÙƒØ¥Ø­Ø¯Ù‰ Ø£Ø´Ù…Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªÙ…Ø¯Ø© Ù…Ù† Ù…Ù„ÙØ§Øª PDF Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ Ø­ÙŠØ« ØªØªÙ…ÙŠØ² Ø¨Ù€:

- **Ù†Ø·Ø§Ù‚ Ø¶Ø®Ù…**: 4.7 Ù…Ù„ÙŠÙˆÙ† ÙˆØ«ÙŠÙ‚Ø© Ø¹Ø¨Ø± 1,808 Ù„ØºØ©
- **Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ‚Ø¯Ù…**: ØªØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª docling Ùˆ RolmOCR Ø§Ù„Ù…ØªØ·ÙˆØ±Ø©
- **ØºÙ†ÙŠØ© Ø¨Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª**: ØªØºØ·ÙŠØ© ÙˆØ§Ø³Ø¹Ø© Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ù„Ù…ÙŠ ÙˆØ§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙˆØ§Ù„ØªÙ‚Ù†ÙŠ
- **ØªØ±Ø®ÙŠØµ Ù…ÙØªÙˆØ­**: ØªØ±Ø®ÙŠØµ ODC-By Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¬Ø§Ø±ÙŠ ÙˆØ§Ù„Ø¨Ø­Ø«ÙŠ
- **ØªØ±ÙƒÙŠØ² Ø§Ù„Ø¬ÙˆØ¯Ø©**: ØªØµÙÙŠØ© Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ ØªÙ†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙŠÙ„

## Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯

### Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©

```bash
# Python 3.8+ Ù…ÙˆØµÙ‰ Ø¨Ù‡
python --version

# Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
pip install datasets transformers torch huggingface_hub
pip install pandas numpy matplotlib seaborn
pip install tqdm rich
```

### ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©

- **Ø°Ø§ÙƒØ±Ø© Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ**: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ 16 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØªØŒ Ù…ÙˆØµÙ‰ Ø¨Ù‡ 32 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª+
- **Ø§Ù„ØªØ®Ø²ÙŠÙ†**: Ù…Ø³Ø§Ø­Ø© Ø­Ø±Ø© 100 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª+ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©
- **Ø§Ù„Ø´Ø¨ÙƒØ©**: Ø¥Ù†ØªØ±Ù†Øª Ù…Ø³ØªÙ‚Ø± Ù„Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„ÙŠ
- **GPU**: Ø§Ø®ØªÙŠØ§Ø±ÙŠ ÙˆÙ„ÙƒÙ† Ù…ÙˆØµÙ‰ Ø¨Ù‡ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©

## Ø§Ù„Ø¨Ø¯Ø¡ Ù…Ø¹ FinePDFs

### ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ

```python
from datasets import load_dataset
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt

# ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)
print("ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª FinePDFs...")
dataset = load_dataset("HuggingFaceFW/finepdfs", "eng_Latn")

print(f"Ø­Ø¬Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {len(dataset['train'])} ÙˆØ«ÙŠÙ‚Ø©")
print(f"Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ØªØ§Ø­Ø©: {dataset['train'].column_names}")

# ÙØ­Øµ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ø£ÙˆÙ„Ù‰
for i, sample in enumerate(dataset['train'].take(3)):
    print(f"\n--- Ø§Ù„Ø¹ÙŠÙ†Ø© {i+1} ---")
    print(f"Ø§Ù„Ù„ØºØ©: {sample['language']}")
    print(f"Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ: {len(sample['text'])} Ø­Ø±Ù")
    print(f"Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù†Øµ: {sample['text'][:200]}...")
```

### Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©

```python
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„Ù„ØºØ§Øª
all_configs = ["eng_Latn", "arb_Arab", "spa_Latn", "fra_Latn", "deu_Latn", 
               "ita_Latn", "por_Latn", "rus_Cyrl", "jpn_Jpan", "kor_Hang"]

language_stats = {}

for config in all_configs[:5]:  # Ø¹ÙŠÙ†Ø© Ù…Ù† Ø£ÙˆÙ„ 5 Ù„ØºØ§Øª
    try:
        ds = load_dataset("HuggingFaceFW/finepdfs", config, split="train")
        language_stats[config] = len(ds)
        print(f"{config}: {len(ds):,} ÙˆØ«ÙŠÙ‚Ø©")
    except Exception as e:
        print(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ {config}: {e}")

# ØªØµÙˆØ± ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù„ØºØ§Øª
plt.figure(figsize=(12, 6))
languages = list(language_stats.keys())
counts = list(language_stats.values())

plt.bar(languages, counts)
plt.title("Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ©")
plt.xlabel("Ø±Ù…Ø² Ø§Ù„Ù„ØºØ©")
plt.ylabel("Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("finepdfs_language_distribution.png", dpi=300, bbox_inches='tight')
plt.show()
```

## Ø¹Ù…Ù„ÙŠØ§Øª Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©

### ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Øµ

```python
import re
from textstat import flesch_reading_ease, flesch_kincaid_grade

def analyze_text_quality(sample_texts, sample_size=1000):
    """ØªØ­Ù„ÙŠÙ„ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Øµ Ù„Ø¹ÙŠÙ†Ø§Øª Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    
    metrics = {
        'avg_length': [],
        'word_count': [],
        'sentence_count': [],
        'reading_ease': [],
        'grade_level': [],
        'special_chars_ratio': []
    }
    
    for text in sample_texts[:sample_size]:
        # Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        text_length = len(text)
        word_count = len(text.split())
        sentence_count = len(re.split(r'[.!?]+', text))
        
        # Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© (Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡)
        try:
            reading_ease = flesch_reading_ease(text)
            grade_level = flesch_kincaid_grade(text)
        except:
            reading_ease = grade_level = 0
        
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©
        special_chars = len(re.findall(r'[^\w\s]', text))
        special_ratio = special_chars / max(text_length, 1)
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        metrics['avg_length'].append(text_length)
        metrics['word_count'].append(word_count)
        metrics['sentence_count'].append(sentence_count)
        metrics['reading_ease'].append(reading_ease)
        metrics['grade_level'].append(grade_level)
        metrics['special_chars_ratio'].append(special_ratio)
    
    return metrics

# ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ¯Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
print("ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Øµ...")
sample_texts = [item['text'] for item in dataset['train'].take(1000)]
quality_metrics = analyze_text_quality(sample_texts)

# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
for metric, values in quality_metrics.items():
    avg_value = sum(values) / len(values)
    print(f"{metric}: {avg_value:.2f}")
```

### Ø§Ù„ØªØµÙÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¬Ø§Ù„

```python
def filter_by_domain(dataset, domain_keywords, min_matches=2):
    """ØªØµÙÙŠØ© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ù…Ø¬Ø§Ù„"""
    
    def contains_domain_keywords(example):
        text_lower = example['text'].lower()
        matches = sum(1 for keyword in domain_keywords if keyword in text_lower)
        return matches >= min_matches
    
    filtered_dataset = dataset.filter(contains_domain_keywords)
    return filtered_dataset

# Ù…Ø«Ø§Ù„: ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ù„Ù…ÙŠ
scientific_keywords = [
    'research', 'study', 'analysis', 'experiment', 'hypothesis',
    'methodology', 'results', 'conclusion', 'peer review',
    'journal', 'publication', 'citation', 'abstract'
]

print("ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ù„Ù…ÙŠ...")
scientific_subset = filter_by_domain(dataset['train'], scientific_keywords)
print(f"Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø¹Ù„Ù…ÙŠØ©: {len(scientific_subset)} / {len(dataset['train'])}")

# Ù…Ø«Ø§Ù„: ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ
legal_keywords = [
    'court', 'judge', 'law', 'legal', 'statute', 'regulation',
    'contract', 'agreement', 'plaintiff', 'defendant',
    'jurisdiction', 'litigation', 'attorney'
]

legal_subset = filter_by_domain(dataset['train'], legal_keywords)
print(f"Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©: {len(legal_subset)} / {len(dataset['train'])}")
```

## Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©

### 1. ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù„ØºÙˆÙŠ

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
import torch

def prepare_training_data(dataset, tokenizer, max_length=512):
    """Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ"""
    
    def tokenize_function(examples):
        # ØªÙ…ÙŠÙŠØ² Ø§Ù„Ù†ØµÙˆØµ
        tokenized = tokenizer(
            examples['text'],
            truncation=True,
            padding=True,
            max_length=max_length,
            return_tensors="pt"
        )
        # ØªØ¹ÙŠÙŠÙ† Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø³Ø¨Ø¨ÙŠ
        tokenized["labels"] = tokenized["input_ids"].clone()
        return tokenized
    
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names
    )
    
    return tokenized_dataset

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø±Ù…Ø² ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬
model_name = "distilgpt2"  # Ù†Ù…ÙˆØ°Ø¬ Ø®ÙÙŠÙ Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(model_name)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© ÙØ±Ø¹ÙŠØ© ØµØºÙŠØ±Ø© Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ)
print("Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
train_subset = dataset['train'].select(range(1000))  # Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø©
tokenized_data = prepare_training_data(train_subset, tokenizer)

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
training_args = TrainingArguments(
    output_dir="./finepdfs-model",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    warmup_steps=100,
    logging_steps=50,
    save_steps=500,
    evaluation_strategy="no",
    save_total_limit=2,
    load_best_model_at_end=False,
)

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¯Ø±Ø¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data,
    tokenizer=tokenizer,
)

print("Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨... (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹)")
# trainer.train()  # Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ¹Ù„ÙŠ
print("Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¬Ø§Ù‡Ø²!")
```

### 2. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def create_document_embeddings(texts, model_name="all-MiniLM-L6-v2"):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"""
    
    model = SentenceTransformer(model_name)
    
    # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø¯ÙØ¹Ø§Øª Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    batch_size = 32
    embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        batch_embeddings = model.encode(batch_texts, show_progress_bar=True)
        embeddings.extend(batch_embeddings)
    
    return np.array(embeddings), model

def find_similar_documents(query, embeddings, texts, model, top_k=5):
    """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø£ÙƒØ«Ø± ØªØ´Ø§Ø¨Ù‡Ø§Ù‹ Ù…Ø¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
    
    # ØªØ±Ù…ÙŠØ² Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
    query_embedding = model.encode([query])
    
    # Ø­Ø³Ø§Ø¨ Ø£ÙˆØ¬Ù‡ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
    similarities = cosine_similarity(query_embedding, embeddings)[0]
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ k Ù…Ø¤Ø´Ø±Ø§Øª
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    results = []
    for idx in top_indices:
        results.append({
            'index': idx,
            'similarity': similarities[idx],
            'text': texts[idx][:200] + "..."
        })
    
    return results

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
print("Ø¥Ù†Ø´Ø§Ø¡ ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚...")
sample_texts = [item['text'] for item in dataset['train'].take(100)]
embeddings, model = create_document_embeddings(sample_texts)

# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ«Ø§Ø¦Ù‚ Ù…ØªØ´Ø§Ø¨Ù‡Ø©
query = "Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"
similar_docs = find_similar_documents(query, embeddings, sample_texts, model)

print(f"\nØ£ÙØ¶Ù„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ù„Ù€: '{query}'")
for i, doc in enumerate(similar_docs):
    print(f"\n{i+1}. Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {doc['similarity']:.3f}")
    print(f"Ø§Ù„Ù†Øµ: {doc['text']}")
```

### 3. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª

```python
def analyze_multilingual_content(language_configs, sample_size=500):
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¹Ø¨Ø± Ø¹Ø¯Ø© Ù„ØºØ§Øª"""
    
    results = {}
    
    for lang_config in language_configs:
        try:
            print(f"ØªØ­Ù…ÙŠÙ„ {lang_config}...")
            ds = load_dataset("HuggingFaceFW/finepdfs", lang_config, split="train")
            
            # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
            sample = ds.take(min(sample_size, len(ds)))
            texts = [item['text'] for item in sample]
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            avg_length = sum(len(text) for text in texts) / len(texts)
            avg_words = sum(len(text.split()) for text in texts) / len(texts)
            
            results[lang_config] = {
                'total_docs': len(ds),
                'avg_length': avg_length,
                'avg_words': avg_words,
                'sample_text': texts[0][:100] + "..." if texts else ""
            }
            
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ {lang_config}: {e}")
            results[lang_config] = {'error': str(e)}
    
    return results

# ØªØ­Ù„ÙŠÙ„ Ø¹Ø¯Ø© Ù„ØºØ§Øª
multilingual_configs = ["eng_Latn", "arb_Arab", "spa_Latn", "fra_Latn"]
multilingual_analysis = analyze_multilingual_content(multilingual_configs)

# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
for lang, stats in multilingual_analysis.items():
    print(f"\n--- {lang} ---")
    if 'error' in stats:
        print(f"Ø®Ø·Ø£: {stats['error']}")
    else:
        print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚: {stats['total_docs']:,}")
        print(f"Ù…ØªÙˆØ³Ø· Ø§Ù„Ø·ÙˆÙ„: {stats['avg_length']:.0f} Ø­Ø±Ù")
        print(f"Ù…ØªÙˆØ³Ø· Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {stats['avg_words']:.0f}")
        print(f"Ø¹ÙŠÙ†Ø©: {stats['sample_text']}")
```

## ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡

### Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©

```python
import gc
from datasets import Dataset

def process_large_dataset_in_chunks(dataset, chunk_size=1000, process_func=None):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø°Ø§ÙƒØ±Ø©"""
    
    total_size = len(dataset)
    results = []
    
    for i in range(0, total_size, chunk_size):
        print(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¬Ø²Ø¡ {i//chunk_size + 1}/{(total_size-1)//chunk_size + 1}")
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø²Ø¡
        chunk = dataset.select(range(i, min(i + chunk_size, total_size)))
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¬Ø²Ø¡
        if process_func:
            chunk_result = process_func(chunk)
            results.append(chunk_result)
        
        # ÙØ±Ø¶ Ø¬Ù…Ø¹ Ø§Ù„Ù‚Ù…Ø§Ù…Ø©
        gc.collect()
    
    return results

def chunk_text_analysis(chunk):
    """Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„Ù„Ø£Ø¬Ø²Ø§Ø¡"""
    texts = [item['text'] for item in chunk]
    
    return {
        'chunk_size': len(texts),
        'avg_length': sum(len(text) for text in texts) / len(texts),
        'total_chars': sum(len(text) for text in texts)
    }

# Ù…Ø«Ø§Ù„: Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£Ø¬Ø²Ø§Ø¡ ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø°Ø§ÙƒØ±Ø©
print("Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£Ø¬Ø²Ø§Ø¡ ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø°Ø§ÙƒØ±Ø©...")
chunk_results = process_large_dataset_in_chunks(
    dataset['train'].select(range(5000)),  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© ÙØ±Ø¹ÙŠØ© Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ
    chunk_size=1000,
    process_func=chunk_text_analysis
)

for i, result in enumerate(chunk_results):
    print(f"Ø§Ù„Ø¬Ø²Ø¡ {i+1}: {result}")
```

### Ø§Ù„ØªØ¯ÙÙ‚ Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©

```python
from datasets import load_dataset

def stream_process_dataset(dataset_name, config, process_func, max_items=None):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„ØªØ¯ÙÙ‚ Ø¯ÙˆÙ† ØªØ­Ù…ÙŠÙ„ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
    
    # ØªØ­Ù…ÙŠÙ„ ÙƒÙ…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯ÙÙ‚
    dataset = load_dataset(dataset_name, config, streaming=True, split="train")
    
    processed_count = 0
    results = []
    
    for item in dataset:
        if max_items and processed_count >= max_items:
            break
        
        result = process_func(item)
        results.append(result)
        processed_count += 1
        
        if processed_count % 100 == 0:
            print(f"ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© {processed_count} Ø¹Ù†ØµØ±...")
    
    return results

def extract_keywords(item):
    """Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
    text = item['text'].lower()
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ø³ÙŠØ· (ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¨Ø·Ø±Ù‚ Ø£ÙƒØ«Ø± ØªØ·ÙˆØ±Ø§Ù‹)
    keywords = []
    for word in ['ai', 'machine learning', 'neural network', 'algorithm']:
        if word in text:
            keywords.append(word)
    return {'keywords': keywords, 'text_length': len(text)}

# Ù…Ø«Ø§Ù„: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ¯ÙÙ‚
print("Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„ØªØ¯ÙÙ‚...")
stream_results = stream_process_dataset(
    "HuggingFaceFW/finepdfs", 
    "eng_Latn",
    extract_keywords,
    max_items=1000
)

print(f"ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© {len(stream_results)} Ø¹Ù†ØµØ± Ø¹Ø¨Ø± Ø§Ù„ØªØ¯ÙÙ‚")
```

## Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©

### 1. Ø¨Ù†Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù†ØµÙˆØµ Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù…Ø¬Ø§Ù„

```python
def build_domain_corpus(dataset, domain_rules, output_path=None):
    """Ø¨Ù†Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù†ØµÙˆØµ Ù…ØªØ®ØµØµØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ø¬Ø§Ù„"""
    
    def evaluate_domain_match(text, rules):
        score = 0
        text_lower = text.lower()
        
        # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (ÙŠØ¬Ø¨ ÙˆØ¬ÙˆØ¯ ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„)
        required = rules.get('required', [])
        if required and any(keyword in text_lower for keyword in required):
            score += 2
        elif required:  # ØªÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ÙƒÙ† Ù„Ù… ØªÙˆØ¬Ø¯
            return 0
        
        # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…ÙƒØ§ÙØ¦Ø©
        bonus = rules.get('bonus', [])
        for keyword in bonus:
            if keyword in text_lower:
                score += 1
        
        # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø¬Ø²Ø§Ø¦ÙŠØ©
        penalty = rules.get('penalty', [])
        for keyword in penalty:
            if keyword in text_lower:
                score -= 2
        
        return max(0, score)
    
    # ØªØ¹Ø±ÙŠÙ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ø¬Ø§Ù„
    scientific_rules = {
        'required': ['research', 'study', 'analysis', 'experiment'],
        'bonus': ['peer review', 'methodology', 'hypothesis', 'statistical'],
        'penalty': ['advertisement', 'promotional', 'buy now']
    }
    
    # ØªØ³Ø¬ÙŠÙ„ ÙˆØªØµÙÙŠØ© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
    scored_docs = []
    for item in dataset:
        score = evaluate_domain_match(item['text'], scientific_rules)
        if score >= 3:  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù„Ù„Ù†Ù‚Ø§Ø·
            scored_docs.append({
                'text': item['text'],
                'language': item.get('language', 'unknown'),
                'domain_score': score
            })
    
    print(f"ØªÙ… Ø¨Ù†Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù†ØµÙˆØµ Ø§Ù„Ù…Ø¬Ø§Ù„ Ù…Ø¹ {len(scored_docs)} ÙˆØ«ÙŠÙ‚Ø©")
    
    if output_path:
        # Ø­ÙØ¸ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
        import json
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(scored_docs, f, ensure_ascii=False, indent=2)
        print(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ÙÙŠ {output_path}")
    
    return scored_docs

# Ø¨Ù†Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù†ØµÙˆØµ Ø¹Ù„Ù…ÙŠØ©
scientific_corpus = build_domain_corpus(
    dataset['train'].take(2000),  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© ÙØ±Ø¹ÙŠØ© Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ
    {},  # Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ø¹Ø±ÙØ© ÙÙŠ Ø§Ù„Ø¯Ø§Ù„Ø©
    "scientific_corpus.json"
)
```

### 2. Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø©

```python
def assess_document_quality(text):
    """ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„ Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"""
    
    # Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    char_count = len(text)
    word_count = len(text.split())
    sentence_count = len(re.split(r'[.!?]+', text))
    
    # Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø¬ÙˆØ¯Ø©
    quality_score = 0
    issues = []
    
    # ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø·ÙˆÙ„
    if word_count < 50:
        issues.append("Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹")
        quality_score -= 2
    elif word_count > 100:
        quality_score += 1
    
    # ÙØ­ÙˆØµØ§Øª Ø§Ù„ØªÙ…Ø§Ø³Ùƒ
    avg_word_length = sum(len(word) for word in text.split()) / max(word_count, 1)
    if avg_word_length > 6:
        issues.append("Ù…ÙØ±Ø¯Ø§Øª Ù…Ø¹Ù‚Ø¯Ø© Ø¨Ø´ÙƒÙ„ Ù…ÙØ±Ø·")
        quality_score -= 1
    
    # ØªÙ†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    unique_words = len(set(text.lower().split()))
    diversity_ratio = unique_words / max(word_count, 1)
    if diversity_ratio > 0.7:
        quality_score += 2
    elif diversity_ratio < 0.3:
        issues.append("ØªÙ†ÙˆØ¹ Ù…ÙØ±Ø¯Ø§Øª Ù…Ù†Ø®ÙØ¶")
        quality_score -= 1
    
    # Ø¶ÙˆØ¶Ø§Ø¡ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©
    special_char_ratio = len(re.findall(r'[^\w\s]', text)) / max(char_count, 1)
    if special_char_ratio > 0.3:
        issues.append("Ø¶ÙˆØ¶Ø§Ø¡ Ø¹Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©")
        quality_score -= 2
    
    return {
        'quality_score': quality_score,
        'issues': issues,
        'metrics': {
            'char_count': char_count,
            'word_count': word_count,
            'sentence_count': sentence_count,
            'diversity_ratio': diversity_ratio,
            'special_char_ratio': special_char_ratio
        }
    }

# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø© Ø¹Ø¨Ø± Ø¹ÙŠÙ†Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
print("ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚...")
quality_results = []

for item in dataset['train'].take(500):
    quality = assess_document_quality(item['text'])
    quality_results.append(quality)

# ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¬ÙˆØ¯Ø©
quality_scores = [result['quality_score'] for result in quality_results]
avg_quality = sum(quality_scores) / len(quality_scores)
high_quality_count = sum(1 for score in quality_scores if score >= 2)

print(f"Ù…ØªÙˆØ³Ø· Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©: {avg_quality:.2f}")
print(f"ÙˆØ«Ø§Ø¦Ù‚ Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©: {high_quality_count}/{len(quality_results)}")
print(f"ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¬ÙˆØ¯Ø©: {Counter(quality_scores)}")
```

## Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ§Ù„ØªØ­Ù‚Ù‚

### Ù†Øµ Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„

```python
#!/usr/bin/env python3
"""
Ù†Øµ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª FinePDFs
ÙŠØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¸Ø§Ø¦Ù ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
"""

import sys
import time
import traceback
from datasets import load_dataset
import pandas as pd
import numpy as np

def test_basic_loading():
    """Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ¸ÙŠÙØ© ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
    print("Ø§Ø®ØªØ¨Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ...")
    try:
        dataset = load_dataset("HuggingFaceFW/finepdfs", "eng_Latn", split="train")
        print(f"âœ“ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ Ù…Ø¹ {len(dataset)} ÙˆØ«ÙŠÙ‚Ø©")
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        first_item = dataset[0]
        required_fields = ['text', 'language']
        for field in required_fields:
            if field not in first_item:
                raise ValueError(f"Ø­Ù‚Ù„ Ù…Ø·Ù„ÙˆØ¨ Ù…ÙÙ‚ÙˆØ¯: {field}")
        
        print("âœ“ ØªÙ… Ø§Ø¬ØªÙŠØ§Ø² Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¨Ù†ÙŠØ© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        return True
        
    except Exception as e:
        print(f"âœ— ÙØ´Ù„ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {e}")
        return False

def test_processing_functions():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø¯ÙˆØ§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    print("\nØ§Ø®ØªØ¨Ø§Ø± Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...")
    try:
        # ØªØ­Ù…ÙŠÙ„ Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø©
        dataset = load_dataset("HuggingFaceFW/finepdfs", "eng_Latn", split="train")
        sample = dataset.take(10)
        
        # Ø§Ø®ØªØ¨Ø§Ø± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ
        texts = [item['text'] for item in sample]
        avg_length = sum(len(text) for text in texts) / len(texts)
        
        if avg_length == 0:
            raise ValueError("Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ ØºÙŠØ± ØµØ­ÙŠØ­Ø©")
        
        print(f"âœ“ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ù†Ø§Ø¬Ø­ (Ù…ØªÙˆØ³Ø· Ø§Ù„Ø·ÙˆÙ„: {avg_length:.0f})")
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØµÙÙŠØ©
        filtered = [text for text in texts if len(text) > 100]
        print(f"âœ“ Ø§Ù„ØªØµÙÙŠØ© Ù†Ø§Ø¬Ø­Ø© ({len(filtered)}/{len(texts)} ÙˆØ«ÙŠÙ‚Ø©)")
        
        return True
        
    except Exception as e:
        print(f"âœ— ÙØ´Ù„ Ø§Ø®ØªØ¨Ø§Ø± Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {e}")
        return False

def test_memory_efficiency():
    """Ø§Ø®ØªØ¨Ø§Ø± ÙƒÙØ§Ø¡Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
    print("\nØ§Ø®ØªØ¨Ø§Ø± ÙƒÙØ§Ø¡Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©...")
    try:
        # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ¯ÙÙ‚
        dataset = load_dataset("HuggingFaceFW/finepdfs", "eng_Latn", 
                             streaming=True, split="train")
        
        count = 0
        for item in dataset:
            count += 1
            if count >= 5:  # Ø§Ø®ØªØ¨Ø§Ø± Ø£ÙˆÙ„ 5 Ø¹Ù†Ø§ØµØ±
                break
        
        if count != 5:
            raise ValueError("ÙØ´Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ¯ÙÙ‚")
        
        print("âœ“ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ¯ÙÙ‚ Ù†Ø§Ø¬Ø­Ø©")
        return True
        
    except Exception as e:
        print(f"âœ— ÙØ´Ù„ Ø§Ø®ØªØ¨Ø§Ø± ÙƒÙØ§Ø¡Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {e}")
        return False

def run_all_tests():
    """ØªØ´ØºÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø´Ø§Ù…Ù„Ø©"""
    print("=" * 50)
    print("Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª FinePDFs")
    print("=" * 50)
    
    tests = [
        test_basic_loading,
        test_processing_functions,
        test_memory_efficiency
    ]
    
    results = []
    start_time = time.time()
    
    for test in tests:
        try:
            result = test()
            results.append(result)
        except Exception as e:
            print(f"âœ— Ø§Ø®ØªØ¨Ø§Ø± {test.__name__} ØªØ¹Ø·Ù„: {e}")
            traceback.print_exc()
            results.append(False)
    
    # Ø§Ù„Ù…Ù„Ø®Øµ
    print("\n" + "=" * 50)
    print("Ù…Ù„Ø®Øµ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±")
    print("=" * 50)
    
    passed = sum(results)
    total = len(results)
    
    print(f"Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¬ØªØ§Ø²Ø©: {passed}/{total}")
    print(f"Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {passed/total*100:.1f}%")
    print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙˆÙ‚Øª: {time.time() - start_time:.1f} Ø«Ø§Ù†ÙŠØ©")
    
    if passed == total:
        print("ğŸ‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ø¬ØªØ§Ø²Øª!")
        return True
    else:
        print("âš ï¸  Ø¨Ø¹Ø¶ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙØ´Ù„Øª. ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø£Ø¹Ù„Ø§Ù‡.")
        return False

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
```

## Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª ÙˆØ§Ù„Ù†ØµØ§Ø¦Ø­

### 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ÙƒÙØ§Ø¡Ø©

```python
# Ø£ÙØ¶Ù„ Ù…Ù…Ø§Ø±Ø³Ø©: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
def load_dataset_efficiently(language_code, max_size=None):
    """ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø«Ù„Ù‰"""
    
    try:
        if max_size and max_size < 10000:
            # Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØµØºÙŠØ±Ø©: ØªØ­Ù…ÙŠÙ„ ÙƒØ§Ù…Ù„
            dataset = load_dataset("HuggingFaceFW/finepdfs", language_code, 
                                 split=f"train[:{max_size}]")
        else:
            # Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø©: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¯ÙÙ‚
            dataset = load_dataset("HuggingFaceFW/finepdfs", language_code, 
                                 streaming=True, split="train")
        
        return dataset
        
    except Exception as e:
        print(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ {language_code}: {e}")
        return None

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
efficient_dataset = load_dataset_efficiently("eng_Latn", max_size=5000)
```

### 2. Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„Ù…ØªØ§Ù†Ø©

```python
def robust_text_processing(text, max_retries=3):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©"""
    
    for attempt in range(max_retries):
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
            if not isinstance(text, str):
                raise TypeError("Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù†Øµ")
            
            if len(text) == 0:
                return {"status": "empty", "result": None}
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ
            processed = {
                "length": len(text),
                "word_count": len(text.split()),
                "status": "success"
            }
            
            return processed
            
        except Exception as e:
            print(f"Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1} ÙØ´Ù„Øª: {e}")
            if attempt == max_retries - 1:
                return {"status": "failed", "error": str(e)}
            
            time.sleep(0.1)  # ØªØ£Ø®ÙŠØ± Ù‚ØµÙŠØ± Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
def process_dataset_safely(dataset, max_items=1000):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
    
    successful = 0
    failed = 0
    results = []
    
    for i, item in enumerate(dataset.take(max_items)):
        try:
            result = robust_text_processing(item['text'])
            if result['status'] == 'success':
                successful += 1
                results.append(result)
            else:
                failed += 1
                
        except Exception as e:
            print(f"ÙØ´Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ù†ØµØ± {i}: {e}")
            failed += 1
    
    print(f"Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {successful} Ù†Ø¬Ø­ØŒ {failed} ÙØ´Ù„")
    return results
```

## Ø§Ù„Ø®Ù„Ø§ØµØ©

ØªÙ…Ø«Ù„ FinePDFs ØªØ­ÙˆÙ„Ø§Ù‹ Ø¬Ø°Ø±ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„Ù‚Ø§Ø¦Ù… Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ØŒ Ø­ÙŠØ« ØªÙ‚Ø¯Ù… ÙˆØµÙˆÙ„Ø§Ù‹ ØºÙŠØ± Ù…Ø³Ø¨ÙˆÙ‚ Ø¥Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù†ØµÙŠ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆÙ…ØªÙ†ÙˆØ¹. ØºØ·Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ:

### Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©

1. **Ø¥ØªÙ‚Ø§Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª**: ÙÙ‡Ù… Ø¨Ù†ÙŠØ© ÙˆÙˆØ¸Ø§Ø¦Ù FinePDFs
2. **Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙØ¹Ø§Ù„Ø©**: ØªÙ‚Ù†ÙŠØ§Øª ÙˆØ§Ø¹ÙŠØ© Ø¨Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª ÙˆØ§Ø³Ø¹Ø© Ø§Ù„Ù†Ø·Ø§Ù‚
3. **Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©**: Ø­Ø§Ù„Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ù‚Ø¹ÙŠØ© Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„
4. **ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø©**: Ø·Ø±Ù‚ Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
5. **Ø§Ù„Ø¯Ø¹Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª**: Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ø§Ù„ØªÙ†ÙˆØ¹ Ø§Ù„Ù„ØºÙˆÙŠ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

### Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©

- **Ø§Ù„ØªØ¬Ø±ÙŠØ¨**: Ø¬Ø±Ø¨ ØªÙƒÙˆÙŠÙ†Ø§Øª Ù„ØºØ§Øª Ù…Ø®ØªÙ„ÙØ© ÙˆÙ…Ø±Ø´Ø­Ø§Øª Ø§Ù„Ù…Ø¬Ø§Ù„
- **Ø§Ù„ØªÙˆØ³Ø¹**: Ø·Ø¨Ù‚ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ÙØ±Ø¹ÙŠØ© Ø£ÙƒØ¨Ø± Ø­Ø³Ø¨ Ù…Ø§ ØªØ³Ù…Ø­ Ø¨Ù‡ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©
- **Ø§Ù„ØªÙƒØ§Ù…Ù„**: Ø§Ø¯Ù…Ø¬ FinePDFs ÙÙŠ Ø®Ø·ÙˆØ· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
- **Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø©**: Ø´Ø§Ø±Ùƒ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª ÙˆØ§Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹

### Ø§Ù„Ù…ØµØ§Ø¯Ø±

- [ØµÙØ­Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª FinePDFs](https://huggingface.co/datasets/HuggingFaceFW/finepdfs)
- [ÙˆØ«Ø§Ø¦Ù‚ Hugging Face Datasets](https://huggingface.co/docs/datasets/)
- [Ù…Ø¹Ø§Ù„Ø¬Ø© PDF Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Docling](https://github.com/DS4SD/docling)

Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù‚Ø§Ø¦Ù… Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù‡Ù†Ø§ Ù…Ø¹ FinePDFs. Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙŠÙˆÙ…! ğŸš€

---

*Ù‡Ù„ Ù„Ø¯ÙŠÙƒ Ø£Ø³Ø¦Ù„Ø© Ø£Ùˆ ØªØ¹Ù„ÙŠÙ‚Ø§ØªØŸ Ø§Ù†Ø¶Ù… Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø´ ÙÙŠ Ù…Ù†ØªØ¯ÙŠØ§Øª Ù…Ø¬ØªÙ…Ø¹ Hugging Face Ø£Ùˆ Ø³Ø§Ù‡Ù… ÙÙŠ Ø§Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.*
