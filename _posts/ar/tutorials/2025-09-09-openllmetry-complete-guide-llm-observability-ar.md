---
title: "ุงูุฏููู ุงูุดุงูู ูู OpenLLMetry: ูุงุจููุฉ ุงูููุงุญุธุฉ ููููุงุฐุฌ ุงููุบููุฉ ุงููุจูุฑุฉ ุจุงุณุชุฎุฏุงู OpenTelemetry"
excerpt: "ุชุนูู ููููุฉ ุชูููุฐ ูุงุจููุฉ ุงูููุงุญุธุฉ ุงูุดุงููุฉ ูุชุทุจููุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุจุงุณุชุฎุฏุงู OpenLLMetryุ ุงูุญู ููุชูุญ ุงููุตุฏุฑ ููุฑุงูุจุฉ ุชุทุจููุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู."
seo_title: "ุฏููู OpenLLMetry: ูุงุจููุฉ ุงูููุงุญุธุฉ ููุฑุงูุจุฉ ุงูููุงุฐุฌ ุงููุบููุฉ - Thaki Cloud"
seo_description: "ุฏููู ุดุงูู ุญูู OpenLLMetry ููุงุจููุฉ ููุงุญุธุฉ ุงูููุงุฐุฌ ุงููุบููุฉ ุงููุจูุฑุฉ. ุชุนูู ุงูุชุซุจูุช ูุงูุชูููู ููุฑุงูุจุฉ ุชุทุจููุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุน ุฃูุซูุฉ ุนูููุฉ."
date: 2025-09-09
categories:
  - tutorials
tags:
  - openllmetry
  - llm-observability
  - opentelemetry
  - ai-monitoring
  - machine-learning
  - python
author_profile: true
toc: true
toc_label: "ุฌุฏูู ุงููุญุชููุงุช"
lang: ar
permalink: /ar/tutorials/openllmetry-complete-guide-llm-observability/
canonical_url: "https://thakicloud.github.io/ar/tutorials/openllmetry-complete-guide-llm-observability/"
---

โฑ๏ธ **ููุช ุงููุฑุงุกุฉ ุงูููุฏุฑ**: 15 ุฏูููุฉ

## ููุฏูุฉ

ูุน ุชุฒุงูุฏ ุชุนููุฏ ุชุทุจููุงุช ุงูููุงุฐุฌ ุงููุบููุฉ ุงููุจูุฑุฉ (LLM) ูุฌุงูุฒูุชูุง ููุฅูุชุงุฌุ ุฃุตุจุญุช ุงููุฑุงูุจุฉ ููุงุจููุฉ ุงูููุงุญุธุฉ ูุชุทูุจุงุช ุฃุณุงุณูุฉ. ูุธูุฑ [OpenLLMetry](https://github.com/traceloop/openllmetry) ูุญู ุดุงูู ูุฌูุจ ูุงุจููุฉ ุงูููุงุญุธุฉ ุนูู ูุณุชูู ุงููุคุณุณุงุช ุฅูู ุชุทุจููุงุช ุงูููุงุฐุฌ ุงููุบููุฉ ุงููุจูุฑุฉ ูู ุฎูุงู ูุนุงููุฑ OpenTelemetry.

OpenLLMetry ูู ููุตุฉ ูุงุจููุฉ ููุงุญุธุฉ ููุชูุญุฉ ุงููุตุฏุฑ ูุตููุฉ ุฎุตูุตุงู ูุชุทุจููุงุช ุงูููุงุฐุฌ ุงููุบููุฉ ุงููุจูุฑุฉ. ูุจูู ุนูู OpenTelemetryุ ูููุฑ ุฑุคูุฉ ูุงููุฉ ูุฃุฏุงุก ุชุทุจูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุงูุชูุงููู ูุงูุณููู ูุน ุงูุญูุงุธ ุนูู ุงูุชูุงูู ูุน ุงูุจููุฉ ุงูุชุญุชูุฉ ุงูุญุงููุฉ ููุงุจููุฉ ุงูููุงุญุธุฉ.

### ููุงุฐุง OpenLLMetry ููู

ุฃุฏูุงุช ุงููุฑุงูุจุฉ ุงูุชูููุฏูุฉ ุชูุตุฑ ุนูุฏูุง ูุชุนูู ุงูุฃูุฑ ุจุชุทุจููุงุช ุงูููุงุฐุฌ ุงููุบููุฉ ุงููุจูุฑุฉ. ูุนุงูุฌ OpenLLMetry ุงูุชุญุฏูุงุช ุงููุฑูุฏุฉ ูุซู:

- **ุชุชุจุน ุงุณุชุฎุฏุงู ุงูุฑููุฒ ุงููููุฒุฉ**: ูุฑุงูุจุฉ ุฑููุฒ ุงูุฅุฏุฎุงู/ุงูุฅุฎุฑุงุฌ ูุงูุชูุงููู ุงููุฑุชุจุทุฉ
- **ุชุญููู ุฒูู ุงูุงุณุชุฌุงุจุฉ**: ุชุชุจุน ุฃููุงุช ุงูุงุณุชุฌุงุจุฉ ุนุจุฑ ูุฒูุฏู ุงูููุงุฐุฌ ุงููุฎุชูููู
- **ูุฑุงูุจุฉ ุงูุฃุฎุทุงุก**: ุงูุชูุงุท ูุชุญููู ุงูุฃุฎุทุงุก ูุงูุฅุฎูุงูุงุช ุงูุฎุงุตุฉ ุจุงูููุงุฐุฌ ุงููุบููุฉ ุงููุจูุฑุฉ
- **ุชุญุณูู ุงูุฃุฏุงุก**: ุชุญุฏูุฏ ุนูุฏ ุงูุงุฒุฏุญุงู ูู ุณูุฑ ุนูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู
- **ุฅุฏุงุฑุฉ ุงูุชูุงููู**: ูุฑุงูุจุฉ ุงูุฅููุงู ุนุจุฑ ูุฒูุฏู ุงูููุงุฐุฌ ุงููุบููุฉ ุงููุชุนุฏุฏุฉ

## ุงููุชุทูุจุงุช ุงูุฃุณุงุณูุฉ

ูุจู ุงูุบูุต ูู OpenLLMetryุ ุชุฃูุฏ ูู ูุฌูุฏ:

- **Python 3.8+** ูุซุจุช ุนูู ูุธุงูู
- **ููู ุฃุณุงุณู** ูููุงููู OpenTelemetry
- **ุชุทุจูู ููุงุฐุฌ ูุบููุฉ ูุจูุฑุฉ** (OpenAIุ Anthropicุ ุฅูุฎ) ูููุฑุงูุจุฉ
- **ุฎูููุฉ ูุงุจููุฉ ุงูููุงุญุธุฉ** (ุงุฎุชูุงุฑูุฉุ ููุฅุนุฏุงุฏุงุช ุงููุชูุฏูุฉ)

## ุงูุฌุฒุก ุงูุฃูู: ุงูุจุฏุก ูุน OpenLLMetry

### ุงูุชุซุจูุช ูุงูุฅุนุฏุงุฏ ุงูุฃุณุงุณู

ููุจุฏุฃ ุจุฃุจุณุท ุฅุนุฏุงุฏ ูููู. ูููุฑ OpenLLMetry SDK ูุฑูุญ ูุฌุนู ุงูุจุฏุก ุณููุงู ููุบุงูุฉ.

#### ุงูุฎุทูุฉ ุงูุฃููู: ุชุซุจูุช OpenLLMetry SDK

```bash
# ุชุซุจูุช SDK ุงูุฃุณุงุณู
pip install traceloop-sdk

# ููุชูุงููุงุช ุงููุญุฏุฏุฉุ ุซุจุช ุงูุญุฒู ุงูุฅุถุงููุฉ
pip install openai anthropic  # ูุซุงู ุนูู ูุฒูุฏู ุงูููุงุฐุฌ ุงููุบููุฉ
```

#### ุงูุฎุทูุฉ ุงูุซุงููุฉ: ุงูุชููุฆุฉ ุงูุฃุณุงุณูุฉ

ุฃุจุณุท ุทุฑููุฉ ูุจุฏุก ูุฑุงูุจุฉ ุชุทุจูู ุงูููุงุฐุฌ ุงููุบููุฉ ุงููุจูุฑุฉ ูู ุจุณุทุฑ ูุงุญุฏ ูู ุงูููุฏ:

```python
from traceloop.sdk import Traceloop

# ุชููุฆุฉ OpenLLMetry ุจุงูุฅุนุฏุงุฏุงุช ุงูุงูุชุฑุงุถูุฉ
Traceloop.init()
```

ููุชุทููุฑ ุงููุญููุ ูุฏ ุชุฑูุฏ ุฑุคูุฉ ุงูุชุชุจุนุงุช ููุฑุงู:

```python
# ุชุนุทูู ุงูุฅุฑุณุงู ุงููุฌูุน ููุฑุคูุฉ ุงูููุฑูุฉ ููุชุชุจุน
Traceloop.init(disable_batch=True)
```

#### ุงูุฎุทูุฉ ุงูุซุงูุซุฉ: ุฃูู ุงุณุชุฏุนุงุก ูุฑุงูุจ ููููุงุฐุฌ ุงููุบููุฉ

ุฅููู ูุซุงู ูุงูู ููุถุญ ุงููุฑุงูุจุฉ ุงูุฃุณุงุณูุฉ:

```python
import openai
from traceloop.sdk import Traceloop

# ุชููุฆุฉ OpenLLMetry
Traceloop.init(disable_batch=True)

# ุชูููู ุนููู OpenAI
client = openai.OpenAI(api_key="your-api-key")

# ุงุณุชุฏุนุงุก ูุฑุงูุจ ููููุงุฐุฌ ุงููุบููุฉ
def generate_response(prompt):
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": prompt}
        ],
        max_tokens=100
    )
    return response.choices[0].message.content

# ุงุฎุชุจุงุฑ ุงูุฏุงูุฉ ุงููุฑุงูุจุฉ
if __name__ == "__main__":
    result = generate_response("ุงุดุฑุญ ุงูุญูุณุจุฉ ุงููููุฉ ุจูุตุทูุญุงุช ุจุณูุทุฉ")
    print(result)
```

ุนูุฏ ุชุดุบูู ูุฐุง ุงูููุฏุ ูููู OpenLLMetry ุชููุงุฆูุงู ุจู:
- ุงูุชูุงุท ุงูุทูุจ ูุงูุงุณุชุฌุงุจุฉ
- ุชุณุฌูู ุงุณุชุฎุฏุงู ุงูุฑููุฒ ูุงูุชูุงููู
- ููุงุณ ุฒูู ุงุณุชุฌุงุจุฉ
- ุชุชุจุน ุฃู ุฃุฎุทุงุก ุชุญุฏุซ

## ุงูุฌุฒุก ุงูุซุงูู: ุงูุชูููู ุงููุชูุฏู

### ุงูุชุชุจุน ุงููุฎุตุต ุจุงุณุชุฎุฏุงู ุงููุฒุฎุฑูุงุช

ูููุฑ OpenLLMetry ูุฒุฎุฑูุงุช ููุชุชุจุน ุงููุฎุตุต ูููุทู ุงูุชุทุจูู:

```python
from traceloop.sdk import Traceloop
from traceloop.sdk.decorators import task, workflow

# ุชููุฆุฉ OpenLLMetry
Traceloop.init()

@workflow(name="document_analysis_pipeline")
def analyze_document(document_text):
    """ุณูุฑ ุงูุนูู ุงูุฑุฆูุณู ูุชุญููู ุงููุซุงุฆู"""
    summary = summarize_text(document_text)
    sentiment = analyze_sentiment(document_text)
    keywords = extract_keywords(document_text)
    
    return {
        "summary": summary,
        "sentiment": sentiment,
        "keywords": keywords
    }

@task(name="text_summarization")
def summarize_text(text):
    """ุชูุฎูุต ุงููุต ุงููุฏุฎู"""
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ูุฎุต ุงููุต ุงูุชุงูู ุจุฅูุฌุงุฒ."},
            {"role": "user", "content": text}
        ],
        max_tokens=150
    )
    return response.choices[0].message.content

@task(name="sentiment_analysis")
def analyze_sentiment(text):
    """ุชุญููู ูุดุงุนุฑ ุงููุต"""
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ุญูู ูุดุงุนุฑ ูุฐุง ุงููุต. ุฃุฌุจ ุจู: ุฅูุฌุงุจูุ ุณูุจูุ ุฃู ูุญุงูุฏ."},
            {"role": "user", "content": text}
        ],
        max_tokens=10
    )
    return response.choices[0].message.content

@task(name="keyword_extraction")
def extract_keywords(text):
    """ุงุณุชุฎุฑุงุฌ ุงููุตุทูุญุงุช ุงูุฑุฆูุณูุฉ ูู ุงููุต"""
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ุงุณุชุฎุฑุฌ 5 ูุตุทูุญุงุช ุฑุฆูุณูุฉ ูู ูุฐุง ุงููุต. ุฃุนุฏูุง ููุงุฆูุฉ ููุตููุฉ ุจููุงุตู."},
            {"role": "user", "content": text}
        ],
        max_tokens=50
    )
    return response.choices[0].message.content
```

### ุงูุชูููู ุงููุงุฆู ุนูู ุงูุจูุฆุฉ

ูููุดุฑ ูู ุงูุฅูุชุงุฌุ ูู ุจุชูููู OpenLLMetry ูู ุฎูุงู ูุชุบูุฑุงุช ุงูุจูุฆุฉ:

```bash
# ุชุนููู ูุชุบูุฑุงุช ุงูุจูุฆุฉ
export TRACELOOP_API_KEY="your-traceloop-api-key"
export TRACELOOP_BATCH_EXPORT="true"
export TRACELOOP_TELEMETRY="false"  # ุชุนุทูู ุงูุชููููุชุฑู ุฅุฐุง ูุฒู ุงูุฃูุฑ
```

```python
import os
from traceloop.sdk import Traceloop

# ุชูููู ุงูุฅูุชุงุฌ
Traceloop.init(
    api_key=os.getenv("TRACELOOP_API_KEY"),
    disable_batch=os.getenv("TRACELOOP_BATCH_EXPORT", "true").lower() == "false",
    telemetry_enabled=os.getenv("TRACELOOP_TELEMETRY", "true").lower() == "true"
)
```

## ุงูุฌุฒุก ุงูุซุงูุซ: ุงูุชูุงูู ูุน ุฃุทุฑ ุงูููุงุฐุฌ ุงููุบููุฉ ุงูุดุงุฆุนุฉ

### ุชูุงูู LangChain

ูุชูุงูู OpenLLMetry ุจุณูุงุณุฉ ูุน ุชุทุจููุงุช LangChain:

```python
from langchain.llms import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from traceloop.sdk import Traceloop

# ุชููุฆุฉ OpenLLMetry
Traceloop.init()

# ุฅูุดุงุก ููููุงุช LangChain
llm = OpenAI(temperature=0.7)
prompt = PromptTemplate(
    input_variables=["topic"],
    template="ุงูุชุจ ุดุฑุญุงู ููุฌุฒุงู ุนู {topic}"
)

# ุฅูุดุงุก ูุชุดุบูู ุงูุณูุณูุฉ
chain = LLMChain(llm=llm, prompt=prompt)

# ุณูุชู ุชุชุจุน ูุฐุง ุชููุงุฆูุงู
result = chain.run(topic="ุงูุชุนูู ุงูุขูู")
print(result)
```

### ุชูุงูู LlamaIndex

ูุชุทุจููุงุช LlamaIndex:

```python
from llama_index import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms import OpenAI
from traceloop.sdk import Traceloop

# ุชููุฆุฉ OpenLLMetry
Traceloop.init()

# ุชูููู LlamaIndex
llm = OpenAI(model="gpt-3.5-turbo")

# ุชุญููู ุงููุซุงุฆู ูุฅูุดุงุก ุงูููุฑุณ
documents = SimpleDirectoryReader("data").load_data()
index = VectorStoreIndex.from_documents(documents)

# ุฅูุดุงุก ูุญุฑู ุงูุงุณุชุนูุงู
query_engine = index.as_query_engine(llm=llm)

# ุงูุงุณุชุนูุงู ูุน ุงูุชุชุจุน ุงูุชููุงุฆู
response = query_engine.query("ูุง ูู ุงูููุงุถูุน ุงูุฑุฆูุณูุฉ ูู ูุฐู ุงููุซุงุฆูุ")
print(response)
```

## ุงูุฌุฒุก ุงูุฑุงุจุน: ูุฑุงูุจุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูุดุนุงุนูุฉ

OpenLLMetry ูุฑุงูุจ ุฃูุถุงู ุนูููุงุช ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูุดุนุงุนูุฉ:

### ุชูุงูู Pinecone

```python
import pinecone
from traceloop.sdk import Traceloop

# ุชููุฆุฉ OpenLLMetry
Traceloop.init()

# ุชููุฆุฉ Pinecone
pinecone.init(
    api_key="your-pinecone-api-key",
    environment="your-environment"
)

# ุฅูุดุงุก ูุฑุฌุน ุงูููุฑุณ
index = pinecone.Index("your-index-name")

# ุนูููุงุช ุดุนุงุนูุฉ ูุฑุงูุจุฉ
def search_similar_documents(query_vector, top_k=5):
    """ุงูุจุญุซ ุนู ูุซุงุฆู ูุดุงุจูุฉ ุจุงุณุชุฎุฏุงู ุงูุชุดุงุจู ุงูุดุนุงุนู"""
    results = index.query(
        vector=query_vector,
        top_k=top_k,
        include_metadata=True
    )
    return results

# ุนูููุฉ ุฅุฏุฑุงุฌ ูุฑุงูุจุฉ
def store_document_embedding(doc_id, embedding, metadata):
    """ุชุฎุฒูู ุชุถููู ุงููุซููุฉ ูู Pinecone"""
    index.upsert([
        (doc_id, embedding, metadata)
    ])
```

### ุชูุงูู Chroma

```python
import chromadb
from traceloop.sdk import Traceloop

# ุชููุฆุฉ OpenLLMetry
Traceloop.init()

# ุชููุฆุฉ ุนููู Chroma
client = chromadb.Client()

# ุงูุญุตูู ุนูู ุฃู ุฅูุดุงุก ูุฌููุนุฉ
collection = client.get_or_create_collection("documents")

# ุงูุนูููุงุช ุงููุฑุงูุจุฉ
def add_documents(documents, embeddings, ids, metadatas):
    """ุฅุถุงูุฉ ูุซุงุฆู ุฅูู ูุฌููุนุฉ Chroma"""
    collection.add(
        documents=documents,
        embeddings=embeddings,
        ids=ids,
        metadatas=metadatas
    )

def query_documents(query_text, n_results=5):
    """ุงุณุชุนูุงู ุงููุซุงุฆู ุงููุดุงุจูุฉ ูู Chroma"""
    results = collection.query(
        query_texts=[query_text],
        n_results=n_results
    )
    return results
```

## ุงูุฌุฒุก ุงูุฎุงูุณ: ุชูุงูู ุฎูููุฉ ูุงุจููุฉ ุงูููุงุญุธุฉ

### ุชูุงูู Datadog

ุฑุจุท OpenLLMetry ุจู Datadog ูููุฑุงูุจุฉ ุนูู ูุณุชูู ุงููุคุณุณุฉ:

```python
from opentelemetry import trace
from opentelemetry.exporter.datadog import DatadogExporter, DatadogSpanProcessor
from opentelemetry.sdk.trace import TracerProvider
from traceloop.sdk import Traceloop

# ุชูููู ูุตุฏุฑ Datadog
tracer_provider = TracerProvider()
datadog_exporter = DatadogExporter(
    agent_url="http://localhost:8126",  # ุฑุงุจุท ูููู Datadog
    service="llm-application"
)

# ุฅุถุงูุฉ ูุนุงูุฌ ูุฏู Datadog
tracer_provider.add_span_processor(
    DatadogSpanProcessor(datadog_exporter)
)

# ุชุนููู ูููุฑ ุงููุชุชุจุน
trace.set_tracer_provider(tracer_provider)

# ุชููุฆุฉ OpenLLMetry ูุน ูุชุชุจุน ูุฎุตุต
Traceloop.init()
```

### ุชูุงูู Honeycomb

ููุงุจููุฉ ููุงุญุธุฉ Honeycomb:

```python
import os
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from traceloop.sdk import Traceloop

# ุชูููู ูุตุฏุฑ Honeycomb
tracer_provider = TracerProvider()

otlp_exporter = OTLPSpanExporter(
    endpoint="https://api.honeycomb.io/v1/traces",
    headers={
        "x-honeycomb-team": os.getenv("HONEYCOMB_API_KEY"),
        "x-honeycomb-dataset": "llm-traces"
    }
)

# ุฅุถุงูุฉ ูุนุงูุฌ ูุฏู ุฏูุนู
tracer_provider.add_span_processor(
    BatchSpanProcessor(otlp_exporter)
)

# ุชุนููู ูููุฑ ุงููุชุชุจุน
trace.set_tracer_provider(tracer_provider)

# ุชููุฆุฉ OpenLLMetry
Traceloop.init()
```

## ุงูุฌุฒุก ุงูุณุงุฏุณ: ุงูููุงููุณ ูุงูุฎุตุงุฆุต ุงููุฎุตุตุฉ

### ุฅุถุงูุฉ ุงูุฎุตุงุฆุต ุงููุฎุตุตุฉ

ุชุญุณูู ุงูุชุชุจุนุงุช ูุน ููุทู ุงูุฃุนูุงู ุงููุฎุตุต:

```python
from traceloop.sdk import Traceloop
from traceloop.sdk.decorators import task
from opentelemetry import trace

# ุชููุฆุฉ OpenLLMetry
Traceloop.init()

@task(name="customer_support_response")
def handle_customer_query(query, customer_id, priority="normal"):
    """ุงูุชุนุงูู ูุน ุงุณุชุนูุงู ุฏุนู ุงูุนููุงุก ูุน ุงูุฎุตุงุฆุต ุงููุฎุตุตุฉ"""
    
    # ุงูุญุตูู ุนูู ุงููุฏู ุงูุญุงูู
    current_span = trace.get_current_span()
    
    # ุฅุถุงูุฉ ุฎุตุงุฆุต ูุฎุตุตุฉ
    current_span.set_attribute("customer.id", customer_id)
    current_span.set_attribute("query.priority", priority)
    current_span.set_attribute("query.length", len(query))
    
    # ุชุญุฏูุฏ ุงููููุฐุฌ ุจูุงุกู ุนูู ุงูุฃููููุฉ
    model = "gpt-4" if priority == "high" else "gpt-3.5-turbo"
    current_span.set_attribute("llm.model", model)
    
    # ุชูููุฏ ุงูุงุณุชุฌุงุจุฉ
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "ุฃูุช ูููู ุฏุนู ุนููุงุก ูููุฏ."},
            {"role": "user", "content": query}
        ]
    )
    
    # ุฅุถุงูุฉ ุฎุตุงุฆุต ุงูุงุณุชุฌุงุจุฉ
    response_text = response.choices[0].message.content
    current_span.set_attribute("response.length", len(response_text))
    current_span.set_attribute("response.satisfactory", "unknown")  # ูููู ุชุญุฏูุฏูุง ุจูุงุณุทุฉ ูููุฐุฌ ML
    
    return response_text
```

### ุฌูุน ุงูููุงููุณ ุงููุฎุตุตุฉ

ุฅูุดุงุก ููุงููุณ ูุฎุตุตุฉ ููุคุดุฑุงุช ุงูุฃุฏุงุก ุงูุฑุฆูุณูุฉ ููุฃุนูุงู:

```python
from opentelemetry import metrics
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader
import time

# ุชูููู ุงูููุงููุณ
metric_reader = PeriodicExportingMetricReader(
    ConsoleMetricExporter(), 
    export_interval_millis=5000
)
metrics.set_meter_provider(MeterProvider(metric_readers=[metric_reader]))

# ุฅูุดุงุก ุนุฏุงุฏุงุช ูุฎุตุตุฉ
meter = metrics.get_meter("llm_application")

# ุฅูุดุงุก ููุงููุณ ูุฎุตุตุฉ
request_counter = meter.create_counter(
    "llm_requests_total",
    description="ุงูุนุฏุฏ ุงูุฅุฌูุงูู ูุทูุจุงุช ุงูููุงุฐุฌ ุงููุบููุฉ"
)

response_time_histogram = meter.create_histogram(
    "llm_response_time",
    description="ููุช ุงุณุชุฌุงุจุฉ ุงูููุงุฐุฌ ุงููุบููุฉ ุจุงูุซูุงูู"
)

token_usage_counter = meter.create_counter(
    "llm_tokens_used",
    description="ุฅุฌูุงูู ุงูุฑููุฒ ุงููุณุชูููุฉ"
)

def monitored_llm_call(prompt, model="gpt-3.5-turbo"):
    """ุงุณุชุฏุนุงุก ุงูููุงุฐุฌ ุงููุบููุฉ ูุน ุงูููุงููุณ ุงููุฎุตุตุฉ"""
    start_time = time.time()
    
    try:
        # ุฒูุงุฏุฉ ุนุฏุงุฏ ุงูุทูุจุงุช
        request_counter.add(1, {"model": model})
        
        # ุงุณุชุฏุนุงุก ุงูููุงุฐุฌ ุงููุบููุฉ
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        
        # ุชุณุฌูู ููุช ุงูุงุณุชุฌุงุจุฉ
        response_time = time.time() - start_time
        response_time_histogram.record(response_time, {"model": model})
        
        # ุชุณุฌูู ุงุณุชุฎุฏุงู ุงูุฑููุฒ
        usage = response.usage
        token_usage_counter.add(usage.total_tokens, {
            "model": model,
            "type": "total"
        })
        token_usage_counter.add(usage.prompt_tokens, {
            "model": model,
            "type": "prompt"
        })
        token_usage_counter.add(usage.completion_tokens, {
            "model": model,
            "type": "completion"
        })
        
        return response.choices[0].message.content
        
    except Exception as e:
        request_counter.add(1, {"model": model, "status": "error"})
        raise
```

## ุงูุฌุฒุก ุงูุณุงุจุน: ุฃูุถู ุงูููุงุฑุณุงุช ููุฅูุชุงุฌ

### ูุนุงูุฌุฉ ุงูุฃุฎุทุงุก ูุงููุฑููุฉ

ุชูููุฐ ูุนุงูุฌุฉ ูููุฉ ููุฃุฎุทุงุก ูุจูุฆุงุช ุงูุฅูุชุงุฌ:

```python
from traceloop.sdk import Traceloop
from opentelemetry import trace
import logging
import sys

# ุชูููู ุงูุชุณุฌูู
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ุชููุฆุฉ OpenLLMetry ูุน ูุนุงูุฌุฉ ุงูุฃุฎุทุงุก
try:
    Traceloop.init(
        disable_batch=False,  # ุชูููู ุงูุชุฌููุน ููุฅูุชุงุฌ
        telemetry_enabled=False  # ุชุนุทูู ุงูุชููููุชุฑู ููุฎุตูุตูุฉ
    )
    logger.info("ุชู ุชููุฆุฉ OpenLLMetry ุจูุฌุงุญ")
except Exception as e:
    logger.error(f"ูุดู ูู ุชููุฆุฉ OpenLLMetry: {e}")
    # ุงููุชุงุจุนุฉ ุจุฏูู ุชุชุจุน ุจุฏูุงู ูู ูุดู ุงูุชุทุจูู
    pass

def safe_llm_call(prompt, max_retries=3, backoff_factor=2):
    """ุงุณุชุฏุนุงุก ุงูููุงุฐุฌ ุงููุบููุฉ ูุน ููุทู ุฅุนุงุฏุฉ ุงููุญุงููุฉ ููุนุงูุฌุฉ ุดุงููุฉ ููุฃุฎุทุงุก"""
    
    span = trace.get_current_span()
    
    for attempt in range(max_retries):
        try:
            span.set_attribute("retry.attempt", attempt + 1)
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                timeout=30  # ุชุนููู ูููุฉ ุฒูููุฉ ููุฅูุชุงุฌ
            )
            
            span.set_attribute("request.successful", True)
            return response.choices[0].message.content
            
        except openai.RateLimitError as e:
            span.set_attribute("error.type", "rate_limit")
            span.set_attribute("error.message", str(e))
            
            if attempt < max_retries - 1:
                wait_time = backoff_factor ** attempt
                logger.warning(f"ุชู ุงููุตูู ูุญุฏ ุงููุนุฏูุ ุงูุชุธุงุฑ {wait_time} ุซุงููุฉ ูุจู ุฅุนุงุฏุฉ ุงููุญุงููุฉ")
                time.sleep(wait_time)
            else:
                span.set_attribute("request.successful", False)
                raise
                
        except openai.APIError as e:
            span.set_attribute("error.type", "api_error")
            span.set_attribute("error.message", str(e))
            span.set_attribute("request.successful", False)
            
            logger.error(f"ุฎุทุฃ ูู API OpenAI: {e}")
            raise
            
        except Exception as e:
            span.set_attribute("error.type", "unknown")
            span.set_attribute("error.message", str(e))
            span.set_attribute("request.successful", False)
            
            logger.error(f"ุฎุทุฃ ุบูุฑ ูุชููุน: {e}")
            raise
```

### ุชุญุณูู ุงูุฃุฏุงุก

ุชุญุณูู OpenLLMetry ููุชุทุจููุงุช ุนุงููุฉ ุงูุฅูุชุงุฌูุฉ:

```python
from traceloop.sdk import Traceloop
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.trace import TracerProvider
import os

# ุชูููู ุนุงูู ุงูุฃุฏุงุก
tracer_provider = TracerProvider()

# ุชูููู ูุนุงูุฌ ุฏูุนู ููุฅูุชุงุฌูุฉ ุงูุนุงููุฉ
batch_processor = BatchSpanProcessor(
    span_exporter=your_exporter,  # ุงููุตุฏุฑ ุงููุฎุชุงุฑ
    max_queue_size=2048,  # ุฒูุงุฏุฉ ุญุฌู ุงูุทุงุจูุฑ
    export_timeout_millis=30000,  # ูููุฉ ุฒูููุฉ 30 ุซุงููุฉ
    max_export_batch_size=512,  # ุฃุญุฌุงู ุฏูุนุงุช ุฃูุจุฑ
    schedule_delay_millis=500  # ุตุงุฏุฑุงุช ุฃูุซุฑ ุชูุฑุงุฑุงู
)

tracer_provider.add_span_processor(batch_processor)

# ุงูุชููุฆุฉ ูุน ุชุญุณููุงุช ุงูุฃุฏุงุก
Traceloop.init(
    disable_batch=False,
    resource_attributes={
        "service.name": "llm-application",
        "service.version": "1.0.0",
        "deployment.environment": os.getenv("ENVIRONMENT", "production")
    }
)
```

### ุงุนุชุจุงุฑุงุช ุงูุฃูุงู ูุงูุฎุตูุตูุฉ

ุชูููุฐ ุฃูุถู ููุงุฑุณุงุช ุงูุฃูุงู:

```python
from traceloop.sdk import Traceloop
from opentelemetry import trace
import hashlib
import re

# ุงูุชููุฆุฉ ูุน ุชูููู ูุฑูุฒ ุนูู ุงูุฎุตูุตูุฉ
Traceloop.init(
    telemetry_enabled=False,  # ุชุนุทูู ุงูุชููููุชุฑู
    api_key=os.getenv("TRACELOOP_API_KEY")  # ุงุณุชุฎุฏุงู ูุชุบูุฑุงุช ุงูุจูุฆุฉ
)

def sanitize_prompt(prompt):
    """ุฅุฒุงูุฉ ุงููุนูููุงุช ุงูุญุณุงุณุฉ ูู ุงููุทุงูุจุงุช ูุจู ุงูุชุชุจุน"""
    
    # ุฅุฒุงูุฉ ุนูุงููู ุงูุจุฑูุฏ ุงูุฅููุชุฑููู
    prompt = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', prompt)
    
    # ุฅุฒุงูุฉ ุฃุฑูุงู ุงููุงุชู
    prompt = re.sub(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', '[PHONE]', prompt)
    
    # ุฅุฒุงูุฉ ุฃุฑูุงู ุจุทุงูุงุช ุงูุงุฆุชูุงู
    prompt = re.sub(r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b', '[CREDIT_CARD]', prompt)
    
    return prompt

def secure_llm_call(prompt, include_prompt_in_trace=False):
    """ุงุณุชุฏุนุงุก ุงูููุงุฐุฌ ุงููุบููุฉ ูุน ุญูุงูุฉ ุงูุฎุตูุตูุฉ"""
    
    span = trace.get_current_span()
    
    # ุชุดููุฑ ุงููุทุงูุจุฉ ุงูุฃุตููุฉ ููุชุชุจุน ุฏูู ูุดู ุงููุญุชูู
    prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()[:16]
    span.set_attribute("prompt.hash", prompt_hash)
    span.set_attribute("prompt.length", len(prompt))
    
    # ุชุถููู ุงููุทุงูุจุฉ ุงูููุธูุฉ ุงุฎุชูุงุฑูุงู
    if include_prompt_in_trace:
        sanitized_prompt = sanitize_prompt(prompt)
        span.set_attribute("prompt.sanitized", sanitized_prompt)
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    
    # ุนุฏู ุชุถููู ูุญุชูู ุงูุงุณุชุฌุงุจุฉ ูู ุงูุชุชุจุนุงุช ููุฎุตูุตูุฉ
    response_text = response.choices[0].message.content
    span.set_attribute("response.length", len(response_text))
    span.set_attribute("response.hash", hashlib.sha256(response_text.encode()).hexdigest()[:16])
    
    return response_text
```

## ุงูุฌุฒุก ุงูุซุงูู: ุงููุฑุงูุจุฉ ูุงูุชูุจูู

### ุฅุนุฏุงุฏ ุงูุชูุจููุงุช

ุชูููู ุงูุชูุจููุงุช ููุดุงูู ุชุทุจููุงุช ุงูููุงุฐุฌ ุงููุบููุฉ ุงูุดุงุฆุนุฉ:

```python
from traceloop.sdk import Traceloop
from opentelemetry import trace
import time

# ุชููุฆุฉ OpenLLMetry
Traceloop.init()

class LLMMonitor:
    def __init__(self):
        self.error_count = 0
        self.total_requests = 0
        self.total_cost = 0.0
        self.response_times = []
    
    def track_request(self, success=True, cost=0.0, response_time=0.0):
        """ุชุชุจุน ููุงููุณ ุงูุทูุจ ููุชูุจูู"""
        self.total_requests += 1
        self.total_cost += cost
        self.response_times.append(response_time)
        
        if not success:
            self.error_count += 1
        
        # ุงูุงุญุชูุงุธ ุจุขุฎุฑ 100 ููุช ุงุณุชุฌุงุจุฉ ููุท ูููุชูุณุท ุงููุชุญุฑู
        if len(self.response_times) > 100:
            self.response_times.pop(0)
        
        # ูุญุต ุดุฑูุท ุงูุชูุจูู
        self.check_alerts()
    
    def check_alerts(self):
        """ูุญุต ุดุฑูุท ุงูุชูุจูู"""
        
        # ุชูุจูู ูุนุฏู ุฎุทุฃ ุนุงูู
        if self.total_requests > 10:
            error_rate = self.error_count / self.total_requests
            if error_rate > 0.1:  # ูุนุฏู ุฎุทุฃ 10%
                self.send_alert(f"ูุนุฏู ุฎุทุฃ ุนุงูู: {error_rate:.2%}")
        
        # ุชูุจูู ููุช ุงุณุชุฌุงุจุฉ ุนุงูู
        if len(self.response_times) > 10:
            avg_response_time = sum(self.response_times[-10:]) / 10
            if avg_response_time > 5.0:  # ูุชูุณุท 5 ุซูุงู
                self.send_alert(f"ููุช ุงุณุชุฌุงุจุฉ ุนุงูู: {avg_response_time:.2f} ุซุงููุฉ")
        
        # ุชูุจูู ุงูุชูููุฉ
        if self.total_cost > 100.0:  # ุนุชุจุฉ 100 ุฏููุงุฑ
            self.send_alert(f"ุชูููุฉ ุนุงููุฉ: ${self.total_cost:.2f}")
    
    def send_alert(self, message):
        """ุฅุฑุณุงู ุชูุจูู (ุชูููุฐ ุทุฑููุฉ ุงูุชูุจูู ุงูููุถูุฉ ูุฏูู)"""
        print(f"ุชูุจูู: {message}")
        # ุชูููุฐ Slack ุฃู ุงูุจุฑูุฏ ุงูุฅููุชุฑููู ุฃู ุชูุจููุงุช ุฃุฎุฑู ููุง

# ูุซูู ูุฑุงูุจ ุนุงู
monitor = LLMMonitor()

def monitored_llm_call_with_alerting(prompt):
    """ุงุณุชุฏุนุงุก ุงูููุงุฐุฌ ุงููุบููุฉ ูุน ุงููุฑุงูุจุฉ ูุงูุชูุจูู"""
    start_time = time.time()
    span = trace.get_current_span()
    
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        
        # ุญุณุงุจ ุงูููุงููุณ
        response_time = time.time() - start_time
        cost = estimate_cost(response.usage)  # ุชูููุฐ ุญุณุงุจ ุงูุชูููุฉ
        
        # ุชุชุจุน ุงูุทูุจ ุงููุงุฌุญ
        monitor.track_request(success=True, cost=cost, response_time=response_time)
        
        # ุฅุถุงูุฉ ุงูููุงููุณ ูููุฏู
        span.set_attribute("request.cost", cost)
        span.set_attribute("request.response_time", response_time)
        
        return response.choices[0].message.content
        
    except Exception as e:
        response_time = time.time() - start_time
        
        # ุชุชุจุน ุงูุทูุจ ุงููุงุดู
        monitor.track_request(success=False, response_time=response_time)
        
        # ุฅุถุงูุฉ ูุนูููุงุช ุงูุฎุทุฃ ูููุฏู
        span.set_attribute("request.failed", True)
        span.set_attribute("error.message", str(e))
        
        raise

def estimate_cost(usage, model="gpt-3.5-turbo"):
    """ุชูุฏูุฑ ุชูููุฉ ุงูุทูุจ ุจูุงุกู ุนูู ุงุณุชุฎุฏุงู ุงูุฑููุฒ"""
    # ุญุณุงุจ ุชูููุฉ ูุจุณุท (ุชุญุฏูุซ ุจุงูุฃุณุนุงุฑ ุงูุญุงููุฉ)
    pricing = {
        "gpt-3.5-turbo": {"input": 0.001, "output": 0.002}  # ููู 1000 ุฑูุฒ
    }
    
    if model in pricing:
        input_cost = (usage.prompt_tokens / 1000) * pricing[model]["input"]
        output_cost = (usage.completion_tokens / 1000) * pricing[model]["output"]
        return input_cost + output_cost
    
    return 0.0
```

## ุงูุงุฎุชุจุงุฑ ูุงูุชุญูู

ูููุดุฆ ุณูุฑูุจุช ุงุฎุชุจุงุฑ ุดุงูู ููุชุญูู ูู ุฅุนุฏุงุฏ OpenLLMetry:

```python
#!/usr/bin/env python3
"""
ุณูุฑูุจุช ุงุฎุชุจุงุฑ OpenLLMetry
ูู ุจุชุดุบูู ูุฐุง ููุชุญูู ูู ุชุซุจูุช ูุชูููู OpenLLMetry.
"""

import os
import sys
import time
from traceloop.sdk import Traceloop
from traceloop.sdk.decorators import task, workflow

def test_basic_initialization():
    """ุงุฎุชุจุงุฑ ุงูุชููุฆุฉ ุงูุฃุณุงุณูุฉ ูู OpenLLMetry"""
    print("ุงุฎุชุจุงุฑ ุงูุชููุฆุฉ ุงูุฃุณุงุณูุฉ...")
    
    try:
        Traceloop.init(disable_batch=True)
        print("โ ุชู ุชููุฆุฉ OpenLLMetry ุจูุฌุงุญ")
        return True
    except Exception as e:
        print(f"โ ูุดู ุงูุชููุฆุฉ: {e}")
        return False

@task(name="test_task")
def test_custom_tracing():
    """ุงุฎุชุจุงุฑ ูุฒุฎุฑูุงุช ุงูุชุชุจุน ุงููุฎุตุตุฉ"""
    print("ุงุฎุชุจุงุฑ ุงูุชุชุจุน ุงููุฎุตุต...")
    time.sleep(0.1)  # ูุญุงูุงุฉ ุงูุนูู
    return "ุชู ุฅูุฌุงุฒ ุงููููุฉ"

@workflow(name="test_workflow")
def test_workflow_tracing():
    """ุงุฎุชุจุงุฑ ุชุชุจุน ูุณุชูู ุณูุฑ ุงูุนูู"""
    print("ุงุฎุชุจุงุฑ ุชุชุจุน ุณูุฑ ุงูุนูู...")
    result = test_custom_tracing()
    return f"ูุชูุฌุฉ ุณูุฑ ุงูุนูู: {result}"

def test_environment_configuration():
    """ุงุฎุชุจุงุฑ ุงูุชูููู ุงููุงุฆู ุนูู ุงูุจูุฆุฉ"""
    print("ุงุฎุชุจุงุฑ ุชูููู ุงูุจูุฆุฉ...")
    
    # ูุญุต ูุชุบูุฑุงุช ุงูุจูุฆุฉ
    required_vars = ["TRACELOOP_API_KEY"]
    optional_vars = ["TRACELOOP_BATCH_EXPORT", "TRACELOOP_TELEMETRY"]
    
    for var in required_vars:
        if not os.getenv(var):
            print(f"โ๏ธ  ุชุญุฐูุฑ: {var} ุบูุฑ ูุญุฏุฏ")
    
    for var in optional_vars:
        value = os.getenv(var, "ุบูุฑ ูุญุฏุฏ")
        print(f"โน๏ธ  {var}: {value}")

def run_tests():
    """ุชุดุบูู ุฌููุน ุงูุงุฎุชุจุงุฑุงุช"""
    print("๐ ุชุดุบูู ุงุฎุชุจุงุฑุงุช OpenLLMetry")
    print("=" * 40)
    
    tests = [
        test_basic_initialization,
        test_environment_configuration,
        test_workflow_tracing
    ]
    
    results = []
    for test in tests:
        try:
            result = test()
            results.append(result)
            print()
        except Exception as e:
            print(f"โ ุงูุงุฎุชุจุงุฑ {test.__name__} ูุดู: {e}")
            results.append(False)
            print()
    
    # ุงูููุฎุต
    passed = sum(1 for r in results if r)
    total = len(results)
    
    print("=" * 40)
    print(f"ูุชุงุฆุฌ ุงูุงุฎุชุจุงุฑ: {passed}/{total} ูุฌุญ")
    
    if passed == total:
        print("๐ ูุฌุญุช ุฌููุน ุงูุงุฎุชุจุงุฑุงุช! OpenLLMetry ุฌุงูุฒ ููุงุณุชุฎุฏุงู.")
    else:
        print("โ๏ธ  ูุดูุช ุจุนุถ ุงูุงุฎุชุจุงุฑุงุช. ุชุญูู ูู ุงูุชูููู ูุงูุชุจุนูุงุช.")

if __name__ == "__main__":
    run_tests()
```

## ุงูุฎูุงุตุฉ

ูููุฑ OpenLLMetry ุญูุงู ุดุงููุงู ููุงุจููุฉ ููุงุญุธุฉ ุชุทุจููุงุช ุงูููุงุฐุฌ ุงููุบููุฉ ุงููุจูุฑุฉุ ูุน ุชูุฏูู ุชูุงูู ุณูุณ ูุน ุงูุจููุฉ ุงูุชุญุชูุฉ ุงูุญุงููุฉ ูู OpenTelemetry ุจูููุง ูุนุงูุฌ ุงุญุชูุงุฌุงุช ุงููุฑุงูุจุฉ ุงููุฑูุฏุฉ ูุชุทุจููุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู.

### ุงูููุงุท ุงูุฑุฆูุณูุฉ

1. **ุฅุนุฏุงุฏ ุจุณูุท**: ุงุจุฏุฃ ุจุจุถุนุฉ ุฃุณุทุฑ ูู ุงูููุฏ
2. **ุชูุงูู ุงูุฃุทุฑ**: ูุนูู ุจุณูุงุณุฉ ูุน LangChain ูLlamaIndex ูุฃุทุฑ ุฃุฎุฑู ุดุงุฆุนุฉ
3. **ุฌุงูุฒ ููุฅูุชุงุฌ**: ูุชุถูู ูุนุงูุฌุฉ ูููุฉ ููุฃุฎุทุงุก ูุชุญุณูู ุงูุฃุฏุงุก ูููุฒุงุช ุงูุฃูุงู
4. **ูุงุจู ููุชูุณุน**: ูุฏุนู ุงูููุงููุณ ูุงูุฎุตุงุฆุต ุงููุฎุตุตุฉ ูุชูุงููุงุช ุงูุฎูููุฉ
5. **ูุนุงู ูู ูุงุญูุฉ ุงูุชูููุฉ**: ููุชูุญ ุงููุตุฏุฑ ูุน ุฏุนู ุนุฏุฉ ุฎูููุงุช ูุงุจููุฉ ููุงุญุธุฉ

### ุงูุฎุทูุงุช ุงูุชุงููุฉ

1. **ุงุจุฏุฃ ุตุบูุฑุงู**: ุงุจุฏุฃ ุจุงููุฑุงูุจุฉ ุงูุฃุณุงุณูุฉ ูู ุจูุฆุฉ ุงูุชุทููุฑ
2. **ุฃุถู ููุงููุณ ูุฎุตุตุฉ**: ููุฐ ุงูุชุชุจุน ุงูุฎุงุต ุจุงูุฃุนูุงู ูุญุงูุฉ ุงูุงุณุชุฎุฏุงู ุงูุฎุงุตุฉ ุจู
3. **ูุดุฑ ุงูุฅูุชุงุฌ**: ุงูููู ูุนุงูุฌุฉ ูููุฉ ููุฃุฎุทุงุก ูุงูุชูุจูู
4. **ุชูุงูู ุงููุฑูู**: ุงุชุตู ุจุงูุจููุฉ ุงูุชุญุชูุฉ ุงูุญุงููุฉ ููุงุจููุฉ ุงูููุงุญุธุฉ
5. **ุงูุชุญุณูู ุงููุณุชูุฑ**: ุงุณุชุฎุฏู ุงูุฑุคู ูุชุญุณูู ุงูุฃุฏุงุก ูุงูุชูุงููู

ูุญูู OpenLLMetry ูุฑุงูุจุฉ ุชุทุจููุงุช ุงูููุงุฐุฌ ุงููุบููุฉ ุงููุจูุฑุฉ ูู ููุฑุฉ ูุงุญูุฉ ุฅูู ูุฏุฑุฉ ูู ุงูุฏุฑุฌุฉ ุงูุฃูููุ ููุง ููููู ูู ุจูุงุก ุชุทุจููุงุช ุฐูุงุก ุงุตุทูุงุนู ุฃูุซุฑ ููุซูููุฉ ูุฃุฏุงุกู ููุนุงููุฉ ูู ูุงุญูุฉ ุงูุชูููุฉ.

ููุฒูุฏ ูู ุงููุนูููุงุชุ ูู ุจุฒูุงุฑุฉ [ูุณุชูุฏุน OpenLLMetry GitHub](https://github.com/traceloop/openllmetry) ุฃู ุชุญูู ูู [ุงููุซุงุฆู ุงูุฑุณููุฉ](https://www.traceloop.com/openllmetry).

---

*ูุฑุงูุจุฉ ุณุนูุฏุฉ! ๐*
