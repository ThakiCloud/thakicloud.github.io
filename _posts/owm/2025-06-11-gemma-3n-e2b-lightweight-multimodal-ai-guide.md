---
title: "Gemma 3n E2B - ëª¨ë°”ì¼ì—ì„œ ëŒì•„ê°€ëŠ” ë©€í‹°ëª¨ë‹¬ AI í˜ì‹ "
date: 2025-06-11
categories: 
  - owm
tags: 
  - gemma
  - multimodal-ai
  - mobile-ai
  - litert-lm
  - google-deepmind
  - edge-computing
  - quantization
  - efficient-ai
author_profile: true
toc: true
toc_label: "Gemma 3n E2B ê°€ì´ë“œ"
---

Google DeepMindê°€ 2025ë…„ ìƒˆí•´ë¥¼ ë§ì•„ ê³µê°œí•œ **Gemma 3n E2B**ëŠ” AI ëª¨ë¸ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•©ë‹ˆë‹¤. **ìŠ¤ë§ˆíŠ¸í°ì—ì„œë„ ëŒì•„ê°€ëŠ” ë©€í‹°ëª¨ë‹¬ AI**ë¼ëŠ” í˜ì‹ ì ì¸ ì»¨ì…‰ìœ¼ë¡œ, í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©´ì„œë„ ê·¹ë„ë¡œ ê²½ëŸ‰í™”ëœ ëª¨ë¸ì…ë‹ˆë‹¤.

## í˜ì‹ ì ì¸ íŠ¹ì§•ë“¤

### ğŸš€ ì„ íƒì  íŒŒë¼ë¯¸í„° í™œì„±í™” ê¸°ìˆ 

Gemma 3nì˜ ê°€ì¥ í˜ì‹ ì ì¸ ì ì€ **ì„ íƒì  íŒŒë¼ë¯¸í„° í™œì„±í™”(Selective Parameter Activation)** ê¸°ìˆ ì…ë‹ˆë‹¤:

```python
# Gemma 3nì˜ íš¨ìœ¨ì„± ë¹„êµ
traditional_model = {
    'total_parameters': '7B',
    'active_parameters': '7B',  # ëª¨ë“  íŒŒë¼ë¯¸í„° í•­ìƒ í™œì„±í™”
    'memory_usage': 'Full',
    'inference_speed': 'Slow on mobile'
}

gemma_3n_e2b = {
    'total_parameters': '6B+',
    'effective_parameters': '2B',  # í•„ìš”í•œ ë¶€ë¶„ë§Œ í™œì„±í™”
    'memory_usage': '60% reduced',
    'inference_speed': '3-4x faster on mobile'
}
```

### ğŸ“± ëª¨ë°”ì¼ ìµœì í™” ì„±ëŠ¥

ì‹¤ì œ ë””ë°”ì´ìŠ¤ì—ì„œì˜ ì„±ëŠ¥ì´ ë§¤ìš° ì¸ìƒì ì…ë‹ˆë‹¤:

| ë””ë°”ì´ìŠ¤ | ë°±ì—”ë“œ | Prefill ì†ë„ | Decode ì†ë„ |
|----------|--------|-------------|------------|
| **MacBook Pro M3** | CPU | 232.5 tokens/sec | 27.6 tokens/sec |
| **Samsung S24 Ultra** | CPU | 110.5 tokens/sec | 16.1 tokens/sec |
| **Samsung S24 Ultra** | GPU | 816.4 tokens/sec | 15.6 tokens/sec |

> ğŸ”¥ **Galaxy S24ì—ì„œ GPU ê°€ì† ì‹œ 800+ tokens/sec!** ì´ëŠ” ë°ìŠ¤í¬í†±ê¸‰ ì„±ëŠ¥ì…ë‹ˆë‹¤.

### ğŸŒ ì§„ì •í•œ ë©€í‹°ëª¨ë‹¬ + ë‹¤êµ­ì–´ ì§€ì›

```python
# Gemma 3n E2B ì…ë ¥ ëŠ¥ë ¥
input_capabilities = {
    'text': {
        'languages': 140,
        'context_length': '32K tokens',
        'use_cases': ['QA', 'ìš”ì•½', 'ì¶”ë¡ ', 'ì½”ë”©']
    },
    'image': {
        'resolutions': ['256x256', '512x512', '768x768'],
        'encoding': '256 tokens per image',
        'use_cases': ['ì´ë¯¸ì§€ ë¶„ì„', 'ì‹œê°ì  ì§ˆë¬¸ ë‹µë³€', 'OCR']
    },
    'audio': {
        'encoding': '6.25 tokens per second',
        'channels': 'Single channel',
        'use_cases': ['ìŒì„± ì¸ì‹', 'ì˜¤ë””ì˜¤ ë¶„ì„', 'ì–¸ì–´ ë²ˆì—­']
    },
    'video': {
        'support': 'Coming soon',
        'potential_uses': ['ë™ì˜ìƒ ìš”ì•½', 'í–‰ë™ ì¸ì‹']
    }
}
```

## LiteRT-LMê³¼ì˜ ì™„ë²½í•œ í†µí•©

### ìƒˆë¡œìš´ ì¶”ë¡  í”„ë ˆì„ì›Œí¬

Gemma 3nëŠ” Google AI Edgeì˜ ìƒˆë¡œìš´ **LiteRT-LM** ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í•¨ê»˜ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤:

```bash
# LiteRT-LM ì„¤ì¹˜ ë° ì„¤ì •
git clone https://github.com/google-ai-edge/litert-lm
cd litert-lm

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
wget https://huggingface.co/google/gemma-3n-E2B-it-litert-lm-preview/resolve/main/model.bin

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python inference_example.py --model gemma-3n-e2b --prompt "í•œêµ­ì˜ ìˆ˜ë„ëŠ”?"
```

### í”Œë«í¼ë³„ í™œìš©ë²•

**Android ê°œë°œììš©:**

```kotlin
// Androidì—ì„œ Gemma 3n ì‚¬ìš© ì˜ˆì‹œ
class GemmaInference {
    private val litert = LiteRTLM.create("gemma-3n-e2b.bin")
    
    fun generateText(prompt: String): String {
        return litert.generate(prompt, maxTokens = 512)
    }
    
    fun analyzeImage(bitmap: Bitmap, question: String): String {
        val encodedImage = ImageEncoder.encode(bitmap)
        val combinedInput = "$question\n[IMAGE]$encodedImage"
        return litert.generate(combinedInput)
    }
}
```

**iOS ê°œë°œììš©:**

```swift
// iOSì—ì„œ LiteRT-LM ì‚¬ìš©
import LiteRTLM

class GemmaWrapper {
    private let model = LiteRTLM(modelPath: "gemma-3n-e2b.bin")
    
    func chat(message: String) -> String {
        return model.generate(prompt: message, maxTokens: 512)
    }
    
    func analyzeAudio(audioData: Data) -> String {
        let encodedAudio = AudioEncoder.encode(audioData)
        return model.generate(prompt: "[AUDIO]\(encodedAudio)")
    }
}
```

## ì‹¤ì „ ë²¤ì¹˜ë§ˆí¬ ë¶„ì„

### ğŸ“Š í•™ìˆ  ì„±ëŠ¥ ì§€í‘œ

| ë²¤ì¹˜ë§ˆí¬ | ë©”íŠ¸ë¦­ | Gemma 3n E2B | Gemma 3n E4B |
|----------|--------|--------------|--------------|
| **MMLU** | Accuracy | 60.1% | 64.9% |
| **HumanEval** | pass@1 | 66.5% | 75.0% |
| **MBPP** | pass@1 | 56.6% | 63.6% |
| **LiveCodeBench** | pass@1 | 13.2% | 13.2% |
| **Global-MMLU-Lite** | Accuracy | 59.0% | 64.5% |

### ğŸ¯ ì‹¤ìš©ì„± í‰ê°€

```python
# ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ í•´ì„
performance_analysis = {
    'coding': {
        'strength': 'HumanEval 66.5% - ì‹¤ìš©ì  ì½”ë”© ê°€ëŠ¥',
        'weakness': 'LiveCodeBench 13.2% - ìµœì‹  ì½”ë”© íŠ¸ë Œë“œ í•œê³„',
        'recommendation': 'ì¼ë°˜ì ì¸ ì½”ë”© ì‘ì—…ì— ì í•©, ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì£¼ì˜'
    },
    'reasoning': {
        'strength': 'MMLU 60.1% - ì¤€ìˆ˜í•œ ì¼ë°˜ ì§€ì‹',
        'weakness': 'HiddenMath 27.7% - ë³µì¡í•œ ìˆ˜í•™ ì¶”ë¡  ì œí•œ',
        'recommendation': 'ê¸°ë³¸ì ì¸ ì¶”ë¡ ê³¼ QAì— íš¨ê³¼ì '
    },
    'multilingual': {
        'strength': 'Global-MMLU 59.0% - ë‹¤êµ­ì–´ ì§€ì› ìš°ìˆ˜',
        'benefit': '140ê°œ ì–¸ì–´ ì§€ì›ìœ¼ë¡œ ê¸€ë¡œë²Œ ì•± ê°œë°œì— ìœ ë¦¬'
    }
}
```

## ê°œë°œìë¥¼ ìœ„í•œ ì‹¤ì „ ê°€ì´ë“œ

### ğŸ› ï¸ í”„ë¡œì íŠ¸ ì„¤ì •

**1. í™˜ê²½ ì¤€ë¹„**

```bash
# Python í™˜ê²½ ì„¤ì •
python -m venv gemma-env
source gemma-env/bin/activate  # Linux/Mac
# gemma-env\Scripts\activate    # Windows

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install torch transformers accelerate
pip install litert-lm  # Preview ë²„ì „
```

**2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì´ˆê¸°í™”**

```python
# Hugging Faceì—ì„œ ëª¨ë¸ ë¡œë“œ
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ë¡œê·¸ì¸ í•„ìš” (Gemma ë¼ì´ì„ ìŠ¤ ë™ì˜)
# huggingface-cli login

model_name = "google/gemma-3n-E2B-it-litert-lm-preview"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

def chat_with_gemma(prompt, max_tokens=512):
    inputs = tokenizer(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(prompt):].strip()

# í…ŒìŠ¤íŠ¸
result = chat_with_gemma("íŒŒì´ì¬ìœ¼ë¡œ ê°„ë‹¨í•œ ì›¹ í¬ë¡¤ëŸ¬ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜")
print(result)
```

### ğŸ“± ëª¨ë°”ì¼ ì•± í†µí•© ì˜ˆì‹œ

**React Native í†µí•©:**

```javascript
// React Nativeì—ì„œ LiteRT-LM ì‚¬ìš©
import { LiteRTLM } from 'litert-lm-react-native';

class GemmaChat extends React.Component {
  constructor(props) {
    super(props);
    this.state = {
      model: null,
      messages: [],
      loading: true
    };
  }
  
  async componentDidMount() {
    try {
      const model = await LiteRTLM.loadModel('gemma-3n-e2b.bin');
      this.setState({ model, loading: false });
    } catch (error) {
      console.error('Model loading failed:', error);
    }
  }
  
  async sendMessage(text) {
    const { model } = this.state;
    const response = await model.generate(text, { maxTokens: 256 });
    
    this.setState(prevState => ({
      messages: [
        ...prevState.messages,
        { role: 'user', content: text },
        { role: 'assistant', content: response }
      ]
    }));
  }
}
```

## í™œìš© ì‚¬ë¡€ë³„ ê°€ì´ë“œ

### ğŸ¨ ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  ë¶„ì„

```python
# ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ë¶„ì„ ì˜ˆì‹œ
def analyze_image_with_context(image_path, question):
    from PIL import Image
    import base64
    from io import BytesIO
    
    # ì´ë¯¸ì§€ ì¸ì½”ë”©
    image = Image.open(image_path).resize((512, 512))
    buffer = BytesIO()
    image.save(buffer, format='PNG')
    img_str = base64.b64encode(buffer.getvalue()).decode()
    
    # ë©€í‹°ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
ë‹¤ìŒ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

[IMAGE_DATA: {img_str[:100]}...]

ë¶„ì„:"""
    
    return chat_with_gemma(prompt, max_tokens=512)

# ì‚¬ìš© ì˜ˆì‹œ
result = analyze_image_with_context(
    "product_screenshot.png",
    "ì´ UIì—ì„œ ê°œì„ í•  ì ì´ ìˆë‚˜ìš”?"
)
print(result)
```

### ğŸ—£ï¸ ìŒì„± ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°

```python
# ì˜¤ë””ì˜¤ ë¶„ì„ íŒŒì´í”„ë¼ì¸
import librosa
import numpy as np

def process_audio_with_gemma(audio_file, task="transcribe"):
    # ì˜¤ë””ì˜¤ ë¡œë“œ ë° ì „ì²˜ë¦¬
    audio, sr = librosa.load(audio_file, sr=16000)
    
    # ì˜¤ë””ì˜¤ë¥¼ 6.25 tokens/secë¡œ ì¸ì½”ë”© (Gemma 3n ê·œê²©)
    audio_tokens = encode_audio_to_tokens(audio, sr)
    
    prompts = {
        "transcribe": f"ë‹¤ìŒ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”:\n[AUDIO]{audio_tokens}",
        "summarize": f"ë‹¤ìŒ ì˜¤ë””ì˜¤ì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:\n[AUDIO]{audio_tokens}",
        "sentiment": f"ë‹¤ìŒ ì˜¤ë””ì˜¤ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:\n[AUDIO]{audio_tokens}"
    }
    
    return chat_with_gemma(prompts[task])

def encode_audio_to_tokens(audio, sr):
    # Gemma 3nì˜ ì˜¤ë””ì˜¤ ì¸ì½”ë”© ë°©ì‹ êµ¬í˜„
    # ì‹¤ì œë¡œëŠ” LiteRT-LMì˜ AudioEncoder ì‚¬ìš©
    chunk_size = sr // 6.25  # 6.25 tokens per second
    tokens = []
    
    for i in range(0, len(audio), int(chunk_size)):
        chunk = audio[i:i+int(chunk_size)]
        # ê°„ë‹¨í•œ íŠ¹ì„± ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡)
        features = np.mean(chunk), np.std(chunk), np.max(chunk)
        tokens.append(f"<audio_{i//int(chunk_size)}>")
    
    return "".join(tokens)
```

## ì„±ëŠ¥ ìµœì í™” íŒ

### âš¡ ì¶”ë¡  ì†ë„ í–¥ìƒ

```python
# ì¶”ë¡  ìµœì í™” ì„¤ì •
optimization_config = {
    'quantization': {
        'method': 'int4_weights_float_activations',
        'memory_reduction': '75%',
        'speed_improvement': '2-3x'
    },
    'caching': {
        'enable_kv_cache': True,
        'enable_xnnpack_cache': True,  # CPU ìµœì í™”
        'enable_gpu_cache': True       # GPU ìµœì í™”
    },
    'threading': {
        'cpu_threads': 4,
        'prefill_parallel': True,
        'decode_parallel': False  # ìˆœì°¨ì  ë””ì½”ë”©
    }
}

# ìµœì í™”ëœ ëª¨ë¸ ë¡œë“œ
from litert_lm import LiteRTLM

model = LiteRTLM(
    model_path="gemma-3n-e2b-int4.bin",
    config=optimization_config
)
```

### ğŸ”‹ ë°°í„°ë¦¬ íš¨ìœ¨ì„±

```python
# ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ìš© ë°°í„°ë¦¬ ìµœì í™”
mobile_config = {
    'inference_mode': 'battery_saver',
    'max_tokens_per_request': 256,  # ì§§ì€ ì‘ë‹µìœ¼ë¡œ ë°°í„°ë¦¬ ì ˆì•½
    'cpu_governor': 'powersave',     # CPU ì£¼íŒŒìˆ˜ ì œí•œ
    'gpu_power_limit': 0.7,          # GPU ì „ë ¥ 70%ë¡œ ì œí•œ
    'thermal_throttling': True       # ê³¼ì—´ ë°©ì§€
}

def efficient_mobile_inference(prompt, config=mobile_config):
    # ë°°í„°ë¦¬ íš¨ìœ¨ì ì¸ ì¶”ë¡ 
    with power_management(config):
        response = model.generate(
            prompt,
            max_tokens=config['max_tokens_per_request'],
            temperature=0.3,  # ë‚®ì€ ì˜¨ë„ë¡œ ì•ˆì •ì„± í™•ë³´
            top_p=0.85
        )
    return response
```

## í”„ë¡œë•ì…˜ ë°°í¬ ê°€ì´ë“œ

### ğŸ­ ì„œë²„ ë°°í¬

```dockerfile
# Dockerfile for Gemma 3n E2B
FROM python:3.9-slim

# LiteRT-LM ë° ì¢…ì†ì„± ì„¤ì¹˜
RUN pip install litert-lm torch transformers

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
COPY models/gemma-3n-e2b.bin /app/models/

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ
COPY src/ /app/src/
WORKDIR /app

# API ì„œë²„ ì‹¤í–‰
CMD ["python", "src/api_server.py"]
```

```python
# FastAPI ì„œë²„ ì˜ˆì‹œ
from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel
import uvicorn

app = FastAPI(title="Gemma 3n E2B API")

class ChatRequest(BaseModel):
    message: str
    max_tokens: int = 512

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    response = model.generate(
        request.message,
        max_tokens=request.max_tokens
    )
    return {"response": response}

@app.post("/analyze-image")
async def analyze_image(file: UploadFile = File(...), question: str = ""):
    # ì´ë¯¸ì§€ ì²˜ë¦¬ ë¡œì§
    image_data = await file.read()
    result = process_multimodal_input(image_data, question)
    return {"analysis": result}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

```python
# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •
import time
import psutil
from dataclasses import dataclass

@dataclass
class InferenceMetrics:
    latency: float
    tokens_per_second: float
    memory_usage: float
    cpu_usage: float
    gpu_usage: float = None

class GemmaMonitor:
    def __init__(self):
        self.metrics_history = []
    
    def measure_inference(self, prompt, model):
        start_time = time.time()
        start_memory = psutil.virtual_memory().used
        
        # ì¶”ë¡  ì‹¤í–‰
        response = model.generate(prompt)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        end_time = time.time()
        latency = end_time - start_time
        tokens = len(response.split())
        tokens_per_second = tokens / latency
        
        metrics = InferenceMetrics(
            latency=latency,
            tokens_per_second=tokens_per_second,
            memory_usage=psutil.virtual_memory().used - start_memory,
            cpu_usage=psutil.cpu_percent()
        )
        
        self.metrics_history.append(metrics)
        return response, metrics
    
    def get_performance_report(self):
        if not self.metrics_history:
            return "No metrics available"
        
        avg_latency = sum(m.latency for m in self.metrics_history) / len(self.metrics_history)
        avg_throughput = sum(m.tokens_per_second for m in self.metrics_history) / len(self.metrics_history)
        
        return f"""
Performance Report:
- Average Latency: {avg_latency:.2f}s
- Average Throughput: {avg_throughput:.1f} tokens/sec
- Total Requests: {len(self.metrics_history)}
        """
```

## ì£¼ì˜ì‚¬í•­ ë° í•œê³„ì 

### âš ï¸ í˜„ì¬ ì œí•œì‚¬í•­

```python
current_limitations = {
    'multimodal': {
        'status': 'Text input only (preview)',
        'coming_soon': ['Full image support', 'Video processing', 'Audio input'],
        'workaround': 'Use base64 encoding for basic image analysis'
    },
    'model_size': {
        'effective_params': '2B',
        'limitation': 'Complex reasoning tasks may be challenging',
        'recommendation': 'Use E4B variant for better performance'
    },
    'language_support': {
        'strong': ['English', 'Korean', 'Japanese', 'Chinese'],
        'limited': 'Some of 140 supported languages',
        'evaluation': 'Primarily English-tested'
    },
    'factual_accuracy': {
        'cutoff_date': 'June 2024',
        'disclaimer': 'May generate outdated or incorrect information',
        'mitigation': 'Always verify critical information'
    }
}
```

### ğŸ›¡ï¸ ì•ˆì „ì„± ê³ ë ¤ì‚¬í•­

```python
# ì•ˆì „í•œ ì‚¬ìš©ì„ ìœ„í•œ ê°€ì´ë“œë¼ì¸
safety_guidelines = {
    'content_filtering': {
        'built_in': 'Basic safety filters included',
        'recommendation': 'Additional content moderation for production',
        'areas': ['CSAM', 'Violence', 'Hate speech', 'Misinformation']
    },
    'privacy': {
        'data_handling': 'No user data sent to Google servers',
        'local_processing': 'All inference runs locally',
        'concern': 'Training data may contain sensitive information'
    },
    'bias_mitigation': {
        'evaluation': 'Tested for representational harms',
        'limitation': 'May reflect training data biases',
        'best_practice': 'Monitor outputs for unfair biases'
    }
}
```

## ë¯¸ë˜ ì „ë§ê³¼ ë¡œë“œë§µ

### ğŸš€ ì˜ˆìƒë˜ëŠ” ë°œì „ ë°©í–¥

```python
future_roadmap = {
    'short_term': {
        'timeline': '2025 Q1-Q2',
        'features': [
            'ì™„ì „í•œ ë©€í‹°ëª¨ë‹¬ ì§€ì›',
            'Video processing ê¸°ëŠ¥',
            'Real-time audio streaming',
            'More quantization options'
        ]
    },
    'medium_term': {
        'timeline': '2025 Q3-Q4',
        'features': [
            'ë” í° ëª¨ë¸ ì‚¬ì´ì¦ˆ (E8B, E16B)',
            'Fine-tuning ì§€ì›',
            'Domain-specific variants',
            'Better multilingual performance'
        ]
    },
    'long_term': {
        'timeline': '2026+',
        'vision': [
            'Sub-1B parameter models',
            'Specialized mobile chipset optimization',
            'Edge AI ecosystem integration',
            'Federated learning capabilities'
        ]
    }
}
```

## ê²°ë¡ 

**Gemma 3n E2B**ëŠ” AIì˜ ë¯¼ì£¼í™”ë¥¼ í•œ ë‹¨ê³„ ë” ë°œì „ì‹œí‚¨ í˜ì‹ ì ì¸ ëª¨ë¸ì…ë‹ˆë‹¤. **ìŠ¤ë§ˆíŠ¸í°ì—ì„œë„ ëŒì•„ê°€ëŠ” ë©€í‹°ëª¨ë‹¬ AI**ë¼ëŠ” ë¹„ì „ì„ í˜„ì‹¤ë¡œ ë§Œë“¤ì–´, ê°œë°œìë“¤ì´ ë” ì ‘ê·¼ ê°€ëŠ¥í•˜ê³  ì‹¤ìš©ì ì¸ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ ìˆ˜ ìˆê²Œ í–ˆìŠµë‹ˆë‹¤.

### í•µì‹¬ ì¥ì  ìš”ì•½

- âœ… **ëª¨ë°”ì¼ ìµœì í™”**: Galaxy S24ì—ì„œë„ 800+ tokens/sec ì„±ëŠ¥
- âœ… **ë©€í‹°ëª¨ë‹¬ ì§€ì›**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ í†µí•© ì²˜ë¦¬
- âœ… **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ì„ íƒì  íŒŒë¼ë¯¸í„° í™œì„±í™”ë¡œ 60% ë©”ëª¨ë¦¬ ì ˆì•½
- âœ… **ë‹¤êµ­ì–´ ì§€ì›**: 140ê°œ ì–¸ì–´ë¡œ ê¸€ë¡œë²Œ ì•± ê°œë°œ ì§€ì›
- âœ… **ì˜¤í”ˆì†ŒìŠ¤**: ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥í•œ ë¼ì´ì„ ìŠ¤

### ì¶”ì²œ ì‚¬ìš© ì¼€ì´ìŠ¤

1. **ëª¨ë°”ì¼ AI ì•± ê°œë°œ** - ì˜¤í”„ë¼ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸, ì‹¤ì‹œê°„ ë²ˆì—­
2. **ì—£ì§€ ë””ë°”ì´ìŠ¤ ë°°í¬** - IoT, ë¡œë´‡í‹±ìŠ¤, ì„ë² ë””ë“œ ì‹œìŠ¤í…œ
3. **í”„ë¡œí† íƒ€ì´í•‘** - ë¹ ë¥¸ AI ê¸°ëŠ¥ ê²€ì¦ ë° MVP ê°œë°œ
4. **êµìœ¡ìš© í”„ë¡œì íŠ¸** - AI í•™ìŠµê³¼ ì‹¤ìŠµìš© ê²½ëŸ‰ ëª¨ë¸

**Gemma 3n E2BëŠ” "AIê°€ í´ë¼ìš°ë“œì—ì„œ ë””ë°”ì´ìŠ¤ë¡œ"ë¼ëŠ” íŒ¨ëŸ¬ë‹¤ì„ ë³€í™”ì˜ ì„ ë‘ì£¼ìì…ë‹ˆë‹¤.** ì•ìœ¼ë¡œ ë” ë§ì€ ê°œë°œìë“¤ì´ ì´ ê¸°ìˆ ì„ í†µí•´ í˜ì‹ ì ì¸ AI ì„œë¹„ìŠ¤ë¥¼ ë§Œë“¤ì–´ë‚¼ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ

- **Hugging Face ëª¨ë¸ í˜ì´ì§€**: [google/gemma-3n-E2B-it-litert-lm-preview](https://huggingface.co/google/gemma-3n-E2B-it-litert-lm-preview)
- **LiteRT-LM GitHub**: [Google AI Edge LiteRT-LM](https://github.com/google-ai-edge/litert-lm)
- **Gemma 3n ê³µì‹ ë¬¸ì„œ**: [Google AI Developer - Gemma 3n](https://ai.google.dev/gemma/docs/gemma-3n)
- **Google AI Studio**: [AI Studio Platform](https://aistudio.google.com)
- **Responsible AI Toolkit**: [Google's Responsible Generative AI Toolkit](https://ai.google.dev/responsible-ai)

---

*ì´ ê°€ì´ë“œëŠ” 2025ë…„ 1ì›” 16ì¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, Gemma 3n ëª¨ë¸ì˜ ì •ì‹ ì¶œì‹œì™€ í•¨ê»˜ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë  ì˜ˆì •ì…ë‹ˆë‹¤.* 