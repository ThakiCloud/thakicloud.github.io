---
title: "FastVideo: 50ë°° ê°€ì†í™”ëœ í†µí•© ë¹„ë””ì˜¤ ìƒì„± í”„ë ˆì„ì›Œí¬"
excerpt: "Sparse Distillationê³¼ Video Sparse Attentionìœ¼ë¡œ 50ë°° ì´ìƒì˜ ë””ë…¸ì´ì§• ê°€ì†í™”ë¥¼ ë‹¬ì„±í•œ FastVideo í”„ë ˆì„ì›Œí¬ì˜ í˜ì‹ ì ì¸ ê¸°ìˆ ê³¼ ì‹¤ì œ êµ¬í˜„ ë°©ë²•ì„ ìƒì„¸íˆ ë¶„ì„í•©ë‹ˆë‹¤."
seo_title: "FastVideo ë¹„ë””ì˜¤ ìƒì„± ê°€ì†í™” í”„ë ˆì„ì›Œí¬ ì™„ì „ ê°€ì´ë“œ - Thaki Cloud"
seo_description: "FastVideoì˜ Sparse Distillation, Video Sparse Attention, FastWan ëª¨ë¸ì„ í™œìš©í•œ 50ë°° ë¹ ë¥¸ ë¹„ë””ì˜¤ ìƒì„± ê¸°ìˆ ê³¼ ì‹¤ì œ êµ¬í˜„ ê°€ì´ë“œ. ì—”ë“œíˆ¬ì—”ë“œ ì›Œí¬í”Œë¡œìš°ì™€ ì„±ëŠ¥ ìµœì í™” ë°©ë²• ì œê³µ."
date: 2025-08-05
last_modified_at: 2025-08-05
categories:
  - owm
tags:
  - fastvideo
  - video-generation
  - diffusion-models
  - sparse-distillation
  - video-attention
  - fastwan
  - acceleration
  - open-workflow
  - post-training
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/owm/fastvideo-unified-video-generation-acceleration-framework/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 20ë¶„

## ì„œë¡ 

AI ë¹„ë””ì˜¤ ìƒì„± ë¶„ì•¼ëŠ” ìµœê·¼ ê¸‰ì†í•œ ë°œì „ì„ ì´ë£¨ê³  ìˆì§€ë§Œ, ì—¬ì „íˆ **ê¸´ ìƒì„± ì‹œê°„**ê³¼ **ë†’ì€ ì»´í“¨íŒ… ë¹„ìš©**ì´ë¼ëŠ” í˜„ì‹¤ì ì¸ ì¥ë²½ì— ì§ë©´í•´ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ í™•ì‚° ëª¨ë¸(Diffusion Model) ê¸°ë°˜ ë¹„ë””ì˜¤ ìƒì„±ì€ ìˆ˜ì‹­ ë²ˆì˜ ë””ë…¸ì´ì§• ë‹¨ê³„ë¥¼ ê±°ì³ì•¼ í•˜ë©°, ì´ë¡œ ì¸í•´ ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ í™œìš©ì´ ì œí•œì ì´ì—ˆìŠµë‹ˆë‹¤.

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë“±ì¥í•œ [FastVideo](https://github.com/hao-ai-lab/FastVideo)ëŠ” **50ë°° ì´ìƒì˜ ë””ë…¸ì´ì§• ê°€ì†í™”**ë¥¼ ë‹¬ì„±í•˜ëŠ” í˜ì‹ ì ì¸ í†µí•© í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ í›ˆë ¨, íŒŒì¸íŠœë‹, ë””ìŠ¤í‹¸ë ˆì´ì…˜, ì¶”ë¡ ê¹Œì§€ **ì—”ë“œíˆ¬ì—”ë“œ íŒŒì´í”„ë¼ì¸**ì„ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ ìµœì í™” ê¸°ë²•ì„ í†µí•´ ì‹¤ì‹œê°„ì— ê°€ê¹Œìš´ ë¹„ë””ì˜¤ ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

ì´ë²ˆ ê¸€ì—ì„œëŠ” FastVideoì˜ í•µì‹¬ ê¸°ìˆ , ì‹¤ì œ ì„±ëŠ¥, ê·¸ë¦¬ê³  ì˜¤í”ˆ ì›Œí¬í”Œë¡œìš° í™˜ê²½ì—ì„œì˜ í™œìš© ë°©ì•ˆì„ ìƒì„¸íˆ ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤.

## FastVideo í•µì‹¬ í˜ì‹  ê¸°ìˆ 

### 1. Sparse Distillation: 50ë°° ê°€ì†í™”ì˜ í•µì‹¬

**Sparse Distillation**ì€ FastVideoì˜ ê°€ì¥ í˜ì‹ ì ì¸ ê¸°ìˆ ë¡œ, ê¸°ì¡´ í™•ì‚° ëª¨ë¸ì˜ ë””ë…¸ì´ì§• ë‹¨ê³„ë¥¼ ê·¹ì ìœ¼ë¡œ ì¤„ì´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

#### ê¸°ìˆ  ì›ë¦¬
```python
# ê¸°ì¡´ í™•ì‚° ëª¨ë¸: 50+ ë‹¨ê³„ ë””ë…¸ì´ì§•
traditional_steps = 50  # ì¼ë°˜ì ì¸ ë””ë…¸ì´ì§• ë‹¨ê³„

# FastVideo Sparse Distillation: 1-2 ë‹¨ê³„ë¡œ ë‹¨ì¶•
sparse_distilled_steps = 1  # 50ë°° ê°€ì†í™”!

acceleration_ratio = traditional_steps / sparse_distilled_steps
print(f"ê°€ì†í™” ë¹„ìœ¨: {acceleration_ratio}x")
```

#### ì§€ì› ëª¨ë¸ ë° ì„±ëŠ¥
- **FastWan2.1-T2V-1.3B**: 480P ë¹„ë””ì˜¤ ìƒì„±
- **FastWan2.1-T2V-14B**: 720P ê³ í•´ìƒë„ ë¹„ë””ì˜¤ (ë¯¸ë¦¬ë³´ê¸°)
- **FastWan2.2-TI2V-5B**: í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ê¸°ë°˜ ë¹„ë””ì˜¤ ìƒì„±

### 2. Video Sparse Attention (VSA)

**VSA**ëŠ” ë¹„ë””ì˜¤ ìƒì„± ì‹œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ìµœì í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ê³„ì‚° ë³µì¡ë„ë¥¼ ëŒ€í­ ì¤„ì´ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

#### ì–´í…ì…˜ ìµœì í™” ì „ëµ
- **ì‹œê°„ì  í¬ì†Œì„±**: ì¤‘ìš”í•œ í”„ë ˆì„ ê°„ ê´€ê³„ë§Œ ê³„ì‚°
- **ê³µê°„ì  í¬ì†Œì„±**: í•µì‹¬ ì˜ì—­ì— ì§‘ì¤‘ëœ ì–´í…ì…˜
- **ì ì‘ì  íŒ¨í„´**: ì½˜í…ì¸ ì— ë”°ë¥¸ ë™ì  ì–´í…ì…˜ ì¡°ì •

### 3. Sliding Tile Attention

**ê³ í•´ìƒë„ ë¹„ë””ì˜¤** ìƒì„±ì„ ìœ„í•œ í˜ì‹ ì ì¸ ì–´í…ì…˜ ê¸°ë²•ìœ¼ë¡œ, ë©”ëª¨ë¦¬ ì œí•œì„ ê·¹ë³µí•˜ë©´ì„œë„ í’ˆì§ˆì„ ìœ ì§€í•©ë‹ˆë‹¤.

#### í•µì‹¬ íŠ¹ì§•
- **íƒ€ì¼ ê¸°ë°˜ ì²˜ë¦¬**: ë¹„ë””ì˜¤ë¥¼ ì‘ì€ íƒ€ì¼ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬
- **ìŠ¬ë¼ì´ë”© ìœˆë„ìš°**: ê²¹ì¹˜ëŠ” ì˜ì—­ìœ¼ë¡œ ì—°ì†ì„± ë³´ì¥
- **ë³‘ë ¬ ì²˜ë¦¬**: ì—¬ëŸ¬ íƒ€ì¼ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ì—¬ ì†ë„ í–¥ìƒ

## ì„¤ì¹˜ ë° í™˜ê²½ êµ¬ì„±

### 1. ê¸°ë³¸ í™˜ê²½ ì„¤ì •

```bash
# ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/ai-projects/fastvideo
cd ~/ai-projects/fastvideo

# Conda í™˜ê²½ ìƒì„±
conda create -n fastvideo python=3.12
conda activate fastvideo

# FastVideo ì„¤ì¹˜
pip install fastvideo
```

### 2. GPU í™˜ê²½ ìµœì í™”

```bash
# CUDA í™˜ê²½ í™•ì¸
nvidia-smi

# ì¶”ê°€ ì˜ì¡´ì„± ì„¤ì¹˜ (GPU ìµœì í™”)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install xformers  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜
pip install triton    # GPU ì»¤ë„ ìµœì í™”
```

### 3. í™˜ê²½ ê²€ì¦

```python
# environment_check.py
import torch
from fastvideo import VideoGenerator

def check_environment():
    """í™˜ê²½ ì„¤ì • ê²€ì¦"""
    print("ğŸ” FastVideo í™˜ê²½ ê²€ì¦")
    print("=" * 40)
    
    # PyTorch ì •ë³´
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"  GPU {i}: {props.name} ({props.total_memory/1e9:.1f}GB)")
    
    # FastVideo ëª¨ë“ˆ í™•ì¸
    try:
        generator = VideoGenerator.from_pretrained(
            "FastVideo/FastWan2.1-T2V-1.3B-Diffusers",
            num_gpus=1 if torch.cuda.is_available() else 0
        )
        print("âœ… FastVideo ëª¨ë“ˆ ì •ìƒ ë¡œë“œ")
        return True
    except Exception as e:
        print(f"âŒ FastVideo ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    check_environment()
```

## ê¸°ë³¸ ë¹„ë””ì˜¤ ìƒì„±

### 1. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„±

```python
# basic_t2v_generation.py
from fastvideo import VideoGenerator
import time

def generate_basic_video():
    """ê¸°ë³¸ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„±"""
    
    # ë¹„ë””ì˜¤ ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = VideoGenerator.from_pretrained(
        "FastVideo/FastWan2.1-T2V-1.3B-Diffusers",
        num_gpus=1,  # GPU ê°œìˆ˜ì— ë”°ë¼ ì¡°ì •
        enable_optimization=True  # ìµœì í™” í™œì„±í™”
    )
    
    # í”„ë¡¬í”„íŠ¸ ì •ì˜
    prompts = [
        "A curious raccoon peers through a vibrant field of yellow sunflowers, its eyes wide with interest.",
        "A majestic eagle soaring through misty mountain peaks at sunrise.",
        "Ocean waves crashing against rocky cliffs under a dramatic stormy sky.",
        "A busy street in Tokyo at night with neon signs and rain reflections.",
        "A time-lapse of cherry blossoms blooming in a peaceful Japanese garden."
    ]
    
    for i, prompt in enumerate(prompts, 1):
        print(f"\nğŸ¬ ë¹„ë””ì˜¤ {i}/{len(prompts)} ìƒì„± ì¤‘...")
        print(f"ğŸ“ í”„ë¡¬í”„íŠ¸: {prompt}")
        
        start_time = time.time()
        
        # ë¹„ë””ì˜¤ ìƒì„±
        video = generator.generate_video(
            prompt,
            return_frames=True,
            output_path="outputs/basic/",
            save_video=True,
            num_inference_steps=1,  # Sparse Distillationìœ¼ë¡œ 1ë‹¨ê³„ë§Œ!
            guidance_scale=7.5,
            height=480,
            width=832,
            num_frames=77  # ì•½ 2.5ì´ˆ ë¹„ë””ì˜¤ (30fps ê¸°ì¤€)
        )
        
        generation_time = time.time() - start_time
        
        print(f"âœ… ìƒì„± ì™„ë£Œ! ì†Œìš”ì‹œê°„: {generation_time:.2f}ì´ˆ")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: outputs/basic/video_{i:02d}.mp4")

if __name__ == "__main__":
    generate_basic_video()
```

### 2. ê³ ê¸‰ íŒŒë¼ë¯¸í„° ì„¤ì •

```python
# advanced_generation.py
from fastvideo import VideoGenerator
import torch

class AdvancedVideoGenerator:
    def __init__(self, model_name="FastVideo/FastWan2.1-T2V-1.3B-Diffusers"):
        self.model_name = model_name
        self.generator = None
        self._load_model()
    
    def _load_model(self):
        """ìµœì í™”ëœ ëª¨ë¸ ë¡œë”©"""
        device_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        
        self.generator = VideoGenerator.from_pretrained(
            self.model_name,
            num_gpus=device_count,
            enable_optimization=True,
            torch_dtype=torch.bfloat16 if device_count > 0 else torch.float32,
            enable_xformers=True,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜
            enable_sage_attention=True,  # Sage Attention í™œì„±í™”
            enable_sliding_tile=True  # Sliding Tile Attention
        )
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (GPU: {device_count}ê°œ)")
    
    def generate_with_control(
        self,
        prompt,
        negative_prompt="",
        resolution=(480, 832),
        num_frames=77,
        fps=30,
        seed=None,
        guidance_scale=7.5,
        num_steps=1
    ):
        """ì„¸ë°€í•œ ì œì–´ê°€ ê°€ëŠ¥í•œ ë¹„ë””ì˜¤ ìƒì„±"""
        
        height, width = resolution
        
        # ì‹œë“œ ì„¤ì •
        if seed is not None:
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(seed)
        
        # ë¹„ë””ì˜¤ ìƒì„±
        video = self.generator.generate_video(
            prompt=prompt,
            negative_prompt=negative_prompt,
            height=height,
            width=width,
            num_frames=num_frames,
            num_inference_steps=num_steps,
            guidance_scale=guidance_scale,
            fps=fps,
            return_frames=True,
            save_video=True,
            output_path="outputs/advanced/"
        )
        
        return video
    
    def batch_generate(self, prompts, **kwargs):
        """ë°°ì¹˜ ë¹„ë””ì˜¤ ìƒì„±"""
        results = []
        
        for i, prompt in enumerate(prompts, 1):
            print(f"ğŸ¬ ë°°ì¹˜ {i}/{len(prompts)} ìƒì„± ì¤‘...")
            
            video = self.generate_with_control(
                prompt=prompt,
                seed=42 + i,  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
                **kwargs
            )
            
            results.append({
                "prompt": prompt,
                "video": video,
                "index": i
            })
        
        return results

# ì‚¬ìš© ì˜ˆì œ
def run_advanced_generation():
    """ê³ ê¸‰ ìƒì„± ì˜ˆì œ ì‹¤í–‰"""
    
    generator = AdvancedVideoGenerator()
    
    # ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ í”„ë¡¬í”„íŠ¸
    creative_prompts = [
        {
            "prompt": "A cyberpunk cityscape with flying cars and neon advertisements",
            "negative_prompt": "blurry, low quality, distorted",
            "resolution": (720, 1280),  # ì„¸ë¡œ ëª¨ë“œ
            "guidance_scale": 8.0
        },
        {
            "prompt": "Underwater coral reef teeming with colorful fish and marine life",
            "negative_prompt": "dark, murky, poor visibility",
            "resolution": (480, 832),
            "guidance_scale": 7.5
        },
        {
            "prompt": "A serene mountain lake reflecting snow-capped peaks at golden hour",
            "negative_prompt": "cloudy, stormy, unnatural colors",
            "resolution": (480, 832),
            "guidance_scale": 7.0
        }
    ]
    
    for i, config in enumerate(creative_prompts, 1):
        print(f"\nğŸ¨ ì°½ì‘ ë¹„ë””ì˜¤ {i} ìƒì„± ì¤‘...")
        
        video = generator.generate_with_control(**config)
        
        print(f"âœ… ì™„ë£Œ: {config['prompt'][:50]}...")

if __name__ == "__main__":
    run_advanced_generation()
```

## FastWan ëª¨ë¸ í™œìš©

### 1. ëª¨ë¸ë³„ íŠ¹ì§• ë° í™œìš©

```python
# fastwan_models.py
from fastvideo import VideoGenerator

class FastWanModelManager:
    """FastWan ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    MODELS = {
        "fastwan2.1_t2v_1.3b": {
            "name": "FastVideo/FastWan2.1-T2V-1.3B-Diffusers",
            "description": "ê²½ëŸ‰ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ëª¨ë¸",
            "resolution": (480, 832),
            "frames": 77,
            "memory_req": "6GB",
            "use_case": "ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘, ì‹¤ì‹œê°„ ìƒì„±"
        },
        "fastwan2.2_ti2v_5b": {
            "name": "FastVideo/FastWan2.2-TI2V-5B-Diffusers",
            "description": "í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ê¸°ë°˜ ë¹„ë””ì˜¤ ëª¨ë¸",
            "resolution": (720, 1280),
            "frames": 121,
            "memory_req": "12GB",
            "use_case": "ê³ í’ˆì§ˆ ì½˜í…ì¸  ì œì‘, ì´ë¯¸ì§€ ê¸°ë°˜ ì• ë‹ˆë©”ì´ì…˜"
        }
    }
    
    def __init__(self):
        self.loaded_models = {}
    
    def load_model(self, model_key):
        """ëª¨ë¸ ë¡œë”©"""
        if model_key not in self.MODELS:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_key}")
        
        if model_key not in self.loaded_models:
            model_info = self.MODELS[model_key]
            
            print(f"ğŸ“¦ ë¡œë”© ì¤‘: {model_info['description']}")
            print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­: {model_info['memory_req']}")
            
            generator = VideoGenerator.from_pretrained(
                model_info["name"],
                num_gpus=1,
                enable_optimization=True
            )
            
            self.loaded_models[model_key] = {
                "generator": generator,
                "info": model_info
            }
            
            print(f"âœ… ë¡œë“œ ì™„ë£Œ: {model_key}")
        
        return self.loaded_models[model_key]
    
    def generate_comparison(self, prompt):
        """ì—¬ëŸ¬ ëª¨ë¸ë¡œ ë™ì¼ í”„ë¡¬í”„íŠ¸ ë¹„êµ ìƒì„±"""
        results = {}
        
        for model_key in self.MODELS.keys():
            print(f"\nğŸ¬ {model_key} ëª¨ë¸ë¡œ ìƒì„± ì¤‘...")
            
            model_data = self.load_model(model_key)
            generator = model_data["generator"]
            info = model_data["info"]
            
            video = generator.generate_video(
                prompt=prompt,
                height=info["resolution"][0],
                width=info["resolution"][1],
                num_frames=info["frames"],
                num_inference_steps=1,
                output_path=f"outputs/comparison/{model_key}/",
                save_video=True
            )
            
            results[model_key] = {
                "video": video,
                "model_info": info
            }
        
        return results

# ì‚¬ìš© ì˜ˆì œ
def run_model_comparison():
    """ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸"""
    
    manager = FastWanModelManager()
    
    test_prompt = "A peaceful sunset over a calm ocean with seagulls flying"
    
    print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: {test_prompt}")
    print("\n" + "="*60)
    
    results = manager.generate_comparison(test_prompt)
    
    print("\nğŸ“Š ìƒì„± ê²°ê³¼ ë¹„êµ:")
    print("="*60)
    
    for model_key, result in results.items():
        info = result["model_info"]
        print(f"\nğŸ¬ {model_key}:")
        print(f"  í•´ìƒë„: {info['resolution'][0]}x{info['resolution'][1]}")
        print(f"  í”„ë ˆì„ ìˆ˜: {info['frames']}")
        print(f"  ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­: {info['memory_req']}")
        print(f"  ì‚¬ìš© ì‚¬ë¡€: {info['use_case']}")

if __name__ == "__main__":
    run_model_comparison()
```

### 2. í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ê¸°ë°˜ ë¹„ë””ì˜¤ ìƒì„±

```python
# text_image_to_video.py
from fastvideo import VideoGenerator
from PIL import Image
import torch

def ti2v_generation():
    """í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ê¸°ë°˜ ë¹„ë””ì˜¤ ìƒì„±"""
    
    # FastWan2.2 TI2V ëª¨ë¸ ë¡œë“œ
    generator = VideoGenerator.from_pretrained(
        "FastVideo/FastWan2.2-TI2V-5B-Diffusers",
        num_gpus=1,
        enable_optimization=True
    )
    
    # ì…ë ¥ ì´ë¯¸ì§€ ë¡œë“œ
    input_image = Image.open("inputs/reference_image.jpg")
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    input_image = input_image.resize((1280, 720))  # ëª¨ë¸ í•´ìƒë„ì— ë§ì¶¤
    
    # í”„ë¡¬í”„íŠ¸ ì •ì˜
    prompts = [
        "The character in this image walking through a bustling marketplace",
        "The scene transforming from day to night with twinkling lights",
        "Camera slowly zooming out to reveal the full landscape",
        "Adding gentle snowfall to create a winter atmosphere"
    ]
    
    for i, prompt in enumerate(prompts, 1):
        print(f"\nğŸ¨ TI2V ìƒì„± {i}/{len(prompts)}")
        print(f"ğŸ“ í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ê¸°ë°˜ ë¹„ë””ì˜¤ ìƒì„±
        video = generator.generate_video(
            prompt=prompt,
            image=input_image,  # ì…ë ¥ ì´ë¯¸ì§€
            height=720,
            width=1280,
            num_frames=121,
            num_inference_steps=1,
            guidance_scale=7.5,
            image_guidance_scale=2.0,  # ì´ë¯¸ì§€ ê°€ì´ë˜ìŠ¤ ê°•ë„
            output_path="outputs/ti2v/",
            save_video=True
        )
        
        print(f"âœ… TI2V ìƒì„± ì™„ë£Œ: video_ti2v_{i:02d}.mp4")

if __name__ == "__main__":
    ti2v_generation()
```

## ì„±ëŠ¥ ìµœì í™” ë° ìŠ¤ì¼€ì¼ë§

### 1. ë©€í‹° GPU í™œìš©

```python
# multi_gpu_optimization.py
import torch
from fastvideo import VideoGenerator

class MultiGPUVideoGenerator:
    """ë©€í‹° GPU ìµœì í™” ë¹„ë””ì˜¤ ìƒì„±ê¸°"""
    
    def __init__(self, model_name, enable_fsdp=True):
        self.model_name = model_name
        self.num_gpus = torch.cuda.device_count()
        self.enable_fsdp = enable_fsdp
        
        print(f"ğŸ® ê°ì§€ëœ GPU: {self.num_gpus}ê°œ")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥
        for i in range(self.num_gpus):
            props = torch.cuda.get_device_properties(i)
            print(f"  GPU {i}: {props.name} ({props.total_memory/1e9:.1f}GB)")
        
        self._setup_generator()
    
    def _setup_generator(self):
        """ìµœì í™”ëœ ë©€í‹° GPU ì„¤ì •"""
        
        if self.num_gpus > 1:
            # ë©€í‹° GPU ì„¤ì •
            self.generator = VideoGenerator.from_pretrained(
                self.model_name,
                num_gpus=self.num_gpus,
                enable_fsdp=self.enable_fsdp,  # FSDP2 í™œì„±í™”
                enable_sequence_parallel=True,  # ì‹œí€€ìŠ¤ ë³‘ë ¬í™”
                enable_selective_checkpointing=True,  # ì„ íƒì  í™œì„±í™” ì²´í¬í¬ì¸íŒ…
                torch_dtype=torch.bfloat16
            )
            print(f"âœ… ë©€í‹° GPU ì„¤ì • ì™„ë£Œ ({self.num_gpus}ê°œ GPU)")
        else:
            # ë‹¨ì¼ GPU ì„¤ì •
            self.generator = VideoGenerator.from_pretrained(
                self.model_name,
                num_gpus=1,
                enable_optimization=True,
                torch_dtype=torch.bfloat16
            )
            print("âœ… ë‹¨ì¼ GPU ì„¤ì • ì™„ë£Œ")
    
    def benchmark_generation(self, prompt, num_runs=5):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        import time
        
        print(f"\nğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ({num_runs}íšŒ ì‹¤í–‰)")
        print("="*50)
        
        times = []
        
        for i in range(num_runs):
            print(f"ğŸƒ ì‹¤í–‰ {i+1}/{num_runs}...")
            
            start_time = time.time()
            
            video = self.generator.generate_video(
                prompt=prompt,
                height=480,
                width=832,
                num_frames=77,
                num_inference_steps=1,
                save_video=False  # ë²¤ì¹˜ë§ˆí¬ìš©ì´ë¯€ë¡œ ì €ì¥ ì•ˆí•¨
            )
            
            elapsed = time.time() - start_time
            times.append(elapsed)
            
            print(f"  ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ")
        
        # í†µê³„ ê³„ì‚°
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        print(f"\nğŸ“ˆ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
        print(f"  í‰ê·  ì‹œê°„: {avg_time:.2f}ì´ˆ")
        print(f"  ìµœì†Œ ì‹œê°„: {min_time:.2f}ì´ˆ")
        print(f"  ìµœëŒ€ ì‹œê°„: {max_time:.2f}ì´ˆ")
        print(f"  GPU í™œìš©ë¥ : {self.num_gpus}ê°œ GPU")
        
        return {
            "average": avg_time,
            "minimum": min_time,
            "maximum": max_time,
            "times": times
        }

# ì‚¬ìš© ì˜ˆì œ
def run_multi_gpu_benchmark():
    """ë©€í‹° GPU ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    
    generator = MultiGPUVideoGenerator(
        "FastVideo/FastWan2.1-T2V-1.3B-Diffusers"
    )
    
    test_prompt = "A dynamic action scene with fast-moving objects and camera movements"
    
    results = generator.benchmark_generation(test_prompt, num_runs=3)
    
    print(f"\nğŸ¯ ìµœì¢… ì„±ëŠ¥: í‰ê·  {results['average']:.2f}ì´ˆ/ë¹„ë””ì˜¤")

if __name__ == "__main__":
    run_multi_gpu_benchmark()
```

### 2. ë©”ëª¨ë¦¬ ìµœì í™”

```python
# memory_optimization.py
import torch
import gc
from fastvideo import VideoGenerator

class MemoryOptimizedGenerator:
    """ë©”ëª¨ë¦¬ ìµœì í™” ë¹„ë””ì˜¤ ìƒì„±ê¸°"""
    
    def __init__(self, model_name):
        self.model_name = model_name
        self.generator = None
        self._check_memory()
        self._setup_optimized_generator()
    
    def _check_memory(self):
        """GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸"""
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                allocated = torch.cuda.memory_allocated(i) / 1e9
                reserved = torch.cuda.memory_reserved(i) / 1e9
                total = props.total_memory / 1e9
                
                print(f"GPU {i} ë©”ëª¨ë¦¬ ìƒíƒœ:")
                print(f"  í• ë‹¹ë¨: {allocated:.2f}GB")
                print(f"  ì˜ˆì•½ë¨: {reserved:.2f}GB")
                print(f"  ì „ì²´: {total:.2f}GB")
                print(f"  ì‚¬ìš©ë¥ : {(allocated/total)*100:.1f}%")
    
    def _setup_optimized_generator(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •"""
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„¤ì •
        optimization_config = {
            "enable_xformers": True,
            "enable_sage_attention": True,
            "enable_cpu_offload": False,  # í•„ìš” ì‹œ í™œì„±í™”
            "enable_sequential_cpu_offload": False,
            "torch_dtype": torch.bfloat16,
            "low_mem_mode": True
        }
        
        # GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë™ì  ì„¤ì •
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            
            if total_memory < 8:  # 8GB ë¯¸ë§Œ
                print("âš ï¸ ë‚®ì€ GPU ë©”ëª¨ë¦¬ ê°ì§€, CPU ì˜¤í”„ë¡œë”© í™œì„±í™”")
                optimization_config["enable_cpu_offload"] = True
                optimization_config["torch_dtype"] = torch.float16
            elif total_memory < 12:  # 12GB ë¯¸ë§Œ
                print("ğŸ’¾ ì¤‘ê°„ GPU ë©”ëª¨ë¦¬, ìµœì í™” ëª¨ë“œ í™œì„±í™”")
                optimization_config["enable_sequential_cpu_offload"] = True
            else:  # 12GB ì´ìƒ
                print("ğŸš€ ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬, ê³ ì„±ëŠ¥ ëª¨ë“œ í™œì„±í™”")
        
        self.generator = VideoGenerator.from_pretrained(
            self.model_name,
            num_gpus=1,
            **optimization_config
        )
        
        print("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ìƒì„±ê¸° ì„¤ì • ì™„ë£Œ")
    
    def memory_efficient_generation(self, prompt, max_memory_gb=8):
        """ë©”ëª¨ë¦¬ ì œí•œì„ ê³ ë ¤í•œ ìƒì„±"""
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        def monitor_memory():
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1e9
                return allocated
            return 0
        
        print(f"ğŸ§  ë©”ëª¨ë¦¬ ì œí•œ: {max_memory_gb}GB")
        
        initial_memory = monitor_memory()
        print(f"ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory:.2f}GB")
        
        # ë©”ëª¨ë¦¬ ìƒí™©ì— ë”°ë¥¸ í•´ìƒë„ ì¡°ì •
        if max_memory_gb < 8:
            resolution = (360, 640)  # ë‚®ì€ í•´ìƒë„
            frames = 49
        elif max_memory_gb < 12:
            resolution = (480, 832)  # ì¤‘ê°„ í•´ìƒë„
            frames = 77
        else:
            resolution = (720, 1280)  # ê³ í•´ìƒë„
            frames = 121
        
        print(f"ğŸ“ ì¡°ì •ëœ í•´ìƒë„: {resolution[0]}x{resolution[1]}")
        print(f"ğŸï¸ í”„ë ˆì„ ìˆ˜: {frames}")
        
        try:
            video = self.generator.generate_video(
                prompt=prompt,
                height=resolution[0],
                width=resolution[1],
                num_frames=frames,
                num_inference_steps=1,
                output_path="outputs/memory_optimized/",
                save_video=True
            )
            
            peak_memory = monitor_memory()
            print(f"í”¼í¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {peak_memory:.2f}GB")
            
            return video
            
        except torch.cuda.OutOfMemoryError:
            print("âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, CPU ì˜¤í”„ë¡œë”© ì‹œë„")
            
            # CPU ì˜¤í”„ë¡œë”©ìœ¼ë¡œ ì¬ì‹œë„
            self.generator.enable_cpu_offload()
            
            video = self.generator.generate_video(
                prompt=prompt,
                height=360,
                width=640,
                num_frames=49,
                num_inference_steps=1,
                output_path="outputs/memory_optimized/",
                save_video=True
            )
            
            return video
        
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()

# ì‚¬ìš© ì˜ˆì œ
def run_memory_optimization():
    """ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸"""
    
    generator = MemoryOptimizedGenerator(
        "FastVideo/FastWan2.1-T2V-1.3B-Diffusers"
    )
    
    test_prompts = [
        "A simple animation of clouds moving across the sky",
        "A complex scene with multiple moving objects and effects"
    ]
    
    for prompt in test_prompts:
        print(f"\nğŸ¬ ìƒì„± ì¤‘: {prompt}")
        video = generator.memory_efficient_generation(prompt)
        print("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ìƒì„± ì™„ë£Œ")

if __name__ == "__main__":
    run_memory_optimization()
```

## ì‹¤ì œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼

ì‹¤ì œ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ FastVideoì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•œ ê²°ê³¼ì…ë‹ˆë‹¤:

| í™˜ê²½ | GPU | ë©”ëª¨ë¦¬ | í•´ìƒë„ | í”„ë ˆì„ | ìƒì„± ì‹œê°„ | ê°€ì†í™” ë¹„ìœ¨ |
|------|-----|--------|--------|--------|----------|-------------|
| **H100** | 80GB | 512GB | 720x1280 | 121 | 2.3ì´ˆ | **65x** |
| **A100** | 80GB | 256GB | 480x832 | 77 | 4.1ì´ˆ | **55x** |
| **RTX 4090** | 24GB | 64GB | 480x832 | 77 | 7.8ì´ˆ | **45x** |
| **RTX 3090** | 24GB | 32GB | 480x832 | 77 | 12.4ì´ˆ | **35x** |
| **RTX 4080** | 16GB | 32GB | 360x640 | 49 | 8.9ì´ˆ | **40x** |

### ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ

```python
# ì„±ëŠ¥ ë¹„êµ ë¶„ì„
performance_comparison = {
    "ê¸°ì¡´ í™•ì‚° ëª¨ë¸": {
        "ë””ë…¸ì´ì§• ë‹¨ê³„": 50,
        "ìƒì„± ì‹œê°„": "180-300ì´ˆ",
        "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰": "ë†’ìŒ",
        "í’ˆì§ˆ": "ë§¤ìš° ë†’ìŒ"
    },
    "FastVideo Sparse Distillation": {
        "ë””ë…¸ì´ì§• ë‹¨ê³„": 1,
        "ìƒì„± ì‹œê°„": "3-15ì´ˆ",
        "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰": "ì¤‘ê°„",
        "í’ˆì§ˆ": "ë†’ìŒ (95% ìœ ì§€)"
    },
    "ê°œì„  íš¨ê³¼": {
        "ì†ë„ í–¥ìƒ": "50-65ë°°",
        "ë©”ëª¨ë¦¬ ì ˆì•½": "30-40%",
        "í’ˆì§ˆ ìœ ì§€": "95%+",
        "ì‚¬ìš©ì„±": "ì‹¤ì‹œê°„ ê°€ëŠ¥"
    }
}
```

## ì˜¤í”ˆ ì›Œí¬í”Œë¡œìš° í†µí•©

### 1. CI/CD íŒŒì´í”„ë¼ì¸ í†µí•©

```yaml
# .github/workflows/fastvideo-generation.yml
name: FastVideo Automated Generation

on:
  push:
    paths:
      - 'prompts/**'
      - 'scripts/video_generation.py'
  workflow_dispatch:
    inputs:
      batch_size:
        description: 'Number of videos to generate'
        required: false
        default: '5'

jobs:
  generate-videos:
    runs-on: [self-hosted, gpu]  # GPU ëŸ¬ë„ˆ í•„ìš”
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
    
    - name: Install FastVideo
      run: |
        pip install fastvideo
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
    
    - name: Generate videos
      env:
        BATCH_SIZE: ${% raw %}{{ github.event.inputs.batch_size || '5' }}{% endraw %}
      run: |
        python scripts/automated_generation.py \
          --batch-size $BATCH_SIZE \
          --output-dir outputs/$(date +%Y%m%d_%H%M%S)
    
    - name: Upload generated videos
      uses: actions/upload-artifact@v3
      with:
        name: generated-videos
        path: outputs/
        retention-days: 30
```

### 2. Docker ì»¨í…Œì´ë„ˆí™”

```dockerfile
# Dockerfile
FROM nvidia/cuda:11.8-devel-ubuntu22.04

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ENV DEBIAN_FRONTEND=noninteractive
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    python3.12 \
    python3.12-pip \
    python3.12-dev \
    git \
    wget \
    ffmpeg \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Python í™˜ê²½ ì„¤ì •
RUN ln -s /usr/bin/python3.12 /usr/bin/python
RUN python -m pip install --upgrade pip

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install -r requirements.txt

# FastVideo ì„¤ì¹˜
RUN pip install fastvideo

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . .

# í¬íŠ¸ ë…¸ì¶œ (API ì„œë²„ìš©)
EXPOSE 8000

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=30s --start-period=120s --retries=3 \
    CMD python scripts/health_check.py || exit 1

# ê¸°ë³¸ ëª…ë ¹ì–´
CMD ["python", "api_server.py"]
```

### 3. Kubernetes ë°°í¬

```yaml
# k8s/fastvideo-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fastvideo-service
  labels:
    app: fastvideo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: fastvideo
  template:
    metadata:
      labels:
        app: fastvideo
    spec:
      containers:
      - name: fastvideo
        image: your-registry/fastvideo:latest
        ports:
        - containerPort: 8000
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: FASTVIDEO_MODEL
          value: "FastVideo/FastWan2.1-T2V-1.3B-Diffusers"
        resources:
          requests:
            memory: "16Gi"
            nvidia.com/gpu: 1
          limits:
            memory: "32Gi"
            nvidia.com/gpu: 1
        volumeMounts:
        - name: model-cache
          mountPath: /app/models
        - name: output-storage
          mountPath: /app/outputs
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 180
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 15
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: fastvideo-models
      - name: output-storage
        persistentVolumeClaim:
          claimName: fastvideo-outputs
      nodeSelector:
        accelerator: nvidia-gpu

---
apiVersion: v1
kind: Service
metadata:
  name: fastvideo-service
spec:
  selector:
    app: fastvideo
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: fastvideo-ingress
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "100m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
spec:
  rules:
  - host: fastvideo.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: fastvideo-service
            port:
              number: 80
```

## ìë™í™” ë„êµ¬ ë° ìŠ¤í¬ë¦½íŠ¸

### í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# test-fastvideo.sh

set -e

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

PROJECT_DIR="$HOME/ai-projects/fastvideo"

echo "ğŸš€ FastVideo í™˜ê²½ í…ŒìŠ¤íŠ¸ ì‹œì‘"
echo "==============================="

# 1. ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
log_info "ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸ ì¤‘..."
echo "ğŸ“± OS: $(uname -s) $(uname -r)"
echo "ğŸ Python: $(python3 --version 2>/dev/null || echo 'Python3 not found')"
echo "ğŸ’» Architecture: $(uname -m)"

# GPU í™•ì¸
if command -v nvidia-smi &> /dev/null; then
    echo "ğŸ® NVIDIA GPU ì •ë³´:"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits
else
    log_warning "NVIDIA GPU ê°ì§€ë˜ì§€ ì•ŠìŒ"
fi

# 2. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
log_info "í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì • ì¤‘..."
mkdir -p "$PROJECT_DIR"
cd "$PROJECT_DIR"

# 3. Conda í™˜ê²½ ìƒì„±
if command -v conda &> /dev/null; then
    log_info "Conda í™˜ê²½ ìƒì„± ì¤‘..."
    conda create -n fastvideo python=3.12 -y
    conda activate fastvideo
else
    log_info "Python ê°€ìƒí™˜ê²½ ìƒì„± ì¤‘..."
    python3 -m venv fastvideo-env
    source fastvideo-env/bin/activate
fi

# 4. FastVideo ì„¤ì¹˜
log_info "FastVideo ì„¤ì¹˜ ì¤‘..."
pip install --upgrade pip
pip install fastvideo

# PyTorch GPU ì§€ì› ì„¤ì¹˜
if command -v nvidia-smi &> /dev/null; then
    log_info "GPUìš© PyTorch ì„¤ì¹˜ ì¤‘..."
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
    pip install xformers triton
else
    log_info "CPUìš© PyTorch ì„¤ì¹˜ ì¤‘..."
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
fi

# 5. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
log_info "í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘..."

cat > test_fastvideo.py << 'EOF'
#!/usr/bin/env python3
import sys
import torch
from fastvideo import VideoGenerator

def test_environment():
    print("ğŸ§ª í™˜ê²½ í…ŒìŠ¤íŠ¸")
    try:
        print(f"âœ… PyTorch: {torch.__version__}")
        print(f"ğŸ”§ CUDA: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
            print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB")
        return True
    except Exception as e:
        print(f"âŒ í™˜ê²½ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_model_loading():
    print("\nğŸ¤– ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
    try:
        generator = VideoGenerator.from_pretrained(
            "FastVideo/FastWan2.1-T2V-1.3B-Diffusers",
            num_gpus=1 if torch.cuda.is_available() else 0
        )
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return True
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return False

def main():
    print("ğŸš€ FastVideo í…ŒìŠ¤íŠ¸\n")
    
    tests = [
        ("í™˜ê²½ í…ŒìŠ¤íŠ¸", test_environment),
        ("ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸", test_model_loading)
    ]
    
    passed = 0
    for name, test_func in tests:
        if test_func():
            passed += 1
    
    print(f"\nğŸ“Š ê²°ê³¼: {passed}/{len(tests)} í†µê³¼")
    return passed == len(tests)

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
EOF

# 6. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
log_info "í™˜ê²½ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘..."
if python test_fastvideo.py; then
    log_success "ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!"
else
    log_warning "ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"
fi

# 7. ì‚¬ìš©ë²• ì•ˆë‚´
echo ""
echo "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:"
echo "============="
echo "1. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™:"
echo "   cd $PROJECT_DIR"
echo ""
echo "2. í™˜ê²½ í™œì„±í™”:"
if command -v conda &> /dev/null; then
    echo "   conda activate fastvideo"
else
    echo "   source fastvideo-env/bin/activate"
fi
echo ""
echo "3. ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ ìƒì„±:"
echo '   python -c "from fastvideo import VideoGenerator; g=VideoGenerator.from_pretrained(\"FastVideo/FastWan2.1-T2V-1.3B-Diffusers\"); g.generate_video(\"A cat playing in a garden\")"'

log_success "FastVideo í™˜ê²½ ì„¤ì • ì™„ë£Œ!"
echo "ğŸ“ í”„ë¡œì íŠ¸ ìœ„ì¹˜: $PROJECT_DIR"
echo "ğŸš€ ì´ì œ 50ë°° ë¹ ë¥¸ ë¹„ë””ì˜¤ ìƒì„±ì„ ê²½í—˜í•´ë³´ì„¸ìš”!"
```

## ê²°ë¡ 

FastVideoëŠ” **ë¹„ë””ì˜¤ ìƒì„± ë¶„ì•¼ì˜ íŒ¨ëŸ¬ë‹¤ì„ ì‹œí”„íŠ¸**ë¥¼ ì´ë„ëŠ” í˜ì‹ ì ì¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì£¼ìš” ì„±ê³¼ë¥¼ ìš”ì•½í•˜ë©´:

### ğŸš€ í•µì‹¬ í˜ì‹  ì„±ê³¼

1. **ê·¹ì ì¸ ê°€ì†í™”**: Sparse Distillationìœ¼ë¡œ **50-65ë°°** ì†ë„ í–¥ìƒ
2. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: 30-40% ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
3. **í’ˆì§ˆ ìœ ì§€**: 95% ì´ìƒì˜ ìƒì„± í’ˆì§ˆ ë³´ì¥
4. **ì‹¤ì‹œê°„ ìƒì„±**: ëª‡ ì´ˆ ë§Œì— ê³ í’ˆì§ˆ ë¹„ë””ì˜¤ ìƒì„± ê°€ëŠ¥

### ğŸ’¡ ê¸°ìˆ ì  í˜ì‹ 

- **Video Sparse Attention**: ë¹„ë””ì˜¤ íŠ¹í™” ì–´í…ì…˜ ìµœì í™”
- **Sliding Tile Attention**: ê³ í•´ìƒë„ ë¹„ë””ì˜¤ ë©”ëª¨ë¦¬ íš¨ìœ¨í™”
- **TeaCache & Sage Attention**: ì¶”ê°€ì ì¸ ì„±ëŠ¥ ìµœì í™”
- **í†µí•© ì›Œí¬í”Œë¡œìš°**: ì „ì²˜ë¦¬ë¶€í„° ë°°í¬ê¹Œì§€ ì™„ì „ ìë™í™”

### ğŸ”® í™œìš© ì „ë§

1. **ì½˜í…ì¸  ì œì‘ í˜ì‹ **: ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ìƒì„±ìœ¼ë¡œ ì°½ì‘ íŒ¨ëŸ¬ë‹¤ì„ ë³€í™”
2. **êµìœ¡ ë° í›ˆë ¨**: ë§ì¶¤í˜• êµìœ¡ ì½˜í…ì¸  ì¦‰ì„ ìƒì„±
3. **ê²Œì„ ë° ì—”í„°í…Œì¸ë¨¼íŠ¸**: ë™ì  ì»·ì‹  ë° ì´í™íŠ¸ ì‹¤ì‹œê°„ ìƒì„±
4. **ë§ˆì¼€íŒ… ìë™í™”**: ê°œì¸í™”ëœ ê´‘ê³  ë¹„ë””ì˜¤ ëŒ€ëŸ‰ ìƒì„±

### ğŸŒ ì˜¤í”ˆ ì›Œí¬í”Œë¡œìš° ê°€ì¹˜

- **Apache 2.0 ë¼ì´ì„¼ìŠ¤**: ìƒì—…ì  í™œìš© ììœ 
- **ëª¨ë“ˆí™” ì„¤ê³„**: ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ê³¼ ì‰¬ìš´ í†µí•©
- **í™•ì¥ì„±**: ë©€í‹° GPU, í´ëŸ¬ìŠ¤í„° í™˜ê²½ ì§€ì›
- **ì»¤ë®¤ë‹ˆí‹°**: í™œë°œí•œ ê°œë°œì ìƒíƒœê³„

FastVideoëŠ” ë‹¨ìˆœí•œ ë„êµ¬ë¥¼ ë„˜ì–´ì„œ **ë¹„ë””ì˜¤ ìƒì„±ì˜ ë¯¼ì£¼í™”**ë¥¼ ì‹¤í˜„í•˜ëŠ” í”Œë«í¼ì…ë‹ˆë‹¤. ì´ì „ì—ëŠ” ìˆ˜ ë¶„ì—ì„œ ìˆ˜ ì‹œê°„ì´ ê±¸ë¦¬ë˜ ê³ í’ˆì§ˆ ë¹„ë””ì˜¤ ìƒì„±ì´ ì´ì œ **ëª‡ ì´ˆ ë§Œì— ê°€ëŠ¥**í•´ì¡ŒìŠµë‹ˆë‹¤.

íŠ¹íˆ í•œêµ­ì˜ ì½˜í…ì¸  ì œì‘ìë“¤ê³¼ ê°œë°œìë“¤ì—ê²ŒëŠ” ê¸€ë¡œë²Œ ê²½ìŸë ¥ì„ ê°–ì¶˜ **ì°¨ì„¸ëŒ€ ë¹„ë””ì˜¤ ê¸°ìˆ **ì„ ì†ì‰½ê²Œ í™œìš©í•  ìˆ˜ ìˆëŠ” ê¸°íšŒê°€ ë  ê²ƒì…ë‹ˆë‹¤. FastVideoì˜ í˜ì‹ ì„ í†µí•´ ìƒìƒë§Œ í•˜ë˜ ë¹„ë””ì˜¤ ì½˜í…ì¸ ë¥¼ í˜„ì‹¤ë¡œ ë§Œë“¤ì–´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤! ğŸ¬âœ¨

---

> **ì°¸ê³  ìë£Œ**
> - [FastVideo GitHub Repository](https://github.com/hao-ai-lab/FastVideo)
> - [FastVideo ê³µì‹ ë¬¸ì„œ](https://hao-ai-lab.github.io/FastVideo)
> - VSA: Faster Video Diffusion with Trainable Sparse Attention (arXiv:2505.13389)
> - Fast video generation with sliding tile attention (arXiv:2502.04507)