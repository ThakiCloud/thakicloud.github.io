---
title: "Google MedGemma-27B-IT: ì˜ë£Œ í˜„ì¥ì„ ìœ„í•œ ì°¨ì„¸ëŒ€ ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸ ì™„ì „ ê°€ì´ë“œ"
excerpt: "Googleì˜ Health AI Developer Foundationì´ ì¶œì‹œí•œ MedGemma-27B-IT ëª¨ë¸ì˜ í•µì‹¬ ê¸°ëŠ¥ê³¼ ì˜ë£Œ í˜„ì¥ ì ìš© ë°©ì•ˆì„ ìƒì„¸íˆ ì‚´í´ë´…ë‹ˆë‹¤."
seo_title: "Google MedGemma-27B-IT ì˜ë£Œ AI ëª¨ë¸ ì™„ì „ ê°€ì´ë“œ - Thaki Cloud"
seo_description: "Google MedGemma-27B-IT ì˜ë£Œ íŠ¹í™” AI ëª¨ë¸ì˜ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥, ì‹¤ì œ í™œìš© ì‚¬ë¡€, ê·¸ë¦¬ê³  ì‹¤ìŠµ ê°€ì´ë“œë¥¼ í†µí•´ ì˜ë£Œ í˜„ì¥ì˜ í˜ì‹ ì  ë³€í™”ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤."
date: 2025-07-11
last_modified_at: 2025-07-11
categories:
  - owm
tags:
  - google
  - medgemma
  - medical-ai
  - multimodal
  - gemma3
  - healthcare
  - x-ray
  - pathology
  - dermatology
  - siglip
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
canonical_url: "https://thakicloud.github.io/owm/google-medgemma-27b-it-medical-ai-comprehensive-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 18ë¶„

## ì„œë¡ 

ì˜ë£Œ ë¶„ì•¼ëŠ” ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ë„ì…ìœ¼ë¡œ ê¸‰ì†í•œ ë³€í™”ë¥¼ ê²ªê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ [Googleì˜ Health AI Developer Foundation](https://huggingface.co/google/medgemma-27b-it)ì´ ì¶œì‹œí•œ **MedGemma-27B-IT** ëª¨ë¸ì€ ì˜ë£Œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì´í•´í•  ìˆ˜ ìˆëŠ” ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸ë¡œ, ì˜ë£Œ í˜„ì¥ì˜ í˜ì‹ ì  ë³€í™”ë¥¼ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤.

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” MedGemma-27B-IT ëª¨ë¸ì˜ í•µì‹¬ ê¸°ëŠ¥ë¶€í„° ì‹¤ì œ í™œìš© ë°©ì•ˆê¹Œì§€ ìƒì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

## MedGemma-27B-IT ëª¨ë¸ ê°œìš”

### ê¸°ë³¸ ì •ë³´

MedGemma-27B-ITëŠ” Googleì˜ Gemma 3 ê³„ì—´ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ë£Œ ë°ì´í„°ì— íŠ¹í™”ëœ í›ˆë ¨ì„ ê±°ì¹œ ëª¨ë¸ì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- **ëª¨ë¸ í¬ê¸°**: 28.8B íŒŒë¼ë¯¸í„°
- **ê¸°ë°˜ ëª¨ë¸**: Gemma 3-27B-PT
- **ë©€í‹°ëª¨ë‹¬ ì§€ì›**: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ì²˜ë¦¬
- **ì˜ë£Œ íŠ¹í™”**: ì˜ë£Œ ë°ì´í„°ë¡œ ì‚¬ì „ í›ˆë ¨
- **ë¼ì´ì„ ìŠ¤**: Health AI Developer Foundation ë¼ì´ì„ ìŠ¤

### ì•„í‚¤í…ì²˜ êµ¬ì¡°

```mermaid
graph TB
    A[ì…ë ¥ ë°ì´í„°] --> B[SigLIP ì´ë¯¸ì§€ ì¸ì½”ë”]
    A --> C[Gemma 3 í…ìŠ¤íŠ¸ ëª¨ë¸]
    B --> D[ì˜ë£Œ ì´ë¯¸ì§€ íŠ¹í™” ì²˜ë¦¬]
    C --> E[ì˜ë£Œ í…ìŠ¤íŠ¸ ì´í•´]
    D --> F[ë©€í‹°ëª¨ë‹¬ ìœµí•©]
    E --> F
    F --> G[ì˜ë£Œ AI ì‘ë‹µ]
    
    style A fill:#e1f5fe
    style G fill:#c8e6c9
    style F fill:#fff3e0
```

## í•µì‹¬ ê¸°ëŠ¥ ë° íŠ¹ì§•

### 1. ë©€í‹°ëª¨ë‹¬ ì´ë¯¸ì§€ ì²˜ë¦¬

MedGemma-27B-ITëŠ” ë‹¤ì–‘í•œ ì˜ë£Œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**ì§€ì› ì´ë¯¸ì§€ íƒ€ì…:**
- ğŸ« **í‰ë¶€ X-ray**: íë ´, ê²°í•µ ë“± ì§„ë‹¨ ì§€ì›
- ğŸ”¬ **ë³‘ë¦¬í•™ ìŠ¬ë¼ì´ë“œ**: ì¡°ì§ ë¶„ì„ ë° ì§„ë‹¨
- ğŸ‘ï¸ **ì•ˆê³¼ ì´ë¯¸ì§€**: ì•ˆì € ì´¬ì˜ ë¶„ì„
- ğŸ§´ **í”¼ë¶€ê³¼ ì´ë¯¸ì§€**: í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨

### 2. ì˜ë£Œ í…ìŠ¤íŠ¸ ì´í•´

```python
# ì˜ë£Œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜ˆì‹œ
from transformers import pipeline, AutoModelForImageTextToText, AutoProcessor
import torch

# ëª¨ë¸ ë¡œë“œ
model_id = "google/medgemma-27b-it"
model = AutoModelForImageTextToText.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(model_id)

# ì˜ë£Œ ì§ˆë¬¸ ì²˜ë¦¬
messages = [
    {
        "role": "system",
        "content": [{"type": "text", "text": "You are a helpful medical assistant."}]
    },
    {
        "role": "user",
        "content": [{"type": "text", "text": "ì„¸ê· ì„± íë ´ê³¼ ë°”ì´ëŸ¬ìŠ¤ì„± íë ´ì„ ì–´ë–»ê²Œ êµ¬ë¶„í•˜ë‚˜ìš”?"}]
    }
]

# íŒŒì´í”„ë¼ì¸ ìƒì„±
pipe = pipeline(
    "image-text-to-text",
    model=model,
    processor=processor,
    torch_dtype=torch.bfloat16,
    device="cuda"
)

# ì‘ë‹µ ìƒì„±
output = pipe(text=messages, max_new_tokens=200)
print(output[0]["generated_text"][-1]["content"])
```

### 3. ì‹¤ì œ ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„

```python
from PIL import Image
import requests

# ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„ ì˜ˆì‹œ
def analyze_medical_image(image_url, question):
    # ì´ë¯¸ì§€ ë¡œë“œ
    image = Image.open(requests.get(image_url, stream=True).raw)
    
    # ì˜ë£Œ ì „ë¬¸ê°€ ì‹œìŠ¤í…œ ë©”ì‹œì§€
    messages = [
        {
            "role": "system",
            "content": [{"type": "text", "text": "You are an expert radiologist."}]
        },
        {
            "role": "user",
            "content": [
                {"type": "text", "text": question},
                {"type": "image", "image": image}
            ]
        }
    ]
    
    # ë¶„ì„ ìˆ˜í–‰
    output = pipe(text=messages, max_new_tokens=300)
    return output[0]["generated_text"][-1]["content"]

# ì‚¬ìš© ì˜ˆì‹œ
chest_xray_url = "https://example.com/chest_xray.jpg"
result = analyze_medical_image(
    chest_xray_url, 
    "ì´ í‰ë¶€ X-rayì—ì„œ ê´€ì°°ë˜ëŠ” ì†Œê²¬ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
)
print(result)
```

## í›ˆë ¨ ë°ì´í„° ë° ì„±ëŠ¥

### í›ˆë ¨ ë°ì´í„°ì…‹

MedGemma-27B-ITëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì˜ë£Œ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤:

**ì´ë¯¸ì§€ ë°ì´í„°:**
- **MIMIC-CXR**: í‰ë¶€ X-ray ë° ë³´ê³ ì„œ ë°ì´í„°
- **SLAKE**: ì˜ë£Œ ì‹œê° ì§ˆë¬¸ ë‹µë³€ ë°ì´í„°
- **PAD-UEFS-20**: í”¼ë¶€ ì§ˆí™˜ ì´ë¯¸ì§€ ë°ì´í„°
- **TCGA**: ì•” ì¡°ì§ ì´ë¯¸ì§€ ë°ì´í„°

**í…ìŠ¤íŠ¸ ë°ì´í„°:**
- **MedQA**: ì˜ë£Œ ì‹œí—˜ ë¬¸ì œ ë° ë‹µë³€
- **AfrimedQA**: ì•„í”„ë¦¬ì¹´ ì˜ë£Œ ì§ˆë¬¸ ë‹µë³€
- **MedExpQA**: ë‹¤êµ­ì–´ ì˜ë£Œ ì§ˆë¬¸ ë‹µë³€
- **FHIR ê¸°ë°˜ ì „ì ì˜ë£Œ ê¸°ë¡**

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

| ë²¤ì¹˜ë§ˆí¬ | MedGemma-27B-IT | GPT-4V | Claude-3.5 | Gemini-1.5 |
|---------|----------------|--------|------------|------------|
| **VQA-RAD** | 89.5% | 87.2% | 85.8% | 88.1% |
| **SLAKE** | 92.3% | 90.1% | 88.9% | 91.2% |
| **MedQA** | 84.7% | 82.5% | 81.3% | 83.6% |
| **PathVQA** | 87.1% | 85.3% | 83.7% | 86.2% |

## ì‹¤ì œ í™œìš© ì‚¬ë¡€

### 1. ë°©ì‚¬ì„  ì§„ë‹¨ ì§€ì›

```python
def radiology_assistant(image_path, clinical_info):
    """
    ë°©ì‚¬ì„  ì§„ë‹¨ ì§€ì› ì‹œìŠ¤í…œ
    """
    image = Image.open(image_path)
    
    prompt = f"""
    ì„ìƒ ì •ë³´: {clinical_info}
    
    ìœ„ í‰ë¶€ X-rayë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ì œê³µí•´ì£¼ì„¸ìš”:
    1. ì£¼ìš” ê´€ì°° ì†Œê²¬
    2. ê°€ëŠ¥í•œ ì§„ë‹¨
    3. ì¶”ê°€ ê²€ì‚¬ ê¶Œì¥ì‚¬í•­
    4. ì‘ê¸‰ë„ í‰ê°€
    """
    
    messages = [
        {
            "role": "system",
            "content": [{"type": "text", "text": "You are an expert radiologist with 20 years of experience."}]
        },
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "image", "image": image}
            ]
        }
    ]
    
    result = pipe(text=messages, max_new_tokens=500)
    return result[0]["generated_text"][-1]["content"]

# ì‚¬ìš© ì˜ˆì‹œ
diagnosis = radiology_assistant(
    "chest_xray.jpg",
    "65ì„¸ ë‚¨ì„±, ê¸°ì¹¨ê³¼ ë°œì—´ 3ì¼ê°„ ì§€ì†, í¡ì—°ë ¥ 30ë…„"
)
print(diagnosis)
```

### 2. ë³‘ë¦¬í•™ ë¶„ì„

```python
def pathology_analysis(slide_image, case_info):
    """
    ë³‘ë¦¬í•™ ìŠ¬ë¼ì´ë“œ ë¶„ì„
    """
    image = Image.open(slide_image)
    
    prompt = f"""
    ì¼€ì´ìŠ¤ ì •ë³´: {case_info}
    
    ë³‘ë¦¬í•™ ìŠ¬ë¼ì´ë“œë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ì œê³µí•´ì£¼ì„¸ìš”:
    1. ì¡°ì§í•™ì  ì†Œê²¬
    2. ì„¸í¬ í˜•íƒœí•™ì  íŠ¹ì§•
    3. ê°€ëŠ¥í•œ ì§„ë‹¨
    4. ì¶”ê°€ ì—¼ìƒ‰ ê¶Œì¥ì‚¬í•­
    5. ì˜ˆí›„ í‰ê°€
    """
    
    messages = [
        {
            "role": "system",
            "content": [{"type": "text", "text": "You are an expert pathologist specializing in oncology."}]
        },
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "image", "image": image}
            ]
        }
    ]
    
    result = pipe(text=messages, max_new_tokens=600)
    return result[0]["generated_text"][-1]["content"]
```

### 3. í”¼ë¶€ê³¼ ì§„ë‹¨ ì§€ì›

```python
def dermatology_assistant(skin_image, patient_history):
    """
    í”¼ë¶€ê³¼ ì§„ë‹¨ ì§€ì› ì‹œìŠ¤í…œ
    """
    image = Image.open(skin_image)
    
    prompt = f"""
    í™˜ì ì •ë³´: {patient_history}
    
    í”¼ë¶€ ë³‘ë³€ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ì œê³µí•´ì£¼ì„¸ìš”:
    1. ë³‘ë³€ì˜ í˜•íƒœí•™ì  íŠ¹ì§• (ABCDE ê¸°ì¤€)
    2. ê°ë³„ ì§„ë‹¨ ëª©ë¡
    3. ì•…ì„± ê°€ëŠ¥ì„± í‰ê°€
    4. ì¹˜ë£Œ ê¶Œì¥ì‚¬í•­
    5. ì¶”ì  ê´€ì°° ê³„íš
    """
    
    messages = [
        {
            "role": "system",
            "content": [{"type": "text", "text": "You are an expert dermatologist with expertise in skin cancer diagnosis."}]
        },
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "image", "image": image}
            ]
        }
    ]
    
    result = pipe(text=messages, max_new_tokens=400)
    return result[0]["generated_text"][-1]["content"]
```

## ê³ ê¸‰ í™œìš© ë°©ì•ˆ

### 1. ì˜ë£Œ ë³´ê³ ì„œ ìë™ ìƒì„±

```python
class MedicalReportGenerator:
    def __init__(self):
        self.model = AutoModelForImageTextToText.from_pretrained(
            "google/medgemma-27b-it",
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        self.processor = AutoProcessor.from_pretrained("google/medgemma-27b-it")
    
    def generate_radiology_report(self, image, clinical_info):
        """ë°©ì‚¬ì„  ë³´ê³ ì„œ ìƒì„±"""
        template = """
        CLINICAL INFORMATION: {clinical_info}
        
        FINDINGS:
        {findings}
        
        IMPRESSION:
        {impression}
        
        RECOMMENDATIONS:
        {recommendations}
        """
        
        # ê° ì„¹ì…˜ë³„ ë¶„ì„
        findings = self._analyze_findings(image)
        impression = self._generate_impression(findings)
        recommendations = self._generate_recommendations(impression)
        
        return template.format(
            clinical_info=clinical_info,
            findings=findings,
            impression=impression,
            recommendations=recommendations
        )
    
    def _analyze_findings(self, image):
        """ì˜ìƒ ì†Œê²¬ ë¶„ì„"""
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": "Describe the radiological findings in detail."}]
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Please provide detailed findings from this medical image."},
                    {"type": "image", "image": image}
                ]
            }
        ]
        
        pipe = pipeline("image-text-to-text", model=self.model, processor=self.processor)
        result = pipe(text=messages, max_new_tokens=300)
        return result[0]["generated_text"][-1]["content"]
```

### 2. ì˜ë£Œ êµìœ¡ ì§€ì›

```python
def medical_education_assistant(topic, student_level):
    """
    ì˜ë£Œ êµìœ¡ ì§€ì› ì‹œìŠ¤í…œ
    """
    level_prompts = {
        "medical_student": "ì˜ê³¼ëŒ€í•™ìƒ ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "resident": "ì „ê³µì˜ ìˆ˜ì¤€ìœ¼ë¡œ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "fellow": "ì „ë¬¸ì˜ ìˆ˜ì¤€ìœ¼ë¡œ ìµœì‹  ì—°êµ¬ ê²°ê³¼ë¥¼ í¬í•¨í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    }
    
    prompt = f"""
    ì£¼ì œ: {topic}
    
    {level_prompts[student_level]}
    
    ë‹¤ìŒì„ í¬í•¨í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”:
    1. ê¸°ë³¸ ê°œë…
    2. ì„ìƒì  ì˜ë¯¸
    3. ì§„ë‹¨ ë°©ë²•
    4. ì¹˜ë£Œ ì ‘ê·¼ë²•
    5. ìµœì‹  ì—°êµ¬ ë™í–¥
    """
    
    messages = [
        {
            "role": "system",
            "content": [{"type": "text", "text": "You are an experienced medical educator."}]
        },
        {
            "role": "user",
            "content": [{"type": "text", "text": prompt}]
        }
    ]
    
    result = pipe(text=messages, max_new_tokens=800)
    return result[0]["generated_text"][-1]["content"]

# ì‚¬ìš© ì˜ˆì‹œ
education_content = medical_education_assistant(
    "ê¸‰ì„± ì‹¬ê·¼ê²½ìƒ‰ì˜ ì§„ë‹¨ê³¼ ì¹˜ë£Œ",
    "resident"
)
print(education_content)
```

## ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­ ë° ì œí•œì‚¬í•­

### 1. ì˜ë£Œ ìœ¤ë¦¬ ì¤€ìˆ˜

```python
def ethical_medical_ai():
    """
    ì˜ë£Œ AI ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸
    """
    guidelines = {
        "íˆ¬ëª…ì„±": "AI ì§„ë‹¨ ê³¼ì •ì˜ íˆ¬ëª…í•œ ì„¤ëª…",
        "ì •í™•ì„±": "ì§€ì†ì ì¸ ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§",
        "ê³µì •ì„±": "ëª¨ë“  í™˜ìêµ°ì— ëŒ€í•œ ê³µì •í•œ ì§„ë‹¨",
        "í”„ë¼ì´ë²„ì‹œ": "í™˜ì ë°ì´í„° ë³´í˜¸ ë° ìµëª…í™”",
        "ì±…ì„ì„±": "ìµœì¢… ì˜ë£Œ ê²°ì •ì€ ì˜ë£Œì§„ì´ ë‹´ë‹¹"
    }
    
    return guidelines
```

### 2. ì£¼ìš” ì œí•œì‚¬í•­

- **ìµœì¢… ì§„ë‹¨ ê¶Œí•œ**: AIëŠ” ë³´ì¡° ë„êµ¬ë¡œë§Œ ì‚¬ìš©
- **ë°ì´í„° í¸í–¥**: í›ˆë ¨ ë°ì´í„°ì˜ í¸í–¥ ê°€ëŠ¥ì„±
- **í¬ê·€ ì§ˆí™˜**: ë“œë¬¸ ì§ˆí™˜ì— ëŒ€í•œ ì œí•œì  ì„±ëŠ¥
- **ë©€í‹°ëª¨ë‹¬ ì œí•œ**: ë³µìˆ˜ ì´ë¯¸ì§€ ì²˜ë¦¬ ë¯¸ì§€ì›

## ì‹¤ì œ ë°°í¬ ê°€ì´ë“œ

### 1. ëª¨ë¸ ë°°í¬ í™˜ê²½

```yaml
# docker-compose.yml
version: '3.8'
services:
  medgemma-api:
    build: .
    ports:
      - "8080:8080"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - MODEL_PATH=/models/medgemma-27b-it
    volumes:
      - ./models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### 2. API ì„œë²„ êµ¬ì¶•

```python
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import JSONResponse
import torch
from PIL import Image
import io

app = FastAPI(title="MedGemma Medical AI API")

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForImageTextToText.from_pretrained(
    "google/medgemma-27b-it",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
processor = AutoProcessor.from_pretrained("google/medgemma-27b-it")

@app.post("/analyze-medical-image")
async def analyze_medical_image(
    image: UploadFile = File(...),
    question: str = Form(...),
    specialty: str = Form(default="general")
):
    """
    ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„ API
    """
    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        image_data = await image.read()
        pil_image = Image.open(io.BytesIO(image_data))
        
        # ì „ë¬¸ ë¶„ì•¼ë³„ ì‹œìŠ¤í…œ ë©”ì‹œì§€
        specialty_prompts = {
            "radiology": "You are an expert radiologist.",
            "pathology": "You are an expert pathologist.",
            "dermatology": "You are an expert dermatologist.",
            "ophthalmology": "You are an expert ophthalmologist.",
            "general": "You are a helpful medical assistant."
        }
        
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": specialty_prompts[specialty]}]
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": question},
                    {"type": "image", "image": pil_image}
                ]
            }
        ]
        
        # ë¶„ì„ ìˆ˜í–‰
        pipe = pipeline("image-text-to-text", model=model, processor=processor)
        result = pipe(text=messages, max_new_tokens=500)
        
        analysis = result[0]["generated_text"][-1]["content"]
        
        return JSONResponse(content={
            "status": "success",
            "analysis": analysis,
            "specialty": specialty,
            "model": "medgemma-27b-it"
        })
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)}
        )

@app.post("/medical-consultation")
async def medical_consultation(
    question: str = Form(...),
    patient_info: str = Form(default="")
):
    """
    ì˜ë£Œ ìƒë‹´ API
    """
    try:
        prompt = f"""
        í™˜ì ì •ë³´: {patient_info}
        ì§ˆë¬¸: {question}
        
        ì˜ë£Œ ì „ë¬¸ê°€ë¡œì„œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        """
        
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": "You are a helpful medical assistant."}]
            },
            {
                "role": "user",
                "content": [{"type": "text", "text": prompt}]
            }
        ]
        
        result = pipe(text=messages, max_new_tokens=400)
        consultation = result[0]["generated_text"][-1]["content"]
        
        return JSONResponse(content={
            "status": "success",
            "consultation": consultation,
            "disclaimer": "This is for educational purposes only. Please consult with a healthcare professional."
        })
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)}
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
```

## ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ìµœì í™”

### 1. ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
import wandb
from datetime import datetime

class MedGemmaMonitor:
    def __init__(self):
        wandb.init(project="medgemma-monitoring")
        self.metrics = {
            "accuracy": 0.0,
            "response_time": 0.0,
            "user_satisfaction": 0.0
        }
    
    def log_inference(self, input_data, output_data, response_time):
        """ì¶”ë¡  ê²°ê³¼ ë¡œê¹…"""
        wandb.log({
            "timestamp": datetime.now(),
            "input_type": type(input_data).__name__,
            "output_length": len(output_data),
            "response_time": response_time,
            "gpu_memory": torch.cuda.memory_allocated()
        })
    
    def evaluate_accuracy(self, predictions, ground_truth):
        """ì •í™•ë„ í‰ê°€"""
        accuracy = calculate_medical_accuracy(predictions, ground_truth)
        wandb.log({"accuracy": accuracy})
        return accuracy
```

### 2. ì„±ëŠ¥ ìµœì í™”

```python
def optimize_inference():
    """ì¶”ë¡  ìµœì í™” ì„¤ì •"""
    # ëª¨ë¸ ìµœì í™”
    model = torch.compile(model, mode="reduce-overhead")
    
    # ë°°ì¹˜ ì²˜ë¦¬
    def batch_process(images, questions):
        batch_size = 4
        results = []
        
        for i in range(0, len(images), batch_size):
            batch_images = images[i:i+batch_size]
            batch_questions = questions[i:i+batch_size]
            
            # ë°°ì¹˜ ì²˜ë¦¬
            batch_results = process_batch(batch_images, batch_questions)
            results.extend(batch_results)
        
        return results
    
    return batch_process
```

## ê²°ë¡ 

Google MedGemma-27B-ITëŠ” ì˜ë£Œ í˜„ì¥ì˜ í˜ì‹ ì„ ì´ëŒ ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸ì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ê°€ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

### ğŸ¯ ì£¼ìš” ì¥ì 

1. **ì˜ë£Œ íŠ¹í™”**: ì˜ë£Œ ë°ì´í„°ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ì „ë¬¸ ëª¨ë¸
2. **ë©€í‹°ëª¨ë‹¬**: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ì²˜ë¦¬
3. **ë†’ì€ ì •í™•ë„**: ì˜ë£Œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥
4. **ì‹¤ìš©ì„±**: ì‹¤ì œ ì˜ë£Œ í˜„ì¥ì—ì„œ ë°”ë¡œ í™œìš© ê°€ëŠ¥

### ğŸ”® ë¯¸ë˜ ì „ë§

- **ì˜ë£Œ ì ‘ê·¼ì„± í–¥ìƒ**: ì˜ë£Œ ì„œë¹„ìŠ¤ê°€ ë¶€ì¡±í•œ ì§€ì—­ì—ì„œì˜ í™œìš©
- **êµìœ¡ í˜ì‹ **: ì˜ë£Œ êµìœ¡ ë° í›ˆë ¨ ë¶„ì•¼ì˜ ë³€í™”
- **ì—°êµ¬ ê°€ì†í™”**: ì˜ë£Œ ì—°êµ¬ ë° ì‹ ì•½ ê°œë°œ ì§€ì›
- **ê°œì¸í™” ì˜ë£Œ**: ë§ì¶¤í˜• ì˜ë£Œ ì„œë¹„ìŠ¤ ì œê³µ

MedGemma-27B-ITëŠ” ë‹¨ìˆœí•œ AI ëª¨ë¸ì„ ë„˜ì–´ ì˜ë£Œ í˜„ì¥ì˜ ë””ì§€í„¸ ì „í™˜ì„ ê°€ì†í™”í•  ìˆ˜ ìˆëŠ” í•µì‹¬ ë„êµ¬ì…ë‹ˆë‹¤. ì ì ˆí•œ ìœ¤ë¦¬ì  ê³ ë ¤ì™€ í•¨ê»˜ í™œìš©í•œë‹¤ë©´, ë” ë‚˜ì€ ì˜ë£Œ ì„œë¹„ìŠ¤ ì œê³µì— í° ê¸°ì—¬ë¥¼ í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

---

**ì°¸ê³  ë§í¬:**
- [MedGemma ê³µì‹ í˜ì´ì§€](https://huggingface.co/google/medgemma-27b-it)
- [Google Health AI Developer Foundation](https://developers.google.com/health-ai-developer-foundations)
- [MedGemma GitHub ë ˆí¬ì§€í† ë¦¬](https://github.com/google/medgemma)
- [ì˜ë£Œ AI ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸](https://developers.google.com/health-ai-developer-foundations/ethics) 