---
title: "MiniMax-M1: ì„¸ê³„ ìµœì´ˆ ì˜¤í”ˆ ì›¨ì´íŠ¸ í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ ì¶”ë¡  ëª¨ë¸"
excerpt: "MiniMax-M1ì˜ í˜ì‹ ì ì¸ í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ ì•„í‚¤í…ì²˜ì™€ ë›°ì–´ë‚œ ì¶”ë¡  ì„±ëŠ¥, ì‹¤ë¬´ ë°°í¬ ê°€ì´ë“œ"
date: 2025-06-17
categories:
  - owm
tags:
  - MiniMax-M1
  - Hybrid Attention
  - Reasoning Model
  - Open Weight Model
  - Function Calling
  - Long Context
  - Mathematical Reasoning
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
---

## ê°œìš”

MiniMax-M1ì€ ì„¸ê³„ ìµœì´ˆì˜ ì˜¤í”ˆ ì›¨ì´íŠ¸, ëŒ€ê·œëª¨ í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ ì¶”ë¡  ëª¨ë¸ì…ë‹ˆë‹¤. MiniMaxì—ì„œ ê°œë°œí•œ ì´ ëª¨ë¸ì€ í˜ì‹ ì ì¸ í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ë›°ì–´ë‚œ ì¶”ë¡  ì„±ëŠ¥ê³¼ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ëŠ¥ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

**GitHub í†µê³„**:
- â­ **110 stars**
- ğŸ´ **1 fork**
- ğŸ‘¥ **3 contributors**
- ğŸ“„ **ê¸°ìˆ  ë³´ê³ ì„œ** í¬í•¨

**ëª¨ë¸ íŠ¹ì§•**:
- ğŸ§  **í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜**: í˜ì‹ ì ì¸ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
- ğŸ”¢ **ë›°ì–´ë‚œ ìˆ˜í•™ ì¶”ë¡ **: AIME 2024ì—ì„œ 86.0% ì„±ëŠ¥
- ğŸ’» **ì½”ë”© ëŠ¥ë ¥**: LiveCodeBenchì—ì„œ 65.0% ì„±ëŠ¥
- ğŸ“š **ê¸´ ì»¨í…ìŠ¤íŠ¸**: ìµœëŒ€ 1M í† í° ì§€ì›
- ğŸ› ï¸ **í•¨ìˆ˜ í˜¸ì¶œ**: êµ¬ì¡°í™”ëœ í•¨ìˆ˜ í˜¸ì¶œ ì§€ì›

---

## í•µì‹¬ ê¸°ìˆ : í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜

### í˜ì‹ ì ì¸ ì•„í‚¤í…ì²˜

MiniMax-M1ì˜ ê°€ì¥ ì¤‘ìš”í•œ í˜ì‹ ì€ **í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜**ì…ë‹ˆë‹¤. ì´ëŠ” ê¸°ì¡´ì˜ ë‹¨ì¼ ì–´í…ì…˜ ë°©ì‹ê³¼ ë‹¬ë¦¬ ì—¬ëŸ¬ ì–´í…ì…˜ íŒ¨í„´ì„ ì¡°í•©í•˜ì—¬ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ ë™ì‹œì— ë‹¬ì„±í•©ë‹ˆë‹¤.

**í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ì˜ ì¥ì **:
- **íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©**: ê¸´ ì‹œí€€ìŠ¤ì—ì„œë„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ìœ ì§€
- **í–¥ìƒëœ ì¶”ë¡  ëŠ¥ë ¥**: ë³µì¡í•œ ë…¼ë¦¬ì  ê´€ê³„ íŒŒì•…
- **í™•ì¥ ê°€ëŠ¥ì„±**: ë‹¤ì–‘í•œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì— ì ì‘

### ì•„í‚¤í…ì²˜ êµ¬ì„±

```python
# MiniMax-M1 ëª¨ë¸ êµ¬ì„± ì˜ˆì‹œ
{
    "architectures": ["MinimaxM1ForCausalLM"],
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 1,
    "eos_token_id": 2,
    "hidden_act": "silu",
    "hidden_size": 4096,
    "initializer_range": 0.02,
    "intermediate_size": 14336,
    "max_position_embeddings": 40960,
    "model_type": "minimax_m1",
    "num_attention_heads": 32,
    "num_hidden_layers": 32,
    "num_key_value_heads": 8,
    "pretraining_tp": 1,
    "rms_norm_eps": 1e-06,
    "rope_scaling": null,
    "rope_theta": 10000.0,
    "tie_word_embeddings": false,
    "torch_dtype": "bfloat16",
    "transformers_version": "4.37.2",
    "use_cache": true,
    "vocab_size": 100352
}
```

---

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ìˆ˜í•™ì  ì¶”ë¡ 

**AIME (American Invitational Mathematics Examination)**:
- **AIME 2024**: 86.0% (GPT-4o 83.3%, Claude-3.5-Sonnet 85.7% ëŒ€ë¹„ ìš°ìˆ˜)
- **AIME 2025**: 76.9% (ìµœì‹  ë¬¸ì œì—ì„œë„ ë†’ì€ ì„±ëŠ¥)
- **MATH-500**: 96.8% (ìˆ˜í•™ ë¬¸ì œ í•´ê²°ì—ì„œ ìµœê³  ìˆ˜ì¤€)

### ì½”ë”© ëŠ¥ë ¥

**LiveCodeBench (2024/8~2025/5)**:
- **MiniMax-M1**: 65.0%
- **GPT-4o**: 62.3%
- **Claude-3.5-Sonnet**: 65.9%
- **o1-preview**: 73.1% (ì°¸ê³ ìš©)

**FullStackBench**:
- **MiniMax-M1**: 68.3%
- ì „ì²´ì ìœ¼ë¡œ ê²½ìŸ ëª¨ë¸ë“¤ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥

### ì¶”ë¡  ë° ì§€ì‹

**GPQA Diamond**:
- **MiniMax-M1**: 70.0%
- ê³¼í•™ì  ì¶”ë¡ ì—ì„œ ì•ˆì •ì ì¸ ì„±ëŠ¥

**ZebraLogic**:
- **MiniMax-M1**: 86.8%
- ë…¼ë¦¬ì  ì¶”ë¡ ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥

**MMLU-Pro**:
- **MiniMax-M1**: 81.1%
- ì¢…í•©ì ì¸ ì§€ì‹ í‰ê°€ì—ì„œ ìš°ìˆ˜í•œ ê²°ê³¼

### ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§

**SWE-bench Verified**:
- **MiniMax-M1**: 56.0%
- ì‹¤ì œ ì†Œí”„íŠ¸ì›¨ì–´ ë²„ê·¸ ìˆ˜ì • ì‘ì—…ì—ì„œ ë†’ì€ ì„±ëŠ¥

### ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬

**OpenAI-MRCR**:
- **128k ì»¨í…ìŠ¤íŠ¸**: 73.4%
- **1M ì»¨í…ìŠ¤íŠ¸**: 56.2%
- ë§¤ìš° ê¸´ ë¬¸ì„œ ì²˜ë¦¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥

**LongBench-v2**:
- **MiniMax-M1**: 61.5%
- ë‹¤ì–‘í•œ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì‘ì—…ì—ì„œ ì•ˆì •ì ì¸ ì„±ëŠ¥

---

## ëª¨ë¸ ë³€í˜•

### MiniMax-M1-40k

**ê¸°ë³¸ ëª¨ë¸**:
- **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: 40,960 í† í°
- **ìš©ë„**: ì¼ë°˜ì ì¸ ì¶”ë¡  ë° ì½”ë”© ì‘ì—…
- **ì„±ëŠ¥**: ê· í˜• ì¡íŒ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±

### MiniMax-M1-80k

**í™•ì¥ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸**:
- **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: 81,920 í† í°
- **ìš©ë„**: ê¸´ ë¬¸ì„œ ë¶„ì„ ë° ë³µì¡í•œ ì¶”ë¡ 
- **ì„±ëŠ¥**: ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ í–¥ìƒëœ ì„±ëŠ¥

---

## ë°°í¬ ê°€ì´ë“œ

### vLLMì„ ì‚¬ìš©í•œ í”„ë¡œë•ì…˜ ë°°í¬

**ê¶Œì¥ ë°°í¬ ë°©ë²•**:
```bash
# vLLM ì„¤ì¹˜
pip install vllm

# MiniMax-M1 ì„œë¹™
python -m vllm.entrypoints.openai.api_server \
    --model MiniMax-AI/MiniMax-M1-40k \
    --served-model-name minimax-m1 \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 4
```

**vLLMì˜ ì¥ì **:
- ğŸ”¥ **ë›°ì–´ë‚œ ì„œë¹™ ì²˜ë¦¬ëŸ‰**: ë†’ì€ ë™ì‹œ ìš”ì²­ ì²˜ë¦¬
- âš¡ **íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬**: ì§€ëŠ¥ì ì¸ ë©”ëª¨ë¦¬ í• ë‹¹
- ğŸ“¦ **ê°•ë ¥í•œ ë°°ì¹˜ ì²˜ë¦¬**: ìš”ì²­ ë°°ì¹˜ ìµœì í™”
- âš™ï¸ **ìµœì í™”ëœ ì„±ëŠ¥**: í•˜ë“œì›¨ì–´ ê°€ì† í™œìš©

### Transformersë¥¼ ì‚¬ìš©í•œ ì§ì ‘ ë°°í¬

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "MiniMax-AI/MiniMax-M1-40k"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

# ì¶”ë¡  ì‹¤í–‰
def generate_response(prompt, max_length=2048):
    inputs = tokenizer(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=1.0,
            top_p=0.95,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(prompt):]

# ì‚¬ìš© ì˜ˆì‹œ
prompt = "Solve this math problem: What is the derivative of x^3 + 2x^2 - 5x + 1?"
response = generate_response(prompt)
print(response)
```

---

## í•¨ìˆ˜ í˜¸ì¶œ ê¸°ëŠ¥

### êµ¬ì¡°í™”ëœ í•¨ìˆ˜ í˜¸ì¶œ

MiniMax-M1ì€ ì™¸ë¶€ í•¨ìˆ˜ í˜¸ì¶œì´ í•„ìš”í•œ ìƒí™©ì„ ì‹ë³„í•˜ê³  êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ í•¨ìˆ˜ í˜¸ì¶œ íŒŒë¼ë¯¸í„°ë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# í•¨ìˆ˜ ì •ì˜ ì˜ˆì‹œ
functions = [
    {
        "name": "calculate_math",
        "description": "Perform mathematical calculations",
        "parameters": {
            "type": "object",
            "properties": {
                "expression": {
                    "type": "string",
                    "description": "Mathematical expression to evaluate"
                },
                "operation": {
                    "type": "string",
                    "enum": ["add", "subtract", "multiply", "divide", "derivative", "integral"],
                    "description": "Type of mathematical operation"
                }
            },
            "required": ["expression", "operation"]
        }
    }
]

# í•¨ìˆ˜ í˜¸ì¶œ í”„ë¡¬í”„íŠ¸
system_prompt = """You are a helpful assistant that can call functions when needed.
Available functions: {functions}

When you need to perform calculations, use the calculate_math function."""

user_prompt = "What is the derivative of 3x^2 + 2x - 1?"

# ëª¨ë¸ ì‘ë‹µì—ì„œ í•¨ìˆ˜ í˜¸ì¶œ ì¶”ì¶œ
response = generate_response(f"{system_prompt}\n\nUser: {user_prompt}")
```

### í•¨ìˆ˜ í˜¸ì¶œ ì‘ë‹µ í˜•ì‹

```json
{
    "function_call": {
        "name": "calculate_math",
        "arguments": {
            "expression": "3x^2 + 2x - 1",
            "operation": "derivative"
        }
    }
}
```

---

## ì‹¤ë¬´ ì ìš© ì‚¬ë¡€

### ìˆ˜í•™ êµìœ¡ í”Œë«í¼

```python
class MathTutor:
    def __init__(self):
        self.model = AutoModelForCausalLM.from_pretrained(
            "MiniMax-AI/MiniMax-M1-40k",
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained("MiniMax-AI/MiniMax-M1-40k")
    
    def solve_problem(self, problem, show_steps=True):
        prompt = f"""Solve this math problem step by step:
        
Problem: {problem}

Please provide:
1. Step-by-step solution
2. Final answer
3. Explanation of key concepts used
"""
        
        response = self.generate_response(prompt)
        return response
    
    def generate_practice_problems(self, topic, difficulty="medium", count=5):
        prompt = f"""Generate {count} {difficulty} level practice problems for {topic}.
        
Format each problem as:
Problem X: [problem statement]
Answer: [solution]
"""
        
        response = self.generate_response(prompt)
        return response

# ì‚¬ìš© ì˜ˆì‹œ
tutor = MathTutor()
solution = tutor.solve_problem("Find the limit of (x^2 - 4)/(x - 2) as x approaches 2")
print(solution)
```

### ì½”ë“œ ë¦¬ë·° ì‹œìŠ¤í…œ

```python
class CodeReviewer:
    def __init__(self):
        self.model = AutoModelForCausalLM.from_pretrained(
            "MiniMax-AI/MiniMax-M1-40k",
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained("MiniMax-AI/MiniMax-M1-40k")
    
    def review_code(self, code, language="python"):
        prompt = f"""Review this {language} code and provide feedback:

```{language}
{code}
```

Please analyze:
1. Code quality and style
2. Potential bugs or issues
3. Performance optimizations
4. Best practices recommendations
5. Security considerations
"""
        
        response = self.generate_response(prompt)
        return response
    
    def suggest_improvements(self, code, language="python"):
        prompt = f"""Suggest improvements for this {language} code:

```{language}
{code}
```

Provide:
1. Refactored code
2. Explanation of changes
3. Benefits of improvements
"""
        
        response = self.generate_response(prompt)
        return response

# ì‚¬ìš© ì˜ˆì‹œ
reviewer = CodeReviewer()
code_to_review = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
"""

review = reviewer.review_code(code_to_review)
print(review)
```

### ê¸´ ë¬¸ì„œ ë¶„ì„ ì‹œìŠ¤í…œ

```python
class DocumentAnalyzer:
    def __init__(self):
        self.model = AutoModelForCausalLM.from_pretrained(
            "MiniMax-AI/MiniMax-M1-80k",  # ê¸´ ì»¨í…ìŠ¤íŠ¸ìš© ëª¨ë¸ ì‚¬ìš©
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained("MiniMax-AI/MiniMax-M1-80k")
    
    def analyze_document(self, document_text, analysis_type="summary"):
        if analysis_type == "summary":
            prompt = f"""Analyze this document and provide a comprehensive summary:

{document_text}

Please provide:
1. Executive summary
2. Key points and findings
3. Important details
4. Conclusions and recommendations
"""
        elif analysis_type == "qa":
            prompt = f"""Based on this document, generate important questions and answers:

{document_text}

Generate 10 important Q&A pairs that cover the main content.
"""
        
        response = self.generate_response(prompt)
        return response
    
    def extract_insights(self, document_text):
        prompt = f"""Extract key insights and patterns from this document:

{document_text}

Identify:
1. Main themes and patterns
2. Critical insights
3. Data trends (if applicable)
4. Strategic implications
5. Action items
"""
        
        response = self.generate_response(prompt)
        return response

# ì‚¬ìš© ì˜ˆì‹œ
analyzer = DocumentAnalyzer()
# ê¸´ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
long_document = "..." # ì‹¤ì œ ê¸´ ë¬¸ì„œ ë‚´ìš©
summary = analyzer.analyze_document(long_document, "summary")
print(summary)
```

---

## API ë° ì„œë¹„ìŠ¤

### ì˜¨ë¼ì¸ ì±—ë´‡

MiniMaxëŠ” ì¼ë°˜ ì‚¬ìš© ë° í‰ê°€ë¥¼ ìœ„í•´ ì˜¨ë¼ì¸ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ìˆëŠ” ì±—ë´‡ì„ ì œê³µí•©ë‹ˆë‹¤.

**ê¸°ëŠ¥**:
- ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰
- ìˆ˜í•™ ë¬¸ì œ í•´ê²°
- ì½”ë“œ ìƒì„± ë° ë””ë²„ê¹…
- ê¸´ ë¬¸ì„œ ë¶„ì„

### MiniMax MCP Server

ê°œë°œìë¥¼ ìœ„í•œ MCP (Model Context Protocol) ì„œë²„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

**ì§€ì› ê¸°ëŠ¥**:
- ğŸ¥ **ë¹„ë””ì˜¤ ìƒì„±**: AI ê¸°ë°˜ ë¹„ë””ì˜¤ ì½˜í…ì¸  ìƒì„±
- ğŸ–¼ï¸ **ì´ë¯¸ì§€ ìƒì„±**: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë³€í™˜
- ğŸ—£ï¸ **ìŒì„± í•©ì„±**: ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ìƒì„±
- ğŸ­ **ìŒì„± ë³µì œ**: ê°œì¸í™”ëœ ìŒì„± ìƒì„±

### API ì‚¬ìš© ì˜ˆì‹œ

```python
import requests

# MiniMax API í˜¸ì¶œ ì˜ˆì‹œ
def call_minimax_api(prompt, model="minimax-m1-40k"):
    url = "https://api.minimax.io/v1/chat/completions"
    headers = {
        "Authorization": "Bearer YOUR_API_KEY",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 1.0,
        "top_p": 0.95,
        "max_tokens": 2048
    }
    
    response = requests.post(url, headers=headers, json=data)
    return response.json()

# ì‚¬ìš© ì˜ˆì‹œ
result = call_minimax_api("Explain quantum computing in simple terms")
print(result["choices"][0]["message"]["content"])
```

---

## ì„±ëŠ¥ ìµœì í™” íŒ

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±

```python
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë¡œë”©
model = AutoModelForCausalLM.from_pretrained(
    "MiniMax-AI/MiniMax-M1-40k",
    torch_dtype=torch.bfloat16,  # ë©”ëª¨ë¦¬ ì ˆì•½
    device_map="auto",           # ìë™ ë””ë°”ì´ìŠ¤ ë°°ì¹˜
    load_in_8bit=True,          # 8ë¹„íŠ¸ ì–‘ìí™” (ì„ íƒì‚¬í•­)
    trust_remote_code=True
)

# ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… (íŒŒì¸íŠœë‹ ì‹œ)
model.gradient_checkpointing_enable()
```

### ì¶”ë¡  ìµœì í™”

```python
# íš¨ìœ¨ì ì¸ ìƒì„± ì„¤ì •
generation_config = {
    "temperature": 1.0,
    "top_p": 0.95,
    "do_sample": True,
    "max_new_tokens": 2048,
    "pad_token_id": tokenizer.eos_token_id,
    "use_cache": True,           # KV ìºì‹œ ì‚¬ìš©
    "repetition_penalty": 1.1    # ë°˜ë³µ ë°©ì§€
}

outputs = model.generate(**inputs, **generation_config)
```

### ë°°ì¹˜ ì²˜ë¦¬

```python
def batch_generate(prompts, batch_size=4):
    results = []
    
    for i in range(0, len(prompts), batch_size):
        batch_prompts = prompts[i:i+batch_size]
        
        # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            batch_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        )
        
        with torch.no_grad():
            outputs = model.generate(**inputs, **generation_config)
        
        # ë””ì½”ë”©
        batch_results = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        results.extend(batch_results)
    
    return results
```

---

## ë¹„êµ ë¶„ì„

### ë‹¤ë¥¸ ì˜¤í”ˆ ëª¨ë¸ê³¼ì˜ ë¹„êµ

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ | AIME 2024 | LiveCodeBench | íŠ¹ì§• |
|------|----------|---------------|-----------|---------------|------|
| **MiniMax-M1** | ~40B | 40k/80k | **86.0%** | **65.0%** | í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ |
| Llama-3.1-70B | 70B | 128k | 83.3% | 62.3% | í‘œì¤€ ì–´í…ì…˜ |
| Claude-3.5-Sonnet | ~200B | 200k | 85.7% | 65.9% | ìƒìš© ëª¨ë¸ |
| GPT-4o | ~200B | 128k | 83.3% | 62.3% | ìƒìš© ëª¨ë¸ |

### ì¥ë‹¨ì  ë¶„ì„

**ì¥ì **:
- âœ… **í˜ì‹ ì ì¸ ì•„í‚¤í…ì²˜**: í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ì˜ íš¨ìœ¨ì„±
- âœ… **ë›°ì–´ë‚œ ì¶”ë¡  ì„±ëŠ¥**: ìˆ˜í•™ ë° ë…¼ë¦¬ì  ì¶”ë¡ ì—ì„œ ìš°ìˆ˜
- âœ… **ì˜¤í”ˆ ì›¨ì´íŠ¸**: ì™„ì „í•œ ëª¨ë¸ ì ‘ê·¼ ê°€ëŠ¥
- âœ… **í•¨ìˆ˜ í˜¸ì¶œ ì§€ì›**: êµ¬ì¡°í™”ëœ ë„êµ¬ ì‚¬ìš©
- âœ… **ê¸´ ì»¨í…ìŠ¤íŠ¸**: ìµœëŒ€ 1M í† í° ì§€ì›

**ê°œì„  ì˜ì—­**:
- ğŸ”„ **ì»¤ë®¤ë‹ˆí‹° ìƒíƒœê³„**: ìƒëŒ€ì ìœ¼ë¡œ ìƒˆë¡œìš´ ëª¨ë¸
- ğŸ”„ **ë¬¸ì„œí™”**: ë” ë§ì€ ì‚¬ìš© ì˜ˆì œ í•„ìš”
- ğŸ”„ **ìµœì í™”**: ì¶”ê°€ì ì¸ ì„±ëŠ¥ íŠœë‹ ê°€ëŠ¥

---

## í–¥í›„ ë°œì „ ë°©í–¥

### ê¸°ìˆ ì  ê°œì„ 

**í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ ê³ ë„í™”**:
- ë” íš¨ìœ¨ì ì¸ ì–´í…ì…˜ íŒ¨í„´ ê°œë°œ
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ê°€ ìµœì í™”
- ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ ì§€ì›

**ì„±ëŠ¥ í–¥ìƒ**:
- ì¶”ë¡  ì†ë„ ê°œì„ 
- ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- í•˜ë“œì›¨ì–´ ê°€ì† ì§€ì› ê°•í™”

### ì‘ìš© ë¶„ì•¼ í™•ì¥

**ì „ë¬¸ ë„ë©”ì¸**:
- ê³¼í•™ ì—°êµ¬ ì§€ì›
- ì˜ë£Œ ì§„ë‹¨ ë³´ì¡°
- ë²•ë¥  ë¬¸ì„œ ë¶„ì„
- ê¸ˆìœµ ë°ì´í„° ë¶„ì„

**ë©€í‹°ëª¨ë‹¬ í™•ì¥**:
- ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í†µí•©
- ì˜¤ë””ì˜¤ ì²˜ë¦¬ ëŠ¥ë ¥
- ë¹„ë””ì˜¤ ì´í•´ ê¸°ëŠ¥

---

## ê²°ë¡ 

MiniMax-M1ì€ í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì¶”ë¡  ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì„ ë™ì‹œì— ë‹¬ì„±í•œ í˜ì‹ ì ì¸ ì˜¤í”ˆ ì›¨ì´íŠ¸ ëª¨ë¸ì…ë‹ˆë‹¤. ì£¼ìš” ì„±ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

**ê¸°ìˆ ì  í˜ì‹ **:
- **í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜**: ìƒˆë¡œìš´ ì–´í…ì…˜ íŒ¨ëŸ¬ë‹¤ì„ ì œì‹œ
- **ë›°ì–´ë‚œ ì¶”ë¡  ëŠ¥ë ¥**: ìˆ˜í•™, ì½”ë”©, ë…¼ë¦¬ì  ì¶”ë¡ ì—ì„œ ìµœê³  ìˆ˜ì¤€
- **ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬**: ìµœëŒ€ 1M í† í°ê¹Œì§€ íš¨ìœ¨ì  ì²˜ë¦¬
- **í•¨ìˆ˜ í˜¸ì¶œ**: êµ¬ì¡°í™”ëœ ë„êµ¬ ì‚¬ìš© ì§€ì›

**ì‹¤ë¬´ì  ê°€ì¹˜**:
- **ì˜¤í”ˆ ì›¨ì´íŠ¸**: ì™„ì „í•œ ëª¨ë¸ ì ‘ê·¼ì„±
- **í”„ë¡œë•ì…˜ ì¤€ë¹„**: vLLM ë“±ì„ í†µí•œ ì‰¬ìš´ ë°°í¬
- **ë‹¤ì–‘í•œ ì‘ìš©**: êµìœ¡, ì—°êµ¬, ê°œë°œ ë“± í­ë„“ì€ í™œìš©
- **í™•ì¥ì„±**: ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ í™˜ê²½ ì§€ì›

**ìƒíƒœê³„ ê¸°ì—¬**:
- **ì˜¤í”ˆì†ŒìŠ¤ ë°œì „**: íˆ¬ëª…í•œ ëª¨ë¸ ê³µê°œ
- **ì—°êµ¬ ì´‰ì§„**: í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ ì—°êµ¬ í™œì„±í™”
- **ì‹¤ìš©ì„±**: ì‹¤ì œ ë¬¸ì œ í•´ê²°ì— ì ìš© ê°€ëŠ¥

MiniMax-M1ì€ ì˜¤í”ˆ ì›¨ì´íŠ¸ ëª¨ë¸ì˜ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ë©°, íŠ¹íˆ ì¶”ë¡ ì´ ì¤‘ìš”í•œ ì‘ì—…ì—ì„œ ìƒìš© ëª¨ë¸ì— í•„ì í•˜ëŠ” ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ì´ë¼ëŠ” í˜ì‹ ì ì¸ ì ‘ê·¼ ë°©ì‹ì€ í–¥í›„ LLM ë°œì „ì˜ ì¤‘ìš”í•œ ë°©í–¥ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤.

ë” ìì„¸í•œ ì •ë³´ëŠ” [MiniMax-M1 GitHub ì €ì¥ì†Œ](https://github.com/MiniMax-AI/MiniMax-M1)ì™€ [ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://www.minimax.io/)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 