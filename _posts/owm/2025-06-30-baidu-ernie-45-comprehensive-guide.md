---
title: "Baidu ERNIE 4.5: 0.3Bë¶€í„° 424Bê¹Œì§€ ì™„ì „ ì˜¤í”ˆì†ŒìŠ¤ AI ëª¨ë¸ ì‹œë¦¬ì¦ˆ ì™„ë²½ ê°€ì´ë“œ"
excerpt: "Baiduê°€ 2025ë…„ 6ì›” ë§ˆì§€ë§‰ ë‚  ê³µê°œí•œ ERNIE 4.5 ëª¨ë¸ ì‹œë¦¬ì¦ˆ ì™„ë²½ ë¶„ì„. MoE ì•„í‚¤í…ì²˜, Vision-Language ëª¨ë¸, Apache 2.0 ë¼ì´ì„ ìŠ¤, ì‹¤ì „ í™œìš©ê¹Œì§€ ì´ì •ë¦¬."
seo_title: "Baidu ERNIE 4.5 ì™„ì „ ì˜¤í”ˆì†ŒìŠ¤ AI ëª¨ë¸ ê°€ì´ë“œ - 0.3B~424B MoE - Thaki Cloud"
seo_description: "Baidu ERNIE 4.5 ì‹œë¦¬ì¦ˆ ì™„ë²½ ë¶„ì„. 0.3B~424B íŒŒë¼ë¯¸í„°, MoE ì•„í‚¤í…ì²˜, Vision-Language ëª¨ë¸, Apache 2.0 ì˜¤í”ˆì†ŒìŠ¤, 128K ì»¨í…ìŠ¤íŠ¸, ì‹¤ì „ ë°°í¬ ê°€ì´ë“œê¹Œì§€ ìƒì„¸ ì •ë¦¬."
date: 2025-06-30
last_modified_at: 2025-06-30
categories:
  - owm
tags:
  - ernie-45
  - baidu
  - open-source-ai
  - moe-architecture
  - vision-language
  - apache-license
  - transformers
  - paddlepaddle
  - chinese-ai
  - multilingual-model
author_profile: true
toc: true
toc_label: "ëª©ì°¨"
toc_icon: "cog"
toc_sticky: true
header:
  teaser: "/assets/images/thumbnails/post-thumbnail.jpg"
  overlay_image: "/assets/images/headers/post-header.jpg"
  overlay_filter: 0.5
canonical_url: "https://thakicloud.github.io/owm/baidu-ernie-45-comprehensive-guide/"
reading_time: true
---

â±ï¸ **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 12ë¶„

## ì„œë¡ 

2025ë…„ 6ì›” 30ì¼, Baiduê°€ ì•½ì†ì„ ì§€ì¼°ìŠµë‹ˆë‹¤! ğŸš€ 6ì›” ë§ˆì§€ë§‰ ë‚ ì— **ERNIE 4.5** ì‹œë¦¬ì¦ˆ 10ê°œ ëª¨ë¸ì„ ëª¨ë‘ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œí–ˆìŠµë‹ˆë‹¤. 

**ERNIE 4.5**ëŠ” 0.3Bë¶€í„° 424Bê¹Œì§€ì˜ ê´‘ë²”ìœ„í•œ íŒŒë¼ë¯¸í„° ë²”ìœ„ë¥¼ ì»¤ë²„í•˜ëŠ” ì™„ì „ ì˜¤í”ˆì†ŒìŠ¤ AI ëª¨ë¸ ì‹œë¦¬ì¦ˆì…ë‹ˆë‹¤. MoE(Mixture of Experts) ì•„í‚¤í…ì²˜, Vision-Language ë©€í‹°ëª¨ë‹¬ ì§€ì›, ê·¸ë¦¬ê³  128K ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ê¹Œì§€ ì§€ì›í•˜ëŠ” ì´ ëª¨ë¸ë“¤ì€ **Apache 2.0 ë¼ì´ì„ ìŠ¤**ë¡œ ìƒì—…ì  í™œìš©ì´ ììœ ë¡­ìŠµë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œëŠ” ERNIE 4.5 ì‹œë¦¬ì¦ˆì˜ ëª¨ë“  ëª¨ë¸ì„ ìƒì„¸íˆ ë¶„ì„í•˜ê³ , ì‹¤ì „ í™œìš© ë°©ë²•ê¹Œì§€ ì™„ë²½ ì •ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

## ERNIE 4.5 ì‹œë¦¬ì¦ˆ ê°œìš”

### í•µì‹¬ íŠ¹ì§•

**ERNIE 4.5**ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜ì‹ ì  íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:

- **ë‹¤ì–‘í•œ í¬ê¸°**: 0.3B Dense ëª¨ë¸ë¶€í„° 424B MoE ëª¨ë¸ê¹Œì§€
- **MoE ì•„í‚¤í…ì²˜**: 47B & 3B active parameter ëª¨ë¸ë“¤
- **Vision-Language**: í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ë©€í‹°ëª¨ë‹¬ ì§€ì›
- **ê¸´ ì»¨í…ìŠ¤íŠ¸**: 128K í† í° ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì§€ì›
- **ì™„ì „ ì˜¤í”ˆ**: Apache 2.0 ë¼ì´ì„ ìŠ¤ë¡œ ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥
- **ë‹¤ì¤‘ í”„ë ˆì„ì›Œí¬**: Transformers & PaddlePaddle ì§€ì›

### ëª¨ë¸ ë¼ì¸ì—…

```mermaid
graph TD
    A["ERNIE 4.5 ì‹œë¦¬ì¦ˆ"] --> B["Dense ëª¨ë¸"]
    A --> C["MoE ëª¨ë¸"]
    A --> D["Vision-Language ëª¨ë¸"]
    
    B --> E["0.3B Dense<br/>ê²½ëŸ‰í™” ëª¨ë¸"]
    
    C --> F["21B Total (3B Active)<br/>ì¤‘í˜• MoE"]
    C --> G["300B Total (47B Active)<br/>ëŒ€í˜• MoE"]
    
    D --> H["28B-A3B VL<br/>ì¤‘í˜• ë©€í‹°ëª¨ë‹¬"]
    D --> I["424B-A47B VL<br/>ëŒ€í˜• ë©€í‹°ëª¨ë‹¬"]
```

## ëª¨ë¸ë³„ ìƒì„¸ ë¶„ì„

### 1. ERNIE-4.5-0.3B: ê²½ëŸ‰í™” Dense ëª¨ë¸

**íŠ¹ì§•**:
- **ì•„í‚¤í…ì²˜**: Dense ëª¨ë¸ (MoE ì•„ë‹˜)
- **íŒŒë¼ë¯¸í„°**: 0.3B (3ì–µ ê°œ)
- **ìš©ë„**: ì—£ì§€ ë””ë°”ì´ìŠ¤, ì‹¤ì‹œê°„ ì¶”ë¡ 
- **ì–¸ì–´**: ì˜ì–´, ì¤‘êµ­ì–´

```python
# ERNIE-4.5-0.3B ì‚¬ìš© ì˜ˆì‹œ
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "baidu/ERNIE-4.5-0.3B-PT"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    trust_remote_code=True,
    torch_dtype=torch.float16,
    device_map="auto"
)

# í…ìŠ¤íŠ¸ ìƒì„±
prompt = "Artificial intelligence is"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(
    **inputs,
    max_length=100,
    temperature=0.7,
    do_sample=True
)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### 2. ERNIE-4.5-21B-A3B: ì¤‘í˜• MoE ëª¨ë¸

**íŠ¹ì§•**:
- **ì´ íŒŒë¼ë¯¸í„°**: 21.9B (ì•½ 220ì–µ ê°œ)
- **í™œì„± íŒŒë¼ë¯¸í„°**: 3B (30ì–µ ê°œ)
- **ì•„í‚¤í…ì²˜**: Mixture of Experts (MoE)
- **íš¨ìœ¨ì„±**: 3B ëª¨ë¸ ìˆ˜ì¤€ì˜ ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ 21B ì„±ëŠ¥

```python
# ERNIE-4.5-21B-A3B MoE ëª¨ë¸ ì‚¬ìš©
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "baidu/ERNIE-4.5-21B-A3B-PT"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# ì¤‘êµ­ì–´ í…ìŠ¤íŠ¸ ìƒì„±
prompt = "äººå·¥æ™ºèƒ½çš„å‘å±•å‰æ™¯"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(
    **inputs,
    max_length=200,
    temperature=0.8,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### 3. ERNIE-4.5-300B-A47B: ëŒ€í˜• MoE ëª¨ë¸

**íŠ¹ì§•**:
- **ì´ íŒŒë¼ë¯¸í„°**: 300.5B (ì•½ 3ì²œì–µ ê°œ)
- **í™œì„± íŒŒë¼ë¯¸í„°**: 47B (470ì–µ ê°œ)
- **ì„±ëŠ¥**: GPT-4 ê¸‰ ì„±ëŠ¥ì„ 47B ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ
- **ë©”ëª¨ë¦¬ ìµœì í™”**: ì–‘ìí™” ë²„ì „ ì œê³µ

```python
# ERNIE-4.5-300B-A47B ê³ ì„±ëŠ¥ ì¶”ë¡ 
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "baidu/ERNIE-4.5-300B-A47B-PT"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš° ì–‘ìí™” ë²„ì „ ì‚¬ìš©
# model_name = "baidu/ERNIE-4.5-300B-A47B-FP8-Paddle"  # FP8 ì–‘ìí™”
# model_name = "baidu/ERNIE-4.5-300B-A47B-2Bits-Paddle"  # 2bit ì–‘ìí™”

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    offload_folder="./offload"  # ë””ìŠ¤í¬ ì˜¤í”„ë¡œë“œ ì‚¬ìš©
)

# ë³µì¡í•œ ì¶”ë¡  ì‘ì—…
prompt = """
Analyze the following complex problem and provide a detailed solution:
A company needs to optimize its supply chain to reduce costs while maintaining quality.
"""

inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(
    **inputs,
    max_length=1000,
    temperature=0.7,
    do_sample=True,
    top_p=0.9
)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### 4. ERNIE-4.5-VL: Vision-Language ë©€í‹°ëª¨ë‹¬ ëª¨ë¸

**íŠ¹ì§•**:
- **ë©€í‹°ëª¨ë‹¬**: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ë™ì‹œ ì²˜ë¦¬
- **Vision Transformer**: ViT ë° UPO ê¸°ìˆ  ì ìš©
- **ë‘ ê°€ì§€ í¬ê¸°**: 28B-A3B, 424B-A47B
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ì´ë¯¸ì§€ ë¶„ì„ê³¼ í…ìŠ¤íŠ¸ ìƒì„± ë™ì‹œ ìˆ˜í–‰

```python
# ERNIE-4.5-VL Vision-Language ëª¨ë¸ ì‚¬ìš©
from transformers import AutoTokenizer, AutoModel
from PIL import Image
import torch
import requests

model_name = "baidu/ERNIE-4.5-VL-28B-A3B-PT"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(
    model_name,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# ì´ë¯¸ì§€ ë¡œë“œ
image_url = "https://example.com/sample_image.jpg"
image = Image.open(requests.get(image_url, stream=True).raw)

# ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ì§ˆë¬¸
question = "What is happening in this image? Describe it in detail."

# ë©€í‹°ëª¨ë‹¬ ì¶”ë¡ 
inputs = {
    "image": image,
    "text": question
}

# Vision-Language ì²˜ë¦¬
with torch.no_grad():
    response = model.chat(
        tokenizer=tokenizer,
        query=question,
        image=image,
        max_length=500,
        temperature=0.7
    )

print(f"Question: {question}")
print(f"Answer: {response}")
```

## ì„±ëŠ¥ ë¹„êµ ë° ë²¤ì¹˜ë§ˆí¬

### ê³„ì‚° íš¨ìœ¨ì„± ë¹„êµ

| ëª¨ë¸ | ì´ íŒŒë¼ë¯¸í„° | í™œì„± íŒŒë¼ë¯¸í„° | ì¶”ë¡  ë¹„ìš© | ì„±ëŠ¥ ë“±ê¸‰ |
|------|-------------|---------------|-----------|-----------|
| ERNIE-4.5-0.3B | 0.3B | 0.3B | ë§¤ìš° ë‚®ìŒ | ê¸°ë³¸ |
| ERNIE-4.5-21B-A3B | 21.9B | 3B | ë‚®ìŒ | ë†’ìŒ |
| ERNIE-4.5-300B-A47B | 300.5B | 47B | ì¤‘ê°„ | ë§¤ìš° ë†’ìŒ |
| ERNIE-4.5-VL-424B-A47B | 423.5B | 47B | ì¤‘ê°„ | ìµœê³  (ë©€í‹°ëª¨ë‹¬) |

### GPU ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­

```python
# ëª¨ë¸ë³„ GPU ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ê³„ì‚°
def estimate_gpu_memory(model_size_b, precision="fp16"):
    """ëª¨ë¸ í¬ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ì¶”ì •"""
    precision_multiplier = {
        "fp32": 4,
        "fp16": 2,
        "bfloat16": 2,
        "int8": 1,
        "int4": 0.5
    }
    
    base_memory = model_size_b * precision_multiplier[precision]
    overhead = base_memory * 0.2  # 20% ì˜¤ë²„í—¤ë“œ
    
    return base_memory + overhead

# ê° ëª¨ë¸ë³„ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
models = {
    "ERNIE-4.5-0.3B": 0.3,
    "ERNIE-4.5-21B-A3B": 3,  # í™œì„± íŒŒë¼ë¯¸í„° ê¸°ì¤€
    "ERNIE-4.5-300B-A47B": 47,  # í™œì„± íŒŒë¼ë¯¸í„° ê¸°ì¤€
    "ERNIE-4.5-VL-424B-A47B": 47
}

for model_name, active_params in models.items():
    memory_fp16 = estimate_gpu_memory(active_params, "fp16")
    memory_int8 = estimate_gpu_memory(active_params, "int8")
    
    print(f"{model_name}:")
    print(f"  FP16: {memory_fp16:.1f}GB")
    print(f"  INT8: {memory_int8:.1f}GB")
    print()
```

## ì‹¤ì „ ë°°í¬ ê°€ì´ë“œ

### 1. ë¡œì»¬ í™˜ê²½ ì„¤ì •

```bash
# í™˜ê²½ ì„¤ì •
pip install torch torchvision transformers accelerate
pip install paddlepaddle-gpu  # PaddlePaddle ë²„ì „ ì‚¬ìš©ì‹œ

# ì–‘ìí™” ì§€ì›ì„ ìœ„í•œ ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install bitsandbytes optimum
pip install auto-gptq  # GPTQ ì–‘ìí™”
```

### 2. Docker ì»¨í…Œì´ë„ˆ ë°°í¬

```dockerfile
# Dockerfile for ERNIE 4.5
FROM nvidia/cuda:12.1-devel-ubuntu22.04

# ê¸°ë³¸ í™˜ê²½ ì„¤ì •
RUN apt-get update && apt-get install -y \
    python3 python3-pip git wget \
    && rm -rf /var/lib/apt/lists/*

# Python ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
COPY requirements.txt .
RUN pip install -r requirements.txt

# ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
RUN mkdir -p /app/models /app/cache

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ
COPY app/ /app/
WORKDIR /app

# í™˜ê²½ ë³€ìˆ˜
ENV TRANSFORMERS_CACHE=/app/cache
ENV HF_HOME=/app/cache

# ì„œë¹„ìŠ¤ ì‹œì‘
EXPOSE 8000
CMD ["python3", "serve.py"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  ernie-45:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./cache:/app/cache
    environment:
      - CUDA_VISIBLE_DEVICES=0,1
      - MODEL_NAME=baidu/ERNIE-4.5-21B-A3B-PT
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
```

### 3. Kubernetes ë°°í¬

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ernie-45-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ernie-45
  template:
    metadata:
      labels:
        app: ernie-45
    spec:
      containers:
      - name: ernie-45
        image: your-registry/ernie-45:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "32Gi"
            nvidia.com/gpu: 2
          limits:
            memory: "64Gi"
            nvidia.com/gpu: 2
        env:
        - name: MODEL_NAME
          value: "baidu/ERNIE-4.5-21B-A3B-PT"
        - name: MAX_BATCH_SIZE
          value: "4"
        volumeMounts:
        - name: model-cache
          mountPath: /app/cache
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-cache-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: ernie-45-service
spec:
  selector:
    app: ernie-45
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

### 4. FastAPI ì„œë¹„ìŠ¤ êµ¬í˜„

```python
# serve.py - FastAPI ì„œë¹„ìŠ¤
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os
from typing import Optional
import asyncio

app = FastAPI(title="ERNIE 4.5 API", version="1.0.0")

class GenerationRequest(BaseModel):
    prompt: str
    max_length: int = 500
    temperature: float = 0.7
    top_p: float = 0.9
    do_sample: bool = True

class GenerationResponse(BaseModel):
    generated_text: str
    model_name: str
    parameters_used: dict

# ëª¨ë¸ ë¡œë”©
MODEL_NAME = os.environ.get("MODEL_NAME", "baidu/ERNIE-4.5-21B-A3B-PT")
print(f"Loading model: {MODEL_NAME}")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

@app.post("/generate", response_model=GenerationResponse)
async def generate_text(request: GenerationRequest):
    try:
        inputs = tokenizer(request.prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=request.max_length,
                temperature=request.temperature,
                top_p=request.top_p,
                do_sample=request.do_sample,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return GenerationResponse(
            generated_text=generated_text,
            model_name=MODEL_NAME,
            parameters_used={
                "max_length": request.max_length,
                "temperature": request.temperature,
                "top_p": request.top_p
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model": MODEL_NAME}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ë©”ëª¨ë¦¬ ìµœì í™”

```python
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ëª¨ë¸ ë¡œë”©
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# 4bit ì–‘ìí™” ì„¤ì •
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    "baidu/ERNIE-4.5-300B-A47B-PT",
    quantization_config=quantization_config,
    trust_remote_code=True,
    device_map="auto"
)

# CPU ì˜¤í”„ë¡œë“œ ì„¤ì •
model = AutoModelForCausalLM.from_pretrained(
    "baidu/ERNIE-4.5-300B-A47B-PT",
    trust_remote_code=True,
    device_map="auto",
    offload_folder="./cpu_offload",
    torch_dtype=torch.bfloat16
)
```

### 2. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```python
# ë°°ì¹˜ ì¶”ë¡  ìµœì í™”
def batch_generate(prompts, model, tokenizer, batch_size=4):
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ íš¨ìœ¨ì ì¸ í…ìŠ¤íŠ¸ ìƒì„±"""
    results = []
    
    for i in range(0, len(prompts), batch_size):
        batch_prompts = prompts[i:i+batch_size]
        
        # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            batch_prompts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        )
        
        # ë°°ì¹˜ ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=200,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        batch_results = [
            tokenizer.decode(output, skip_special_tokens=True) 
            for output in outputs
        ]
        results.extend(batch_results)
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
prompts = [
    "Explain artificial intelligence",
    "What is machine learning?",
    "How does deep learning work?",
    "Describe neural networks"
]

results = batch_generate(prompts, model, tokenizer)
```

### 3. ìºì‹± ë° ëª¨ë¸ ìµœì í™”

```python
# ëª¨ë¸ ìºì‹± ë° ì»´íŒŒì¼ ìµœì í™”
import torch._dynamo as dynamo

# PyTorch 2.0 ì»´íŒŒì¼ ìµœì í™”
model = torch.compile(model, mode="reduce-overhead")

# KV ìºì‹œ ìµœì í™”
def optimized_generate(prompt, model, tokenizer):
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # KV ìºì‹œë¥¼ í™œìš©í•œ íš¨ìœ¨ì  ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=500,
            use_cache=True,  # KV ìºì‹œ í™œìš©
            temperature=0.7,
            do_sample=True
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## ë¼ì´ì„ ìŠ¤ ë° ìƒì—…ì  í™œìš©

### Apache 2.0 ë¼ì´ì„ ìŠ¤ íŠ¹ì§•

**ERNIE 4.5**ëŠ” **Apache 2.0 ë¼ì´ì„ ìŠ¤**ë¡œ ê³µê°œë˜ì–´ ë‹¤ìŒê³¼ ê°™ì€ ììœ ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

- âœ… **ìƒì—…ì  ì‚¬ìš©**: ì œí’ˆì´ë‚˜ ì„œë¹„ìŠ¤ì— ììœ ë¡­ê²Œ í™œìš© ê°€ëŠ¥
- âœ… **ìˆ˜ì • ë° ë°°í¬**: ëª¨ë¸ì„ ìˆ˜ì •í•˜ê³  ì¬ë°°í¬ ê°€ëŠ¥
- âœ… **íŠ¹í—ˆ ê¶Œë¦¬**: Apache 2.0ì˜ íŠ¹í—ˆ ë³´í˜¸ ì¡°í•­ ì ìš©
- âœ… **ì†ŒìŠ¤ ê³µê°œ ì˜ë¬´ ì—†ìŒ**: ìˆ˜ì •ëœ ì½”ë“œ ê³µê°œ ì˜ë¬´ ì—†ìŒ

### ìƒì—…ì  í™œìš© ê°€ì´ë“œ

```python
# ìƒì—…ì  ì„œë¹„ìŠ¤ ì˜ˆì‹œ
class ERNIECommercialService:
    def __init__(self, model_name="baidu/ERNIE-4.5-21B-A3B-PT"):
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
    
    def generate_content(self, prompt, customer_id=None):
        """ìƒì—…ì  ì½˜í…ì¸  ìƒì„± ì„œë¹„ìŠ¤"""
        # ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ ì¶”ì 
        if customer_id:
            self.track_usage(customer_id)
        
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.model.generate(
            **inputs,
            max_length=1000,
            temperature=0.7,
            do_sample=True
        )
        
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    def track_usage(self, customer_id):
        """ì‚¬ìš©ëŸ‰ ì¶”ì  (ê³¼ê¸ˆ ëª©ì )"""
        # ìƒì—…ì  ì„œë¹„ìŠ¤ë¥¼ ìœ„í•œ ì‚¬ìš©ëŸ‰ ì¶”ì  ë¡œì§
        pass

# ìƒì—…ì  ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
commercial_service = ERNIECommercialService()
```

## ì»¤ë®¤ë‹ˆí‹° ë° ì§€ì›

### ê³µì‹ ë¦¬ì†ŒìŠ¤

- **Hugging Face ì»¬ë ‰ì…˜**: [ERNIE 4.5 Collection](https://huggingface.co/collections/baidu/ernie-45-6861cd4c9be84540645f35c9)
- **ëª¨ë¸ ì¹´ë“œ**: ê° ëª¨ë¸ë³„ ìƒì„¸ ë¬¸ì„œ ì œê³µ
- **GitHub**: Baidu ê³µì‹ ì €ì¥ì†Œ (ì—…ë°ì´íŠ¸ ì˜ˆì •)
- **ë…¼ë¬¸**: ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ (ì¶œê°„ ì˜ˆì •)

### ì»¤ë®¤ë‹ˆí‹° í™œë™

```python
# ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬ ì˜ˆì‹œ
def contribute_to_ernie_community():
    """ERNIE 4.5 ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬ ë°©ë²•"""
    
    contributions = [
        "ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ê³µìœ ",
        "ìƒˆë¡œìš´ í™œìš© ì‚¬ë¡€ ê°œë°œ",
        "ìµœì í™” ê¸°ë²• ê³µìœ ",
        "ë²„ê·¸ ë¦¬í¬íŠ¸ ë° ìˆ˜ì •",
        "ë²ˆì—­ ë° ë‹¤êµ­ì–´ ì§€ì›",
        "êµìœ¡ ìë£Œ ì œì‘"
    ]
    
    return contributions

# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê¸°ì—¬ ì˜ˆì‹œ
def benchmark_ernie_performance():
    """ì»¤ë®¤ë‹ˆí‹° ë²¤ì¹˜ë§ˆí¬ ê¸°ì—¬"""
    import time
    
    model_name = "baidu/ERNIE-4.5-21B-A3B-PT"
    prompts = ["Test prompt " + str(i) for i in range(100)]
    
    start_time = time.time()
    results = batch_generate(prompts, model, tokenizer)
    end_time = time.time()
    
    benchmark_results = {
        "model": model_name,
        "total_prompts": len(prompts),
        "total_time": end_time - start_time,
        "throughput": len(prompts) / (end_time - start_time),
        "average_length": sum(len(r) for r in results) / len(results)
    }
    
    return benchmark_results
```

## ë¯¸ë˜ ì „ë§ ë° ë¡œë“œë§µ

### ì˜ˆìƒ ì—…ë°ì´íŠ¸

1. **ë” í° ëª¨ë¸**: 1T+ íŒŒë¼ë¯¸í„° ëª¨ë¸ ì¶œì‹œ ì˜ˆì •
2. **íŠ¹í™” ëª¨ë¸**: ë„ë©”ì¸ë³„ íŠ¹í™” ë²„ì „ (ì˜ë£Œ, ë²•ë¥ , ê¸ˆìœµ)
3. **ë©€í‹°ëª¨ë‹¬ í™•ì¥**: ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ ì§€ì› í™•ëŒ€
4. **íš¨ìœ¨ì„± ê°œì„ **: ë” íš¨ìœ¨ì ì¸ MoE ì•„í‚¤í…ì²˜
5. **íˆ´ í†µí•©**: LangChain, AutoGen ë“±ê³¼ì˜ ë„¤ì´í‹°ë¸Œ í†µí•©

### ê¸°ìˆ ì  í˜ì‹  í¬ì¸íŠ¸

```python
# í–¥í›„ ê¸°ìˆ  ë°œì „ ë°©í–¥ ì˜ˆì¸¡
future_innovations = {
    "architecture": [
        "Dynamic MoE routing",
        "Adaptive expert selection", 
        "Cross-modal expert sharing"
    ],
    "efficiency": [
        "Sub-linear scaling MoE",
        "Memory-efficient attention",
        "Dynamic batching optimization"
    ],
    "capabilities": [
        "Code generation specialization",
        "Scientific reasoning enhancement",
        "Real-time multimodal processing"
    ]
}

# ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 
performance_roadmap = {
    "2025 Q3": "Code generation 50% improvement",
    "2025 Q4": "Multimodal latency 3x reduction", 
    "2026 Q1": "128K+ context stable support",
    "2026 Q2": "Domain-specific expert modules"
}
```

## ê²°ë¡ 

**Baidu ERNIE 4.5**ëŠ” 2025ë…„ ì˜¤í”ˆì†ŒìŠ¤ AI ìƒíƒœê³„ì— ì¤‘ëŒ€í•œ ë³€í™”ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤. ğŸ”¥

### ì£¼ìš” í˜ì‹ ì 

1. **ì™„ì „í•œ ì˜¤í”ˆì†ŒìŠ¤**: Apache 2.0ìœ¼ë¡œ ìƒì—…ì  í™œìš© ììœ 
2. **ìŠ¤ì¼€ì¼ëŸ¬ë¸” ì•„í‚¤í…ì²˜**: 0.3Bë¶€í„° 424Bê¹Œì§€ ë‹¤ì–‘í•œ ì„ íƒ
3. **MoE íš¨ìœ¨ì„±**: 47B í™œì„±ìœ¼ë¡œ 424B ì„±ëŠ¥ ë‹¬ì„±
4. **ë©€í‹°ëª¨ë‹¬ ì§€ì›**: í…ìŠ¤íŠ¸+ë¹„ì „ í†µí•© ì²˜ë¦¬
5. **ì‹¤ìš©ì  ë°°í¬**: ë‹¤ì–‘í•œ í™˜ê²½ ì§€ì›

### í™œìš© ê¶Œì¥ì‚¬í•­

- **ìŠ¤íƒ€íŠ¸ì—…**: 0.3B ë˜ëŠ” 21B-A3B ëª¨ë¸ë¡œ ì‹œì‘
- **ì¤‘ê²¬ê¸°ì—…**: 300B-A47Bë¡œ ê³ ì„±ëŠ¥ ì„œë¹„ìŠ¤ êµ¬ì¶•
- **ëŒ€ê¸°ì—…**: VL ëª¨ë¸ë¡œ ë©€í‹°ëª¨ë‹¬ í˜ì‹  ì¶”ì§„
- **ì—°êµ¬ê¸°ê´€**: ì „ì²´ ì‹œë¦¬ì¦ˆë¡œ í¬ê´„ì  ì—°êµ¬ ìˆ˜í–‰

Baiduê°€ ì•½ì†ì„ ì§€í‚¤ë©° ê³µê°œí•œ ERNIE 4.5 ì‹œë¦¬ì¦ˆëŠ” ì˜¤í”ˆì†ŒìŠ¤ AIì˜ ìƒˆë¡œìš´ ì¥ì„ ì—´ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì—¬ëŸ¬ë¶„ì˜ í”„ë¡œì íŠ¸ì— ì´ ê°•ë ¥í•œ ëª¨ë¸ë“¤ì„ í™œìš©í•´ í˜ì‹ ì„ ë§Œë“¤ì–´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤! ğŸš€

---

**ì°¸ê³  ë§í¬**:
- [ERNIE 4.5 Hugging Face ì»¬ë ‰ì…˜](https://huggingface.co/collections/baidu/ernie-45-6861cd4c9be84540645f35c9)
- [Apache 2.0 ë¼ì´ì„ ìŠ¤ ì „ë¬¸](https://www.apache.org/licenses/LICENSE-2.0)
- [Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¬¸ì„œ](https://huggingface.co/docs/transformers/)
- [PaddlePaddle ê³µì‹ ë¬¸ì„œ](https://www.paddlepaddle.org.cn/) 