#!/bin/bash

# NextStep-1 í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
# ì‘ì„±ì¼: 2025-08-19
# ìš©ë„: StepFun NextStep-1 ëª¨ë¸ ë¡œì»¬ í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶•

set -e

echo "ğŸš€ NextStep-1 í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì‹œì‘..."

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# ë¡œê·¸ í•¨ìˆ˜
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
check_system() {
    log_info "ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸ ì¤‘..."
    
    echo "ğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´:"
    echo "   OS: $(uname -s) $(uname -r)"
    echo "   ì•„í‚¤í…ì²˜: $(uname -m)"
    echo "   ë©”ëª¨ë¦¬: $(sysctl -n hw.memsize | awk '{printf "%.0f GB", $1/1024/1024/1024}')"
    
    # Python ë²„ì „ í™•ì¸
    if command -v python3 &> /dev/null; then
        echo "   Python: $(python3 --version)"
    else
        log_error "Python3ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."
        exit 1
    fi
    
    # Conda í™•ì¸
    if command -v conda &> /dev/null; then
        echo "   Conda: $(conda --version)"
    else
        log_warning "Condaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. Miniforge ì„¤ì¹˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
    fi
    
    # Git LFS í™•ì¸
    if command -v git-lfs &> /dev/null; then
        echo "   Git LFS: $(git lfs version | head -n1)"
    else
        log_warning "Git LFSê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."
        log_info "ì„¤ì¹˜ ëª…ë ¹: brew install git-lfs"
    fi
}

# GPU ì§€ì› í™•ì¸
check_gpu_support() {
    log_info "GPU ì§€ì› í™•ì¸ ì¤‘..."
    
    # NVIDIA GPU í™•ì¸ (CUDA)
    if command -v nvidia-smi &> /dev/null; then
        echo "ğŸ”¥ NVIDIA GPU ê°ì§€ë¨:"
        nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits | \
        awk -F, '{printf "   GPU: %s (VRAM: %d GB)\n", $1, $2/1024}'
        export USE_CUDA=true
    # Apple Silicon GPU í™•ì¸ (MPS)
    elif [[ "$(uname -m)" == "arm64" ]] && [[ "$(uname -s)" == "Darwin" ]]; then
        echo "ğŸ Apple Silicon GPU (Metal Performance Shaders) ì§€ì›"
        export USE_MPS=true
    else
        echo "âš ï¸  GPU ê°€ì† ë¯¸ì§€ì› - CPUë§Œ ì‚¬ìš©"
        export USE_CPU_ONLY=true
    fi
}

# Conda í™˜ê²½ ì„¤ì •
setup_conda_env() {
    log_info "Conda í™˜ê²½ ì„¤ì • ì¤‘..."
    
    ENV_NAME="nextstep"
    
    # ê¸°ì¡´ í™˜ê²½ í™•ì¸
    if conda env list | grep -q "^${ENV_NAME} "; then
        log_warning "ê¸°ì¡´ ${ENV_NAME} í™˜ê²½ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤."
        read -p "ê¸°ì¡´ í™˜ê²½ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ë§Œë“œì‹œê² ìŠµë‹ˆê¹Œ? (y/N): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            conda env remove -n ${ENV_NAME} -y
        else
            log_info "ê¸°ì¡´ í™˜ê²½ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            conda activate ${ENV_NAME}
            return
        fi
    fi
    
    # ìƒˆ í™˜ê²½ ìƒì„±
    log_info "Python 3.11 í™˜ê²½ ìƒì„± ì¤‘..."
    conda create -n ${ENV_NAME} python=3.11 -y
    
    # í™˜ê²½ í™œì„±í™”
    log_info "í™˜ê²½ í™œì„±í™” ì¤‘..."
    conda activate ${ENV_NAME}
    
    log_success "Conda í™˜ê²½ '${ENV_NAME}' ìƒì„± ì™„ë£Œ"
}

# íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì„¤ì¹˜
install_package_managers() {
    log_info "íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì„¤ì¹˜ ì¤‘..."
    
    # uv ì„¤ì¹˜ (ì„ íƒì‚¬í•­)
    if ! command -v uv &> /dev/null; then
        log_info "uv íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì„¤ì¹˜ ì¤‘..."
        pip install uv
        log_success "uv ì„¤ì¹˜ ì™„ë£Œ"
    else
        log_info "uv ì´ë¯¸ ì„¤ì¹˜ë¨: $(uv --version)"
    fi
}

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
download_model() {
    log_info "NextStep-1 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."
    
    MODEL_DIR="NextStep-1-Large-Pretrain"
    
    # Git LFS ì²´í¬
    if ! command -v git-lfs &> /dev/null; then
        log_error "Git LFSê°€ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:"
        echo "brew install git-lfs"
        exit 1
    fi
    
    # ê¸°ì¡´ ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸
    if [ -d "$MODEL_DIR" ]; then
        log_warning "ê¸°ì¡´ ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤."
        read -p "ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            rm -rf "$MODEL_DIR"
        else
            log_info "ê¸°ì¡´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            cd "$MODEL_DIR"
            return
        fi
    fi
    
    # ëª¨ë¸ í´ë¡  (ëŒ€ìš©ëŸ‰ íŒŒì¼ ì œì™¸)
    log_info "ëª¨ë¸ ì €ì¥ì†Œ í´ë¡  ì¤‘... (ëŒ€ìš©ëŸ‰ íŒŒì¼ ì œì™¸)"
    GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/stepfun-ai/NextStep-1-Large-Pretrain
    cd "$MODEL_DIR"
    
    # Hugging Face CLI í™•ì¸ ë° ì„¤ì¹˜
    if ! command -v huggingface-cli &> /dev/null; then
        log_info "Hugging Face CLI ì„¤ì¹˜ ì¤‘..."
        pip install --upgrade huggingface_hub
    fi
    
    log_success "ëª¨ë¸ ì €ì¥ì†Œ í´ë¡  ì™„ë£Œ"
}

# ì˜ì¡´ì„± ì„¤ì¹˜
install_dependencies() {
    log_info "ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
    
    # requirements.txt í™•ì¸
    if [ ! -f "requirements.txt" ]; then
        log_warning "requirements.txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
        
        # ê¸°ë³¸ íŒ¨í‚¤ì§€ ëª©ë¡ ìƒì„±
        cat > requirements.txt << EOF
torch>=2.0.0
torchvision>=0.15.0
transformers>=4.35.0
accelerate>=0.24.0
diffusers>=0.24.0
Pillow>=9.5.0
numpy>=1.24.0
scipy>=1.10.0
opencv-python>=4.8.0
matplotlib>=3.7.0
tqdm>=4.65.0
safetensors>=0.4.0
huggingface-hub>=0.17.0
EOF
        log_info "ê¸°ë³¸ requirements.txt ìƒì„±ë¨"
    fi
    
    # PyTorch GPU ì§€ì› ì„¤ì¹˜
    if [[ "${USE_CUDA}" == "true" ]]; then
        log_info "CUDA ì§€ì› PyTorch ì„¤ì¹˜ ì¤‘..."
        if command -v uv &> /dev/null; then
            uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
        else
            pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
        fi
    elif [[ "${USE_MPS}" == "true" ]]; then
        log_info "Apple Silicon MPS ì§€ì› PyTorch ì„¤ì¹˜ ì¤‘..."
        if command -v uv &> /dev/null; then
            uv pip install torch torchvision
        else
            pip install torch torchvision
        fi
    else
        log_info "CPU ì „ìš© PyTorch ì„¤ì¹˜ ì¤‘..."
        if command -v uv &> /dev/null; then
            uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
        else
            pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
        fi
    fi
    
    # ë‚˜ë¨¸ì§€ ì˜ì¡´ì„± ì„¤ì¹˜
    log_info "ê¸°íƒ€ ì˜ì¡´ì„± ì„¤ì¹˜ ì¤‘..."
    if command -v uv &> /dev/null; then
        uv pip install -r requirements.txt
    else
        pip install -r requirements.txt
    fi
    
    log_success "ëª¨ë“  ì˜ì¡´ì„± ì„¤ì¹˜ ì™„ë£Œ"
}

# VAE ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
download_vae_model() {
    log_info "VAE ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."
    
    # VAE ë””ë ‰í† ë¦¬ ìƒì„±
    mkdir -p vae
    
    # Hugging Face CLIë¡œ VAE ë‹¤ìš´ë¡œë“œ
    log_info "VAE checkpoint ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 2-3GB)"
    huggingface-cli download stepfun-ai/NextStep-1-Large-Pretrain "vae/checkpoint.pt" --local-dir ./
    
    if [ -f "vae/checkpoint.pt" ]; then
        log_success "VAE ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ"
        echo "   íŒŒì¼ í¬ê¸°: $(du -h vae/checkpoint.pt | cut -f1)"
    else
        log_error "VAE ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"
        exit 1
    fi
}

# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
create_test_script() {
    log_info "í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘..."
    
    cat > test_nextstep.py << 'EOF'
#!/usr/bin/env python3
"""
NextStep-1 ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
macOS í™˜ê²½ì—ì„œ ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„± í…ŒìŠ¤íŠ¸
"""

import torch
import sys
import os
from datetime import datetime
from pathlib import Path

def check_environment():
    """í™˜ê²½ ì ê²€"""
    print("ğŸ” í™˜ê²½ ì ê²€ ì‹œì‘...")
    
    # PyTorch ë²„ì „
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    
    # ë””ë°”ì´ìŠ¤ í™•ì¸
    if torch.cuda.is_available():
        device = "cuda"
        print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"CUDA ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = "mps"
        print("Apple Silicon MPS ì‚¬ìš© ê°€ëŠ¥")
    else:
        device = "cpu"
        print("CPU ì „ìš© ëª¨ë“œ")
    
    print(f"ì„ íƒëœ ë””ë°”ì´ìŠ¤: {device}")
    return device

def test_basic_import():
    """ê¸°ë³¸ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“¦ ëª¨ë“ˆ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸...")
    
    try:
        from transformers import AutoTokenizer, AutoModel
        print("âœ… Transformers ì„í¬íŠ¸ ì„±ê³µ")
    except ImportError as e:
        print(f"âŒ Transformers ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        return False
    
    try:
        from PIL import Image
        print("âœ… PIL ì„í¬íŠ¸ ì„±ê³µ")
    except ImportError as e:
        print(f"âŒ PIL ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        return False
    
    return True

def test_model_loading():
    """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¤– ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸...")
    
    try:
        from transformers import AutoTokenizer, AutoModel
        
        model_path = "."  # í˜„ì¬ ë””ë ‰í† ë¦¬
        
        print("í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            local_files_only=True,
            trust_remote_code=True
        )
        print("âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ")
        
        print("ëª¨ë¸ ë¡œë”© ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model = AutoModel.from_pretrained(
            model_path,
            local_files_only=True,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
        )
        print("âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ")
        
        # ëª¨ë¸ ì •ë³´
        total_params = sum(p.numel() for p in model.parameters())
        print(f"ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
        
        return tokenizer, model
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None

def test_simple_generation(tokenizer, model, device):
    """ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¨ ì´ë¯¸ì§€ ìƒì„± í…ŒìŠ¤íŠ¸...")
    
    try:
        # íŒŒì´í”„ë¼ì¸ import ì‹œë„
        try:
            from models.gen_pipeline import NextStepPipeline
            pipeline_available = True
        except ImportError:
            print("âš ï¸ gen_pipeline ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ê¸°ë³¸ ìƒì„± í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            pipeline_available = False
        
        if not pipeline_available:
            return False
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        pipeline = NextStepPipeline(tokenizer=tokenizer, model=model)
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (MPSëŠ” ì¼ë¶€ ì—°ì‚°ì—ì„œ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ)
        if device == "mps":
            print("âš ï¸ MPSì—ì„œëŠ” ì¼ë¶€ ì—°ì‚°ì´ ì§€ì›ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            device = "cpu"  # MPS ëŒ€ì‹  CPU ì‚¬ìš©
        
        pipeline = pipeline.to(device=device, dtype=torch.float16)
        
        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
        test_prompt = "A simple red apple on a white background"
        
        print(f"í”„ë¡¬í”„íŠ¸: {test_prompt}")
        print("ì´ë¯¸ì§€ ìƒì„± ì¤‘... (ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        
        image = pipeline.generate_image(
            test_prompt,
            hw=(512, 512),
            num_images_per_caption=1,
            positive_prompt="masterpiece, best quality",
            negative_prompt="lowres, bad quality",
            cfg=7.5,
            num_sampling_steps=20,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë‹¨ê³„ ìˆ˜ ê°ì†Œ
            seed=42
        )[0]
        
        # ì´ë¯¸ì§€ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"test_output_{timestamp}.jpg"
        image.save(output_path)
        
        print(f"âœ… ì´ë¯¸ì§€ ìƒì„± ì„±ê³µ: {output_path}")
        print(f"ì´ë¯¸ì§€ í¬ê¸°: {image.size}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ NextStep-1 ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # í™˜ê²½ ì ê²€
    device = check_environment()
    
    # ê¸°ë³¸ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸
    if not test_basic_import():
        print("\nâŒ ê¸°ë³¸ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        sys.exit(1)
    
    # ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    tokenizer, model = test_model_loading()
    if tokenizer is None or model is None:
        print("\nâŒ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        sys.exit(1)
    
    # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
    if test_simple_generation(tokenizer, model, device):
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("1. ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”")
        print("2. ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì‹¤í—˜í•´ë³´ì„¸ìš”")
        print("3. ì„¤ì •ê°’ë“¤ì„ ì¡°ì •í•´ë³´ì„¸ìš”")
    else:
        print("\nâš ï¸ ì´ë¯¸ì§€ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        print("ëª¨ë¸ ë¡œë”©ì€ ì„±ê³µí–ˆìœ¼ë‹ˆ ì„¤ì •ì„ í™•ì¸í•´ë³´ì„¸ìš”")

if __name__ == "__main__":
    main()
EOF

    chmod +x test_nextstep.py
    log_success "í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì™„ë£Œ: test_nextstep.py"
}

# ì‚¬ìš©ë²• ì•ˆë‚´ ìƒì„±
create_usage_guide() {
    log_info "ì‚¬ìš©ë²• ê°€ì´ë“œ ìƒì„± ì¤‘..."
    
    cat > README.md << 'EOF'
# NextStep-1 í…ŒìŠ¤íŠ¸ í™˜ê²½

StepFun NextStep-1 ëª¨ë¸ì˜ macOS í…ŒìŠ¤íŠ¸ í™˜ê²½ì…ë‹ˆë‹¤.

## ì„¤ì¹˜ëœ êµ¬ì„± ìš”ì†Œ

- NextStep-1 14B ë©”ì¸ ëª¨ë¸
- 157M Flow Matching Head
- VAE ëª¨ë¸ (vae/checkpoint.pt)
- í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

## ì‚¬ìš©ë²•

### 1. í™˜ê²½ í™œì„±í™”
```bash
conda activate nextstep
cd NextStep-1-Large-Pretrain
```

### 2. ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
python test_nextstep.py
```

### 3. ì»¤ìŠ¤í…€ ì´ë¯¸ì§€ ìƒì„±
```python
import torch
from transformers import AutoTokenizer, AutoModel
from models.gen_pipeline import NextStepPipeline

# ëª¨ë¸ ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained(".", local_files_only=True, trust_remote_code=True)
model = AutoModel.from_pretrained(".", local_files_only=True, trust_remote_code=True)

# íŒŒì´í”„ë¼ì¸ ìƒì„±
pipeline = NextStepPipeline(tokenizer=tokenizer, model=model)
device = "cuda" if torch.cuda.is_available() else "cpu"
pipeline = pipeline.to(device=device, dtype=torch.bfloat16)

# ì´ë¯¸ì§€ ìƒì„±
image = pipeline.generate_image(
    "Your custom prompt here",
    hw=(1024, 1024),
    cfg=7.5,
    num_sampling_steps=28,
    seed=3407
)[0]

image.save("output.jpg")
```

## ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ëª…

- `hw`: ì´ë¯¸ì§€ í¬ê¸° (ë†’ì´, ë„ˆë¹„)
- `cfg`: Classifier-Free Guidance ê°•ë„ (7.0-9.0 ê¶Œì¥)
- `num_sampling_steps`: ìƒì„± ë‹¨ê³„ ìˆ˜ (ë§ì„ìˆ˜ë¡ í’ˆì§ˆ í–¥ìƒ, ì‹œê°„ ì¦ê°€)
- `seed`: ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œê°’

## ë¬¸ì œ í•´ê²°

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
- ì´ë¯¸ì§€ í¬ê¸°ë¥¼ 512x512ë¡œ ì¤„ì´ê¸°
- ë°°ì¹˜ í¬ê¸°ë¥¼ 1ë¡œ ì„¤ì •
- `torch_dtype=torch.float16` ì‚¬ìš©

### MPS ê´€ë ¨ ì˜¤ë¥˜ (Apple Silicon)
- CPU ëª¨ë“œë¡œ ì „í™˜: `device="cpu"`
- PyTorch ìµœì‹  ë²„ì „ ì‚¬ìš©

### ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨
- Git LFS ì„¤ì¹˜ í™•ì¸: `brew install git-lfs`
- ì¸í„°ë„· ì—°ê²° í™•ì¸
- ë””ìŠ¤í¬ ìš©ëŸ‰ í™•ì¸ (ìµœì†Œ 50GB)

## ì„±ëŠ¥ ìµœì í™”

1. **GPU ì‚¬ìš©**: NVIDIA GPU ë˜ëŠ” Apple Silicon ê¶Œì¥
2. **ë©”ëª¨ë¦¬**: 24GB+ VRAM ë˜ëŠ” 32GB+ RAM ê¶Œì¥
3. **ì €ì¥ê³µê°„**: 50GB+ ì—¬ìœ  ê³µê°„ í•„ìš”

## ì§€ì› ëª¨ë¸ í¬ê¸°

- NextStep-1-Large-Pretrain: ~15B íŒŒë¼ë¯¸í„°
- VAE: ~157M íŒŒë¼ë¯¸í„°
- ì´ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: ~30GB
EOF

    log_success "ì‚¬ìš©ë²• ê°€ì´ë“œ ìƒì„± ì™„ë£Œ: README.md"
}

# Alias ì„¤ì •
setup_aliases() {
    log_info "í¸ì˜ alias ì„¤ì • ì¤‘..."
    
    cat > setup_nextstep_aliases.sh << 'EOF'
#!/bin/bash
# NextStep-1 í…ŒìŠ¤íŠ¸ìš© í¸ì˜ alias ì„¤ì •

# Conda í™˜ê²½ ê´€ë ¨
alias nextstep-env="conda activate nextstep"
alias nextstep-deenv="conda deactivate"

# í…ŒìŠ¤íŠ¸ ê´€ë ¨
alias nextstep-test="cd NextStep-1-Large-Pretrain && python test_nextstep.py"
alias nextstep-dir="cd NextStep-1-Large-Pretrain"

# ëª¨ë‹ˆí„°ë§ ê´€ë ¨
alias nextstep-gpu="watch -n 1 nvidia-smi"  # NVIDIA GPUë§Œ
alias nextstep-mem="ps aux | grep python | grep -v grep"

# ë¡œê·¸ ê´€ë ¨
alias nextstep-log="tail -f nextstep_generation.log"

echo "âœ… NextStep-1 alias ì„¤ì • ì™„ë£Œ"
echo "ì‚¬ìš©ë²•:"
echo "  nextstep-env     : í™˜ê²½ í™œì„±í™”"
echo "  nextstep-test    : ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"
echo "  nextstep-dir     : ëª¨ë¸ ë””ë ‰í† ë¦¬ ì´ë™"
echo "  nextstep-gpu     : GPU ìƒíƒœ ëª¨ë‹ˆí„°ë§"

# zshrcì— ì¶”ê°€í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ ì‹¤í–‰:
echo ""
echo "ğŸ”§ ì˜êµ¬ ì„¤ì •ì„ ì›í•œë‹¤ë©´:"
echo "echo 'source $(pwd)/setup_nextstep_aliases.sh' >> ~/.zshrc"
EOF

    chmod +x setup_nextstep_aliases.sh
    log_success "Alias ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì™„ë£Œ"
}

# ìµœì¢… ì •ë¦¬ ë° ì•ˆë‚´
final_setup() {
    log_info "ìµœì¢… ì„¤ì • ì™„ë£Œ ì¤‘..."
    
    # ë””ë ‰í† ë¦¬ ì •ë³´
    echo ""
    echo "ğŸ“ ì„¤ì¹˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°:"
    echo "   $(pwd)/"
    echo "   â”œâ”€â”€ NextStep-1-Large-Pretrain/"
    echo "   â”‚   â”œâ”€â”€ models/"
    echo "   â”‚   â”œâ”€â”€ vae/checkpoint.pt"
    echo "   â”‚   â”œâ”€â”€ test_nextstep.py"
    echo "   â”‚   â””â”€â”€ README.md"
    echo "   â””â”€â”€ setup_nextstep_aliases.sh"
    
    # ì‚¬ìš©ë²• ì•ˆë‚´
    echo ""
    log_success "ğŸ‰ NextStep-1 í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì™„ë£Œ!"
    echo ""
    echo "ğŸš€ ì‹œì‘í•˜ê¸°:"
    echo "   1. conda activate nextstep"
    echo "   2. cd NextStep-1-Large-Pretrain"
    echo "   3. python test_nextstep.py"
    echo ""
    echo "ğŸ“š ì¶”ê°€ ì •ë³´:"
    echo "   - ì‚¬ìš©ë²•: cat NextStep-1-Large-Pretrain/README.md"
    echo "   - Alias: source setup_nextstep_aliases.sh"
    echo "   - ë¡œê·¸: tail -f NextStep-1-Large-Pretrain/nextstep_generation.log"
    
    # ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì¬ì•ˆë‚´
    echo ""
    echo "âš ï¸  ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­:"
    echo "   - ë©”ëª¨ë¦¬: 24GB+ VRAM ë˜ëŠ” 32GB+ RAM"
    echo "   - ì €ì¥ê³µê°„: 50GB+ ì—¬ìœ ê³µê°„"
    echo "   - ë„¤íŠ¸ì›Œí¬: ëª¨ë¸ ë‹¤ìš´ë¡œë“œìš© ì•ˆì •ì  ì—°ê²°"
}

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
main() {
    echo "ğŸ¯ NextStep-1 í…ŒìŠ¤íŠ¸ í™˜ê²½ ìë™ ì„¤ì •"
    echo "   StepFun AIì˜ 14B ìë™íšŒê·€ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸"
    echo "   macOS ìµœì í™” ë²„ì „"
    echo ""
    
    # ë‹¨ê³„ë³„ ì‹¤í–‰
    check_system
    check_gpu_support
    
    # ì‚¬ìš©ì í™•ì¸
    echo ""
    read -p "ì„¤ì¹˜ë¥¼ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        log_info "ì„¤ì¹˜ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
        exit 0
    fi
    
    # ì„¤ì¹˜ ì§„í–‰
    setup_conda_env
    install_package_managers
    download_model
    install_dependencies
    download_vae_model
    create_test_script
    create_usage_guide
    setup_aliases
    final_setup
    
    log_success "ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰"
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
